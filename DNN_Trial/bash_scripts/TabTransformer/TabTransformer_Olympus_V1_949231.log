

----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/boston.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/boston.yml', data_parallel=False, dataset='Boston', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[3], num_classes=1, num_features=13, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Boston...
Dataset loaded! 

X b4 encoding : [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01
 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00] 

(506, 13)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [3]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [3]
Cat Idx Part II: [3] 
ENDE 
 

X after Nominal Encoding: [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01
 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00] 
 

Scaling the data...
X after Scaling: [-0.41978194  0.28482986 -1.2879095   0.         -0.14421743  0.41367189
 -0.12001342  0.1402136  -0.98284286 -0.66660821 -1.45900038  0.44105193
 -1.0755623 ] 
 

One Hot Encoding...
X after One Hot Encoding: [ 1.          0.         -0.41978194  0.28482986 -1.2879095  -0.14421743
  0.41367189 -0.12001342  0.1402136  -0.98284286 -0.66660821 -1.45900038
  0.44105193 -1.0755623 ] 
 

args.num_features: 14
args.cat_idx: None
Cat Dims: []
New Shape: (506, 14)
False 
 

Using an existing study with name 'TabTransformer_Boston' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 536.13904
Epoch 1: Val Loss 536.13855
Epoch 2: Val Loss 536.13812
Epoch 3: Val Loss 536.13763
Epoch 4: Val Loss 536.13721
Epoch 5: Val Loss 536.13672
Epoch 6: Val Loss 536.13629
Epoch 7: Val Loss 536.13580
Epoch 8: Val Loss 536.13538
Epoch 9: Val Loss 536.13489
Epoch 10: Val Loss 536.13434
Epoch 11: Val Loss 536.13397
Epoch 12: Val Loss 536.13348
Epoch 13: Val Loss 536.13300
Epoch 14: Val Loss 536.13257
Epoch 15: Val Loss 536.13208
Epoch 16: Val Loss 536.13159
Epoch 17: Val Loss 536.13123
Epoch 18: Val Loss 536.13074
Epoch 19: Val Loss 536.13025
Epoch 20: Val Loss 536.12982
Epoch 21: Val Loss 536.12933
Epoch 22: Val Loss 536.12885
Epoch 23: Val Loss 536.12842
Epoch 24: Val Loss 536.12793
Epoch 25: Val Loss 536.12744
Epoch 26: Val Loss 536.12701
Epoch 27: Val Loss 536.12653
Epoch 28: Val Loss 536.12610
Epoch 29: Val Loss 536.12561
Epoch 30: Val Loss 536.12518
Epoch 31: Val Loss 536.12476
Epoch 32: Val Loss 536.12427
Epoch 33: Val Loss 536.12378
Epoch 34: Val Loss 536.12335
Epoch 35: Val Loss 536.12286
Epoch 36: Val Loss 536.12244
Epoch 37: Val Loss 536.12201
Epoch 38: Val Loss 536.12152
Epoch 39: Val Loss 536.12109
Epoch 40: Val Loss 536.12061
Epoch 41: Val Loss 536.12012
Epoch 42: Val Loss 536.11963
Epoch 43: Val Loss 536.11920
Epoch 44: Val Loss 536.11871
Epoch 45: Val Loss 536.11829
Epoch 46: Val Loss 536.11780
Epoch 47: Val Loss 536.11737
Epoch 48: Val Loss 536.11694
Epoch 49: Val Loss 536.11646
Epoch 50: Val Loss 536.11597
Epoch 51: Val Loss 536.11554
Epoch 52: Val Loss 536.11505
Epoch 53: Val Loss 536.11462
Epoch 54: Val Loss 536.11414
Epoch 55: Val Loss 536.11371
Epoch 56: Val Loss 536.11322
Epoch 57: Val Loss 536.11279
Epoch 58: Val Loss 536.11230
Epoch 59: Val Loss 536.11182
Epoch 60: Val Loss 536.11139
Epoch 61: Val Loss 536.11090
Epoch 62: Val Loss 536.11047
Epoch 63: Val Loss 536.10999
Epoch 64: Val Loss 536.10956
Epoch 65: Val Loss 536.10907
Epoch 66: Val Loss 536.10864
Epoch 67: Val Loss 536.10822
Epoch 68: Val Loss 536.10773
Epoch 69: Val Loss 536.10724
Epoch 70: Val Loss 536.10681
Epoch 71: Val Loss 536.10632
Epoch 72: Val Loss 536.10590
Epoch 73: Val Loss 536.10541
Epoch 74: Val Loss 536.10498
Epoch 75: Val Loss 536.10455
Epoch 76: Val Loss 536.10406
Epoch 77: Val Loss 536.10358
Epoch 78: Val Loss 536.10309
Epoch 79: Val Loss 536.10266
Epoch 80: Val Loss 536.10217
Epoch 81: Val Loss 536.10175
Epoch 82: Val Loss 536.10126
Epoch 83: Val Loss 536.10083
Epoch 84: Val Loss 536.10034
Epoch 85: Val Loss 536.09991
Epoch 86: Val Loss 536.09943
Epoch 87: Val Loss 536.09900
Epoch 88: Val Loss 536.09851
Epoch 89: Val Loss 536.09808
Epoch 90: Val Loss 536.09760
Epoch 91: Val Loss 536.09711
Epoch 92: Val Loss 536.09668
Epoch 93: Val Loss 536.09619
Epoch 94: Val Loss 536.09576
Epoch 95: Val Loss 536.09528
Epoch 96: Val Loss 536.09485
Epoch 97: Val Loss 536.09436
Epoch 98: Val Loss 536.09393
Epoch 99: Val Loss 536.09344
{'MSE - mean': 536.0934380336388, 'MSE - std': 0.0, 'R2 - mean': -6.17966709907851, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 610.41577
Epoch 1: Val Loss 610.41522
Epoch 2: Val Loss 610.41467
Epoch 3: Val Loss 610.41412
Epoch 4: Val Loss 610.41357
Epoch 5: Val Loss 610.41302
Epoch 6: Val Loss 610.41248
Epoch 7: Val Loss 610.41193
Epoch 8: Val Loss 610.41138
Epoch 9: Val Loss 610.41083
Epoch 10: Val Loss 610.41028
Epoch 11: Val Loss 610.40973
Epoch 12: Val Loss 610.40912
Epoch 13: Val Loss 610.40857
Epoch 14: Val Loss 610.40808
Epoch 15: Val Loss 610.40747
Epoch 16: Val Loss 610.40692
Epoch 17: Val Loss 610.40637
Epoch 18: Val Loss 610.40588
Epoch 19: Val Loss 610.40533
Epoch 20: Val Loss 610.40466
Epoch 21: Val Loss 610.40424
Epoch 22: Val Loss 610.40363
Epoch 23: Val Loss 610.40314
Epoch 24: Val Loss 610.40259
Epoch 25: Val Loss 610.40198
Epoch 26: Val Loss 610.40143
Epoch 27: Val Loss 610.40094
Epoch 28: Val Loss 610.40039
Epoch 29: Val Loss 610.39978
Epoch 30: Val Loss 610.39923
Epoch 31: Val Loss 610.39868
Epoch 32: Val Loss 610.39813
Epoch 33: Val Loss 610.39758
Epoch 34: Val Loss 610.39709
Epoch 35: Val Loss 610.39655
Epoch 36: Val Loss 610.39594
Epoch 37: Val Loss 610.39545
Epoch 38: Val Loss 610.39490
Epoch 39: Val Loss 610.39435
Epoch 40: Val Loss 610.39380
Epoch 41: Val Loss 610.39319
Epoch 42: Val Loss 610.39264
Epoch 43: Val Loss 610.39215
Epoch 44: Val Loss 610.39160
Epoch 45: Val Loss 610.39105
Epoch 46: Val Loss 610.39044
Epoch 47: Val Loss 610.38989
Epoch 48: Val Loss 610.38934
Epoch 49: Val Loss 610.38873
Epoch 50: Val Loss 610.38824
Epoch 51: Val Loss 610.38763
Epoch 52: Val Loss 610.38708
Epoch 53: Val Loss 610.38654
Epoch 54: Val Loss 610.38593
Epoch 55: Val Loss 610.38538
Epoch 56: Val Loss 610.38483
Epoch 57: Val Loss 610.38428
Epoch 58: Val Loss 610.38373
Epoch 59: Val Loss 610.38318
Epoch 60: Val Loss 610.38269
Epoch 61: Val Loss 610.38208
Epoch 62: Val Loss 610.38159
Epoch 63: Val Loss 610.38104
Epoch 64: Val Loss 610.38043
Epoch 65: Val Loss 610.37994
Epoch 66: Val Loss 610.37933
Epoch 67: Val Loss 610.37878
Epoch 68: Val Loss 610.37823
Epoch 69: Val Loss 610.37769
Epoch 70: Val Loss 610.37714
Epoch 71: Val Loss 610.37659
Epoch 72: Val Loss 610.37610
Epoch 73: Val Loss 610.37543
Epoch 74: Val Loss 610.37494
Epoch 75: Val Loss 610.37439
Epoch 76: Val Loss 610.37378
Epoch 77: Val Loss 610.37323
Epoch 78: Val Loss 610.37274
Epoch 79: Val Loss 610.37219
Epoch 80: Val Loss 610.37164
Epoch 81: Val Loss 610.37109
Epoch 82: Val Loss 610.37061
Epoch 83: Val Loss 610.37006
Epoch 84: Val Loss 610.36951
Epoch 85: Val Loss 610.36896
Epoch 86: Val Loss 610.36841
Epoch 87: Val Loss 610.36786
Epoch 88: Val Loss 610.36731
Epoch 89: Val Loss 610.36682
Epoch 90: Val Loss 610.36627
Epoch 91: Val Loss 610.36566
Epoch 92: Val Loss 610.36511
Epoch 93: Val Loss 610.36456
Epoch 94: Val Loss 610.36401
Epoch 95: Val Loss 610.36340
Epoch 96: Val Loss 610.36292
Epoch 97: Val Loss 610.36237
Epoch 98: Val Loss 610.36176
Epoch 99: Val Loss 610.36121
{'MSE - mean': 573.2273407310483, 'MSE - std': 37.13390269740961, 'R2 - mean': -6.379008631667043, 'R2 - std': 0.19934153258853238} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 659.74261
Epoch 1: Val Loss 659.74231
Epoch 2: Val Loss 659.74213
Epoch 3: Val Loss 659.74188
Epoch 4: Val Loss 659.74164
Epoch 5: Val Loss 659.74133
Epoch 6: Val Loss 659.74115
Epoch 7: Val Loss 659.74097
Epoch 8: Val Loss 659.74072
Epoch 9: Val Loss 659.74042
Epoch 10: Val Loss 659.74017
Epoch 11: Val Loss 659.73993
Epoch 12: Val Loss 659.73969
Epoch 13: Val Loss 659.73944
Epoch 14: Val Loss 659.73914
Epoch 15: Val Loss 659.73901
Epoch 16: Val Loss 659.73871
Epoch 17: Val Loss 659.73853
Epoch 18: Val Loss 659.73822
Epoch 19: Val Loss 659.73810
Epoch 20: Val Loss 659.73779
Epoch 21: Val Loss 659.73755
Epoch 22: Val Loss 659.73730
Epoch 23: Val Loss 659.73706
Epoch 24: Val Loss 659.73682
Epoch 25: Val Loss 659.73651
Epoch 26: Val Loss 659.73639
Epoch 27: Val Loss 659.73608
Epoch 28: Val Loss 659.73584
Epoch 29: Val Loss 659.73560
Epoch 30: Val Loss 659.73529
Epoch 31: Val Loss 659.73517
Epoch 32: Val Loss 659.73480
Epoch 33: Val Loss 659.73468
Epoch 34: Val Loss 659.73438
Epoch 35: Val Loss 659.73413
Epoch 36: Val Loss 659.73389
Epoch 37: Val Loss 659.73358
Epoch 38: Val Loss 659.73346
Epoch 39: Val Loss 659.73315
Epoch 40: Val Loss 659.73297
Epoch 41: Val Loss 659.73267
Epoch 42: Val Loss 659.73242
Epoch 43: Val Loss 659.73218
Epoch 44: Val Loss 659.73187
Epoch 45: Val Loss 659.73175
Epoch 46: Val Loss 659.73145
Epoch 47: Val Loss 659.73120
Epoch 48: Val Loss 659.73096
Epoch 49: Val Loss 659.73065
Epoch 50: Val Loss 659.73047
Epoch 51: Val Loss 659.73029
Epoch 52: Val Loss 659.72998
Epoch 53: Val Loss 659.72974
Epoch 54: Val Loss 659.72955
Epoch 55: Val Loss 659.72925
Epoch 56: Val Loss 659.72900
Epoch 57: Val Loss 659.72882
Epoch 58: Val Loss 659.72858
Epoch 59: Val Loss 659.72833
Epoch 60: Val Loss 659.72809
Epoch 61: Val Loss 659.72784
Epoch 62: Val Loss 659.72754
Epoch 63: Val Loss 659.72742
Epoch 64: Val Loss 659.72711
Epoch 65: Val Loss 659.72693
Epoch 66: Val Loss 659.72662
Epoch 67: Val Loss 659.72650
Epoch 68: Val Loss 659.72614
Epoch 69: Val Loss 659.72583
Epoch 70: Val Loss 659.72571
Epoch 71: Val Loss 659.72540
Epoch 72: Val Loss 659.72522
Epoch 73: Val Loss 659.72491
Epoch 74: Val Loss 659.72467
Epoch 75: Val Loss 659.72449
Epoch 76: Val Loss 659.72424
Epoch 77: Val Loss 659.72400
Epoch 78: Val Loss 659.72369
Epoch 79: Val Loss 659.72351
Epoch 80: Val Loss 659.72333
Epoch 81: Val Loss 659.72302
Epoch 82: Val Loss 659.72284
Epoch 83: Val Loss 659.72253
Epoch 84: Val Loss 659.72229
Epoch 85: Val Loss 659.72205
Epoch 86: Val Loss 659.72186
Epoch 87: Val Loss 659.72162
Epoch 88: Val Loss 659.72137
Epoch 89: Val Loss 659.72113
Epoch 90: Val Loss 659.72089
Epoch 91: Val Loss 659.72070
Epoch 92: Val Loss 659.72046
Epoch 93: Val Loss 659.72015
Epoch 94: Val Loss 659.71997
Epoch 95: Val Loss 659.71967
Epoch 96: Val Loss 659.71942
Epoch 97: Val Loss 659.71918
Epoch 98: Val Loss 659.71887
Epoch 99: Val Loss 659.71875
{'MSE - mean': 602.0577999857934, 'MSE - std': 50.81018842751406, 'R2 - mean': -6.223620752760357, 'R2 - std': 0.27346325206884453} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 534.37500
Epoch 1: Val Loss 534.37469
Epoch 2: Val Loss 534.37445
Epoch 3: Val Loss 534.37415
Epoch 4: Val Loss 534.37384
Epoch 5: Val Loss 534.37360
Epoch 6: Val Loss 534.37329
Epoch 7: Val Loss 534.37299
Epoch 8: Val Loss 534.37268
Epoch 9: Val Loss 534.37244
Epoch 10: Val Loss 534.37213
Epoch 11: Val Loss 534.37183
Epoch 12: Val Loss 534.37158
Epoch 13: Val Loss 534.37128
Epoch 14: Val Loss 534.37097
Epoch 15: Val Loss 534.37073
Epoch 16: Val Loss 534.37042
Epoch 17: Val Loss 534.37018
Epoch 18: Val Loss 534.36981
Epoch 19: Val Loss 534.36957
Epoch 20: Val Loss 534.36926
Epoch 21: Val Loss 534.36902
Epoch 22: Val Loss 534.36871
Epoch 23: Val Loss 534.36841
Epoch 24: Val Loss 534.36810
Epoch 25: Val Loss 534.36786
Epoch 26: Val Loss 534.36755
Epoch 27: Val Loss 534.36725
Epoch 28: Val Loss 534.36700
Epoch 29: Val Loss 534.36670
Epoch 30: Val Loss 534.36639
Epoch 31: Val Loss 534.36609
Epoch 32: Val Loss 534.36584
Epoch 33: Val Loss 534.36554
Epoch 34: Val Loss 534.36530
Epoch 35: Val Loss 534.36493
Epoch 36: Val Loss 534.36469
Epoch 37: Val Loss 534.36438
Epoch 38: Val Loss 534.36407
Epoch 39: Val Loss 534.36383
Epoch 40: Val Loss 534.36353
Epoch 41: Val Loss 534.36328
Epoch 42: Val Loss 534.36298
Epoch 43: Val Loss 534.36267
Epoch 44: Val Loss 534.36249
Epoch 45: Val Loss 534.36212
Epoch 46: Val Loss 534.36182
Epoch 47: Val Loss 534.36163
Epoch 48: Val Loss 534.36127
Epoch 49: Val Loss 534.36096
Epoch 50: Val Loss 534.36072
Epoch 51: Val Loss 534.36047
Epoch 52: Val Loss 534.36017
Epoch 53: Val Loss 534.35986
Epoch 54: Val Loss 534.35962
Epoch 55: Val Loss 534.35931
Epoch 56: Val Loss 534.35901
Epoch 57: Val Loss 534.35864
Epoch 58: Val Loss 534.35846
Epoch 59: Val Loss 534.35815
Epoch 60: Val Loss 534.35785
Epoch 61: Val Loss 534.35760
Epoch 62: Val Loss 534.35730
Epoch 63: Val Loss 534.35699
Epoch 64: Val Loss 534.35675
Epoch 65: Val Loss 534.35645
Epoch 66: Val Loss 534.35614
Epoch 67: Val Loss 534.35583
Epoch 68: Val Loss 534.35559
Epoch 69: Val Loss 534.35529
Epoch 70: Val Loss 534.35498
Epoch 71: Val Loss 534.35474
Epoch 72: Val Loss 534.35443
Epoch 73: Val Loss 534.35419
Epoch 74: Val Loss 534.35388
Epoch 75: Val Loss 534.35358
Epoch 76: Val Loss 534.35327
Epoch 77: Val Loss 534.35303
Epoch 78: Val Loss 534.35272
Epoch 79: Val Loss 534.35248
Epoch 80: Val Loss 534.35217
Epoch 81: Val Loss 534.35193
Epoch 82: Val Loss 534.35162
Epoch 83: Val Loss 534.35132
Epoch 84: Val Loss 534.35101
Epoch 85: Val Loss 534.35077
Epoch 86: Val Loss 534.35046
Epoch 87: Val Loss 534.35016
Epoch 88: Val Loss 534.34991
Epoch 89: Val Loss 534.34961
Epoch 90: Val Loss 534.34930
Epoch 91: Val Loss 534.34900
Epoch 92: Val Loss 534.34875
Epoch 93: Val Loss 534.34845
Epoch 94: Val Loss 534.34814
Epoch 95: Val Loss 534.34790
Epoch 96: Val Loss 534.34760
Epoch 97: Val Loss 534.34729
Epoch 98: Val Loss 534.34705
Epoch 99: Val Loss 534.34674
{'MSE - mean': 585.1300374574657, 'MSE - std': 52.87630725264363, 'R2 - mean': -6.188117710007079, 'R2 - std': 0.24467940413584968} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 546.86700
Epoch 1: Val Loss 546.86639
Epoch 2: Val Loss 546.86578
Epoch 3: Val Loss 546.86523
Epoch 4: Val Loss 546.86462
Epoch 5: Val Loss 546.86401
Epoch 6: Val Loss 546.86340
Epoch 7: Val Loss 546.86285
Epoch 8: Val Loss 546.86224
Epoch 9: Val Loss 546.86163
Epoch 10: Val Loss 546.86108
Epoch 11: Val Loss 546.86047
Epoch 12: Val Loss 546.85980
Epoch 13: Val Loss 546.85919
Epoch 14: Val Loss 546.85858
Epoch 15: Val Loss 546.85803
Epoch 16: Val Loss 546.85748
Epoch 17: Val Loss 546.85687
Epoch 18: Val Loss 546.85626
Epoch 19: Val Loss 546.85565
Epoch 20: Val Loss 546.85510
Epoch 21: Val Loss 546.85449
Epoch 22: Val Loss 546.85388
Epoch 23: Val Loss 546.85327
Epoch 24: Val Loss 546.85272
Epoch 25: Val Loss 546.85211
Epoch 26: Val Loss 546.85150
Epoch 27: Val Loss 546.85083
Epoch 28: Val Loss 546.85028
Epoch 29: Val Loss 546.84967
Epoch 30: Val Loss 546.84906
Epoch 31: Val Loss 546.84845
Epoch 32: Val Loss 546.84784
Epoch 33: Val Loss 546.84729
Epoch 34: Val Loss 546.84662
Epoch 35: Val Loss 546.84607
Epoch 36: Val Loss 546.84552
Epoch 37: Val Loss 546.84485
Epoch 38: Val Loss 546.84430
Epoch 39: Val Loss 546.84369
Epoch 40: Val Loss 546.84308
Epoch 41: Val Loss 546.84253
Epoch 42: Val Loss 546.84186
Epoch 43: Val Loss 546.84131
Epoch 44: Val Loss 546.84070
Epoch 45: Val Loss 546.84009
Epoch 46: Val Loss 546.83954
Epoch 47: Val Loss 546.83893
Epoch 48: Val Loss 546.83832
Epoch 49: Val Loss 546.83777
Epoch 50: Val Loss 546.83716
Epoch 51: Val Loss 546.83655
Epoch 52: Val Loss 546.83594
Epoch 53: Val Loss 546.83539
Epoch 54: Val Loss 546.83484
Epoch 55: Val Loss 546.83423
Epoch 56: Val Loss 546.83362
Epoch 57: Val Loss 546.83301
Epoch 58: Val Loss 546.83240
Epoch 59: Val Loss 546.83179
Epoch 60: Val Loss 546.83124
Epoch 61: Val Loss 546.83057
Epoch 62: Val Loss 546.82996
Epoch 63: Val Loss 546.82941
Epoch 64: Val Loss 546.82880
Epoch 65: Val Loss 546.82825
Epoch 66: Val Loss 546.82764
Epoch 67: Val Loss 546.82703
Epoch 68: Val Loss 546.82648
Epoch 69: Val Loss 546.82587
Epoch 70: Val Loss 546.82526
Epoch 71: Val Loss 546.82465
Epoch 72: Val Loss 546.82410
Epoch 73: Val Loss 546.82349
Epoch 74: Val Loss 546.82288
Epoch 75: Val Loss 546.82227
Epoch 76: Val Loss 546.82172
Epoch 77: Val Loss 546.82111
Epoch 78: Val Loss 546.82050
Epoch 79: Val Loss 546.81982
Epoch 80: Val Loss 546.81927
Epoch 81: Val Loss 546.81866
Epoch 82: Val Loss 546.81805
Epoch 83: Val Loss 546.81744
Epoch 84: Val Loss 546.81683
Epoch 85: Val Loss 546.81628
Epoch 86: Val Loss 546.81567
Epoch 87: Val Loss 546.81506
Epoch 88: Val Loss 546.81445
Epoch 89: Val Loss 546.81384
Epoch 90: Val Loss 546.81329
Epoch 91: Val Loss 546.81262
Epoch 92: Val Loss 546.81213
Epoch 93: Val Loss 546.81146
Epoch 94: Val Loss 546.81085
Epoch 95: Val Loss 546.81024
Epoch 96: Val Loss 546.80969
Epoch 97: Val Loss 546.80914
Epoch 98: Val Loss 546.80847
Epoch 99: Val Loss 546.80792
{'MSE - mean': 577.4656129937296, 'MSE - std': 49.71616140031383, 'R2 - mean': -5.948629982723778, 'R2 - std': 0.5266041157491093} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 577.4656129937296 and parameters: {'dim': 32, 'depth': 1, 'heads': 8, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.5}. Best is trial 2 with value: 577.4656129937296.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 581.09375
Epoch 1: Val Loss 581.09314
Epoch 2: Val Loss 581.09253
Epoch 3: Val Loss 581.09192
Epoch 4: Val Loss 581.09131
Epoch 5: Val Loss 581.09076
Epoch 6: Val Loss 581.09015
Epoch 7: Val Loss 581.08954
Epoch 8: Val Loss 581.08893
Epoch 9: Val Loss 581.08838
Epoch 10: Val Loss 581.08783
Epoch 11: Val Loss 581.08722
Epoch 12: Val Loss 581.08667
Epoch 13: Val Loss 581.08606
Epoch 14: Val Loss 581.08545
Epoch 15: Val Loss 581.08484
Epoch 16: Val Loss 581.08423
Epoch 17: Val Loss 581.08368
Epoch 18: Val Loss 581.08307
Epoch 19: Val Loss 581.08246
Epoch 20: Val Loss 581.08185
Epoch 21: Val Loss 581.08130
Epoch 22: Val Loss 581.08069
Epoch 23: Val Loss 581.08008
Epoch 24: Val Loss 581.07953
Epoch 25: Val Loss 581.07892
Epoch 26: Val Loss 581.07831
Epoch 27: Val Loss 581.07776
Epoch 28: Val Loss 581.07721
Epoch 29: Val Loss 581.07660
Epoch 30: Val Loss 581.07599
Epoch 31: Val Loss 581.07544
Epoch 32: Val Loss 581.07483
Epoch 33: Val Loss 581.07422
Epoch 34: Val Loss 581.07367
Epoch 35: Val Loss 581.07312
Epoch 36: Val Loss 581.07245
Epoch 37: Val Loss 581.07184
Epoch 38: Val Loss 581.07123
Epoch 39: Val Loss 581.07068
Epoch 40: Val Loss 581.07013
Epoch 41: Val Loss 581.06952
Epoch 42: Val Loss 581.06885
Epoch 43: Val Loss 581.06830
Epoch 44: Val Loss 581.06775
Epoch 45: Val Loss 581.06714
Epoch 46: Val Loss 581.06659
Epoch 47: Val Loss 581.06598
Epoch 48: Val Loss 581.06537
Epoch 49: Val Loss 581.06482
Epoch 50: Val Loss 581.06421
Epoch 51: Val Loss 581.06360
Epoch 52: Val Loss 581.06299
Epoch 53: Val Loss 581.06244
Epoch 54: Val Loss 581.06183
Epoch 55: Val Loss 581.06116
Epoch 56: Val Loss 581.06055
Epoch 57: Val Loss 581.06006
Epoch 58: Val Loss 581.05945
Epoch 59: Val Loss 581.05878
Epoch 60: Val Loss 581.05823
Epoch 61: Val Loss 581.05762
Epoch 62: Val Loss 581.05695
Epoch 63: Val Loss 581.05640
Epoch 64: Val Loss 581.05579
Epoch 65: Val Loss 581.05518
Epoch 66: Val Loss 581.05463
Epoch 67: Val Loss 581.05402
Epoch 68: Val Loss 581.05341
Epoch 69: Val Loss 581.05280
Epoch 70: Val Loss 581.05225
Epoch 71: Val Loss 581.05164
Epoch 72: Val Loss 581.05103
Epoch 73: Val Loss 581.05042
Epoch 74: Val Loss 581.04980
Epoch 75: Val Loss 581.04926
Epoch 76: Val Loss 581.04865
Epoch 77: Val Loss 581.04810
Epoch 78: Val Loss 581.04749
Epoch 79: Val Loss 581.04688
Epoch 80: Val Loss 581.04626
Epoch 81: Val Loss 581.04565
Epoch 82: Val Loss 581.04510
Epoch 83: Val Loss 581.04443
Epoch 84: Val Loss 581.04388
Epoch 85: Val Loss 581.04333
Epoch 86: Val Loss 581.04272
Epoch 87: Val Loss 581.04218
Epoch 88: Val Loss 581.04150
Epoch 89: Val Loss 581.04089
Epoch 90: Val Loss 581.04041
Epoch 91: Val Loss 581.03979
Epoch 92: Val Loss 581.03925
Epoch 93: Val Loss 581.03864
Epoch 94: Val Loss 581.03802
Epoch 95: Val Loss 581.03748
Epoch 96: Val Loss 581.03687
Epoch 97: Val Loss 581.03632
Epoch 98: Val Loss 581.03571
Epoch 99: Val Loss 581.03510
{'MSE - mean': 581.035087187628, 'MSE - std': 0.0, 'R2 - mean': -6.781551130699468, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 637.36566
Epoch 1: Val Loss 637.36499
Epoch 2: Val Loss 637.36426
Epoch 3: Val Loss 637.36353
Epoch 4: Val Loss 637.36279
Epoch 5: Val Loss 637.36206
Epoch 6: Val Loss 637.36133
Epoch 7: Val Loss 637.36060
Epoch 8: Val Loss 637.35986
Epoch 9: Val Loss 637.35919
Epoch 10: Val Loss 637.35846
Epoch 11: Val Loss 637.35773
Epoch 12: Val Loss 637.35706
Epoch 13: Val Loss 637.35626
Epoch 14: Val Loss 637.35559
Epoch 15: Val Loss 637.35480
Epoch 16: Val Loss 637.35413
Epoch 17: Val Loss 637.35339
Epoch 18: Val Loss 637.35266
Epoch 19: Val Loss 637.35193
Epoch 20: Val Loss 637.35126
Epoch 21: Val Loss 637.35046
Epoch 22: Val Loss 637.34979
Epoch 23: Val Loss 637.34906
Epoch 24: Val Loss 637.34839
Epoch 25: Val Loss 637.34766
Epoch 26: Val Loss 637.34698
Epoch 27: Val Loss 637.34619
Epoch 28: Val Loss 637.34552
Epoch 29: Val Loss 637.34479
Epoch 30: Val Loss 637.34406
Epoch 31: Val Loss 637.34326
Epoch 32: Val Loss 637.34259
Epoch 33: Val Loss 637.34186
Epoch 34: Val Loss 637.34119
Epoch 35: Val Loss 637.34045
Epoch 36: Val Loss 637.33972
Epoch 37: Val Loss 637.33905
Epoch 38: Val Loss 637.33838
Epoch 39: Val Loss 637.33759
Epoch 40: Val Loss 637.33685
Epoch 41: Val Loss 637.33624
Epoch 42: Val Loss 637.33551
Epoch 43: Val Loss 637.33478
Epoch 44: Val Loss 637.33405
Epoch 45: Val Loss 637.33337
Epoch 46: Val Loss 637.33264
Epoch 47: Val Loss 637.33197
Epoch 48: Val Loss 637.33124
Epoch 49: Val Loss 637.33044
Epoch 50: Val Loss 637.32971
Epoch 51: Val Loss 637.32904
Epoch 52: Val Loss 637.32837
Epoch 53: Val Loss 637.32764
Epoch 54: Val Loss 637.32690
Epoch 55: Val Loss 637.32617
Epoch 56: Val Loss 637.32550
Epoch 57: Val Loss 637.32477
Epoch 58: Val Loss 637.32404
Epoch 59: Val Loss 637.32330
Epoch 60: Val Loss 637.32263
Epoch 61: Val Loss 637.32190
Epoch 62: Val Loss 637.32117
Epoch 63: Val Loss 637.32056
Epoch 64: Val Loss 637.31976
Epoch 65: Val Loss 637.31909
Epoch 66: Val Loss 637.31842
Epoch 67: Val Loss 637.31769
Epoch 68: Val Loss 637.31696
Epoch 69: Val Loss 637.31622
Epoch 70: Val Loss 637.31549
Epoch 71: Val Loss 637.31482
Epoch 72: Val Loss 637.31409
Epoch 73: Val Loss 637.31329
Epoch 74: Val Loss 637.31256
Epoch 75: Val Loss 637.31189
Epoch 76: Val Loss 637.31116
Epoch 77: Val Loss 637.31049
Epoch 78: Val Loss 637.30975
Epoch 79: Val Loss 637.30902
Epoch 80: Val Loss 637.30829
Epoch 81: Val Loss 637.30762
Epoch 82: Val Loss 637.30695
Epoch 83: Val Loss 637.30621
Epoch 84: Val Loss 637.30548
Epoch 85: Val Loss 637.30475
Epoch 86: Val Loss 637.30408
Epoch 87: Val Loss 637.30328
Epoch 88: Val Loss 637.30261
Epoch 89: Val Loss 637.30182
Epoch 90: Val Loss 637.30115
Epoch 91: Val Loss 637.30042
Epoch 92: Val Loss 637.29962
Epoch 93: Val Loss 637.29889
Epoch 94: Val Loss 637.29816
Epoch 95: Val Loss 637.29749
Epoch 96: Val Loss 637.29681
Epoch 97: Val Loss 637.29602
Epoch 98: Val Loss 637.29529
Epoch 99: Val Loss 637.29456
{'MSE - mean': 609.1648424907238, 'MSE - std': 28.12975530309575, 'R2 - mean': -6.8471552272868355, 'R2 - std': 0.06560409658736788} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 652.37933
Epoch 1: Val Loss 652.37854
Epoch 2: Val Loss 652.37775
Epoch 3: Val Loss 652.37695
Epoch 4: Val Loss 652.37616
Epoch 5: Val Loss 652.37537
Epoch 6: Val Loss 652.37469
Epoch 7: Val Loss 652.37390
Epoch 8: Val Loss 652.37311
Epoch 9: Val Loss 652.37238
Epoch 10: Val Loss 652.37152
Epoch 11: Val Loss 652.37073
Epoch 12: Val Loss 652.36993
Epoch 13: Val Loss 652.36920
Epoch 14: Val Loss 652.36835
Epoch 15: Val Loss 652.36755
Epoch 16: Val Loss 652.36676
Epoch 17: Val Loss 652.36603
Epoch 18: Val Loss 652.36523
Epoch 19: Val Loss 652.36444
Epoch 20: Val Loss 652.36371
Epoch 21: Val Loss 652.36292
Epoch 22: Val Loss 652.36212
Epoch 23: Val Loss 652.36139
Epoch 24: Val Loss 652.36060
Epoch 25: Val Loss 652.35980
Epoch 26: Val Loss 652.35907
Epoch 27: Val Loss 652.35828
Epoch 28: Val Loss 652.35748
Epoch 29: Val Loss 652.35675
Epoch 30: Val Loss 652.35596
Epoch 31: Val Loss 652.35516
Epoch 32: Val Loss 652.35443
Epoch 33: Val Loss 652.35364
Epoch 34: Val Loss 652.35284
Epoch 35: Val Loss 652.35211
Epoch 36: Val Loss 652.35132
Epoch 37: Val Loss 652.35052
Epoch 38: Val Loss 652.34979
Epoch 39: Val Loss 652.34900
Epoch 40: Val Loss 652.34814
Epoch 41: Val Loss 652.34747
Epoch 42: Val Loss 652.34668
Epoch 43: Val Loss 652.34589
Epoch 44: Val Loss 652.34503
Epoch 45: Val Loss 652.34436
Epoch 46: Val Loss 652.34351
Epoch 47: Val Loss 652.34283
Epoch 48: Val Loss 652.34204
Epoch 49: Val Loss 652.34119
Epoch 50: Val Loss 652.34052
Epoch 51: Val Loss 652.33972
Epoch 52: Val Loss 652.33893
Epoch 53: Val Loss 652.33807
Epoch 54: Val Loss 652.33734
Epoch 55: Val Loss 652.33655
Epoch 56: Val Loss 652.33569
Epoch 57: Val Loss 652.33502
Epoch 58: Val Loss 652.33423
Epoch 59: Val Loss 652.33337
Epoch 60: Val Loss 652.33270
Epoch 61: Val Loss 652.33191
Epoch 62: Val Loss 652.33112
Epoch 63: Val Loss 652.33038
Epoch 64: Val Loss 652.32959
Epoch 65: Val Loss 652.32880
Epoch 66: Val Loss 652.32806
Epoch 67: Val Loss 652.32721
Epoch 68: Val Loss 652.32642
Epoch 69: Val Loss 652.32562
Epoch 70: Val Loss 652.32489
Epoch 71: Val Loss 652.32410
Epoch 72: Val Loss 652.32330
Epoch 73: Val Loss 652.32257
Epoch 74: Val Loss 652.32172
Epoch 75: Val Loss 652.32098
Epoch 76: Val Loss 652.32013
Epoch 77: Val Loss 652.31940
Epoch 78: Val Loss 652.31866
Epoch 79: Val Loss 652.31793
Epoch 80: Val Loss 652.31714
Epoch 81: Val Loss 652.31635
Epoch 82: Val Loss 652.31561
Epoch 83: Val Loss 652.31476
Epoch 84: Val Loss 652.31403
Epoch 85: Val Loss 652.31317
Epoch 86: Val Loss 652.31250
Epoch 87: Val Loss 652.31165
Epoch 88: Val Loss 652.31085
Epoch 89: Val Loss 652.31012
Epoch 90: Val Loss 652.30933
Epoch 91: Val Loss 652.30853
Epoch 92: Val Loss 652.30786
Epoch 93: Val Loss 652.30707
Epoch 94: Val Loss 652.30627
Epoch 95: Val Loss 652.30554
Epoch 96: Val Loss 652.30475
Epoch 97: Val Loss 652.30396
Epoch 98: Val Loss 652.30322
Epoch 99: Val Loss 652.30243
{'MSE - mean': 623.5440345148681, 'MSE - std': 30.676447243378707, 'R2 - mean': -6.509814649499425, 'R2 - std': 0.48006936565103153} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 528.16467
Epoch 1: Val Loss 528.16431
Epoch 2: Val Loss 528.16400
Epoch 3: Val Loss 528.16370
Epoch 4: Val Loss 528.16333
Epoch 5: Val Loss 528.16302
Epoch 6: Val Loss 528.16266
Epoch 7: Val Loss 528.16241
Epoch 8: Val Loss 528.16205
Epoch 9: Val Loss 528.16174
Epoch 10: Val Loss 528.16138
Epoch 11: Val Loss 528.16101
Epoch 12: Val Loss 528.16071
Epoch 13: Val Loss 528.16040
Epoch 14: Val Loss 528.16010
Epoch 15: Val Loss 528.15979
Epoch 16: Val Loss 528.15948
Epoch 17: Val Loss 528.15912
Epoch 18: Val Loss 528.15881
Epoch 19: Val Loss 528.15851
Epoch 20: Val Loss 528.15814
Epoch 21: Val Loss 528.15784
Epoch 22: Val Loss 528.15747
Epoch 23: Val Loss 528.15717
Epoch 24: Val Loss 528.15686
Epoch 25: Val Loss 528.15649
Epoch 26: Val Loss 528.15619
Epoch 27: Val Loss 528.15588
Epoch 28: Val Loss 528.15552
Epoch 29: Val Loss 528.15521
Epoch 30: Val Loss 528.15491
Epoch 31: Val Loss 528.15454
Epoch 32: Val Loss 528.15424
Epoch 33: Val Loss 528.15393
Epoch 34: Val Loss 528.15356
Epoch 35: Val Loss 528.15320
Epoch 36: Val Loss 528.15289
Epoch 37: Val Loss 528.15259
Epoch 38: Val Loss 528.15228
Epoch 39: Val Loss 528.15198
Epoch 40: Val Loss 528.15161
Epoch 41: Val Loss 528.15131
Epoch 42: Val Loss 528.15100
Epoch 43: Val Loss 528.15057
Epoch 44: Val Loss 528.15027
Epoch 45: Val Loss 528.14996
Epoch 46: Val Loss 528.14966
Epoch 47: Val Loss 528.14929
Epoch 48: Val Loss 528.14899
Epoch 49: Val Loss 528.14868
Epoch 50: Val Loss 528.14838
Epoch 51: Val Loss 528.14807
Epoch 52: Val Loss 528.14771
Epoch 53: Val Loss 528.14740
Epoch 54: Val Loss 528.14709
Epoch 55: Val Loss 528.14679
Epoch 56: Val Loss 528.14642
Epoch 57: Val Loss 528.14612
Epoch 58: Val Loss 528.14581
Epoch 59: Val Loss 528.14551
Epoch 60: Val Loss 528.14520
Epoch 61: Val Loss 528.14490
Epoch 62: Val Loss 528.14453
Epoch 63: Val Loss 528.14417
Epoch 64: Val Loss 528.14392
Epoch 65: Val Loss 528.14355
Epoch 66: Val Loss 528.14325
Epoch 67: Val Loss 528.14288
Epoch 68: Val Loss 528.14264
Epoch 69: Val Loss 528.14227
Epoch 70: Val Loss 528.14197
Epoch 71: Val Loss 528.14160
Epoch 72: Val Loss 528.14130
Epoch 73: Val Loss 528.14099
Epoch 74: Val Loss 528.14062
Epoch 75: Val Loss 528.14032
Epoch 76: Val Loss 528.14008
Epoch 77: Val Loss 528.13971
Epoch 78: Val Loss 528.13940
Epoch 79: Val Loss 528.13910
Epoch 80: Val Loss 528.13873
Epoch 81: Val Loss 528.13843
Epoch 82: Val Loss 528.13806
Epoch 83: Val Loss 528.13776
Epoch 84: Val Loss 528.13739
Epoch 85: Val Loss 528.13708
Epoch 86: Val Loss 528.13678
Epoch 87: Val Loss 528.13641
Epoch 88: Val Loss 528.13611
Epoch 89: Val Loss 528.13580
Epoch 90: Val Loss 528.13550
Epoch 91: Val Loss 528.13519
Epoch 92: Val Loss 528.13483
Epoch 93: Val Loss 528.13452
Epoch 94: Val Loss 528.13422
Epoch 95: Val Loss 528.13391
Epoch 96: Val Loss 528.13361
Epoch 97: Val Loss 528.13324
Epoch 98: Val Loss 528.13293
Epoch 99: Val Loss 528.13257
{'MSE - mean': 599.6911796291307, 'MSE - std': 49.11882908017044, 'R2 - mean': -6.382174408549082, 'R2 - std': 0.47087794618884293} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 591.97839
Epoch 1: Val Loss 591.97766
Epoch 2: Val Loss 591.97693
Epoch 3: Val Loss 591.97626
Epoch 4: Val Loss 591.97546
Epoch 5: Val Loss 591.97479
Epoch 6: Val Loss 591.97406
Epoch 7: Val Loss 591.97339
Epoch 8: Val Loss 591.97266
Epoch 9: Val Loss 591.97192
Epoch 10: Val Loss 591.97119
Epoch 11: Val Loss 591.97052
Epoch 12: Val Loss 591.96979
Epoch 13: Val Loss 591.96912
Epoch 14: Val Loss 591.96838
Epoch 15: Val Loss 591.96765
Epoch 16: Val Loss 591.96686
Epoch 17: Val Loss 591.96619
Epoch 18: Val Loss 591.96552
Epoch 19: Val Loss 591.96478
Epoch 20: Val Loss 591.96411
Epoch 21: Val Loss 591.96338
Epoch 22: Val Loss 591.96265
Epoch 23: Val Loss 591.96191
Epoch 24: Val Loss 591.96124
Epoch 25: Val Loss 591.96057
Epoch 26: Val Loss 591.95978
Epoch 27: Val Loss 591.95905
Epoch 28: Val Loss 591.95837
Epoch 29: Val Loss 591.95764
Epoch 30: Val Loss 591.95691
Epoch 31: Val Loss 591.95624
Epoch 32: Val Loss 591.95544
Epoch 33: Val Loss 591.95477
Epoch 34: Val Loss 591.95404
Epoch 35: Val Loss 591.95331
Epoch 36: Val Loss 591.95258
Epoch 37: Val Loss 591.95184
Epoch 38: Val Loss 591.95111
Epoch 39: Val Loss 591.95038
Epoch 40: Val Loss 591.94965
Epoch 41: Val Loss 591.94891
Epoch 42: Val Loss 591.94824
Epoch 43: Val Loss 591.94745
Epoch 44: Val Loss 591.94678
Epoch 45: Val Loss 591.94611
Epoch 46: Val Loss 591.94531
Epoch 47: Val Loss 591.94458
Epoch 48: Val Loss 591.94391
Epoch 49: Val Loss 591.94324
Epoch 50: Val Loss 591.94250
Epoch 51: Val Loss 591.94177
Epoch 52: Val Loss 591.94104
Epoch 53: Val Loss 591.94037
Epoch 54: Val Loss 591.93964
Epoch 55: Val Loss 591.93896
Epoch 56: Val Loss 591.93823
Epoch 57: Val Loss 591.93756
Epoch 58: Val Loss 591.93683
Epoch 59: Val Loss 591.93616
Epoch 60: Val Loss 591.93542
Epoch 61: Val Loss 591.93475
Epoch 62: Val Loss 591.93402
Epoch 63: Val Loss 591.93329
Epoch 64: Val Loss 591.93268
Epoch 65: Val Loss 591.93195
Epoch 66: Val Loss 591.93121
Epoch 67: Val Loss 591.93048
Epoch 68: Val Loss 591.92975
Epoch 69: Val Loss 591.92908
Epoch 70: Val Loss 591.92834
Epoch 71: Val Loss 591.92761
Epoch 72: Val Loss 591.92688
Epoch 73: Val Loss 591.92621
Epoch 74: Val Loss 591.92548
Epoch 75: Val Loss 591.92480
Epoch 76: Val Loss 591.92407
Epoch 77: Val Loss 591.92340
Epoch 78: Val Loss 591.92267
Epoch 79: Val Loss 591.92200
Epoch 80: Val Loss 591.92126
Epoch 81: Val Loss 591.92053
Epoch 82: Val Loss 591.91986
Epoch 83: Val Loss 591.91913
Epoch 84: Val Loss 591.91840
Epoch 85: Val Loss 591.91766
Epoch 86: Val Loss 591.91693
Epoch 87: Val Loss 591.91632
Epoch 88: Val Loss 591.91559
Epoch 89: Val Loss 591.91486
Epoch 90: Val Loss 591.91418
Epoch 91: Val Loss 591.91345
Epoch 92: Val Loss 591.91272
Epoch 93: Val Loss 591.91199
Epoch 94: Val Loss 591.91132
Epoch 95: Val Loss 591.91064
Epoch 96: Val Loss 591.90997
Epoch 97: Val Loss 591.90924
Epoch 98: Val Loss 591.90851
Epoch 99: Val Loss 591.90778
{'MSE - mean': 598.1345028239284, 'MSE - std': 44.04339299905284, 'R2 - mean': -6.202695750932967, 'R2 - std': 0.5533815919637631} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 598.1345028239284 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -5, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 2 with value: 577.4656129937296.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 562.56970
Epoch 1: Val Loss 562.08826
Epoch 2: Val Loss 561.61963
Epoch 3: Val Loss 561.17352
Epoch 4: Val Loss 560.74823
Epoch 5: Val Loss 560.33521
Epoch 6: Val Loss 559.93213
Epoch 7: Val Loss 559.53955
Epoch 8: Val Loss 559.16119
Epoch 9: Val Loss 558.80280
Epoch 10: Val Loss 558.46216
Epoch 11: Val Loss 558.15076
Epoch 12: Val Loss 557.86346
Epoch 13: Val Loss 557.58606
Epoch 14: Val Loss 557.31824
Epoch 15: Val Loss 557.05664
Epoch 16: Val Loss 556.80377
Epoch 17: Val Loss 556.55634
Epoch 18: Val Loss 556.31183
Epoch 19: Val Loss 556.07172
Epoch 20: Val Loss 555.83575
Epoch 21: Val Loss 555.60199
Epoch 22: Val Loss 555.36987
Epoch 23: Val Loss 555.14185
Epoch 24: Val Loss 554.91919
Epoch 25: Val Loss 554.69611
Epoch 26: Val Loss 554.47534
Epoch 27: Val Loss 554.25732
Epoch 28: Val Loss 554.04431
Epoch 29: Val Loss 553.83478
Epoch 30: Val Loss 553.62769
Epoch 31: Val Loss 553.42242
Epoch 32: Val Loss 553.21881
Epoch 33: Val Loss 553.01935
Epoch 34: Val Loss 552.82080
Epoch 35: Val Loss 552.62262
Epoch 36: Val Loss 552.42731
Epoch 37: Val Loss 552.23151
Epoch 38: Val Loss 552.03412
Epoch 39: Val Loss 551.83771
Epoch 40: Val Loss 551.64227
Epoch 41: Val Loss 551.44733
Epoch 42: Val Loss 551.25421
Epoch 43: Val Loss 551.06525
Epoch 44: Val Loss 550.88043
Epoch 45: Val Loss 550.69684
Epoch 46: Val Loss 550.51440
Epoch 47: Val Loss 550.33368
Epoch 48: Val Loss 550.15576
Epoch 49: Val Loss 549.98077
Epoch 50: Val Loss 549.80493
Epoch 51: Val Loss 549.62891
Epoch 52: Val Loss 549.45306
Epoch 53: Val Loss 549.27783
Epoch 54: Val Loss 549.10236
Epoch 55: Val Loss 548.92725
Epoch 56: Val Loss 548.75177
Epoch 57: Val Loss 548.57422
Epoch 58: Val Loss 548.39832
Epoch 59: Val Loss 548.22241
Epoch 60: Val Loss 548.04572
Epoch 61: Val Loss 547.87061
Epoch 62: Val Loss 547.69806
Epoch 63: Val Loss 547.52539
Epoch 64: Val Loss 547.35352
Epoch 65: Val Loss 547.18085
Epoch 66: Val Loss 547.00824
Epoch 67: Val Loss 546.83649
Epoch 68: Val Loss 546.66467
Epoch 69: Val Loss 546.49365
Epoch 70: Val Loss 546.32275
Epoch 71: Val Loss 546.15277
Epoch 72: Val Loss 545.98285
Epoch 73: Val Loss 545.81201
Epoch 74: Val Loss 545.64197
Epoch 75: Val Loss 545.47162
Epoch 76: Val Loss 545.29987
Epoch 77: Val Loss 545.12823
Epoch 78: Val Loss 544.95636
Epoch 79: Val Loss 544.78632
Epoch 80: Val Loss 544.61542
Epoch 81: Val Loss 544.44519
Epoch 82: Val Loss 544.27399
Epoch 83: Val Loss 544.10339
Epoch 84: Val Loss 543.93280
Epoch 85: Val Loss 543.76288
Epoch 86: Val Loss 543.59344
Epoch 87: Val Loss 543.42426
Epoch 88: Val Loss 543.25568
Epoch 89: Val Loss 543.08759
Epoch 90: Val Loss 542.91730
Epoch 91: Val Loss 542.74652
Epoch 92: Val Loss 542.57587
Epoch 93: Val Loss 542.40497
Epoch 94: Val Loss 542.23572
Epoch 95: Val Loss 542.06689
Epoch 96: Val Loss 541.89624
Epoch 97: Val Loss 541.72467
Epoch 98: Val Loss 541.55530
Epoch 99: Val Loss 541.38574
{'MSE - mean': 541.3857164533634, 'MSE - std': 0.0, 'R2 - mean': -6.250544290540937, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 611.31940
Epoch 1: Val Loss 611.10187
Epoch 2: Val Loss 610.87152
Epoch 3: Val Loss 610.62885
Epoch 4: Val Loss 610.37512
Epoch 5: Val Loss 610.10675
Epoch 6: Val Loss 609.82086
Epoch 7: Val Loss 609.51721
Epoch 8: Val Loss 609.18964
Epoch 9: Val Loss 608.83691
Epoch 10: Val Loss 608.46411
Epoch 11: Val Loss 608.07416
Epoch 12: Val Loss 607.65997
Epoch 13: Val Loss 607.21875
Epoch 14: Val Loss 606.74994
Epoch 15: Val Loss 606.24847
Epoch 16: Val Loss 605.70789
Epoch 17: Val Loss 605.12842
Epoch 18: Val Loss 604.51019
Epoch 19: Val Loss 603.84100
Epoch 20: Val Loss 603.11053
Epoch 21: Val Loss 602.32214
Epoch 22: Val Loss 601.46216
Epoch 23: Val Loss 600.52448
Epoch 24: Val Loss 599.49878
Epoch 25: Val Loss 598.36639
Epoch 26: Val Loss 597.13245
Epoch 27: Val Loss 595.76373
Epoch 28: Val Loss 594.24536
Epoch 29: Val Loss 592.58215
Epoch 30: Val Loss 590.76306
Epoch 31: Val Loss 588.77472
Epoch 32: Val Loss 586.60236
Epoch 33: Val Loss 584.25024
Epoch 34: Val Loss 581.68488
Epoch 35: Val Loss 578.87134
Epoch 36: Val Loss 575.80457
Epoch 37: Val Loss 572.50861
Epoch 38: Val Loss 568.93237
Epoch 39: Val Loss 565.08026
Epoch 40: Val Loss 560.92371
Epoch 41: Val Loss 556.53003
Epoch 42: Val Loss 551.87732
Epoch 43: Val Loss 546.97150
Epoch 44: Val Loss 541.73999
Epoch 45: Val Loss 536.14124
Epoch 46: Val Loss 530.15741
Epoch 47: Val Loss 523.86682
Epoch 48: Val Loss 517.24823
Epoch 49: Val Loss 510.29803
Epoch 50: Val Loss 502.96295
Epoch 51: Val Loss 495.15213
Epoch 52: Val Loss 486.91312
Epoch 53: Val Loss 478.34842
Epoch 54: Val Loss 469.44495
Epoch 55: Val Loss 460.23935
Epoch 56: Val Loss 450.67853
Epoch 57: Val Loss 440.72702
Epoch 58: Val Loss 430.41782
Epoch 59: Val Loss 419.76508
Epoch 60: Val Loss 408.82883
Epoch 61: Val Loss 397.56482
Epoch 62: Val Loss 385.89334
Epoch 63: Val Loss 373.86069
Epoch 64: Val Loss 361.73746
Epoch 65: Val Loss 349.36810
Epoch 66: Val Loss 336.86346
Epoch 67: Val Loss 324.16727
Epoch 68: Val Loss 311.42236
Epoch 69: Val Loss 298.70270
Epoch 70: Val Loss 286.19623
Epoch 71: Val Loss 273.71832
Epoch 72: Val Loss 261.44781
Epoch 73: Val Loss 249.38499
Epoch 74: Val Loss 237.54718
Epoch 75: Val Loss 225.95459
Epoch 76: Val Loss 214.59866
Epoch 77: Val Loss 203.53783
Epoch 78: Val Loss 192.66913
Epoch 79: Val Loss 182.26930
Epoch 80: Val Loss 172.34239
Epoch 81: Val Loss 162.87170
Epoch 82: Val Loss 154.01643
Epoch 83: Val Loss 145.83195
Epoch 84: Val Loss 138.35873
Epoch 85: Val Loss 131.42233
Epoch 86: Val Loss 125.01906
Epoch 87: Val Loss 119.36075
Epoch 88: Val Loss 114.17095
Epoch 89: Val Loss 109.68263
Epoch 90: Val Loss 105.65722
Epoch 91: Val Loss 102.08726
Epoch 92: Val Loss 98.87263
Epoch 93: Val Loss 95.82561
Epoch 94: Val Loss 93.14451
Epoch 95: Val Loss 90.73920
Epoch 96: Val Loss 88.56371
Epoch 97: Val Loss 86.70958
Epoch 98: Val Loss 85.00687
Epoch 99: Val Loss 83.40147
{'MSE - mean': 312.3935917791989, 'MSE - std': 228.99212467416456, 'R2 - mean': -3.143035624472008, 'R2 - std': 3.107508666068928} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 681.24860
Epoch 1: Val Loss 680.93439
Epoch 2: Val Loss 680.59637
Epoch 3: Val Loss 680.22491
Epoch 4: Val Loss 679.83313
Epoch 5: Val Loss 679.42676
Epoch 6: Val Loss 679.01477
Epoch 7: Val Loss 678.59283
Epoch 8: Val Loss 678.16589
Epoch 9: Val Loss 677.73004
Epoch 10: Val Loss 677.26685
Epoch 11: Val Loss 676.78485
Epoch 12: Val Loss 676.26471
Epoch 13: Val Loss 675.72028
Epoch 14: Val Loss 675.14612
Epoch 15: Val Loss 674.52600
Epoch 16: Val Loss 673.86835
Epoch 17: Val Loss 673.17529
Epoch 18: Val Loss 672.42242
Epoch 19: Val Loss 671.62799
Epoch 20: Val Loss 670.78857
Epoch 21: Val Loss 669.90576
Epoch 22: Val Loss 668.97290
Epoch 23: Val Loss 667.96478
Epoch 24: Val Loss 666.90051
Epoch 25: Val Loss 665.76514
Epoch 26: Val Loss 664.55157
Epoch 27: Val Loss 663.28943
Epoch 28: Val Loss 661.91632
Epoch 29: Val Loss 660.45233
Epoch 30: Val Loss 658.90076
Epoch 31: Val Loss 657.26489
Epoch 32: Val Loss 655.54004
Epoch 33: Val Loss 653.68127
Epoch 34: Val Loss 651.69165
Epoch 35: Val Loss 649.64240
Epoch 36: Val Loss 647.47186
Epoch 37: Val Loss 645.19281
Epoch 38: Val Loss 642.84558
Epoch 39: Val Loss 640.42499
Epoch 40: Val Loss 637.88098
Epoch 41: Val Loss 635.23413
Epoch 42: Val Loss 632.51111
Epoch 43: Val Loss 629.65533
Epoch 44: Val Loss 626.68292
Epoch 45: Val Loss 623.60742
Epoch 46: Val Loss 620.41785
Epoch 47: Val Loss 617.07385
Epoch 48: Val Loss 613.67169
Epoch 49: Val Loss 610.21930
Epoch 50: Val Loss 606.67487
Epoch 51: Val Loss 603.10583
Epoch 52: Val Loss 599.47943
Epoch 53: Val Loss 595.78815
Epoch 54: Val Loss 592.05017
Epoch 55: Val Loss 588.19720
Epoch 56: Val Loss 584.27191
Epoch 57: Val Loss 580.32111
Epoch 58: Val Loss 576.30823
Epoch 59: Val Loss 572.21851
Epoch 60: Val Loss 568.00470
Epoch 61: Val Loss 563.77240
Epoch 62: Val Loss 559.43323
Epoch 63: Val Loss 555.01935
Epoch 64: Val Loss 550.47375
Epoch 65: Val Loss 545.83490
Epoch 66: Val Loss 541.14923
Epoch 67: Val Loss 536.40900
Epoch 68: Val Loss 531.60992
Epoch 69: Val Loss 526.72198
Epoch 70: Val Loss 521.63391
Epoch 71: Val Loss 516.47815
Epoch 72: Val Loss 511.17838
Epoch 73: Val Loss 505.66901
Epoch 74: Val Loss 499.87415
Epoch 75: Val Loss 493.82596
Epoch 76: Val Loss 487.38647
Epoch 77: Val Loss 480.80212
Epoch 78: Val Loss 473.86487
Epoch 79: Val Loss 466.51321
Epoch 80: Val Loss 459.08743
Epoch 81: Val Loss 451.76898
Epoch 82: Val Loss 444.49728
Epoch 83: Val Loss 437.12878
Epoch 84: Val Loss 429.57812
Epoch 85: Val Loss 421.72653
Epoch 86: Val Loss 413.81641
Epoch 87: Val Loss 405.97498
Epoch 88: Val Loss 397.87869
Epoch 89: Val Loss 389.62317
Epoch 90: Val Loss 380.85431
Epoch 91: Val Loss 372.06879
Epoch 92: Val Loss 363.26114
Epoch 93: Val Loss 354.62390
Epoch 94: Val Loss 346.13516
Epoch 95: Val Loss 337.86942
Epoch 96: Val Loss 329.77872
Epoch 97: Val Loss 321.73370
Epoch 98: Val Loss 314.01993
Epoch 99: Val Loss 306.36951
{'MSE - mean': 310.3855660385339, 'MSE - std': 186.99285131596812, 'R2 - mean': -2.8321186784593184, 'R2 - std': 2.575088108738167} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 538.03937
Epoch 1: Val Loss 537.45526
Epoch 2: Val Loss 536.85333
Epoch 3: Val Loss 536.23547
Epoch 4: Val Loss 535.58447
Epoch 5: Val Loss 534.90771
Epoch 6: Val Loss 534.21167
Epoch 7: Val Loss 533.49603
Epoch 8: Val Loss 532.76526
Epoch 9: Val Loss 532.01202
Epoch 10: Val Loss 531.22223
Epoch 11: Val Loss 530.40424
Epoch 12: Val Loss 529.54364
Epoch 13: Val Loss 528.64563
Epoch 14: Val Loss 527.71838
Epoch 15: Val Loss 526.75903
Epoch 16: Val Loss 525.74719
Epoch 17: Val Loss 524.67896
Epoch 18: Val Loss 523.56866
Epoch 19: Val Loss 522.41101
Epoch 20: Val Loss 521.20056
Epoch 21: Val Loss 519.94141
Epoch 22: Val Loss 518.61755
Epoch 23: Val Loss 517.24310
Epoch 24: Val Loss 515.82318
Epoch 25: Val Loss 514.35077
Epoch 26: Val Loss 512.82220
Epoch 27: Val Loss 511.22818
Epoch 28: Val Loss 509.56250
Epoch 29: Val Loss 507.81119
Epoch 30: Val Loss 505.94543
Epoch 31: Val Loss 503.97006
Epoch 32: Val Loss 501.90933
Epoch 33: Val Loss 499.74225
Epoch 34: Val Loss 497.45908
Epoch 35: Val Loss 495.09229
Epoch 36: Val Loss 492.58411
Epoch 37: Val Loss 489.96094
Epoch 38: Val Loss 487.22299
Epoch 39: Val Loss 484.35333
Epoch 40: Val Loss 481.28918
Epoch 41: Val Loss 478.05167
Epoch 42: Val Loss 474.67828
Epoch 43: Val Loss 471.17032
Epoch 44: Val Loss 467.53976
Epoch 45: Val Loss 463.74637
Epoch 46: Val Loss 459.83258
Epoch 47: Val Loss 455.77399
Epoch 48: Val Loss 451.48511
Epoch 49: Val Loss 447.09119
Epoch 50: Val Loss 442.46140
Epoch 51: Val Loss 437.65582
Epoch 52: Val Loss 432.67627
Epoch 53: Val Loss 427.53824
Epoch 54: Val Loss 422.32394
Epoch 55: Val Loss 416.87946
Epoch 56: Val Loss 411.24310
Epoch 57: Val Loss 405.41064
Epoch 58: Val Loss 399.38574
Epoch 59: Val Loss 393.15247
Epoch 60: Val Loss 386.73749
Epoch 61: Val Loss 380.18201
Epoch 62: Val Loss 373.57526
Epoch 63: Val Loss 366.80396
Epoch 64: Val Loss 359.87692
Epoch 65: Val Loss 352.68387
Epoch 66: Val Loss 345.22516
Epoch 67: Val Loss 337.74390
Epoch 68: Val Loss 330.04044
Epoch 69: Val Loss 322.25638
Epoch 70: Val Loss 314.25195
Epoch 71: Val Loss 306.09177
Epoch 72: Val Loss 297.73486
Epoch 73: Val Loss 289.30820
Epoch 74: Val Loss 280.91260
Epoch 75: Val Loss 272.63577
Epoch 76: Val Loss 264.48419
Epoch 77: Val Loss 256.45178
Epoch 78: Val Loss 248.40417
Epoch 79: Val Loss 240.47072
Epoch 80: Val Loss 232.59270
Epoch 81: Val Loss 224.71124
Epoch 82: Val Loss 216.87086
Epoch 83: Val Loss 209.17572
Epoch 84: Val Loss 201.48788
Epoch 85: Val Loss 194.03728
Epoch 86: Val Loss 186.87820
Epoch 87: Val Loss 179.88741
Epoch 88: Val Loss 173.00024
Epoch 89: Val Loss 166.29065
Epoch 90: Val Loss 159.72847
Epoch 91: Val Loss 153.53627
Epoch 92: Val Loss 147.72638
Epoch 93: Val Loss 142.28947
Epoch 94: Val Loss 137.10284
Epoch 95: Val Loss 132.20213
Epoch 96: Val Loss 127.50539
Epoch 97: Val Loss 123.09080
Epoch 98: Val Loss 118.85862
Epoch 99: Val Loss 114.94527
{'MSE - mean': 261.52549327908645, 'MSE - std': 182.72018214371485, 'R2 - mean': -2.2549266606458795, 'R2 - std': 2.4439232705030203} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 570.07129
Epoch 1: Val Loss 569.02417
Epoch 2: Val Loss 568.01660
Epoch 3: Val Loss 567.04553
Epoch 4: Val Loss 566.11310
Epoch 5: Val Loss 565.20923
Epoch 6: Val Loss 564.32300
Epoch 7: Val Loss 563.45184
Epoch 8: Val Loss 562.58008
Epoch 9: Val Loss 561.69775
Epoch 10: Val Loss 560.81476
Epoch 11: Val Loss 559.93439
Epoch 12: Val Loss 559.06860
Epoch 13: Val Loss 558.20056
Epoch 14: Val Loss 557.34717
Epoch 15: Val Loss 556.54089
Epoch 16: Val Loss 555.76782
Epoch 17: Val Loss 555.01282
Epoch 18: Val Loss 554.26709
Epoch 19: Val Loss 553.53217
Epoch 20: Val Loss 552.79565
Epoch 21: Val Loss 552.06866
Epoch 22: Val Loss 551.35315
Epoch 23: Val Loss 550.65924
Epoch 24: Val Loss 549.97101
Epoch 25: Val Loss 549.29364
Epoch 26: Val Loss 548.61871
Epoch 27: Val Loss 547.94000
Epoch 28: Val Loss 547.25574
Epoch 29: Val Loss 546.57849
Epoch 30: Val Loss 545.88885
Epoch 31: Val Loss 545.19043
Epoch 32: Val Loss 544.48157
Epoch 33: Val Loss 543.76483
Epoch 34: Val Loss 543.03723
Epoch 35: Val Loss 542.29742
Epoch 36: Val Loss 541.54700
Epoch 37: Val Loss 540.77454
Epoch 38: Val Loss 539.96552
Epoch 39: Val Loss 539.09271
Epoch 40: Val Loss 538.17859
Epoch 41: Val Loss 537.23596
Epoch 42: Val Loss 536.24744
Epoch 43: Val Loss 535.19025
Epoch 44: Val Loss 534.04871
Epoch 45: Val Loss 532.81226
Epoch 46: Val Loss 531.44678
Epoch 47: Val Loss 529.96667
Epoch 48: Val Loss 528.35138
Epoch 49: Val Loss 526.62347
Epoch 50: Val Loss 524.79578
Epoch 51: Val Loss 522.79626
Epoch 52: Val Loss 520.63831
Epoch 53: Val Loss 518.30994
Epoch 54: Val Loss 515.78986
Epoch 55: Val Loss 513.12177
Epoch 56: Val Loss 510.29416
Epoch 57: Val Loss 507.27182
Epoch 58: Val Loss 504.00351
Epoch 59: Val Loss 500.47430
Epoch 60: Val Loss 496.64774
Epoch 61: Val Loss 492.57614
Epoch 62: Val Loss 488.21426
Epoch 63: Val Loss 483.55933
Epoch 64: Val Loss 478.54242
Epoch 65: Val Loss 473.21210
Epoch 66: Val Loss 467.64850
Epoch 67: Val Loss 461.87051
Epoch 68: Val Loss 455.78363
Epoch 69: Val Loss 449.36633
Epoch 70: Val Loss 442.63876
Epoch 71: Val Loss 435.59604
Epoch 72: Val Loss 428.27304
Epoch 73: Val Loss 420.62360
Epoch 74: Val Loss 412.53922
Epoch 75: Val Loss 404.10806
Epoch 76: Val Loss 395.41232
Epoch 77: Val Loss 386.51855
Epoch 78: Val Loss 377.39948
Epoch 79: Val Loss 368.01987
Epoch 80: Val Loss 358.52359
Epoch 81: Val Loss 349.06073
Epoch 82: Val Loss 339.56616
Epoch 83: Val Loss 329.71902
Epoch 84: Val Loss 319.92316
Epoch 85: Val Loss 310.09378
Epoch 86: Val Loss 300.16290
Epoch 87: Val Loss 290.39017
Epoch 88: Val Loss 280.83121
Epoch 89: Val Loss 271.61877
Epoch 90: Val Loss 262.49240
Epoch 91: Val Loss 253.53070
Epoch 92: Val Loss 244.79695
Epoch 93: Val Loss 236.36757
Epoch 94: Val Loss 228.18802
Epoch 95: Val Loss 220.36502
Epoch 96: Val Loss 212.75856
Epoch 97: Val Loss 205.49037
Epoch 98: Val Loss 198.88884
Epoch 99: Val Loss 192.64023
{'MSE - mean': 247.74844253395108, 'MSE - std': 165.73641868654576, 'R2 - mean': -2.0260441620249496, 'R2 - std': 2.233328805703447} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 247.74844253395108 and parameters: {'dim': 32, 'depth': 6, 'heads': 8, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 573.65009
Epoch 1: Val Loss 573.64825
Epoch 2: Val Loss 573.64636
Epoch 3: Val Loss 573.64453
Epoch 4: Val Loss 573.64276
Epoch 5: Val Loss 573.64093
Epoch 6: Val Loss 573.63904
Epoch 7: Val Loss 573.63715
Epoch 8: Val Loss 573.63538
Epoch 9: Val Loss 573.63348
Epoch 10: Val Loss 573.63165
Epoch 11: Val Loss 573.62982
Epoch 12: Val Loss 573.62799
Epoch 13: Val Loss 573.62616
Epoch 14: Val Loss 573.62433
Epoch 15: Val Loss 573.62244
Epoch 16: Val Loss 573.62067
Epoch 17: Val Loss 573.61877
Epoch 18: Val Loss 573.61700
Epoch 19: Val Loss 573.61505
Epoch 20: Val Loss 573.61328
Epoch 21: Val Loss 573.61145
Epoch 22: Val Loss 573.60956
Epoch 23: Val Loss 573.60779
Epoch 24: Val Loss 573.60596
Epoch 25: Val Loss 573.60413
Epoch 26: Val Loss 573.60229
Epoch 27: Val Loss 573.60046
Epoch 28: Val Loss 573.59857
Epoch 29: Val Loss 573.59674
Epoch 30: Val Loss 573.59485
Epoch 31: Val Loss 573.59308
Epoch 32: Val Loss 573.59125
Epoch 33: Val Loss 573.58942
Epoch 34: Val Loss 573.58752
Epoch 35: Val Loss 573.58575
Epoch 36: Val Loss 573.58386
Epoch 37: Val Loss 573.58203
Epoch 38: Val Loss 573.58014
Epoch 39: Val Loss 573.57831
Epoch 40: Val Loss 573.57648
Epoch 41: Val Loss 573.57471
Epoch 42: Val Loss 573.57275
Epoch 43: Val Loss 573.57104
Epoch 44: Val Loss 573.56921
Epoch 45: Val Loss 573.56732
Epoch 46: Val Loss 573.56561
Epoch 47: Val Loss 573.56372
Epoch 48: Val Loss 573.56195
Epoch 49: Val Loss 573.56006
Epoch 50: Val Loss 573.55823
Epoch 51: Val Loss 573.55640
Epoch 52: Val Loss 573.55450
Epoch 53: Val Loss 573.55261
Epoch 54: Val Loss 573.55084
Epoch 55: Val Loss 573.54895
Epoch 56: Val Loss 573.54712
Epoch 57: Val Loss 573.54535
Epoch 58: Val Loss 573.54346
Epoch 59: Val Loss 573.54163
Epoch 60: Val Loss 573.53979
Epoch 61: Val Loss 573.53796
Epoch 62: Val Loss 573.53613
Epoch 63: Val Loss 573.53436
Epoch 64: Val Loss 573.53247
Epoch 65: Val Loss 573.53064
Epoch 66: Val Loss 573.52881
Epoch 67: Val Loss 573.52698
Epoch 68: Val Loss 573.52521
Epoch 69: Val Loss 573.52338
Epoch 70: Val Loss 573.52155
Epoch 71: Val Loss 573.51971
Epoch 72: Val Loss 573.51794
Epoch 73: Val Loss 573.51611
Epoch 74: Val Loss 573.51428
Epoch 75: Val Loss 573.51245
Epoch 76: Val Loss 573.51068
Epoch 77: Val Loss 573.50891
Epoch 78: Val Loss 573.50708
Epoch 79: Val Loss 573.50525
Epoch 80: Val Loss 573.50342
Epoch 81: Val Loss 573.50165
Epoch 82: Val Loss 573.49982
Epoch 83: Val Loss 573.49792
Epoch 84: Val Loss 573.49615
Epoch 85: Val Loss 573.49426
Epoch 86: Val Loss 573.49249
Epoch 87: Val Loss 573.49066
Epoch 88: Val Loss 573.48883
Epoch 89: Val Loss 573.48706
Epoch 90: Val Loss 573.48523
Epoch 91: Val Loss 573.48346
Epoch 92: Val Loss 573.48151
Epoch 93: Val Loss 573.47968
Epoch 94: Val Loss 573.47778
Epoch 95: Val Loss 573.47607
Epoch 96: Val Loss 573.47412
Epoch 97: Val Loss 573.47235
Epoch 98: Val Loss 573.47052
Epoch 99: Val Loss 573.46875
{'MSE - mean': 573.4687026531542, 'MSE - std': 0.0, 'R2 - mean': -6.68021782152785, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 604.14032
Epoch 1: Val Loss 604.13165
Epoch 2: Val Loss 604.12305
Epoch 3: Val Loss 604.11444
Epoch 4: Val Loss 604.10590
Epoch 5: Val Loss 604.09729
Epoch 6: Val Loss 604.08881
Epoch 7: Val Loss 604.08020
Epoch 8: Val Loss 604.07172
Epoch 9: Val Loss 604.06342
Epoch 10: Val Loss 604.05505
Epoch 11: Val Loss 604.04651
Epoch 12: Val Loss 604.03802
Epoch 13: Val Loss 604.02954
Epoch 14: Val Loss 604.02100
Epoch 15: Val Loss 604.01227
Epoch 16: Val Loss 604.00360
Epoch 17: Val Loss 603.99506
Epoch 18: Val Loss 603.98651
Epoch 19: Val Loss 603.97809
Epoch 20: Val Loss 603.96967
Epoch 21: Val Loss 603.96106
Epoch 22: Val Loss 603.95239
Epoch 23: Val Loss 603.94391
Epoch 24: Val Loss 603.93555
Epoch 25: Val Loss 603.92719
Epoch 26: Val Loss 603.91864
Epoch 27: Val Loss 603.91010
Epoch 28: Val Loss 603.90155
Epoch 29: Val Loss 603.89307
Epoch 30: Val Loss 603.88458
Epoch 31: Val Loss 603.87610
Epoch 32: Val Loss 603.86761
Epoch 33: Val Loss 603.85901
Epoch 34: Val Loss 603.85052
Epoch 35: Val Loss 603.84204
Epoch 36: Val Loss 603.83362
Epoch 37: Val Loss 603.82526
Epoch 38: Val Loss 603.81683
Epoch 39: Val Loss 603.80823
Epoch 40: Val Loss 603.79980
Epoch 41: Val Loss 603.79138
Epoch 42: Val Loss 603.78290
Epoch 43: Val Loss 603.77441
Epoch 44: Val Loss 603.76587
Epoch 45: Val Loss 603.75720
Epoch 46: Val Loss 603.74860
Epoch 47: Val Loss 603.73987
Epoch 48: Val Loss 603.73126
Epoch 49: Val Loss 603.72260
Epoch 50: Val Loss 603.71399
Epoch 51: Val Loss 603.70544
Epoch 52: Val Loss 603.69708
Epoch 53: Val Loss 603.68860
Epoch 54: Val Loss 603.68011
Epoch 55: Val Loss 603.67157
Epoch 56: Val Loss 603.66315
Epoch 57: Val Loss 603.65472
Epoch 58: Val Loss 603.64648
Epoch 59: Val Loss 603.63806
Epoch 60: Val Loss 603.62976
Epoch 61: Val Loss 603.62146
Epoch 62: Val Loss 603.61310
Epoch 63: Val Loss 603.60474
Epoch 64: Val Loss 603.59631
Epoch 65: Val Loss 603.58795
Epoch 66: Val Loss 603.57941
Epoch 67: Val Loss 603.57092
Epoch 68: Val Loss 603.56244
Epoch 69: Val Loss 603.55414
Epoch 70: Val Loss 603.54572
Epoch 71: Val Loss 603.53741
Epoch 72: Val Loss 603.52905
Epoch 73: Val Loss 603.52063
Epoch 74: Val Loss 603.51227
Epoch 75: Val Loss 603.50391
Epoch 76: Val Loss 603.49530
Epoch 77: Val Loss 603.48663
Epoch 78: Val Loss 603.47791
Epoch 79: Val Loss 603.46930
Epoch 80: Val Loss 603.46069
Epoch 81: Val Loss 603.45203
Epoch 82: Val Loss 603.44336
Epoch 83: Val Loss 603.43457
Epoch 84: Val Loss 603.42578
Epoch 85: Val Loss 603.41693
Epoch 86: Val Loss 603.40814
Epoch 87: Val Loss 603.39954
Epoch 88: Val Loss 603.39081
Epoch 89: Val Loss 603.38196
Epoch 90: Val Loss 603.37335
Epoch 91: Val Loss 603.36469
Epoch 92: Val Loss 603.35602
Epoch 93: Val Loss 603.34729
Epoch 94: Val Loss 603.33875
Epoch 95: Val Loss 603.33026
Epoch 96: Val Loss 603.32159
Epoch 97: Val Loss 603.31305
Epoch 98: Val Loss 603.30444
Epoch 99: Val Loss 603.29572
{'MSE - mean': 588.3822217778186, 'MSE - std': 14.91351912466439, 'R2 - mean': -6.585420745671506, 'R2 - std': 0.09479707585634412} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 706.37946
Epoch 1: Val Loss 706.37524
Epoch 2: Val Loss 706.37097
Epoch 3: Val Loss 706.36676
Epoch 4: Val Loss 706.36261
Epoch 5: Val Loss 706.35852
Epoch 6: Val Loss 706.35431
Epoch 7: Val Loss 706.35016
Epoch 8: Val Loss 706.34607
Epoch 9: Val Loss 706.34186
Epoch 10: Val Loss 706.33771
Epoch 11: Val Loss 706.33362
Epoch 12: Val Loss 706.32941
Epoch 13: Val Loss 706.32532
Epoch 14: Val Loss 706.32104
Epoch 15: Val Loss 706.31696
Epoch 16: Val Loss 706.31281
Epoch 17: Val Loss 706.30872
Epoch 18: Val Loss 706.30450
Epoch 19: Val Loss 706.30029
Epoch 20: Val Loss 706.29608
Epoch 21: Val Loss 706.29193
Epoch 22: Val Loss 706.28772
Epoch 23: Val Loss 706.28363
Epoch 24: Val Loss 706.27948
Epoch 25: Val Loss 706.27533
Epoch 26: Val Loss 706.27118
Epoch 27: Val Loss 706.26703
Epoch 28: Val Loss 706.26300
Epoch 29: Val Loss 706.25879
Epoch 30: Val Loss 706.25470
Epoch 31: Val Loss 706.25061
Epoch 32: Val Loss 706.24652
Epoch 33: Val Loss 706.24243
Epoch 34: Val Loss 706.23840
Epoch 35: Val Loss 706.23431
Epoch 36: Val Loss 706.23016
Epoch 37: Val Loss 706.22601
Epoch 38: Val Loss 706.22192
Epoch 39: Val Loss 706.21783
Epoch 40: Val Loss 706.21362
Epoch 41: Val Loss 706.20959
Epoch 42: Val Loss 706.20557
Epoch 43: Val Loss 706.20142
Epoch 44: Val Loss 706.19745
Epoch 45: Val Loss 706.19336
Epoch 46: Val Loss 706.18933
Epoch 47: Val Loss 706.18524
Epoch 48: Val Loss 706.18115
Epoch 49: Val Loss 706.17712
Epoch 50: Val Loss 706.17310
Epoch 51: Val Loss 706.16901
Epoch 52: Val Loss 706.16492
Epoch 53: Val Loss 706.16083
Epoch 54: Val Loss 706.15668
Epoch 55: Val Loss 706.15259
Epoch 56: Val Loss 706.14850
Epoch 57: Val Loss 706.14441
Epoch 58: Val Loss 706.14032
Epoch 59: Val Loss 706.13623
Epoch 60: Val Loss 706.13208
Epoch 61: Val Loss 706.12793
Epoch 62: Val Loss 706.12384
Epoch 63: Val Loss 706.11975
Epoch 64: Val Loss 706.11560
Epoch 65: Val Loss 706.11151
Epoch 66: Val Loss 706.10736
Epoch 67: Val Loss 706.10333
Epoch 68: Val Loss 706.09924
Epoch 69: Val Loss 706.09515
Epoch 70: Val Loss 706.09100
Epoch 71: Val Loss 706.08691
Epoch 72: Val Loss 706.08276
Epoch 73: Val Loss 706.07867
Epoch 74: Val Loss 706.07458
Epoch 75: Val Loss 706.07043
Epoch 76: Val Loss 706.06622
Epoch 77: Val Loss 706.06207
Epoch 78: Val Loss 706.05792
Epoch 79: Val Loss 706.05365
Epoch 80: Val Loss 706.04950
Epoch 81: Val Loss 706.04529
Epoch 82: Val Loss 706.04114
Epoch 83: Val Loss 706.03687
Epoch 84: Val Loss 706.03265
Epoch 85: Val Loss 706.02844
Epoch 86: Val Loss 706.02417
Epoch 87: Val Loss 706.02002
Epoch 88: Val Loss 706.01593
Epoch 89: Val Loss 706.01166
Epoch 90: Val Loss 706.00757
Epoch 91: Val Loss 706.00330
Epoch 92: Val Loss 705.99915
Epoch 93: Val Loss 705.99493
Epoch 94: Val Loss 705.99072
Epoch 95: Val Loss 705.98651
Epoch 96: Val Loss 705.98242
Epoch 97: Val Loss 705.97833
Epoch 98: Val Loss 705.97418
Epoch 99: Val Loss 705.96997
{'MSE - mean': 627.5781371188888, 'MSE - std': 56.753105001428615, 'R2 - mean': -6.522776322823891, 'R2 - std': 0.11764199004506833} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 561.27850
Epoch 1: Val Loss 561.27429
Epoch 2: Val Loss 561.27008
Epoch 3: Val Loss 561.26575
Epoch 4: Val Loss 561.26154
Epoch 5: Val Loss 561.25708
Epoch 6: Val Loss 561.25269
Epoch 7: Val Loss 561.24829
Epoch 8: Val Loss 561.24377
Epoch 9: Val Loss 561.23938
Epoch 10: Val Loss 561.23492
Epoch 11: Val Loss 561.23041
Epoch 12: Val Loss 561.22595
Epoch 13: Val Loss 561.22137
Epoch 14: Val Loss 561.21680
Epoch 15: Val Loss 561.21222
Epoch 16: Val Loss 561.20776
Epoch 17: Val Loss 561.20325
Epoch 18: Val Loss 561.19891
Epoch 19: Val Loss 561.19458
Epoch 20: Val Loss 561.19019
Epoch 21: Val Loss 561.18585
Epoch 22: Val Loss 561.18146
Epoch 23: Val Loss 561.17706
Epoch 24: Val Loss 561.17255
Epoch 25: Val Loss 561.16803
Epoch 26: Val Loss 561.16345
Epoch 27: Val Loss 561.15900
Epoch 28: Val Loss 561.15448
Epoch 29: Val Loss 561.14996
Epoch 30: Val Loss 561.14551
Epoch 31: Val Loss 561.14099
Epoch 32: Val Loss 561.13641
Epoch 33: Val Loss 561.13184
Epoch 34: Val Loss 561.12726
Epoch 35: Val Loss 561.12268
Epoch 36: Val Loss 561.11798
Epoch 37: Val Loss 561.11322
Epoch 38: Val Loss 561.10846
Epoch 39: Val Loss 561.10352
Epoch 40: Val Loss 561.09869
Epoch 41: Val Loss 561.09369
Epoch 42: Val Loss 561.08862
Epoch 43: Val Loss 561.08362
Epoch 44: Val Loss 561.07867
Epoch 45: Val Loss 561.07373
Epoch 46: Val Loss 561.06879
Epoch 47: Val Loss 561.06378
Epoch 48: Val Loss 561.05878
Epoch 49: Val Loss 561.05377
Epoch 50: Val Loss 561.04889
Epoch 51: Val Loss 561.04382
Epoch 52: Val Loss 561.03882
Epoch 53: Val Loss 561.03381
Epoch 54: Val Loss 561.02887
Epoch 55: Val Loss 561.02386
Epoch 56: Val Loss 561.01886
Epoch 57: Val Loss 561.01373
Epoch 58: Val Loss 561.00867
Epoch 59: Val Loss 561.00360
Epoch 60: Val Loss 560.99860
Epoch 61: Val Loss 560.99347
Epoch 62: Val Loss 560.98840
Epoch 63: Val Loss 560.98322
Epoch 64: Val Loss 560.97815
Epoch 65: Val Loss 560.97302
Epoch 66: Val Loss 560.96783
Epoch 67: Val Loss 560.96265
Epoch 68: Val Loss 560.95752
Epoch 69: Val Loss 560.95251
Epoch 70: Val Loss 560.94739
Epoch 71: Val Loss 560.94214
Epoch 72: Val Loss 560.93683
Epoch 73: Val Loss 560.93164
Epoch 74: Val Loss 560.92657
Epoch 75: Val Loss 560.92126
Epoch 76: Val Loss 560.91602
Epoch 77: Val Loss 560.91083
Epoch 78: Val Loss 560.90558
Epoch 79: Val Loss 560.90027
Epoch 80: Val Loss 560.89490
Epoch 81: Val Loss 560.88947
Epoch 82: Val Loss 560.88422
Epoch 83: Val Loss 560.87891
Epoch 84: Val Loss 560.87354
Epoch 85: Val Loss 560.86810
Epoch 86: Val Loss 560.86279
Epoch 87: Val Loss 560.85742
Epoch 88: Val Loss 560.85211
Epoch 89: Val Loss 560.84662
Epoch 90: Val Loss 560.84125
Epoch 91: Val Loss 560.83575
Epoch 92: Val Loss 560.83038
Epoch 93: Val Loss 560.82495
Epoch 94: Val Loss 560.81946
Epoch 95: Val Loss 560.81403
Epoch 96: Val Loss 560.80859
Epoch 97: Val Loss 560.80304
Epoch 98: Val Loss 560.79755
Epoch 99: Val Loss 560.79193
{'MSE - mean': 610.8815910393344, 'MSE - std': 57.0263986977737, 'R2 - mean': -6.500102856295765, 'R2 - std': 0.10918785014610886} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 595.80695
Epoch 1: Val Loss 595.79944
Epoch 2: Val Loss 595.79199
Epoch 3: Val Loss 595.78455
Epoch 4: Val Loss 595.77704
Epoch 5: Val Loss 595.76959
Epoch 6: Val Loss 595.76215
Epoch 7: Val Loss 595.75470
Epoch 8: Val Loss 595.74725
Epoch 9: Val Loss 595.73987
Epoch 10: Val Loss 595.73242
Epoch 11: Val Loss 595.72510
Epoch 12: Val Loss 595.71753
Epoch 13: Val Loss 595.71008
Epoch 14: Val Loss 595.70258
Epoch 15: Val Loss 595.69501
Epoch 16: Val Loss 595.68750
Epoch 17: Val Loss 595.67999
Epoch 18: Val Loss 595.67249
Epoch 19: Val Loss 595.66504
Epoch 20: Val Loss 595.65765
Epoch 21: Val Loss 595.65021
Epoch 22: Val Loss 595.64270
Epoch 23: Val Loss 595.63531
Epoch 24: Val Loss 595.62787
Epoch 25: Val Loss 595.62048
Epoch 26: Val Loss 595.61310
Epoch 27: Val Loss 595.60571
Epoch 28: Val Loss 595.59833
Epoch 29: Val Loss 595.59082
Epoch 30: Val Loss 595.58344
Epoch 31: Val Loss 595.57593
Epoch 32: Val Loss 595.56860
Epoch 33: Val Loss 595.56134
Epoch 34: Val Loss 595.55396
Epoch 35: Val Loss 595.54657
Epoch 36: Val Loss 595.53918
Epoch 37: Val Loss 595.53180
Epoch 38: Val Loss 595.52441
Epoch 39: Val Loss 595.51709
Epoch 40: Val Loss 595.50970
Epoch 41: Val Loss 595.50232
Epoch 42: Val Loss 595.49493
Epoch 43: Val Loss 595.48755
Epoch 44: Val Loss 595.48016
Epoch 45: Val Loss 595.47284
Epoch 46: Val Loss 595.46539
Epoch 47: Val Loss 595.45795
Epoch 48: Val Loss 595.45050
Epoch 49: Val Loss 595.44305
Epoch 50: Val Loss 595.43555
Epoch 51: Val Loss 595.42810
Epoch 52: Val Loss 595.42072
Epoch 53: Val Loss 595.41333
Epoch 54: Val Loss 595.40594
Epoch 55: Val Loss 595.39856
Epoch 56: Val Loss 595.39130
Epoch 57: Val Loss 595.38397
Epoch 58: Val Loss 595.37659
Epoch 59: Val Loss 595.36920
Epoch 60: Val Loss 595.36182
Epoch 61: Val Loss 595.35443
Epoch 62: Val Loss 595.34717
Epoch 63: Val Loss 595.33978
Epoch 64: Val Loss 595.33246
Epoch 65: Val Loss 595.32507
Epoch 66: Val Loss 595.31775
Epoch 67: Val Loss 595.31042
Epoch 68: Val Loss 595.30304
Epoch 69: Val Loss 595.29584
Epoch 70: Val Loss 595.28857
Epoch 71: Val Loss 595.28131
Epoch 72: Val Loss 595.27393
Epoch 73: Val Loss 595.26660
Epoch 74: Val Loss 595.25928
Epoch 75: Val Loss 595.25195
Epoch 76: Val Loss 595.24463
Epoch 77: Val Loss 595.23724
Epoch 78: Val Loss 595.22986
Epoch 79: Val Loss 595.22247
Epoch 80: Val Loss 595.21509
Epoch 81: Val Loss 595.20776
Epoch 82: Val Loss 595.20032
Epoch 83: Val Loss 595.19299
Epoch 84: Val Loss 595.18567
Epoch 85: Val Loss 595.17828
Epoch 86: Val Loss 595.17084
Epoch 87: Val Loss 595.16357
Epoch 88: Val Loss 595.15625
Epoch 89: Val Loss 595.14899
Epoch 90: Val Loss 595.14178
Epoch 91: Val Loss 595.13452
Epoch 92: Val Loss 595.12708
Epoch 93: Val Loss 595.11975
Epoch 94: Val Loss 595.11249
Epoch 95: Val Loss 595.10510
Epoch 96: Val Loss 595.09790
Epoch 97: Val Loss 595.09070
Epoch 98: Val Loss 595.08337
Epoch 99: Val Loss 595.07605
{'MSE - mean': 607.7204869313802, 'MSE - std': 51.39628814880757, 'R2 - mean': -6.303980661061926, 'R2 - std': 0.40421931070766903} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 607.7204869313802 and parameters: {'dim': 32, 'depth': 6, 'heads': 8, 'weight_decay': -2, 'learning_rate': -5, 'dropout': 0.4}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 540.92664
Epoch 1: Val Loss 540.92639
Epoch 2: Val Loss 540.92621
Epoch 3: Val Loss 540.92590
Epoch 4: Val Loss 540.92566
Epoch 5: Val Loss 540.92542
Epoch 6: Val Loss 540.92517
Epoch 7: Val Loss 540.92487
Epoch 8: Val Loss 540.92462
Epoch 9: Val Loss 540.92438
Epoch 10: Val Loss 540.92413
Epoch 11: Val Loss 540.92389
Epoch 12: Val Loss 540.92365
Epoch 13: Val Loss 540.92334
Epoch 14: Val Loss 540.92310
Epoch 15: Val Loss 540.92291
Epoch 16: Val Loss 540.92267
Epoch 17: Val Loss 540.92236
Epoch 18: Val Loss 540.92212
Epoch 19: Val Loss 540.92188
Epoch 20: Val Loss 540.92157
Epoch 21: Val Loss 540.92139
Epoch 22: Val Loss 540.92114
Epoch 23: Val Loss 540.92084
Epoch 24: Val Loss 540.92059
Epoch 25: Val Loss 540.92035
Epoch 26: Val Loss 540.92010
Epoch 27: Val Loss 540.91992
Epoch 28: Val Loss 540.91962
Epoch 29: Val Loss 540.91937
Epoch 30: Val Loss 540.91913
Epoch 31: Val Loss 540.91882
Epoch 32: Val Loss 540.91858
Epoch 33: Val Loss 540.91833
Epoch 34: Val Loss 540.91809
Epoch 35: Val Loss 540.91779
Epoch 36: Val Loss 540.91760
Epoch 37: Val Loss 540.91736
Epoch 38: Val Loss 540.91705
Epoch 39: Val Loss 540.91681
Epoch 40: Val Loss 540.91650
Epoch 41: Val Loss 540.91632
Epoch 42: Val Loss 540.91608
Epoch 43: Val Loss 540.91583
Epoch 44: Val Loss 540.91559
Epoch 45: Val Loss 540.91528
Epoch 46: Val Loss 540.91510
Epoch 47: Val Loss 540.91479
Epoch 48: Val Loss 540.91455
Epoch 49: Val Loss 540.91431
Epoch 50: Val Loss 540.91406
Epoch 51: Val Loss 540.91382
Epoch 52: Val Loss 540.91357
Epoch 53: Val Loss 540.91327
Epoch 54: Val Loss 540.91309
Epoch 55: Val Loss 540.91278
Epoch 56: Val Loss 540.91254
Epoch 57: Val Loss 540.91223
Epoch 58: Val Loss 540.91199
Epoch 59: Val Loss 540.91180
Epoch 60: Val Loss 540.91150
Epoch 61: Val Loss 540.91132
Epoch 62: Val Loss 540.91101
Epoch 63: Val Loss 540.91077
Epoch 64: Val Loss 540.91058
Epoch 65: Val Loss 540.91028
Epoch 66: Val Loss 540.91003
Epoch 67: Val Loss 540.90979
Epoch 68: Val Loss 540.90955
Epoch 69: Val Loss 540.90930
Epoch 70: Val Loss 540.90900
Epoch 71: Val Loss 540.90875
Epoch 72: Val Loss 540.90857
Epoch 73: Val Loss 540.90826
Epoch 74: Val Loss 540.90802
Epoch 75: Val Loss 540.90778
Epoch 76: Val Loss 540.90747
Epoch 77: Val Loss 540.90729
Epoch 78: Val Loss 540.90704
Epoch 79: Val Loss 540.90674
Epoch 80: Val Loss 540.90649
Epoch 81: Val Loss 540.90625
Epoch 82: Val Loss 540.90601
Epoch 83: Val Loss 540.90576
Epoch 84: Val Loss 540.90552
Epoch 85: Val Loss 540.90521
Epoch 86: Val Loss 540.90497
Epoch 87: Val Loss 540.90472
Epoch 88: Val Loss 540.90448
Epoch 89: Val Loss 540.90417
Epoch 90: Val Loss 540.90399
Epoch 91: Val Loss 540.90369
Epoch 92: Val Loss 540.90350
Epoch 93: Val Loss 540.90320
Epoch 94: Val Loss 540.90295
Epoch 95: Val Loss 540.90277
Epoch 96: Val Loss 540.90253
Epoch 97: Val Loss 540.90222
Epoch 98: Val Loss 540.90192
Epoch 99: Val Loss 540.90173
{'MSE - mean': 540.9016933236138, 'MSE - std': 0.0, 'R2 - mean': -6.244061978515997, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 654.22247
Epoch 1: Val Loss 654.22168
Epoch 2: Val Loss 654.22089
Epoch 3: Val Loss 654.22015
Epoch 4: Val Loss 654.21942
Epoch 5: Val Loss 654.21875
Epoch 6: Val Loss 654.21796
Epoch 7: Val Loss 654.21716
Epoch 8: Val Loss 654.21649
Epoch 9: Val Loss 654.21570
Epoch 10: Val Loss 654.21503
Epoch 11: Val Loss 654.21423
Epoch 12: Val Loss 654.21350
Epoch 13: Val Loss 654.21289
Epoch 14: Val Loss 654.21204
Epoch 15: Val Loss 654.21130
Epoch 16: Val Loss 654.21051
Epoch 17: Val Loss 654.20984
Epoch 18: Val Loss 654.20905
Epoch 19: Val Loss 654.20837
Epoch 20: Val Loss 654.20758
Epoch 21: Val Loss 654.20691
Epoch 22: Val Loss 654.20624
Epoch 23: Val Loss 654.20544
Epoch 24: Val Loss 654.20465
Epoch 25: Val Loss 654.20398
Epoch 26: Val Loss 654.20325
Epoch 27: Val Loss 654.20258
Epoch 28: Val Loss 654.20172
Epoch 29: Val Loss 654.20105
Epoch 30: Val Loss 654.20032
Epoch 31: Val Loss 654.19958
Epoch 32: Val Loss 654.19885
Epoch 33: Val Loss 654.19818
Epoch 34: Val Loss 654.19739
Epoch 35: Val Loss 654.19659
Epoch 36: Val Loss 654.19592
Epoch 37: Val Loss 654.19525
Epoch 38: Val Loss 654.19446
Epoch 39: Val Loss 654.19373
Epoch 40: Val Loss 654.19299
Epoch 41: Val Loss 654.19226
Epoch 42: Val Loss 654.19153
Epoch 43: Val Loss 654.19073
Epoch 44: Val Loss 654.18994
Epoch 45: Val Loss 654.18927
Epoch 46: Val Loss 654.18848
Epoch 47: Val Loss 654.18781
Epoch 48: Val Loss 654.18701
Epoch 49: Val Loss 654.18634
Epoch 50: Val Loss 654.18561
Epoch 51: Val Loss 654.18488
Epoch 52: Val Loss 654.18408
Epoch 53: Val Loss 654.18329
Epoch 54: Val Loss 654.18262
Epoch 55: Val Loss 654.18195
Epoch 56: Val Loss 654.18115
Epoch 57: Val Loss 654.18036
Epoch 58: Val Loss 654.17975
Epoch 59: Val Loss 654.17896
Epoch 60: Val Loss 654.17822
Epoch 61: Val Loss 654.17749
Epoch 62: Val Loss 654.17682
Epoch 63: Val Loss 654.17603
Epoch 64: Val Loss 654.17529
Epoch 65: Val Loss 654.17456
Epoch 66: Val Loss 654.17389
Epoch 67: Val Loss 654.17310
Epoch 68: Val Loss 654.17242
Epoch 69: Val Loss 654.17169
Epoch 70: Val Loss 654.17096
Epoch 71: Val Loss 654.17017
Epoch 72: Val Loss 654.16949
Epoch 73: Val Loss 654.16876
Epoch 74: Val Loss 654.16809
Epoch 75: Val Loss 654.16736
Epoch 76: Val Loss 654.16663
Epoch 77: Val Loss 654.16589
Epoch 78: Val Loss 654.16522
Epoch 79: Val Loss 654.16443
Epoch 80: Val Loss 654.16364
Epoch 81: Val Loss 654.16296
Epoch 82: Val Loss 654.16229
Epoch 83: Val Loss 654.16156
Epoch 84: Val Loss 654.16083
Epoch 85: Val Loss 654.16010
Epoch 86: Val Loss 654.15936
Epoch 87: Val Loss 654.15863
Epoch 88: Val Loss 654.15796
Epoch 89: Val Loss 654.15717
Epoch 90: Val Loss 654.15637
Epoch 91: Val Loss 654.15570
Epoch 92: Val Loss 654.15491
Epoch 93: Val Loss 654.15424
Epoch 94: Val Loss 654.15344
Epoch 95: Val Loss 654.15271
Epoch 96: Val Loss 654.15204
Epoch 97: Val Loss 654.15131
Epoch 98: Val Loss 654.15057
Epoch 99: Val Loss 654.14990
{'MSE - mean': 597.5257861842067, 'MSE - std': 56.624092860592896, 'R2 - mean': -6.6830496865305005, 'R2 - std': 0.43898770801450304} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 695.72046
Epoch 1: Val Loss 695.72015
Epoch 2: Val Loss 695.71967
Epoch 3: Val Loss 695.71936
Epoch 4: Val Loss 695.71899
Epoch 5: Val Loss 695.71857
Epoch 6: Val Loss 695.71814
Epoch 7: Val Loss 695.71783
Epoch 8: Val Loss 695.71735
Epoch 9: Val Loss 695.71704
Epoch 10: Val Loss 695.71667
Epoch 11: Val Loss 695.71625
Epoch 12: Val Loss 695.71588
Epoch 13: Val Loss 695.71552
Epoch 14: Val Loss 695.71509
Epoch 15: Val Loss 695.71478
Epoch 16: Val Loss 695.71442
Epoch 17: Val Loss 695.71405
Epoch 18: Val Loss 695.71362
Epoch 19: Val Loss 695.71326
Epoch 20: Val Loss 695.71289
Epoch 21: Val Loss 695.71246
Epoch 22: Val Loss 695.71210
Epoch 23: Val Loss 695.71167
Epoch 24: Val Loss 695.71130
Epoch 25: Val Loss 695.71088
Epoch 26: Val Loss 695.71051
Epoch 27: Val Loss 695.71014
Epoch 28: Val Loss 695.70978
Epoch 29: Val Loss 695.70947
Epoch 30: Val Loss 695.70898
Epoch 31: Val Loss 695.70868
Epoch 32: Val Loss 695.70819
Epoch 33: Val Loss 695.70789
Epoch 34: Val Loss 695.70746
Epoch 35: Val Loss 695.70715
Epoch 36: Val Loss 695.70667
Epoch 37: Val Loss 695.70636
Epoch 38: Val Loss 695.70587
Epoch 39: Val Loss 695.70557
Epoch 40: Val Loss 695.70514
Epoch 41: Val Loss 695.70471
Epoch 42: Val Loss 695.70435
Epoch 43: Val Loss 695.70398
Epoch 44: Val Loss 695.70355
Epoch 45: Val Loss 695.70319
Epoch 46: Val Loss 695.70282
Epoch 47: Val Loss 695.70239
Epoch 48: Val Loss 695.70203
Epoch 49: Val Loss 695.70172
Epoch 50: Val Loss 695.70142
Epoch 51: Val Loss 695.70093
Epoch 52: Val Loss 695.70050
Epoch 53: Val Loss 695.70020
Epoch 54: Val Loss 695.69977
Epoch 55: Val Loss 695.69940
Epoch 56: Val Loss 695.69910
Epoch 57: Val Loss 695.69861
Epoch 58: Val Loss 695.69830
Epoch 59: Val Loss 695.69788
Epoch 60: Val Loss 695.69757
Epoch 61: Val Loss 695.69714
Epoch 62: Val Loss 695.69678
Epoch 63: Val Loss 695.69641
Epoch 64: Val Loss 695.69598
Epoch 65: Val Loss 695.69562
Epoch 66: Val Loss 695.69525
Epoch 67: Val Loss 695.69482
Epoch 68: Val Loss 695.69446
Epoch 69: Val Loss 695.69409
Epoch 70: Val Loss 695.69366
Epoch 71: Val Loss 695.69330
Epoch 72: Val Loss 695.69293
Epoch 73: Val Loss 695.69250
Epoch 74: Val Loss 695.69214
Epoch 75: Val Loss 695.69177
Epoch 76: Val Loss 695.69141
Epoch 77: Val Loss 695.69104
Epoch 78: Val Loss 695.69061
Epoch 79: Val Loss 695.69019
Epoch 80: Val Loss 695.68988
Epoch 81: Val Loss 695.68951
Epoch 82: Val Loss 695.68921
Epoch 83: Val Loss 695.68872
Epoch 84: Val Loss 695.68835
Epoch 85: Val Loss 695.68793
Epoch 86: Val Loss 695.68756
Epoch 87: Val Loss 695.68719
Epoch 88: Val Loss 695.68677
Epoch 89: Val Loss 695.68640
Epoch 90: Val Loss 695.68610
Epoch 91: Val Loss 695.68561
Epoch 92: Val Loss 695.68530
Epoch 93: Val Loss 695.68488
Epoch 94: Val Loss 695.68457
Epoch 95: Val Loss 695.68414
Epoch 96: Val Loss 695.68378
Epoch 97: Val Loss 695.68341
Epoch 98: Val Loss 695.68298
Epoch 99: Val Loss 695.68262
{'MSE - mean': 630.244727558127, 'MSE - std': 65.41088220459525, 'R2 - mean': -6.55193034969145, 'R2 - std': 0.4035567281619477} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 562.89734
Epoch 1: Val Loss 562.89667
Epoch 2: Val Loss 562.89594
Epoch 3: Val Loss 562.89520
Epoch 4: Val Loss 562.89453
Epoch 5: Val Loss 562.89380
Epoch 6: Val Loss 562.89307
Epoch 7: Val Loss 562.89240
Epoch 8: Val Loss 562.89172
Epoch 9: Val Loss 562.89093
Epoch 10: Val Loss 562.89026
Epoch 11: Val Loss 562.88947
Epoch 12: Val Loss 562.88885
Epoch 13: Val Loss 562.88812
Epoch 14: Val Loss 562.88745
Epoch 15: Val Loss 562.88672
Epoch 16: Val Loss 562.88599
Epoch 17: Val Loss 562.88525
Epoch 18: Val Loss 562.88458
Epoch 19: Val Loss 562.88385
Epoch 20: Val Loss 562.88318
Epoch 21: Val Loss 562.88245
Epoch 22: Val Loss 562.88171
Epoch 23: Val Loss 562.88104
Epoch 24: Val Loss 562.88031
Epoch 25: Val Loss 562.87964
Epoch 26: Val Loss 562.87891
Epoch 27: Val Loss 562.87817
Epoch 28: Val Loss 562.87756
Epoch 29: Val Loss 562.87683
Epoch 30: Val Loss 562.87610
Epoch 31: Val Loss 562.87537
Epoch 32: Val Loss 562.87463
Epoch 33: Val Loss 562.87390
Epoch 34: Val Loss 562.87323
Epoch 35: Val Loss 562.87250
Epoch 36: Val Loss 562.87183
Epoch 37: Val Loss 562.87109
Epoch 38: Val Loss 562.87042
Epoch 39: Val Loss 562.86963
Epoch 40: Val Loss 562.86896
Epoch 41: Val Loss 562.86829
Epoch 42: Val Loss 562.86755
Epoch 43: Val Loss 562.86688
Epoch 44: Val Loss 562.86615
Epoch 45: Val Loss 562.86542
Epoch 46: Val Loss 562.86475
Epoch 47: Val Loss 562.86401
Epoch 48: Val Loss 562.86328
Epoch 49: Val Loss 562.86261
Epoch 50: Val Loss 562.86194
Epoch 51: Val Loss 562.86121
Epoch 52: Val Loss 562.86047
Epoch 53: Val Loss 562.85980
Epoch 54: Val Loss 562.85907
Epoch 55: Val Loss 562.85834
Epoch 56: Val Loss 562.85767
Epoch 57: Val Loss 562.85699
Epoch 58: Val Loss 562.85626
Epoch 59: Val Loss 562.85553
Epoch 60: Val Loss 562.85486
Epoch 61: Val Loss 562.85413
Epoch 62: Val Loss 562.85339
Epoch 63: Val Loss 562.85272
Epoch 64: Val Loss 562.85199
Epoch 65: Val Loss 562.85126
Epoch 66: Val Loss 562.85052
Epoch 67: Val Loss 562.84985
Epoch 68: Val Loss 562.84912
Epoch 69: Val Loss 562.84845
Epoch 70: Val Loss 562.84766
Epoch 71: Val Loss 562.84692
Epoch 72: Val Loss 562.84619
Epoch 73: Val Loss 562.84552
Epoch 74: Val Loss 562.84485
Epoch 75: Val Loss 562.84406
Epoch 76: Val Loss 562.84338
Epoch 77: Val Loss 562.84265
Epoch 78: Val Loss 562.84198
Epoch 79: Val Loss 562.84125
Epoch 80: Val Loss 562.84052
Epoch 81: Val Loss 562.83978
Epoch 82: Val Loss 562.83905
Epoch 83: Val Loss 562.83832
Epoch 84: Val Loss 562.83765
Epoch 85: Val Loss 562.83691
Epoch 86: Val Loss 562.83618
Epoch 87: Val Loss 562.83551
Epoch 88: Val Loss 562.83478
Epoch 89: Val Loss 562.83405
Epoch 90: Val Loss 562.83331
Epoch 91: Val Loss 562.83264
Epoch 92: Val Loss 562.83185
Epoch 93: Val Loss 562.83118
Epoch 94: Val Loss 562.83044
Epoch 95: Val Loss 562.82971
Epoch 96: Val Loss 562.82898
Epoch 97: Val Loss 562.82825
Epoch 98: Val Loss 562.82751
Epoch 99: Val Loss 562.82678
{'MSE - mean': 613.3902516245062, 'MSE - std': 63.72721326151488, 'R2 - mean': -6.528710328232643, 'R2 - std': 0.35179686300935725} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 559.75885
Epoch 1: Val Loss 559.75836
Epoch 2: Val Loss 559.75787
Epoch 3: Val Loss 559.75745
Epoch 4: Val Loss 559.75702
Epoch 5: Val Loss 559.75659
Epoch 6: Val Loss 559.75610
Epoch 7: Val Loss 559.75562
Epoch 8: Val Loss 559.75519
Epoch 9: Val Loss 559.75476
Epoch 10: Val Loss 559.75427
Epoch 11: Val Loss 559.75385
Epoch 12: Val Loss 559.75342
Epoch 13: Val Loss 559.75293
Epoch 14: Val Loss 559.75244
Epoch 15: Val Loss 559.75201
Epoch 16: Val Loss 559.75153
Epoch 17: Val Loss 559.75110
Epoch 18: Val Loss 559.75067
Epoch 19: Val Loss 559.75024
Epoch 20: Val Loss 559.74976
Epoch 21: Val Loss 559.74927
Epoch 22: Val Loss 559.74884
Epoch 23: Val Loss 559.74835
Epoch 24: Val Loss 559.74792
Epoch 25: Val Loss 559.74750
Epoch 26: Val Loss 559.74695
Epoch 27: Val Loss 559.74652
Epoch 28: Val Loss 559.74603
Epoch 29: Val Loss 559.74567
Epoch 30: Val Loss 559.74518
Epoch 31: Val Loss 559.74475
Epoch 32: Val Loss 559.74426
Epoch 33: Val Loss 559.74384
Epoch 34: Val Loss 559.74341
Epoch 35: Val Loss 559.74298
Epoch 36: Val Loss 559.74249
Epoch 37: Val Loss 559.74200
Epoch 38: Val Loss 559.74158
Epoch 39: Val Loss 559.74109
Epoch 40: Val Loss 559.74066
Epoch 41: Val Loss 559.74023
Epoch 42: Val Loss 559.73975
Epoch 43: Val Loss 559.73926
Epoch 44: Val Loss 559.73877
Epoch 45: Val Loss 559.73828
Epoch 46: Val Loss 559.73785
Epoch 47: Val Loss 559.73737
Epoch 48: Val Loss 559.73694
Epoch 49: Val Loss 559.73651
Epoch 50: Val Loss 559.73608
Epoch 51: Val Loss 559.73560
Epoch 52: Val Loss 559.73517
Epoch 53: Val Loss 559.73468
Epoch 54: Val Loss 559.73419
Epoch 55: Val Loss 559.73376
Epoch 56: Val Loss 559.73328
Epoch 57: Val Loss 559.73285
Epoch 58: Val Loss 559.73242
Epoch 59: Val Loss 559.73199
Epoch 60: Val Loss 559.73145
Epoch 61: Val Loss 559.73102
Epoch 62: Val Loss 559.73059
Epoch 63: Val Loss 559.73010
Epoch 64: Val Loss 559.72968
Epoch 65: Val Loss 559.72919
Epoch 66: Val Loss 559.72876
Epoch 67: Val Loss 559.72827
Epoch 68: Val Loss 559.72784
Epoch 69: Val Loss 559.72736
Epoch 70: Val Loss 559.72693
Epoch 71: Val Loss 559.72650
Epoch 72: Val Loss 559.72601
Epoch 73: Val Loss 559.72552
Epoch 74: Val Loss 559.72510
Epoch 75: Val Loss 559.72467
Epoch 76: Val Loss 559.72418
Epoch 77: Val Loss 559.72375
Epoch 78: Val Loss 559.72333
Epoch 79: Val Loss 559.72284
Epoch 80: Val Loss 559.72235
Epoch 81: Val Loss 559.72192
Epoch 82: Val Loss 559.72144
Epoch 83: Val Loss 559.72101
Epoch 84: Val Loss 559.72052
Epoch 85: Val Loss 559.72003
Epoch 86: Val Loss 559.71960
Epoch 87: Val Loss 559.71912
Epoch 88: Val Loss 559.71869
Epoch 89: Val Loss 559.71826
Epoch 90: Val Loss 559.71777
Epoch 91: Val Loss 559.71729
Epoch 92: Val Loss 559.71686
Epoch 93: Val Loss 559.71643
Epoch 94: Val Loss 559.71594
Epoch 95: Val Loss 559.71552
Epoch 96: Val Loss 559.71503
Epoch 97: Val Loss 559.71466
Epoch 98: Val Loss 559.71417
Epoch 99: Val Loss 559.71368
{'MSE - mean': 602.6549458609712, 'MSE - std': 60.9090578437167, 'R2 - mean': -6.249382582345668, 'R2 - std': 0.6411745353238641} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 602.6549458609712 and parameters: {'dim': 32, 'depth': 3, 'heads': 8, 'weight_decay': -3, 'learning_rate': -6, 'dropout': 0.5}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 555.56805
Epoch 1: Val Loss 555.56085
Epoch 2: Val Loss 555.55365
Epoch 3: Val Loss 555.54645
Epoch 4: Val Loss 555.53931
Epoch 5: Val Loss 555.53204
Epoch 6: Val Loss 555.52484
Epoch 7: Val Loss 555.51764
Epoch 8: Val Loss 555.51056
Epoch 9: Val Loss 555.50342
Epoch 10: Val Loss 555.49622
Epoch 11: Val Loss 555.48901
Epoch 12: Val Loss 555.48187
Epoch 13: Val Loss 555.47467
Epoch 14: Val Loss 555.46747
Epoch 15: Val Loss 555.46027
Epoch 16: Val Loss 555.45306
Epoch 17: Val Loss 555.44586
Epoch 18: Val Loss 555.43866
Epoch 19: Val Loss 555.43134
Epoch 20: Val Loss 555.42413
Epoch 21: Val Loss 555.41693
Epoch 22: Val Loss 555.40973
Epoch 23: Val Loss 555.40253
Epoch 24: Val Loss 555.39514
Epoch 25: Val Loss 555.38782
Epoch 26: Val Loss 555.38055
Epoch 27: Val Loss 555.37323
Epoch 28: Val Loss 555.36603
Epoch 29: Val Loss 555.35889
Epoch 30: Val Loss 555.35162
Epoch 31: Val Loss 555.34430
Epoch 32: Val Loss 555.33698
Epoch 33: Val Loss 555.32965
Epoch 34: Val Loss 555.32239
Epoch 35: Val Loss 555.31519
Epoch 36: Val Loss 555.30792
Epoch 37: Val Loss 555.30072
Epoch 38: Val Loss 555.29358
Epoch 39: Val Loss 555.28625
Epoch 40: Val Loss 555.27899
Epoch 41: Val Loss 555.27167
Epoch 42: Val Loss 555.26440
Epoch 43: Val Loss 555.25720
Epoch 44: Val Loss 555.25000
Epoch 45: Val Loss 555.24286
Epoch 46: Val Loss 555.23584
Epoch 47: Val Loss 555.22870
Epoch 48: Val Loss 555.22162
Epoch 49: Val Loss 555.21442
Epoch 50: Val Loss 555.20715
Epoch 51: Val Loss 555.19995
Epoch 52: Val Loss 555.19281
Epoch 53: Val Loss 555.18567
Epoch 54: Val Loss 555.17853
Epoch 55: Val Loss 555.17139
Epoch 56: Val Loss 555.16412
Epoch 57: Val Loss 555.15686
Epoch 58: Val Loss 555.14960
Epoch 59: Val Loss 555.14246
Epoch 60: Val Loss 555.13538
Epoch 61: Val Loss 555.12817
Epoch 62: Val Loss 555.12097
Epoch 63: Val Loss 555.11359
Epoch 64: Val Loss 555.10632
Epoch 65: Val Loss 555.09912
Epoch 66: Val Loss 555.09186
Epoch 67: Val Loss 555.08459
Epoch 68: Val Loss 555.07739
Epoch 69: Val Loss 555.07019
Epoch 70: Val Loss 555.06293
Epoch 71: Val Loss 555.05579
Epoch 72: Val Loss 555.04858
Epoch 73: Val Loss 555.04132
Epoch 74: Val Loss 555.03394
Epoch 75: Val Loss 555.02655
Epoch 76: Val Loss 555.01910
Epoch 77: Val Loss 555.01172
Epoch 78: Val Loss 555.00433
Epoch 79: Val Loss 554.99701
Epoch 80: Val Loss 554.98962
Epoch 81: Val Loss 554.98230
Epoch 82: Val Loss 554.97498
Epoch 83: Val Loss 554.96759
Epoch 84: Val Loss 554.96021
Epoch 85: Val Loss 554.95282
Epoch 86: Val Loss 554.94550
Epoch 87: Val Loss 554.93811
Epoch 88: Val Loss 554.93073
Epoch 89: Val Loss 554.92328
Epoch 90: Val Loss 554.91602
Epoch 91: Val Loss 554.90857
Epoch 92: Val Loss 554.90112
Epoch 93: Val Loss 554.89368
Epoch 94: Val Loss 554.88611
Epoch 95: Val Loss 554.87866
Epoch 96: Val Loss 554.87122
Epoch 97: Val Loss 554.86389
Epoch 98: Val Loss 554.85651
Epoch 99: Val Loss 554.84918
{'MSE - mean': 554.8491720280563, 'MSE - std': 0.0, 'R2 - mean': -6.43085451665391, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 617.47253
Epoch 1: Val Loss 617.47070
Epoch 2: Val Loss 617.46881
Epoch 3: Val Loss 617.46698
Epoch 4: Val Loss 617.46521
Epoch 5: Val Loss 617.46332
Epoch 6: Val Loss 617.46149
Epoch 7: Val Loss 617.45966
Epoch 8: Val Loss 617.45770
Epoch 9: Val Loss 617.45587
Epoch 10: Val Loss 617.45398
Epoch 11: Val Loss 617.45215
Epoch 12: Val Loss 617.45026
Epoch 13: Val Loss 617.44849
Epoch 14: Val Loss 617.44659
Epoch 15: Val Loss 617.44470
Epoch 16: Val Loss 617.44293
Epoch 17: Val Loss 617.44104
Epoch 18: Val Loss 617.43927
Epoch 19: Val Loss 617.43732
Epoch 20: Val Loss 617.43555
Epoch 21: Val Loss 617.43365
Epoch 22: Val Loss 617.43182
Epoch 23: Val Loss 617.42993
Epoch 24: Val Loss 617.42810
Epoch 25: Val Loss 617.42621
Epoch 26: Val Loss 617.42432
Epoch 27: Val Loss 617.42255
Epoch 28: Val Loss 617.42065
Epoch 29: Val Loss 617.41876
Epoch 30: Val Loss 617.41681
Epoch 31: Val Loss 617.41498
Epoch 32: Val Loss 617.41315
Epoch 33: Val Loss 617.41119
Epoch 34: Val Loss 617.40930
Epoch 35: Val Loss 617.40747
Epoch 36: Val Loss 617.40558
Epoch 37: Val Loss 617.40375
Epoch 38: Val Loss 617.40186
Epoch 39: Val Loss 617.40002
Epoch 40: Val Loss 617.39825
Epoch 41: Val Loss 617.39642
Epoch 42: Val Loss 617.39453
Epoch 43: Val Loss 617.39270
Epoch 44: Val Loss 617.39087
Epoch 45: Val Loss 617.38904
Epoch 46: Val Loss 617.38715
Epoch 47: Val Loss 617.38525
Epoch 48: Val Loss 617.38348
Epoch 49: Val Loss 617.38165
Epoch 50: Val Loss 617.37982
Epoch 51: Val Loss 617.37793
Epoch 52: Val Loss 617.37616
Epoch 53: Val Loss 617.37427
Epoch 54: Val Loss 617.37244
Epoch 55: Val Loss 617.37054
Epoch 56: Val Loss 617.36865
Epoch 57: Val Loss 617.36682
Epoch 58: Val Loss 617.36499
Epoch 59: Val Loss 617.36310
Epoch 60: Val Loss 617.36133
Epoch 61: Val Loss 617.35938
Epoch 62: Val Loss 617.35760
Epoch 63: Val Loss 617.35571
Epoch 64: Val Loss 617.35382
Epoch 65: Val Loss 617.35199
Epoch 66: Val Loss 617.35010
Epoch 67: Val Loss 617.34827
Epoch 68: Val Loss 617.34644
Epoch 69: Val Loss 617.34454
Epoch 70: Val Loss 617.34271
Epoch 71: Val Loss 617.34088
Epoch 72: Val Loss 617.33905
Epoch 73: Val Loss 617.33716
Epoch 74: Val Loss 617.33539
Epoch 75: Val Loss 617.33344
Epoch 76: Val Loss 617.33160
Epoch 77: Val Loss 617.32971
Epoch 78: Val Loss 617.32782
Epoch 79: Val Loss 617.32605
Epoch 80: Val Loss 617.32416
Epoch 81: Val Loss 617.32227
Epoch 82: Val Loss 617.32037
Epoch 83: Val Loss 617.31854
Epoch 84: Val Loss 617.31665
Epoch 85: Val Loss 617.31482
Epoch 86: Val Loss 617.31293
Epoch 87: Val Loss 617.31104
Epoch 88: Val Loss 617.30920
Epoch 89: Val Loss 617.30737
Epoch 90: Val Loss 617.30548
Epoch 91: Val Loss 617.30365
Epoch 92: Val Loss 617.30182
Epoch 93: Val Loss 617.30005
Epoch 94: Val Loss 617.29816
Epoch 95: Val Loss 617.29626
Epoch 96: Val Loss 617.29437
Epoch 97: Val Loss 617.29254
Epoch 98: Val Loss 617.29059
Epoch 99: Val Loss 617.28882
{'MSE - mean': 586.0689925446184, 'MSE - std': 31.219820516562095, 'R2 - mean': -6.547609288413249, 'R2 - std': 0.11675477175933935} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 682.55029
Epoch 1: Val Loss 682.54761
Epoch 2: Val Loss 682.54517
Epoch 3: Val Loss 682.54260
Epoch 4: Val Loss 682.54004
Epoch 5: Val Loss 682.53748
Epoch 6: Val Loss 682.53503
Epoch 7: Val Loss 682.53253
Epoch 8: Val Loss 682.53003
Epoch 9: Val Loss 682.52753
Epoch 10: Val Loss 682.52502
Epoch 11: Val Loss 682.52252
Epoch 12: Val Loss 682.51996
Epoch 13: Val Loss 682.51746
Epoch 14: Val Loss 682.51501
Epoch 15: Val Loss 682.51239
Epoch 16: Val Loss 682.50983
Epoch 17: Val Loss 682.50726
Epoch 18: Val Loss 682.50476
Epoch 19: Val Loss 682.50226
Epoch 20: Val Loss 682.49976
Epoch 21: Val Loss 682.49725
Epoch 22: Val Loss 682.49475
Epoch 23: Val Loss 682.49225
Epoch 24: Val Loss 682.48981
Epoch 25: Val Loss 682.48712
Epoch 26: Val Loss 682.48468
Epoch 27: Val Loss 682.48218
Epoch 28: Val Loss 682.47955
Epoch 29: Val Loss 682.47711
Epoch 30: Val Loss 682.47461
Epoch 31: Val Loss 682.47205
Epoch 32: Val Loss 682.46948
Epoch 33: Val Loss 682.46704
Epoch 34: Val Loss 682.46454
Epoch 35: Val Loss 682.46210
Epoch 36: Val Loss 682.45959
Epoch 37: Val Loss 682.45715
Epoch 38: Val Loss 682.45465
Epoch 39: Val Loss 682.45209
Epoch 40: Val Loss 682.44965
Epoch 41: Val Loss 682.44708
Epoch 42: Val Loss 682.44458
Epoch 43: Val Loss 682.44214
Epoch 44: Val Loss 682.43964
Epoch 45: Val Loss 682.43719
Epoch 46: Val Loss 682.43463
Epoch 47: Val Loss 682.43213
Epoch 48: Val Loss 682.42969
Epoch 49: Val Loss 682.42712
Epoch 50: Val Loss 682.42462
Epoch 51: Val Loss 682.42218
Epoch 52: Val Loss 682.41968
Epoch 53: Val Loss 682.41724
Epoch 54: Val Loss 682.41473
Epoch 55: Val Loss 682.41217
Epoch 56: Val Loss 682.40973
Epoch 57: Val Loss 682.40717
Epoch 58: Val Loss 682.40466
Epoch 59: Val Loss 682.40222
Epoch 60: Val Loss 682.39984
Epoch 61: Val Loss 682.39734
Epoch 62: Val Loss 682.39496
Epoch 63: Val Loss 682.39246
Epoch 64: Val Loss 682.39001
Epoch 65: Val Loss 682.38757
Epoch 66: Val Loss 682.38507
Epoch 67: Val Loss 682.38257
Epoch 68: Val Loss 682.38007
Epoch 69: Val Loss 682.37762
Epoch 70: Val Loss 682.37512
Epoch 71: Val Loss 682.37268
Epoch 72: Val Loss 682.37018
Epoch 73: Val Loss 682.36774
Epoch 74: Val Loss 682.36523
Epoch 75: Val Loss 682.36279
Epoch 76: Val Loss 682.36029
Epoch 77: Val Loss 682.35779
Epoch 78: Val Loss 682.35535
Epoch 79: Val Loss 682.35297
Epoch 80: Val Loss 682.35040
Epoch 81: Val Loss 682.34790
Epoch 82: Val Loss 682.34552
Epoch 83: Val Loss 682.34296
Epoch 84: Val Loss 682.34052
Epoch 85: Val Loss 682.33801
Epoch 86: Val Loss 682.33551
Epoch 87: Val Loss 682.33301
Epoch 88: Val Loss 682.33051
Epoch 89: Val Loss 682.32812
Epoch 90: Val Loss 682.32562
Epoch 91: Val Loss 682.32318
Epoch 92: Val Loss 682.32068
Epoch 93: Val Loss 682.31824
Epoch 94: Val Loss 682.31580
Epoch 95: Val Loss 682.31335
Epoch 96: Val Loss 682.31085
Epoch 97: Val Loss 682.30847
Epoch 98: Val Loss 682.30585
Epoch 99: Val Loss 682.30334
{'MSE - mean': 618.1471383833137, 'MSE - std': 52.03652252301373, 'R2 - mean': -6.414905623168444, 'R2 - std': 0.21049539193181274} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 547.46344
Epoch 1: Val Loss 547.46063
Epoch 2: Val Loss 547.45782
Epoch 3: Val Loss 547.45496
Epoch 4: Val Loss 547.45221
Epoch 5: Val Loss 547.44940
Epoch 6: Val Loss 547.44672
Epoch 7: Val Loss 547.44391
Epoch 8: Val Loss 547.44110
Epoch 9: Val Loss 547.43829
Epoch 10: Val Loss 547.43555
Epoch 11: Val Loss 547.43268
Epoch 12: Val Loss 547.42987
Epoch 13: Val Loss 547.42700
Epoch 14: Val Loss 547.42413
Epoch 15: Val Loss 547.42133
Epoch 16: Val Loss 547.41846
Epoch 17: Val Loss 547.41559
Epoch 18: Val Loss 547.41272
Epoch 19: Val Loss 547.40985
Epoch 20: Val Loss 547.40704
Epoch 21: Val Loss 547.40424
Epoch 22: Val Loss 547.40143
Epoch 23: Val Loss 547.39868
Epoch 24: Val Loss 547.39581
Epoch 25: Val Loss 547.39301
Epoch 26: Val Loss 547.39014
Epoch 27: Val Loss 547.38727
Epoch 28: Val Loss 547.38440
Epoch 29: Val Loss 547.38153
Epoch 30: Val Loss 547.37866
Epoch 31: Val Loss 547.37579
Epoch 32: Val Loss 547.37292
Epoch 33: Val Loss 547.37006
Epoch 34: Val Loss 547.36719
Epoch 35: Val Loss 547.36438
Epoch 36: Val Loss 547.36151
Epoch 37: Val Loss 547.35864
Epoch 38: Val Loss 547.35583
Epoch 39: Val Loss 547.35291
Epoch 40: Val Loss 547.35010
Epoch 41: Val Loss 547.34723
Epoch 42: Val Loss 547.34436
Epoch 43: Val Loss 547.34143
Epoch 44: Val Loss 547.33862
Epoch 45: Val Loss 547.33575
Epoch 46: Val Loss 547.33282
Epoch 47: Val Loss 547.32983
Epoch 48: Val Loss 547.32678
Epoch 49: Val Loss 547.32385
Epoch 50: Val Loss 547.32086
Epoch 51: Val Loss 547.31787
Epoch 52: Val Loss 547.31488
Epoch 53: Val Loss 547.31189
Epoch 54: Val Loss 547.30902
Epoch 55: Val Loss 547.30609
Epoch 56: Val Loss 547.30322
Epoch 57: Val Loss 547.30035
Epoch 58: Val Loss 547.29742
Epoch 59: Val Loss 547.29449
Epoch 60: Val Loss 547.29156
Epoch 61: Val Loss 547.28864
Epoch 62: Val Loss 547.28564
Epoch 63: Val Loss 547.28271
Epoch 64: Val Loss 547.27979
Epoch 65: Val Loss 547.27686
Epoch 66: Val Loss 547.27393
Epoch 67: Val Loss 547.27100
Epoch 68: Val Loss 547.26807
Epoch 69: Val Loss 547.26501
Epoch 70: Val Loss 547.26208
Epoch 71: Val Loss 547.25916
Epoch 72: Val Loss 547.25629
Epoch 73: Val Loss 547.25336
Epoch 74: Val Loss 547.25043
Epoch 75: Val Loss 547.24756
Epoch 76: Val Loss 547.24463
Epoch 77: Val Loss 547.24170
Epoch 78: Val Loss 547.23877
Epoch 79: Val Loss 547.23584
Epoch 80: Val Loss 547.23291
Epoch 81: Val Loss 547.23004
Epoch 82: Val Loss 547.22711
Epoch 83: Val Loss 547.22418
Epoch 84: Val Loss 547.22119
Epoch 85: Val Loss 547.21826
Epoch 86: Val Loss 547.21533
Epoch 87: Val Loss 547.21246
Epoch 88: Val Loss 547.20953
Epoch 89: Val Loss 547.20660
Epoch 90: Val Loss 547.20355
Epoch 91: Val Loss 547.20062
Epoch 92: Val Loss 547.19757
Epoch 93: Val Loss 547.19452
Epoch 94: Val Loss 547.19153
Epoch 95: Val Loss 547.18842
Epoch 96: Val Loss 547.18542
Epoch 97: Val Loss 547.18243
Epoch 98: Val Loss 547.17950
Epoch 99: Val Loss 547.17657
{'MSE - mean': 600.4044998975882, 'MSE - std': 54.54588360748718, 'R2 - mean': -6.374089278600188, 'R2 - std': 0.19552277225463904} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 550.11920
Epoch 1: Val Loss 550.11749
Epoch 2: Val Loss 550.11578
Epoch 3: Val Loss 550.11414
Epoch 4: Val Loss 550.11237
Epoch 5: Val Loss 550.11060
Epoch 6: Val Loss 550.10889
Epoch 7: Val Loss 550.10730
Epoch 8: Val Loss 550.10547
Epoch 9: Val Loss 550.10382
Epoch 10: Val Loss 550.10211
Epoch 11: Val Loss 550.10040
Epoch 12: Val Loss 550.09875
Epoch 13: Val Loss 550.09705
Epoch 14: Val Loss 550.09534
Epoch 15: Val Loss 550.09369
Epoch 16: Val Loss 550.09192
Epoch 17: Val Loss 550.09027
Epoch 18: Val Loss 550.08850
Epoch 19: Val Loss 550.08679
Epoch 20: Val Loss 550.08508
Epoch 21: Val Loss 550.08337
Epoch 22: Val Loss 550.08167
Epoch 23: Val Loss 550.07996
Epoch 24: Val Loss 550.07819
Epoch 25: Val Loss 550.07648
Epoch 26: Val Loss 550.07471
Epoch 27: Val Loss 550.07300
Epoch 28: Val Loss 550.07129
Epoch 29: Val Loss 550.06958
Epoch 30: Val Loss 550.06793
Epoch 31: Val Loss 550.06610
Epoch 32: Val Loss 550.06445
Epoch 33: Val Loss 550.06274
Epoch 34: Val Loss 550.06104
Epoch 35: Val Loss 550.05939
Epoch 36: Val Loss 550.05762
Epoch 37: Val Loss 550.05591
Epoch 38: Val Loss 550.05420
Epoch 39: Val Loss 550.05249
Epoch 40: Val Loss 550.05072
Epoch 41: Val Loss 550.04901
Epoch 42: Val Loss 550.04730
Epoch 43: Val Loss 550.04565
Epoch 44: Val Loss 550.04395
Epoch 45: Val Loss 550.04224
Epoch 46: Val Loss 550.04053
Epoch 47: Val Loss 550.03882
Epoch 48: Val Loss 550.03711
Epoch 49: Val Loss 550.03540
Epoch 50: Val Loss 550.03369
Epoch 51: Val Loss 550.03198
Epoch 52: Val Loss 550.03027
Epoch 53: Val Loss 550.02863
Epoch 54: Val Loss 550.02686
Epoch 55: Val Loss 550.02515
Epoch 56: Val Loss 550.02344
Epoch 57: Val Loss 550.02173
Epoch 58: Val Loss 550.01996
Epoch 59: Val Loss 550.01831
Epoch 60: Val Loss 550.01654
Epoch 61: Val Loss 550.01489
Epoch 62: Val Loss 550.01324
Epoch 63: Val Loss 550.01154
Epoch 64: Val Loss 550.00989
Epoch 65: Val Loss 550.00812
Epoch 66: Val Loss 550.00641
Epoch 67: Val Loss 550.00470
Epoch 68: Val Loss 550.00299
Epoch 69: Val Loss 550.00128
Epoch 70: Val Loss 549.99957
Epoch 71: Val Loss 549.99786
Epoch 72: Val Loss 549.99609
Epoch 73: Val Loss 549.99445
Epoch 74: Val Loss 549.99274
Epoch 75: Val Loss 549.99103
Epoch 76: Val Loss 549.98926
Epoch 77: Val Loss 549.98761
Epoch 78: Val Loss 549.98590
Epoch 79: Val Loss 549.98419
Epoch 80: Val Loss 549.98248
Epoch 81: Val Loss 549.98077
Epoch 82: Val Loss 549.97900
Epoch 83: Val Loss 549.97736
Epoch 84: Val Loss 549.97565
Epoch 85: Val Loss 549.97394
Epoch 86: Val Loss 549.97217
Epoch 87: Val Loss 549.97046
Epoch 88: Val Loss 549.96875
Epoch 89: Val Loss 549.96704
Epoch 90: Val Loss 549.96533
Epoch 91: Val Loss 549.96362
Epoch 92: Val Loss 549.96185
Epoch 93: Val Loss 549.96021
Epoch 94: Val Loss 549.95850
Epoch 95: Val Loss 549.95673
Epoch 96: Val Loss 549.95508
Epoch 97: Val Loss 549.95331
Epoch 98: Val Loss 549.95166
Epoch 99: Val Loss 549.94995
{'MSE - mean': 590.3135877848222, 'MSE - std': 52.79686320121206, 'R2 - mean': -6.104291870418098, 'R2 - std': 0.5672264886473509} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 590.3135877848222 and parameters: {'dim': 32, 'depth': 6, 'heads': 8, 'weight_decay': -6, 'learning_rate': -5, 'dropout': 0.4}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 522.06714
Epoch 1: Val Loss 522.06183
Epoch 2: Val Loss 522.05658
Epoch 3: Val Loss 522.05133
Epoch 4: Val Loss 522.04602
Epoch 5: Val Loss 522.04077
Epoch 6: Val Loss 522.03546
Epoch 7: Val Loss 522.03027
Epoch 8: Val Loss 522.02509
Epoch 9: Val Loss 522.01984
Epoch 10: Val Loss 522.01459
Epoch 11: Val Loss 522.00922
Epoch 12: Val Loss 522.00385
Epoch 13: Val Loss 521.99847
Epoch 14: Val Loss 521.99304
Epoch 15: Val Loss 521.98767
Epoch 16: Val Loss 521.98224
Epoch 17: Val Loss 521.97687
Epoch 18: Val Loss 521.97150
Epoch 19: Val Loss 521.96606
Epoch 20: Val Loss 521.96063
Epoch 21: Val Loss 521.95526
Epoch 22: Val Loss 521.95001
Epoch 23: Val Loss 521.94470
Epoch 24: Val Loss 521.93945
Epoch 25: Val Loss 521.93420
Epoch 26: Val Loss 521.92896
Epoch 27: Val Loss 521.92371
Epoch 28: Val Loss 521.91846
Epoch 29: Val Loss 521.91321
Epoch 30: Val Loss 521.90796
Epoch 31: Val Loss 521.90271
Epoch 32: Val Loss 521.89728
Epoch 33: Val Loss 521.89203
Epoch 34: Val Loss 521.88672
Epoch 35: Val Loss 521.88147
Epoch 36: Val Loss 521.87616
Epoch 37: Val Loss 521.87073
Epoch 38: Val Loss 521.86523
Epoch 39: Val Loss 521.85986
Epoch 40: Val Loss 521.85449
Epoch 41: Val Loss 521.84924
Epoch 42: Val Loss 521.84399
Epoch 43: Val Loss 521.83868
Epoch 44: Val Loss 521.83350
Epoch 45: Val Loss 521.82819
Epoch 46: Val Loss 521.82294
Epoch 47: Val Loss 521.81750
Epoch 48: Val Loss 521.81201
Epoch 49: Val Loss 521.80640
Epoch 50: Val Loss 521.80078
Epoch 51: Val Loss 521.79504
Epoch 52: Val Loss 521.78949
Epoch 53: Val Loss 521.78381
Epoch 54: Val Loss 521.77826
Epoch 55: Val Loss 521.77277
Epoch 56: Val Loss 521.76727
Epoch 57: Val Loss 521.76184
Epoch 58: Val Loss 521.75629
Epoch 59: Val Loss 521.75085
Epoch 60: Val Loss 521.74542
Epoch 61: Val Loss 521.73993
Epoch 62: Val Loss 521.73438
Epoch 63: Val Loss 521.72882
Epoch 64: Val Loss 521.72314
Epoch 65: Val Loss 521.71759
Epoch 66: Val Loss 521.71204
Epoch 67: Val Loss 521.70654
Epoch 68: Val Loss 521.70099
Epoch 69: Val Loss 521.69556
Epoch 70: Val Loss 521.69006
Epoch 71: Val Loss 521.68451
Epoch 72: Val Loss 521.67902
Epoch 73: Val Loss 521.67340
Epoch 74: Val Loss 521.66766
Epoch 75: Val Loss 521.66199
Epoch 76: Val Loss 521.65637
Epoch 77: Val Loss 521.65070
Epoch 78: Val Loss 521.64502
Epoch 79: Val Loss 521.63947
Epoch 80: Val Loss 521.63391
Epoch 81: Val Loss 521.62842
Epoch 82: Val Loss 521.62280
Epoch 83: Val Loss 521.61731
Epoch 84: Val Loss 521.61194
Epoch 85: Val Loss 521.60638
Epoch 86: Val Loss 521.60095
Epoch 87: Val Loss 521.59552
Epoch 88: Val Loss 521.59009
Epoch 89: Val Loss 521.58466
Epoch 90: Val Loss 521.57910
Epoch 91: Val Loss 521.57349
Epoch 92: Val Loss 521.56781
Epoch 93: Val Loss 521.56207
Epoch 94: Val Loss 521.55646
Epoch 95: Val Loss 521.55078
Epoch 96: Val Loss 521.54523
Epoch 97: Val Loss 521.53955
Epoch 98: Val Loss 521.53381
Epoch 99: Val Loss 521.52826
{'MSE - mean': 521.5282301632292, 'MSE - std': 0.0, 'R2 - mean': -5.984601581914211, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 604.04816
Epoch 1: Val Loss 604.03766
Epoch 2: Val Loss 604.02722
Epoch 3: Val Loss 604.01685
Epoch 4: Val Loss 604.00659
Epoch 5: Val Loss 603.99628
Epoch 6: Val Loss 603.98578
Epoch 7: Val Loss 603.97534
Epoch 8: Val Loss 603.96478
Epoch 9: Val Loss 603.95422
Epoch 10: Val Loss 603.94373
Epoch 11: Val Loss 603.93329
Epoch 12: Val Loss 603.92273
Epoch 13: Val Loss 603.91217
Epoch 14: Val Loss 603.90155
Epoch 15: Val Loss 603.89087
Epoch 16: Val Loss 603.88013
Epoch 17: Val Loss 603.86951
Epoch 18: Val Loss 603.85889
Epoch 19: Val Loss 603.84827
Epoch 20: Val Loss 603.83771
Epoch 21: Val Loss 603.82721
Epoch 22: Val Loss 603.81683
Epoch 23: Val Loss 603.80640
Epoch 24: Val Loss 603.79596
Epoch 25: Val Loss 603.78534
Epoch 26: Val Loss 603.77460
Epoch 27: Val Loss 603.76410
Epoch 28: Val Loss 603.75354
Epoch 29: Val Loss 603.74310
Epoch 30: Val Loss 603.73260
Epoch 31: Val Loss 603.72198
Epoch 32: Val Loss 603.71124
Epoch 33: Val Loss 603.70068
Epoch 34: Val Loss 603.69037
Epoch 35: Val Loss 603.68011
Epoch 36: Val Loss 603.67004
Epoch 37: Val Loss 603.65997
Epoch 38: Val Loss 603.64954
Epoch 39: Val Loss 603.63910
Epoch 40: Val Loss 603.62854
Epoch 41: Val Loss 603.61804
Epoch 42: Val Loss 603.60748
Epoch 43: Val Loss 603.59686
Epoch 44: Val Loss 603.58636
Epoch 45: Val Loss 603.57593
Epoch 46: Val Loss 603.56561
Epoch 47: Val Loss 603.55518
Epoch 48: Val Loss 603.54462
Epoch 49: Val Loss 603.53394
Epoch 50: Val Loss 603.52313
Epoch 51: Val Loss 603.51245
Epoch 52: Val Loss 603.50183
Epoch 53: Val Loss 603.49127
Epoch 54: Val Loss 603.48041
Epoch 55: Val Loss 603.46967
Epoch 56: Val Loss 603.45911
Epoch 57: Val Loss 603.44873
Epoch 58: Val Loss 603.43835
Epoch 59: Val Loss 603.42804
Epoch 60: Val Loss 603.41785
Epoch 61: Val Loss 603.40759
Epoch 62: Val Loss 603.39746
Epoch 63: Val Loss 603.38715
Epoch 64: Val Loss 603.37677
Epoch 65: Val Loss 603.36639
Epoch 66: Val Loss 603.35590
Epoch 67: Val Loss 603.34521
Epoch 68: Val Loss 603.33423
Epoch 69: Val Loss 603.32318
Epoch 70: Val Loss 603.31219
Epoch 71: Val Loss 603.30133
Epoch 72: Val Loss 603.29065
Epoch 73: Val Loss 603.28009
Epoch 74: Val Loss 603.26947
Epoch 75: Val Loss 603.25885
Epoch 76: Val Loss 603.24811
Epoch 77: Val Loss 603.23749
Epoch 78: Val Loss 603.22687
Epoch 79: Val Loss 603.21625
Epoch 80: Val Loss 603.20557
Epoch 81: Val Loss 603.19507
Epoch 82: Val Loss 603.18457
Epoch 83: Val Loss 603.17401
Epoch 84: Val Loss 603.16351
Epoch 85: Val Loss 603.15302
Epoch 86: Val Loss 603.14258
Epoch 87: Val Loss 603.13226
Epoch 88: Val Loss 603.12189
Epoch 89: Val Loss 603.11139
Epoch 90: Val Loss 603.10095
Epoch 91: Val Loss 603.09039
Epoch 92: Val Loss 603.07996
Epoch 93: Val Loss 603.06958
Epoch 94: Val Loss 603.05927
Epoch 95: Val Loss 603.04889
Epoch 96: Val Loss 603.03839
Epoch 97: Val Loss 603.02802
Epoch 98: Val Loss 603.01764
Epoch 99: Val Loss 603.00720
{'MSE - mean': 562.2677190126136, 'MSE - std': 40.73948884938443, 'R2 - mean': -6.235821387939364, 'R2 - std': 0.2512198060251527} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 665.46857
Epoch 1: Val Loss 665.46021
Epoch 2: Val Loss 665.45178
Epoch 3: Val Loss 665.44342
Epoch 4: Val Loss 665.43518
Epoch 5: Val Loss 665.42694
Epoch 6: Val Loss 665.41870
Epoch 7: Val Loss 665.41040
Epoch 8: Val Loss 665.40204
Epoch 9: Val Loss 665.39386
Epoch 10: Val Loss 665.38556
Epoch 11: Val Loss 665.37726
Epoch 12: Val Loss 665.36877
Epoch 13: Val Loss 665.36035
Epoch 14: Val Loss 665.35193
Epoch 15: Val Loss 665.34344
Epoch 16: Val Loss 665.33490
Epoch 17: Val Loss 665.32648
Epoch 18: Val Loss 665.31805
Epoch 19: Val Loss 665.30957
Epoch 20: Val Loss 665.30096
Epoch 21: Val Loss 665.29236
Epoch 22: Val Loss 665.28394
Epoch 23: Val Loss 665.27539
Epoch 24: Val Loss 665.26678
Epoch 25: Val Loss 665.25812
Epoch 26: Val Loss 665.24951
Epoch 27: Val Loss 665.24097
Epoch 28: Val Loss 665.23242
Epoch 29: Val Loss 665.22382
Epoch 30: Val Loss 665.21539
Epoch 31: Val Loss 665.20685
Epoch 32: Val Loss 665.19855
Epoch 33: Val Loss 665.18994
Epoch 34: Val Loss 665.18146
Epoch 35: Val Loss 665.17279
Epoch 36: Val Loss 665.16437
Epoch 37: Val Loss 665.15582
Epoch 38: Val Loss 665.14740
Epoch 39: Val Loss 665.13892
Epoch 40: Val Loss 665.13043
Epoch 41: Val Loss 665.12189
Epoch 42: Val Loss 665.11322
Epoch 43: Val Loss 665.10474
Epoch 44: Val Loss 665.09607
Epoch 45: Val Loss 665.08752
Epoch 46: Val Loss 665.07892
Epoch 47: Val Loss 665.07037
Epoch 48: Val Loss 665.06171
Epoch 49: Val Loss 665.05322
Epoch 50: Val Loss 665.04462
Epoch 51: Val Loss 665.03601
Epoch 52: Val Loss 665.02753
Epoch 53: Val Loss 665.01904
Epoch 54: Val Loss 665.01038
Epoch 55: Val Loss 665.00171
Epoch 56: Val Loss 664.99286
Epoch 57: Val Loss 664.98407
Epoch 58: Val Loss 664.97522
Epoch 59: Val Loss 664.96649
Epoch 60: Val Loss 664.95776
Epoch 61: Val Loss 664.94910
Epoch 62: Val Loss 664.94043
Epoch 63: Val Loss 664.93195
Epoch 64: Val Loss 664.92340
Epoch 65: Val Loss 664.91492
Epoch 66: Val Loss 664.90637
Epoch 67: Val Loss 664.89789
Epoch 68: Val Loss 664.88940
Epoch 69: Val Loss 664.88086
Epoch 70: Val Loss 664.87250
Epoch 71: Val Loss 664.86414
Epoch 72: Val Loss 664.85565
Epoch 73: Val Loss 664.84698
Epoch 74: Val Loss 664.83856
Epoch 75: Val Loss 664.82983
Epoch 76: Val Loss 664.82117
Epoch 77: Val Loss 664.81256
Epoch 78: Val Loss 664.80389
Epoch 79: Val Loss 664.79529
Epoch 80: Val Loss 664.78650
Epoch 81: Val Loss 664.77777
Epoch 82: Val Loss 664.76910
Epoch 83: Val Loss 664.76050
Epoch 84: Val Loss 664.75183
Epoch 85: Val Loss 664.74316
Epoch 86: Val Loss 664.73444
Epoch 87: Val Loss 664.72571
Epoch 88: Val Loss 664.71686
Epoch 89: Val Loss 664.70807
Epoch 90: Val Loss 664.69946
Epoch 91: Val Loss 664.69073
Epoch 92: Val Loss 664.68207
Epoch 93: Val Loss 664.67340
Epoch 94: Val Loss 664.66473
Epoch 95: Val Loss 664.65607
Epoch 96: Val Loss 664.64740
Epoch 97: Val Loss 664.63873
Epoch 98: Val Loss 664.63007
Epoch 99: Val Loss 664.62152
{'MSE - mean': 596.3856413213692, 'MSE - std': 58.604913444226845, 'R2 - mean': -6.1452870962369905, 'R2 - std': 0.24179986884676183} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 524.93903
Epoch 1: Val Loss 524.93256
Epoch 2: Val Loss 524.92603
Epoch 3: Val Loss 524.91956
Epoch 4: Val Loss 524.91296
Epoch 5: Val Loss 524.90649
Epoch 6: Val Loss 524.90002
Epoch 7: Val Loss 524.89362
Epoch 8: Val Loss 524.88727
Epoch 9: Val Loss 524.88092
Epoch 10: Val Loss 524.87445
Epoch 11: Val Loss 524.86804
Epoch 12: Val Loss 524.86176
Epoch 13: Val Loss 524.85541
Epoch 14: Val Loss 524.84906
Epoch 15: Val Loss 524.84265
Epoch 16: Val Loss 524.83630
Epoch 17: Val Loss 524.82990
Epoch 18: Val Loss 524.82343
Epoch 19: Val Loss 524.81696
Epoch 20: Val Loss 524.81055
Epoch 21: Val Loss 524.80420
Epoch 22: Val Loss 524.79779
Epoch 23: Val Loss 524.79144
Epoch 24: Val Loss 524.78510
Epoch 25: Val Loss 524.77875
Epoch 26: Val Loss 524.77240
Epoch 27: Val Loss 524.76605
Epoch 28: Val Loss 524.75970
Epoch 29: Val Loss 524.75336
Epoch 30: Val Loss 524.74707
Epoch 31: Val Loss 524.74072
Epoch 32: Val Loss 524.73444
Epoch 33: Val Loss 524.72809
Epoch 34: Val Loss 524.72180
Epoch 35: Val Loss 524.71558
Epoch 36: Val Loss 524.70905
Epoch 37: Val Loss 524.70264
Epoch 38: Val Loss 524.69623
Epoch 39: Val Loss 524.68988
Epoch 40: Val Loss 524.68353
Epoch 41: Val Loss 524.67731
Epoch 42: Val Loss 524.67108
Epoch 43: Val Loss 524.66473
Epoch 44: Val Loss 524.65851
Epoch 45: Val Loss 524.65222
Epoch 46: Val Loss 524.64594
Epoch 47: Val Loss 524.63959
Epoch 48: Val Loss 524.63330
Epoch 49: Val Loss 524.62701
Epoch 50: Val Loss 524.62067
Epoch 51: Val Loss 524.61432
Epoch 52: Val Loss 524.60797
Epoch 53: Val Loss 524.60168
Epoch 54: Val Loss 524.59534
Epoch 55: Val Loss 524.58911
Epoch 56: Val Loss 524.58289
Epoch 57: Val Loss 524.57672
Epoch 58: Val Loss 524.57037
Epoch 59: Val Loss 524.56415
Epoch 60: Val Loss 524.55792
Epoch 61: Val Loss 524.55176
Epoch 62: Val Loss 524.54565
Epoch 63: Val Loss 524.53949
Epoch 64: Val Loss 524.53326
Epoch 65: Val Loss 524.52698
Epoch 66: Val Loss 524.52087
Epoch 67: Val Loss 524.51477
Epoch 68: Val Loss 524.50873
Epoch 69: Val Loss 524.50256
Epoch 70: Val Loss 524.49658
Epoch 71: Val Loss 524.49054
Epoch 72: Val Loss 524.48438
Epoch 73: Val Loss 524.47827
Epoch 74: Val Loss 524.47217
Epoch 75: Val Loss 524.46613
Epoch 76: Val Loss 524.46002
Epoch 77: Val Loss 524.45398
Epoch 78: Val Loss 524.44794
Epoch 79: Val Loss 524.44183
Epoch 80: Val Loss 524.43573
Epoch 81: Val Loss 524.42950
Epoch 82: Val Loss 524.42328
Epoch 83: Val Loss 524.41711
Epoch 84: Val Loss 524.41101
Epoch 85: Val Loss 524.40479
Epoch 86: Val Loss 524.39868
Epoch 87: Val Loss 524.39258
Epoch 88: Val Loss 524.38641
Epoch 89: Val Loss 524.38037
Epoch 90: Val Loss 524.37427
Epoch 91: Val Loss 524.36816
Epoch 92: Val Loss 524.36230
Epoch 93: Val Loss 524.35645
Epoch 94: Val Loss 524.35052
Epoch 95: Val Loss 524.34467
Epoch 96: Val Loss 524.33868
Epoch 97: Val Loss 524.33264
Epoch 98: Val Loss 524.32660
Epoch 99: Val Loss 524.32056
{'MSE - mean': 578.3693590334942, 'MSE - std': 59.57903316002362, 'R2 - mean': -6.0961484525394125, 'R2 - std': 0.22604026487342013} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 579.06854
Epoch 1: Val Loss 579.06567
Epoch 2: Val Loss 579.06287
Epoch 3: Val Loss 579.06000
Epoch 4: Val Loss 579.05719
Epoch 5: Val Loss 579.05432
Epoch 6: Val Loss 579.05151
Epoch 7: Val Loss 579.04865
Epoch 8: Val Loss 579.04578
Epoch 9: Val Loss 579.04297
Epoch 10: Val Loss 579.04022
Epoch 11: Val Loss 579.03754
Epoch 12: Val Loss 579.03479
Epoch 13: Val Loss 579.03204
Epoch 14: Val Loss 579.02930
Epoch 15: Val Loss 579.02661
Epoch 16: Val Loss 579.02386
Epoch 17: Val Loss 579.02124
Epoch 18: Val Loss 579.01843
Epoch 19: Val Loss 579.01569
Epoch 20: Val Loss 579.01294
Epoch 21: Val Loss 579.01013
Epoch 22: Val Loss 579.00745
Epoch 23: Val Loss 579.00470
Epoch 24: Val Loss 579.00195
Epoch 25: Val Loss 578.99921
Epoch 26: Val Loss 578.99646
Epoch 27: Val Loss 578.99359
Epoch 28: Val Loss 578.99078
Epoch 29: Val Loss 578.98798
Epoch 30: Val Loss 578.98523
Epoch 31: Val Loss 578.98236
Epoch 32: Val Loss 578.97955
Epoch 33: Val Loss 578.97681
Epoch 34: Val Loss 578.97406
Epoch 35: Val Loss 578.97131
Epoch 36: Val Loss 578.96869
Epoch 37: Val Loss 578.96594
Epoch 38: Val Loss 578.96320
Epoch 39: Val Loss 578.96045
Epoch 40: Val Loss 578.95782
Epoch 41: Val Loss 578.95514
Epoch 42: Val Loss 578.95258
Epoch 43: Val Loss 578.94989
Epoch 44: Val Loss 578.94714
Epoch 45: Val Loss 578.94446
Epoch 46: Val Loss 578.94183
Epoch 47: Val Loss 578.93909
Epoch 48: Val Loss 578.93646
Epoch 49: Val Loss 578.93384
Epoch 50: Val Loss 578.93109
Epoch 51: Val Loss 578.92853
Epoch 52: Val Loss 578.92590
Epoch 53: Val Loss 578.92334
Epoch 54: Val Loss 578.92072
Epoch 55: Val Loss 578.91809
Epoch 56: Val Loss 578.91553
Epoch 57: Val Loss 578.91296
Epoch 58: Val Loss 578.91034
Epoch 59: Val Loss 578.90778
Epoch 60: Val Loss 578.90521
Epoch 61: Val Loss 578.90259
Epoch 62: Val Loss 578.89996
Epoch 63: Val Loss 578.89734
Epoch 64: Val Loss 578.89478
Epoch 65: Val Loss 578.89215
Epoch 66: Val Loss 578.88953
Epoch 67: Val Loss 578.88696
Epoch 68: Val Loss 578.88440
Epoch 69: Val Loss 578.88190
Epoch 70: Val Loss 578.87933
Epoch 71: Val Loss 578.87677
Epoch 72: Val Loss 578.87421
Epoch 73: Val Loss 578.87158
Epoch 74: Val Loss 578.86890
Epoch 75: Val Loss 578.86633
Epoch 76: Val Loss 578.86371
Epoch 77: Val Loss 578.86108
Epoch 78: Val Loss 578.85846
Epoch 79: Val Loss 578.85590
Epoch 80: Val Loss 578.85327
Epoch 81: Val Loss 578.85077
Epoch 82: Val Loss 578.84814
Epoch 83: Val Loss 578.84552
Epoch 84: Val Loss 578.84302
Epoch 85: Val Loss 578.84052
Epoch 86: Val Loss 578.83789
Epoch 87: Val Loss 578.83533
Epoch 88: Val Loss 578.83276
Epoch 89: Val Loss 578.83014
Epoch 90: Val Loss 578.82758
Epoch 91: Val Loss 578.82501
Epoch 92: Val Loss 578.82239
Epoch 93: Val Loss 578.81982
Epoch 94: Val Loss 578.81726
Epoch 95: Val Loss 578.81464
Epoch 96: Val Loss 578.81213
Epoch 97: Val Loss 578.80945
Epoch 98: Val Loss 578.80701
Epoch 99: Val Loss 578.80444
{'MSE - mean': 578.4563741239967, 'MSE - std': 53.28939144267324, 'R2 - mean': -5.9451636126923635, 'R2 - std': 0.3634020480539317} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 578.4563741239967 and parameters: {'dim': 64, 'depth': 6, 'heads': 4, 'weight_decay': -5, 'learning_rate': -5, 'dropout': 0.3}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 551.31146
Epoch 1: Val Loss 551.30969
Epoch 2: Val Loss 551.30792
Epoch 3: Val Loss 551.30621
Epoch 4: Val Loss 551.30450
Epoch 5: Val Loss 551.30273
Epoch 6: Val Loss 551.30096
Epoch 7: Val Loss 551.29926
Epoch 8: Val Loss 551.29749
Epoch 9: Val Loss 551.29578
Epoch 10: Val Loss 551.29401
Epoch 11: Val Loss 551.29224
Epoch 12: Val Loss 551.29047
Epoch 13: Val Loss 551.28870
Epoch 14: Val Loss 551.28693
Epoch 15: Val Loss 551.28522
Epoch 16: Val Loss 551.28345
Epoch 17: Val Loss 551.28174
Epoch 18: Val Loss 551.27991
Epoch 19: Val Loss 551.27820
Epoch 20: Val Loss 551.27643
Epoch 21: Val Loss 551.27472
Epoch 22: Val Loss 551.27295
Epoch 23: Val Loss 551.27124
Epoch 24: Val Loss 551.26947
Epoch 25: Val Loss 551.26776
Epoch 26: Val Loss 551.26605
Epoch 27: Val Loss 551.26428
Epoch 28: Val Loss 551.26257
Epoch 29: Val Loss 551.26080
Epoch 30: Val Loss 551.25903
Epoch 31: Val Loss 551.25732
Epoch 32: Val Loss 551.25555
Epoch 33: Val Loss 551.25385
Epoch 34: Val Loss 551.25214
Epoch 35: Val Loss 551.25043
Epoch 36: Val Loss 551.24866
Epoch 37: Val Loss 551.24689
Epoch 38: Val Loss 551.24518
Epoch 39: Val Loss 551.24347
Epoch 40: Val Loss 551.24176
Epoch 41: Val Loss 551.23999
Epoch 42: Val Loss 551.23828
Epoch 43: Val Loss 551.23651
Epoch 44: Val Loss 551.23480
Epoch 45: Val Loss 551.23303
Epoch 46: Val Loss 551.23132
Epoch 47: Val Loss 551.22955
Epoch 48: Val Loss 551.22778
Epoch 49: Val Loss 551.22607
Epoch 50: Val Loss 551.22430
Epoch 51: Val Loss 551.22253
Epoch 52: Val Loss 551.22083
Epoch 53: Val Loss 551.21906
Epoch 54: Val Loss 551.21729
Epoch 55: Val Loss 551.21558
Epoch 56: Val Loss 551.21381
Epoch 57: Val Loss 551.21198
Epoch 58: Val Loss 551.21027
Epoch 59: Val Loss 551.20850
Epoch 60: Val Loss 551.20673
Epoch 61: Val Loss 551.20502
Epoch 62: Val Loss 551.20325
Epoch 63: Val Loss 551.20154
Epoch 64: Val Loss 551.19977
Epoch 65: Val Loss 551.19800
Epoch 66: Val Loss 551.19635
Epoch 67: Val Loss 551.19458
Epoch 68: Val Loss 551.19281
Epoch 69: Val Loss 551.19098
Epoch 70: Val Loss 551.18927
Epoch 71: Val Loss 551.18750
Epoch 72: Val Loss 551.18567
Epoch 73: Val Loss 551.18396
Epoch 74: Val Loss 551.18225
Epoch 75: Val Loss 551.18048
Epoch 76: Val Loss 551.17871
Epoch 77: Val Loss 551.17694
Epoch 78: Val Loss 551.17517
Epoch 79: Val Loss 551.17340
Epoch 80: Val Loss 551.17163
Epoch 81: Val Loss 551.16986
Epoch 82: Val Loss 551.16809
Epoch 83: Val Loss 551.16638
Epoch 84: Val Loss 551.16467
Epoch 85: Val Loss 551.16302
Epoch 86: Val Loss 551.16125
Epoch 87: Val Loss 551.15955
Epoch 88: Val Loss 551.15778
Epoch 89: Val Loss 551.15607
Epoch 90: Val Loss 551.15430
Epoch 91: Val Loss 551.15259
Epoch 92: Val Loss 551.15082
Epoch 93: Val Loss 551.14905
Epoch 94: Val Loss 551.14740
Epoch 95: Val Loss 551.14563
Epoch 96: Val Loss 551.14386
Epoch 97: Val Loss 551.14215
Epoch 98: Val Loss 551.14044
Epoch 99: Val Loss 551.13873
{'MSE - mean': 551.1386806376622, 'MSE - std': 0.0, 'R2 - mean': -6.381161513406677, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 635.76630
Epoch 1: Val Loss 635.75897
Epoch 2: Val Loss 635.75159
Epoch 3: Val Loss 635.74420
Epoch 4: Val Loss 635.73682
Epoch 5: Val Loss 635.72955
Epoch 6: Val Loss 635.72229
Epoch 7: Val Loss 635.71503
Epoch 8: Val Loss 635.70782
Epoch 9: Val Loss 635.70062
Epoch 10: Val Loss 635.69336
Epoch 11: Val Loss 635.68616
Epoch 12: Val Loss 635.67896
Epoch 13: Val Loss 635.67163
Epoch 14: Val Loss 635.66431
Epoch 15: Val Loss 635.65692
Epoch 16: Val Loss 635.64960
Epoch 17: Val Loss 635.64227
Epoch 18: Val Loss 635.63507
Epoch 19: Val Loss 635.62799
Epoch 20: Val Loss 635.62079
Epoch 21: Val Loss 635.61371
Epoch 22: Val Loss 635.60651
Epoch 23: Val Loss 635.59930
Epoch 24: Val Loss 635.59204
Epoch 25: Val Loss 635.58502
Epoch 26: Val Loss 635.57794
Epoch 27: Val Loss 635.57098
Epoch 28: Val Loss 635.56396
Epoch 29: Val Loss 635.55682
Epoch 30: Val Loss 635.54962
Epoch 31: Val Loss 635.54248
Epoch 32: Val Loss 635.53534
Epoch 33: Val Loss 635.52808
Epoch 34: Val Loss 635.52081
Epoch 35: Val Loss 635.51361
Epoch 36: Val Loss 635.50635
Epoch 37: Val Loss 635.49908
Epoch 38: Val Loss 635.49182
Epoch 39: Val Loss 635.48444
Epoch 40: Val Loss 635.47723
Epoch 41: Val Loss 635.46997
Epoch 42: Val Loss 635.46252
Epoch 43: Val Loss 635.45526
Epoch 44: Val Loss 635.44800
Epoch 45: Val Loss 635.44067
Epoch 46: Val Loss 635.43341
Epoch 47: Val Loss 635.42621
Epoch 48: Val Loss 635.41901
Epoch 49: Val Loss 635.41174
Epoch 50: Val Loss 635.40466
Epoch 51: Val Loss 635.39752
Epoch 52: Val Loss 635.39020
Epoch 53: Val Loss 635.38293
Epoch 54: Val Loss 635.37585
Epoch 55: Val Loss 635.36871
Epoch 56: Val Loss 635.36169
Epoch 57: Val Loss 635.35468
Epoch 58: Val Loss 635.34753
Epoch 59: Val Loss 635.34021
Epoch 60: Val Loss 635.33289
Epoch 61: Val Loss 635.32556
Epoch 62: Val Loss 635.31836
Epoch 63: Val Loss 635.31104
Epoch 64: Val Loss 635.30365
Epoch 65: Val Loss 635.29614
Epoch 66: Val Loss 635.28888
Epoch 67: Val Loss 635.28162
Epoch 68: Val Loss 635.27448
Epoch 69: Val Loss 635.26733
Epoch 70: Val Loss 635.26013
Epoch 71: Val Loss 635.25305
Epoch 72: Val Loss 635.24579
Epoch 73: Val Loss 635.23871
Epoch 74: Val Loss 635.23157
Epoch 75: Val Loss 635.22461
Epoch 76: Val Loss 635.21747
Epoch 77: Val Loss 635.21045
Epoch 78: Val Loss 635.20349
Epoch 79: Val Loss 635.19647
Epoch 80: Val Loss 635.18945
Epoch 81: Val Loss 635.18243
Epoch 82: Val Loss 635.17535
Epoch 83: Val Loss 635.16833
Epoch 84: Val Loss 635.16132
Epoch 85: Val Loss 635.15430
Epoch 86: Val Loss 635.14709
Epoch 87: Val Loss 635.14008
Epoch 88: Val Loss 635.13306
Epoch 89: Val Loss 635.12622
Epoch 90: Val Loss 635.11926
Epoch 91: Val Loss 635.11237
Epoch 92: Val Loss 635.10559
Epoch 93: Val Loss 635.09869
Epoch 94: Val Loss 635.09186
Epoch 95: Val Loss 635.08527
Epoch 96: Val Loss 635.07880
Epoch 97: Val Loss 635.07214
Epoch 98: Val Loss 635.06549
Epoch 99: Val Loss 635.05884
{'MSE - mean': 593.0987729718986, 'MSE - std': 41.96009233423632, 'R2 - mean': -6.633080799113727, 'R2 - std': 0.25191928570705} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 704.48218
Epoch 1: Val Loss 704.47546
Epoch 2: Val Loss 704.46875
Epoch 3: Val Loss 704.46191
Epoch 4: Val Loss 704.45520
Epoch 5: Val Loss 704.44849
Epoch 6: Val Loss 704.44196
Epoch 7: Val Loss 704.43542
Epoch 8: Val Loss 704.42883
Epoch 9: Val Loss 704.42212
Epoch 10: Val Loss 704.41547
Epoch 11: Val Loss 704.40887
Epoch 12: Val Loss 704.40222
Epoch 13: Val Loss 704.39563
Epoch 14: Val Loss 704.38916
Epoch 15: Val Loss 704.38263
Epoch 16: Val Loss 704.37628
Epoch 17: Val Loss 704.36987
Epoch 18: Val Loss 704.36353
Epoch 19: Val Loss 704.35712
Epoch 20: Val Loss 704.35065
Epoch 21: Val Loss 704.34418
Epoch 22: Val Loss 704.33789
Epoch 23: Val Loss 704.33154
Epoch 24: Val Loss 704.32520
Epoch 25: Val Loss 704.31885
Epoch 26: Val Loss 704.31256
Epoch 27: Val Loss 704.30609
Epoch 28: Val Loss 704.29950
Epoch 29: Val Loss 704.29285
Epoch 30: Val Loss 704.28619
Epoch 31: Val Loss 704.27948
Epoch 32: Val Loss 704.27295
Epoch 33: Val Loss 704.26648
Epoch 34: Val Loss 704.25989
Epoch 35: Val Loss 704.25342
Epoch 36: Val Loss 704.24677
Epoch 37: Val Loss 704.23999
Epoch 38: Val Loss 704.23328
Epoch 39: Val Loss 704.22668
Epoch 40: Val Loss 704.22003
Epoch 41: Val Loss 704.21332
Epoch 42: Val Loss 704.20654
Epoch 43: Val Loss 704.19995
Epoch 44: Val Loss 704.19330
Epoch 45: Val Loss 704.18671
Epoch 46: Val Loss 704.18030
Epoch 47: Val Loss 704.17383
Epoch 48: Val Loss 704.16748
Epoch 49: Val Loss 704.16113
Epoch 50: Val Loss 704.15466
Epoch 51: Val Loss 704.14801
Epoch 52: Val Loss 704.14154
Epoch 53: Val Loss 704.13483
Epoch 54: Val Loss 704.12842
Epoch 55: Val Loss 704.12213
Epoch 56: Val Loss 704.11597
Epoch 57: Val Loss 704.10950
Epoch 58: Val Loss 704.10315
Epoch 59: Val Loss 704.09674
Epoch 60: Val Loss 704.09027
Epoch 61: Val Loss 704.08374
Epoch 62: Val Loss 704.07721
Epoch 63: Val Loss 704.07068
Epoch 64: Val Loss 704.06433
Epoch 65: Val Loss 704.05817
Epoch 66: Val Loss 704.05188
Epoch 67: Val Loss 704.04578
Epoch 68: Val Loss 704.03943
Epoch 69: Val Loss 704.03308
Epoch 70: Val Loss 704.02673
Epoch 71: Val Loss 704.02039
Epoch 72: Val Loss 704.01404
Epoch 73: Val Loss 704.00787
Epoch 74: Val Loss 704.00171
Epoch 75: Val Loss 703.99548
Epoch 76: Val Loss 703.98926
Epoch 77: Val Loss 703.98303
Epoch 78: Val Loss 703.97662
Epoch 79: Val Loss 703.97028
Epoch 80: Val Loss 703.96405
Epoch 81: Val Loss 703.95770
Epoch 82: Val Loss 703.95142
Epoch 83: Val Loss 703.94525
Epoch 84: Val Loss 703.93896
Epoch 85: Val Loss 703.93268
Epoch 86: Val Loss 703.92633
Epoch 87: Val Loss 703.91992
Epoch 88: Val Loss 703.91351
Epoch 89: Val Loss 703.90717
Epoch 90: Val Loss 703.90082
Epoch 91: Val Loss 703.89441
Epoch 92: Val Loss 703.88806
Epoch 93: Val Loss 703.88165
Epoch 94: Val Loss 703.87524
Epoch 95: Val Loss 703.86896
Epoch 96: Val Loss 703.86267
Epoch 97: Val Loss 703.85651
Epoch 98: Val Loss 703.85022
Epoch 99: Val Loss 703.84406
{'MSE - mean': 630.0138782959615, 'MSE - std': 62.443704523662014, 'R2 - mean': -6.547124370152543, 'R2 - std': 0.2389265571264216} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 559.40277
Epoch 1: Val Loss 559.39679
Epoch 2: Val Loss 559.39087
Epoch 3: Val Loss 559.38495
Epoch 4: Val Loss 559.37891
Epoch 5: Val Loss 559.37292
Epoch 6: Val Loss 559.36688
Epoch 7: Val Loss 559.36102
Epoch 8: Val Loss 559.35516
Epoch 9: Val Loss 559.34918
Epoch 10: Val Loss 559.34326
Epoch 11: Val Loss 559.33734
Epoch 12: Val Loss 559.33136
Epoch 13: Val Loss 559.32544
Epoch 14: Val Loss 559.31952
Epoch 15: Val Loss 559.31360
Epoch 16: Val Loss 559.30762
Epoch 17: Val Loss 559.30170
Epoch 18: Val Loss 559.29572
Epoch 19: Val Loss 559.28973
Epoch 20: Val Loss 559.28381
Epoch 21: Val Loss 559.27777
Epoch 22: Val Loss 559.27185
Epoch 23: Val Loss 559.26587
Epoch 24: Val Loss 559.25995
Epoch 25: Val Loss 559.25403
Epoch 26: Val Loss 559.24811
Epoch 27: Val Loss 559.24219
Epoch 28: Val Loss 559.23633
Epoch 29: Val Loss 559.23047
Epoch 30: Val Loss 559.22455
Epoch 31: Val Loss 559.21857
Epoch 32: Val Loss 559.21265
Epoch 33: Val Loss 559.20673
Epoch 34: Val Loss 559.20093
Epoch 35: Val Loss 559.19495
Epoch 36: Val Loss 559.18903
Epoch 37: Val Loss 559.18323
Epoch 38: Val Loss 559.17731
Epoch 39: Val Loss 559.17151
Epoch 40: Val Loss 559.16565
Epoch 41: Val Loss 559.15979
Epoch 42: Val Loss 559.15399
Epoch 43: Val Loss 559.14813
Epoch 44: Val Loss 559.14233
Epoch 45: Val Loss 559.13641
Epoch 46: Val Loss 559.13062
Epoch 47: Val Loss 559.12476
Epoch 48: Val Loss 559.11896
Epoch 49: Val Loss 559.11310
Epoch 50: Val Loss 559.10718
Epoch 51: Val Loss 559.10126
Epoch 52: Val Loss 559.09540
Epoch 53: Val Loss 559.08948
Epoch 54: Val Loss 559.08368
Epoch 55: Val Loss 559.07782
Epoch 56: Val Loss 559.07202
Epoch 57: Val Loss 559.06610
Epoch 58: Val Loss 559.06018
Epoch 59: Val Loss 559.05438
Epoch 60: Val Loss 559.04846
Epoch 61: Val Loss 559.04266
Epoch 62: Val Loss 559.03687
Epoch 63: Val Loss 559.03119
Epoch 64: Val Loss 559.02527
Epoch 65: Val Loss 559.01935
Epoch 66: Val Loss 559.01349
Epoch 67: Val Loss 559.00763
Epoch 68: Val Loss 559.00189
Epoch 69: Val Loss 558.99603
Epoch 70: Val Loss 558.99017
Epoch 71: Val Loss 558.98438
Epoch 72: Val Loss 558.97858
Epoch 73: Val Loss 558.97278
Epoch 74: Val Loss 558.96692
Epoch 75: Val Loss 558.96106
Epoch 76: Val Loss 558.95520
Epoch 77: Val Loss 558.94922
Epoch 78: Val Loss 558.94348
Epoch 79: Val Loss 558.93762
Epoch 80: Val Loss 558.93182
Epoch 81: Val Loss 558.92615
Epoch 82: Val Loss 558.92041
Epoch 83: Val Loss 558.91461
Epoch 84: Val Loss 558.90881
Epoch 85: Val Loss 558.90302
Epoch 86: Val Loss 558.89722
Epoch 87: Val Loss 558.89142
Epoch 88: Val Loss 558.88556
Epoch 89: Val Loss 558.87976
Epoch 90: Val Loss 558.87390
Epoch 91: Val Loss 558.86810
Epoch 92: Val Loss 558.86230
Epoch 93: Val Loss 558.85645
Epoch 94: Val Loss 558.85052
Epoch 95: Val Loss 558.84467
Epoch 96: Val Loss 558.83868
Epoch 97: Val Loss 558.83282
Epoch 98: Val Loss 558.82697
Epoch 99: Val Loss 558.82117
{'MSE - mean': 612.2156932794696, 'MSE - std': 62.247396295930564, 'R2 - mean': -6.511834172382494, 'R2 - std': 0.21575592448681755} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 592.39551
Epoch 1: Val Loss 592.38489
Epoch 2: Val Loss 592.37427
Epoch 3: Val Loss 592.36365
Epoch 4: Val Loss 592.35303
Epoch 5: Val Loss 592.34247
Epoch 6: Val Loss 592.33185
Epoch 7: Val Loss 592.32129
Epoch 8: Val Loss 592.31079
Epoch 9: Val Loss 592.30017
Epoch 10: Val Loss 592.28943
Epoch 11: Val Loss 592.27869
Epoch 12: Val Loss 592.26794
Epoch 13: Val Loss 592.25732
Epoch 14: Val Loss 592.24683
Epoch 15: Val Loss 592.23633
Epoch 16: Val Loss 592.22577
Epoch 17: Val Loss 592.21515
Epoch 18: Val Loss 592.20441
Epoch 19: Val Loss 592.19342
Epoch 20: Val Loss 592.18262
Epoch 21: Val Loss 592.17194
Epoch 22: Val Loss 592.16125
Epoch 23: Val Loss 592.15057
Epoch 24: Val Loss 592.13983
Epoch 25: Val Loss 592.12909
Epoch 26: Val Loss 592.11853
Epoch 27: Val Loss 592.10797
Epoch 28: Val Loss 592.09753
Epoch 29: Val Loss 592.08685
Epoch 30: Val Loss 592.07623
Epoch 31: Val Loss 592.06573
Epoch 32: Val Loss 592.05542
Epoch 33: Val Loss 592.04492
Epoch 34: Val Loss 592.03436
Epoch 35: Val Loss 592.02380
Epoch 36: Val Loss 592.01331
Epoch 37: Val Loss 592.00275
Epoch 38: Val Loss 591.99231
Epoch 39: Val Loss 591.98175
Epoch 40: Val Loss 591.97119
Epoch 41: Val Loss 591.96057
Epoch 42: Val Loss 591.95001
Epoch 43: Val Loss 591.93945
Epoch 44: Val Loss 591.92889
Epoch 45: Val Loss 591.91809
Epoch 46: Val Loss 591.90723
Epoch 47: Val Loss 591.89648
Epoch 48: Val Loss 591.88580
Epoch 49: Val Loss 591.87494
Epoch 50: Val Loss 591.86414
Epoch 51: Val Loss 591.85339
Epoch 52: Val Loss 591.84271
Epoch 53: Val Loss 591.83203
Epoch 54: Val Loss 591.82147
Epoch 55: Val Loss 591.81104
Epoch 56: Val Loss 591.80042
Epoch 57: Val Loss 591.78979
Epoch 58: Val Loss 591.77924
Epoch 59: Val Loss 591.76874
Epoch 60: Val Loss 591.75806
Epoch 61: Val Loss 591.74738
Epoch 62: Val Loss 591.73669
Epoch 63: Val Loss 591.72583
Epoch 64: Val Loss 591.71509
Epoch 65: Val Loss 591.70422
Epoch 66: Val Loss 591.69330
Epoch 67: Val Loss 591.68237
Epoch 68: Val Loss 591.67151
Epoch 69: Val Loss 591.66058
Epoch 70: Val Loss 591.64966
Epoch 71: Val Loss 591.63867
Epoch 72: Val Loss 591.62762
Epoch 73: Val Loss 591.61639
Epoch 74: Val Loss 591.60516
Epoch 75: Val Loss 591.59399
Epoch 76: Val Loss 591.58301
Epoch 77: Val Loss 591.57190
Epoch 78: Val Loss 591.56085
Epoch 79: Val Loss 591.54980
Epoch 80: Val Loss 591.53870
Epoch 81: Val Loss 591.52771
Epoch 82: Val Loss 591.51666
Epoch 83: Val Loss 591.50580
Epoch 84: Val Loss 591.49506
Epoch 85: Val Loss 591.48431
Epoch 86: Val Loss 591.47345
Epoch 87: Val Loss 591.46265
Epoch 88: Val Loss 591.45184
Epoch 89: Val Loss 591.44110
Epoch 90: Val Loss 591.43036
Epoch 91: Val Loss 591.41949
Epoch 92: Val Loss 591.40863
Epoch 93: Val Loss 591.39758
Epoch 94: Val Loss 591.38647
Epoch 95: Val Loss 591.37555
Epoch 96: Val Loss 591.36450
Epoch 97: Val Loss 591.35370
Epoch 98: Val Loss 591.34290
Epoch 99: Val Loss 591.33185
{'MSE - mean': 608.0389147376901, 'MSE - std': 56.29895729448623, 'R2 - mean': -6.305161472958028, 'R2 - std': 0.45617421442965433} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 608.0389147376901 and parameters: {'dim': 128, 'depth': 12, 'heads': 4, 'weight_decay': -3, 'learning_rate': -5, 'dropout': 0.2}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 513.42749
Epoch 1: Val Loss 513.41888
Epoch 2: Val Loss 513.41046
Epoch 3: Val Loss 513.40204
Epoch 4: Val Loss 513.39380
Epoch 5: Val Loss 513.38562
Epoch 6: Val Loss 513.37726
Epoch 7: Val Loss 513.36902
Epoch 8: Val Loss 513.36072
Epoch 9: Val Loss 513.35242
Epoch 10: Val Loss 513.34406
Epoch 11: Val Loss 513.33575
Epoch 12: Val Loss 513.32745
Epoch 13: Val Loss 513.31903
Epoch 14: Val Loss 513.31061
Epoch 15: Val Loss 513.30219
Epoch 16: Val Loss 513.29388
Epoch 17: Val Loss 513.28564
Epoch 18: Val Loss 513.27734
Epoch 19: Val Loss 513.26910
Epoch 20: Val Loss 513.26080
Epoch 21: Val Loss 513.25262
Epoch 22: Val Loss 513.24438
Epoch 23: Val Loss 513.23621
Epoch 24: Val Loss 513.22791
Epoch 25: Val Loss 513.21979
Epoch 26: Val Loss 513.21143
Epoch 27: Val Loss 513.20306
Epoch 28: Val Loss 513.19476
Epoch 29: Val Loss 513.18628
Epoch 30: Val Loss 513.17792
Epoch 31: Val Loss 513.16956
Epoch 32: Val Loss 513.16119
Epoch 33: Val Loss 513.15289
Epoch 34: Val Loss 513.14471
Epoch 35: Val Loss 513.13654
Epoch 36: Val Loss 513.12823
Epoch 37: Val Loss 513.12006
Epoch 38: Val Loss 513.11200
Epoch 39: Val Loss 513.10388
Epoch 40: Val Loss 513.09583
Epoch 41: Val Loss 513.08765
Epoch 42: Val Loss 513.07928
Epoch 43: Val Loss 513.07104
Epoch 44: Val Loss 513.06274
Epoch 45: Val Loss 513.05438
Epoch 46: Val Loss 513.04626
Epoch 47: Val Loss 513.03815
Epoch 48: Val Loss 513.03009
Epoch 49: Val Loss 513.02191
Epoch 50: Val Loss 513.01361
Epoch 51: Val Loss 513.00543
Epoch 52: Val Loss 512.99744
Epoch 53: Val Loss 512.98914
Epoch 54: Val Loss 512.98090
Epoch 55: Val Loss 512.97253
Epoch 56: Val Loss 512.96423
Epoch 57: Val Loss 512.95599
Epoch 58: Val Loss 512.94751
Epoch 59: Val Loss 512.93927
Epoch 60: Val Loss 512.93091
Epoch 61: Val Loss 512.92236
Epoch 62: Val Loss 512.91376
Epoch 63: Val Loss 512.90540
Epoch 64: Val Loss 512.89709
Epoch 65: Val Loss 512.88873
Epoch 66: Val Loss 512.88037
Epoch 67: Val Loss 512.87201
Epoch 68: Val Loss 512.86353
Epoch 69: Val Loss 512.85510
Epoch 70: Val Loss 512.84692
Epoch 71: Val Loss 512.83856
Epoch 72: Val Loss 512.83014
Epoch 73: Val Loss 512.82166
Epoch 74: Val Loss 512.81311
Epoch 75: Val Loss 512.80469
Epoch 76: Val Loss 512.79620
Epoch 77: Val Loss 512.78772
Epoch 78: Val Loss 512.77936
Epoch 79: Val Loss 512.77100
Epoch 80: Val Loss 512.76270
Epoch 81: Val Loss 512.75427
Epoch 82: Val Loss 512.74579
Epoch 83: Val Loss 512.73743
Epoch 84: Val Loss 512.72900
Epoch 85: Val Loss 512.72076
Epoch 86: Val Loss 512.71240
Epoch 87: Val Loss 512.70392
Epoch 88: Val Loss 512.69550
Epoch 89: Val Loss 512.68707
Epoch 90: Val Loss 512.67865
Epoch 91: Val Loss 512.67010
Epoch 92: Val Loss 512.66150
Epoch 93: Val Loss 512.65289
Epoch 94: Val Loss 512.64441
Epoch 95: Val Loss 512.63586
Epoch 96: Val Loss 512.62750
Epoch 97: Val Loss 512.61914
Epoch 98: Val Loss 512.61084
Epoch 99: Val Loss 512.60223
{'MSE - mean': 512.6022547533275, 'MSE - std': 0.0, 'R2 - mean': -5.8650598613277465, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 605.37366
Epoch 1: Val Loss 605.36694
Epoch 2: Val Loss 605.36023
Epoch 3: Val Loss 605.35358
Epoch 4: Val Loss 605.34705
Epoch 5: Val Loss 605.34052
Epoch 6: Val Loss 605.33392
Epoch 7: Val Loss 605.32733
Epoch 8: Val Loss 605.32068
Epoch 9: Val Loss 605.31409
Epoch 10: Val Loss 605.30762
Epoch 11: Val Loss 605.30115
Epoch 12: Val Loss 605.29456
Epoch 13: Val Loss 605.28802
Epoch 14: Val Loss 605.28156
Epoch 15: Val Loss 605.27515
Epoch 16: Val Loss 605.26868
Epoch 17: Val Loss 605.26208
Epoch 18: Val Loss 605.25549
Epoch 19: Val Loss 605.24872
Epoch 20: Val Loss 605.24194
Epoch 21: Val Loss 605.23517
Epoch 22: Val Loss 605.22821
Epoch 23: Val Loss 605.22131
Epoch 24: Val Loss 605.21448
Epoch 25: Val Loss 605.20746
Epoch 26: Val Loss 605.20050
Epoch 27: Val Loss 605.19366
Epoch 28: Val Loss 605.18695
Epoch 29: Val Loss 605.18018
Epoch 30: Val Loss 605.17340
Epoch 31: Val Loss 605.16656
Epoch 32: Val Loss 605.15967
Epoch 33: Val Loss 605.15271
Epoch 34: Val Loss 605.14581
Epoch 35: Val Loss 605.13916
Epoch 36: Val Loss 605.13263
Epoch 37: Val Loss 605.12598
Epoch 38: Val Loss 605.11938
Epoch 39: Val Loss 605.11279
Epoch 40: Val Loss 605.10614
Epoch 41: Val Loss 605.09949
Epoch 42: Val Loss 605.09283
Epoch 43: Val Loss 605.08624
Epoch 44: Val Loss 605.07965
Epoch 45: Val Loss 605.07306
Epoch 46: Val Loss 605.06659
Epoch 47: Val Loss 605.05994
Epoch 48: Val Loss 605.05334
Epoch 49: Val Loss 605.04700
Epoch 50: Val Loss 605.04047
Epoch 51: Val Loss 605.03394
Epoch 52: Val Loss 605.02734
Epoch 53: Val Loss 605.02094
Epoch 54: Val Loss 605.01453
Epoch 55: Val Loss 605.00793
Epoch 56: Val Loss 605.00092
Epoch 57: Val Loss 604.99408
Epoch 58: Val Loss 604.98737
Epoch 59: Val Loss 604.98059
Epoch 60: Val Loss 604.97400
Epoch 61: Val Loss 604.96747
Epoch 62: Val Loss 604.96088
Epoch 63: Val Loss 604.95422
Epoch 64: Val Loss 604.94763
Epoch 65: Val Loss 604.94104
Epoch 66: Val Loss 604.93427
Epoch 67: Val Loss 604.92743
Epoch 68: Val Loss 604.92072
Epoch 69: Val Loss 604.91394
Epoch 70: Val Loss 604.90692
Epoch 71: Val Loss 604.89996
Epoch 72: Val Loss 604.89325
Epoch 73: Val Loss 604.88641
Epoch 74: Val Loss 604.87958
Epoch 75: Val Loss 604.87280
Epoch 76: Val Loss 604.86603
Epoch 77: Val Loss 604.85931
Epoch 78: Val Loss 604.85254
Epoch 79: Val Loss 604.84601
Epoch 80: Val Loss 604.83954
Epoch 81: Val Loss 604.83301
Epoch 82: Val Loss 604.82648
Epoch 83: Val Loss 604.81970
Epoch 84: Val Loss 604.81293
Epoch 85: Val Loss 604.80609
Epoch 86: Val Loss 604.79926
Epoch 87: Val Loss 604.79242
Epoch 88: Val Loss 604.78552
Epoch 89: Val Loss 604.77850
Epoch 90: Val Loss 604.77155
Epoch 91: Val Loss 604.76459
Epoch 92: Val Loss 604.75775
Epoch 93: Val Loss 604.75073
Epoch 94: Val Loss 604.74359
Epoch 95: Val Loss 604.73657
Epoch 96: Val Loss 604.72974
Epoch 97: Val Loss 604.72284
Epoch 98: Val Loss 604.71594
Epoch 99: Val Loss 604.70917
{'MSE - mean': 558.6557046591308, 'MSE - std': 46.053449905803234, 'R2 - mean': -6.186616359122668, 'R2 - std': 0.32155649779492146} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 701.06250
Epoch 1: Val Loss 701.05365
Epoch 2: Val Loss 701.04480
Epoch 3: Val Loss 701.03595
Epoch 4: Val Loss 701.02704
Epoch 5: Val Loss 701.01825
Epoch 6: Val Loss 701.00958
Epoch 7: Val Loss 701.00092
Epoch 8: Val Loss 700.99219
Epoch 9: Val Loss 700.98346
Epoch 10: Val Loss 700.97461
Epoch 11: Val Loss 700.96564
Epoch 12: Val Loss 700.95667
Epoch 13: Val Loss 700.94775
Epoch 14: Val Loss 700.93903
Epoch 15: Val Loss 700.93011
Epoch 16: Val Loss 700.92126
Epoch 17: Val Loss 700.91229
Epoch 18: Val Loss 700.90332
Epoch 19: Val Loss 700.89447
Epoch 20: Val Loss 700.88556
Epoch 21: Val Loss 700.87671
Epoch 22: Val Loss 700.86774
Epoch 23: Val Loss 700.85883
Epoch 24: Val Loss 700.85022
Epoch 25: Val Loss 700.84137
Epoch 26: Val Loss 700.83246
Epoch 27: Val Loss 700.82379
Epoch 28: Val Loss 700.81494
Epoch 29: Val Loss 700.80615
Epoch 30: Val Loss 700.79779
Epoch 31: Val Loss 700.78912
Epoch 32: Val Loss 700.78033
Epoch 33: Val Loss 700.77148
Epoch 34: Val Loss 700.76257
Epoch 35: Val Loss 700.75372
Epoch 36: Val Loss 700.74463
Epoch 37: Val Loss 700.73547
Epoch 38: Val Loss 700.72650
Epoch 39: Val Loss 700.71741
Epoch 40: Val Loss 700.70856
Epoch 41: Val Loss 700.69965
Epoch 42: Val Loss 700.69080
Epoch 43: Val Loss 700.68176
Epoch 44: Val Loss 700.67249
Epoch 45: Val Loss 700.66284
Epoch 46: Val Loss 700.65350
Epoch 47: Val Loss 700.64404
Epoch 48: Val Loss 700.63440
Epoch 49: Val Loss 700.62482
Epoch 50: Val Loss 700.61554
Epoch 51: Val Loss 700.60645
Epoch 52: Val Loss 700.59729
Epoch 53: Val Loss 700.58801
Epoch 54: Val Loss 700.57867
Epoch 55: Val Loss 700.56909
Epoch 56: Val Loss 700.55939
Epoch 57: Val Loss 700.55011
Epoch 58: Val Loss 700.54102
Epoch 59: Val Loss 700.53168
Epoch 60: Val Loss 700.52228
Epoch 61: Val Loss 700.51276
Epoch 62: Val Loss 700.50330
Epoch 63: Val Loss 700.49377
Epoch 64: Val Loss 700.48407
Epoch 65: Val Loss 700.47461
Epoch 66: Val Loss 700.46521
Epoch 67: Val Loss 700.45557
Epoch 68: Val Loss 700.44617
Epoch 69: Val Loss 700.43665
Epoch 70: Val Loss 700.42712
Epoch 71: Val Loss 700.41779
Epoch 72: Val Loss 700.40863
Epoch 73: Val Loss 700.39917
Epoch 74: Val Loss 700.39001
Epoch 75: Val Loss 700.38043
Epoch 76: Val Loss 700.37109
Epoch 77: Val Loss 700.36169
Epoch 78: Val Loss 700.35242
Epoch 79: Val Loss 700.34314
Epoch 80: Val Loss 700.33386
Epoch 81: Val Loss 700.32471
Epoch 82: Val Loss 700.31512
Epoch 83: Val Loss 700.30554
Epoch 84: Val Loss 700.29608
Epoch 85: Val Loss 700.28668
Epoch 86: Val Loss 700.27716
Epoch 87: Val Loss 700.26764
Epoch 88: Val Loss 700.25806
Epoch 89: Val Loss 700.24854
Epoch 90: Val Loss 700.23871
Epoch 91: Val Loss 700.22900
Epoch 92: Val Loss 700.21930
Epoch 93: Val Loss 700.20923
Epoch 94: Val Loss 700.19904
Epoch 95: Val Loss 700.18890
Epoch 96: Val Loss 700.17847
Epoch 97: Val Loss 700.16797
Epoch 98: Val Loss 700.15802
Epoch 99: Val Loss 700.14795
{'MSE - mean': 605.8197932344452, 'MSE - std': 76.56924535628573, 'R2 - mean': -6.236571516914059, 'R2 - std': 0.27188862259391405} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 546.66382
Epoch 1: Val Loss 546.65375
Epoch 2: Val Loss 546.64374
Epoch 3: Val Loss 546.63367
Epoch 4: Val Loss 546.62366
Epoch 5: Val Loss 546.61365
Epoch 6: Val Loss 546.60376
Epoch 7: Val Loss 546.59381
Epoch 8: Val Loss 546.58398
Epoch 9: Val Loss 546.57416
Epoch 10: Val Loss 546.56433
Epoch 11: Val Loss 546.55450
Epoch 12: Val Loss 546.54456
Epoch 13: Val Loss 546.53473
Epoch 14: Val Loss 546.52496
Epoch 15: Val Loss 546.51514
Epoch 16: Val Loss 546.50537
Epoch 17: Val Loss 546.49561
Epoch 18: Val Loss 546.48578
Epoch 19: Val Loss 546.47583
Epoch 20: Val Loss 546.46594
Epoch 21: Val Loss 546.45605
Epoch 22: Val Loss 546.44623
Epoch 23: Val Loss 546.43628
Epoch 24: Val Loss 546.42633
Epoch 25: Val Loss 546.41650
Epoch 26: Val Loss 546.40680
Epoch 27: Val Loss 546.39709
Epoch 28: Val Loss 546.38739
Epoch 29: Val Loss 546.37769
Epoch 30: Val Loss 546.36786
Epoch 31: Val Loss 546.35822
Epoch 32: Val Loss 546.34863
Epoch 33: Val Loss 546.33887
Epoch 34: Val Loss 546.32910
Epoch 35: Val Loss 546.31921
Epoch 36: Val Loss 546.30939
Epoch 37: Val Loss 546.29962
Epoch 38: Val Loss 546.28986
Epoch 39: Val Loss 546.28015
Epoch 40: Val Loss 546.27039
Epoch 41: Val Loss 546.26080
Epoch 42: Val Loss 546.25110
Epoch 43: Val Loss 546.24127
Epoch 44: Val Loss 546.23132
Epoch 45: Val Loss 546.22156
Epoch 46: Val Loss 546.21179
Epoch 47: Val Loss 546.20197
Epoch 48: Val Loss 546.19214
Epoch 49: Val Loss 546.18225
Epoch 50: Val Loss 546.17249
Epoch 51: Val Loss 546.16284
Epoch 52: Val Loss 546.15320
Epoch 53: Val Loss 546.14343
Epoch 54: Val Loss 546.13379
Epoch 55: Val Loss 546.12427
Epoch 56: Val Loss 546.11462
Epoch 57: Val Loss 546.10510
Epoch 58: Val Loss 546.09540
Epoch 59: Val Loss 546.08588
Epoch 60: Val Loss 546.07642
Epoch 61: Val Loss 546.06696
Epoch 62: Val Loss 546.05737
Epoch 63: Val Loss 546.04773
Epoch 64: Val Loss 546.03821
Epoch 65: Val Loss 546.02863
Epoch 66: Val Loss 546.01917
Epoch 67: Val Loss 546.00952
Epoch 68: Val Loss 546.00000
Epoch 69: Val Loss 545.99042
Epoch 70: Val Loss 545.98090
Epoch 71: Val Loss 545.97137
Epoch 72: Val Loss 545.96185
Epoch 73: Val Loss 545.95227
Epoch 74: Val Loss 545.94269
Epoch 75: Val Loss 545.93304
Epoch 76: Val Loss 545.92358
Epoch 77: Val Loss 545.91406
Epoch 78: Val Loss 545.90454
Epoch 79: Val Loss 545.89508
Epoch 80: Val Loss 545.88568
Epoch 81: Val Loss 545.87628
Epoch 82: Val Loss 545.86688
Epoch 83: Val Loss 545.85748
Epoch 84: Val Loss 545.84821
Epoch 85: Val Loss 545.83887
Epoch 86: Val Loss 545.82947
Epoch 87: Val Loss 545.82001
Epoch 88: Val Loss 545.81049
Epoch 89: Val Loss 545.80090
Epoch 90: Val Loss 545.79114
Epoch 91: Val Loss 545.78149
Epoch 92: Val Loss 545.77185
Epoch 93: Val Loss 545.76233
Epoch 94: Val Loss 545.75287
Epoch 95: Val Loss 545.74341
Epoch 96: Val Loss 545.73395
Epoch 97: Val Loss 545.72461
Epoch 98: Val Loss 545.71527
Epoch 99: Val Loss 545.70593
{'MSE - mean': 590.7913352072585, 'MSE - std': 71.23693320776837, 'R2 - mean': -6.235466217375416, 'R2 - std': 0.23547023672477643} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 561.07892
Epoch 1: Val Loss 561.07343
Epoch 2: Val Loss 561.06793
Epoch 3: Val Loss 561.06244
Epoch 4: Val Loss 561.05688
Epoch 5: Val Loss 561.05133
Epoch 6: Val Loss 561.04578
Epoch 7: Val Loss 561.04022
Epoch 8: Val Loss 561.03467
Epoch 9: Val Loss 561.02899
Epoch 10: Val Loss 561.02344
Epoch 11: Val Loss 561.01794
Epoch 12: Val Loss 561.01245
Epoch 13: Val Loss 561.00696
Epoch 14: Val Loss 561.00146
Epoch 15: Val Loss 560.99597
Epoch 16: Val Loss 560.99054
Epoch 17: Val Loss 560.98505
Epoch 18: Val Loss 560.97955
Epoch 19: Val Loss 560.97418
Epoch 20: Val Loss 560.96869
Epoch 21: Val Loss 560.96320
Epoch 22: Val Loss 560.95770
Epoch 23: Val Loss 560.95221
Epoch 24: Val Loss 560.94678
Epoch 25: Val Loss 560.94135
Epoch 26: Val Loss 560.93585
Epoch 27: Val Loss 560.93048
Epoch 28: Val Loss 560.92493
Epoch 29: Val Loss 560.91956
Epoch 30: Val Loss 560.91412
Epoch 31: Val Loss 560.90869
Epoch 32: Val Loss 560.90332
Epoch 33: Val Loss 560.89789
Epoch 34: Val Loss 560.89246
Epoch 35: Val Loss 560.88708
Epoch 36: Val Loss 560.88159
Epoch 37: Val Loss 560.87610
Epoch 38: Val Loss 560.87067
Epoch 39: Val Loss 560.86523
Epoch 40: Val Loss 560.85974
Epoch 41: Val Loss 560.85431
Epoch 42: Val Loss 560.84882
Epoch 43: Val Loss 560.84344
Epoch 44: Val Loss 560.83795
Epoch 45: Val Loss 560.83258
Epoch 46: Val Loss 560.82709
Epoch 47: Val Loss 560.82172
Epoch 48: Val Loss 560.81628
Epoch 49: Val Loss 560.81073
Epoch 50: Val Loss 560.80530
Epoch 51: Val Loss 560.79980
Epoch 52: Val Loss 560.79437
Epoch 53: Val Loss 560.78888
Epoch 54: Val Loss 560.78339
Epoch 55: Val Loss 560.77795
Epoch 56: Val Loss 560.77252
Epoch 57: Val Loss 560.76709
Epoch 58: Val Loss 560.76160
Epoch 59: Val Loss 560.75604
Epoch 60: Val Loss 560.75055
Epoch 61: Val Loss 560.74512
Epoch 62: Val Loss 560.73969
Epoch 63: Val Loss 560.73431
Epoch 64: Val Loss 560.72888
Epoch 65: Val Loss 560.72339
Epoch 66: Val Loss 560.71790
Epoch 67: Val Loss 560.71246
Epoch 68: Val Loss 560.70703
Epoch 69: Val Loss 560.70166
Epoch 70: Val Loss 560.69617
Epoch 71: Val Loss 560.69073
Epoch 72: Val Loss 560.68530
Epoch 73: Val Loss 560.67981
Epoch 74: Val Loss 560.67438
Epoch 75: Val Loss 560.66895
Epoch 76: Val Loss 560.66357
Epoch 77: Val Loss 560.65820
Epoch 78: Val Loss 560.65277
Epoch 79: Val Loss 560.64734
Epoch 80: Val Loss 560.64191
Epoch 81: Val Loss 560.63641
Epoch 82: Val Loss 560.63098
Epoch 83: Val Loss 560.62555
Epoch 84: Val Loss 560.62006
Epoch 85: Val Loss 560.61469
Epoch 86: Val Loss 560.60931
Epoch 87: Val Loss 560.60370
Epoch 88: Val Loss 560.59827
Epoch 89: Val Loss 560.59283
Epoch 90: Val Loss 560.58740
Epoch 91: Val Loss 560.58191
Epoch 92: Val Loss 560.57648
Epoch 93: Val Loss 560.57111
Epoch 94: Val Loss 560.56567
Epoch 95: Val Loss 560.56030
Epoch 96: Val Loss 560.55493
Epoch 97: Val Loss 560.54944
Epoch 98: Val Loss 560.54401
Epoch 99: Val Loss 560.53857
{'MSE - mean': 584.7407895893377, 'MSE - std': 64.85519972505368, 'R2 - mean': -6.016594735374672, 'R2 - std': 0.4857734949947213} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 584.7407895893377 and parameters: {'dim': 64, 'depth': 3, 'heads': 8, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 553.59149
Epoch 1: Val Loss 553.58722
Epoch 2: Val Loss 553.58307
Epoch 3: Val Loss 553.57892
Epoch 4: Val Loss 553.57471
Epoch 5: Val Loss 553.57050
Epoch 6: Val Loss 553.56635
Epoch 7: Val Loss 553.56219
Epoch 8: Val Loss 553.55798
Epoch 9: Val Loss 553.55389
Epoch 10: Val Loss 553.54974
Epoch 11: Val Loss 553.54559
Epoch 12: Val Loss 553.54138
Epoch 13: Val Loss 553.53723
Epoch 14: Val Loss 553.53302
Epoch 15: Val Loss 553.52887
Epoch 16: Val Loss 553.52478
Epoch 17: Val Loss 553.52063
Epoch 18: Val Loss 553.51648
Epoch 19: Val Loss 553.51227
Epoch 20: Val Loss 553.50824
Epoch 21: Val Loss 553.50409
Epoch 22: Val Loss 553.49988
Epoch 23: Val Loss 553.49573
Epoch 24: Val Loss 553.49158
Epoch 25: Val Loss 553.48749
Epoch 26: Val Loss 553.48322
Epoch 27: Val Loss 553.47913
Epoch 28: Val Loss 553.47491
Epoch 29: Val Loss 553.47076
Epoch 30: Val Loss 553.46649
Epoch 31: Val Loss 553.46234
Epoch 32: Val Loss 553.45819
Epoch 33: Val Loss 553.45404
Epoch 34: Val Loss 553.44995
Epoch 35: Val Loss 553.44580
Epoch 36: Val Loss 553.44171
Epoch 37: Val Loss 553.43756
Epoch 38: Val Loss 553.43341
Epoch 39: Val Loss 553.42920
Epoch 40: Val Loss 553.42505
Epoch 41: Val Loss 553.42090
Epoch 42: Val Loss 553.41675
Epoch 43: Val Loss 553.41248
Epoch 44: Val Loss 553.40826
Epoch 45: Val Loss 553.40411
Epoch 46: Val Loss 553.39984
Epoch 47: Val Loss 553.39557
Epoch 48: Val Loss 553.39136
Epoch 49: Val Loss 553.38690
Epoch 50: Val Loss 553.38269
Epoch 51: Val Loss 553.37842
Epoch 52: Val Loss 553.37427
Epoch 53: Val Loss 553.37000
Epoch 54: Val Loss 553.36584
Epoch 55: Val Loss 553.36163
Epoch 56: Val Loss 553.35748
Epoch 57: Val Loss 553.35333
Epoch 58: Val Loss 553.34912
Epoch 59: Val Loss 553.34491
Epoch 60: Val Loss 553.34070
Epoch 61: Val Loss 553.33643
Epoch 62: Val Loss 553.33221
Epoch 63: Val Loss 553.32806
Epoch 64: Val Loss 553.32391
Epoch 65: Val Loss 553.31970
Epoch 66: Val Loss 553.31561
Epoch 67: Val Loss 553.31146
Epoch 68: Val Loss 553.30725
Epoch 69: Val Loss 553.30310
Epoch 70: Val Loss 553.29895
Epoch 71: Val Loss 553.29480
Epoch 72: Val Loss 553.29071
Epoch 73: Val Loss 553.28656
Epoch 74: Val Loss 553.28241
Epoch 75: Val Loss 553.27826
Epoch 76: Val Loss 553.27417
Epoch 77: Val Loss 553.27002
Epoch 78: Val Loss 553.26593
Epoch 79: Val Loss 553.26184
Epoch 80: Val Loss 553.25769
Epoch 81: Val Loss 553.25342
Epoch 82: Val Loss 553.24921
Epoch 83: Val Loss 553.24493
Epoch 84: Val Loss 553.24072
Epoch 85: Val Loss 553.23663
Epoch 86: Val Loss 553.23242
Epoch 87: Val Loss 553.22827
Epoch 88: Val Loss 553.22412
Epoch 89: Val Loss 553.21991
Epoch 90: Val Loss 553.21570
Epoch 91: Val Loss 553.21155
Epoch 92: Val Loss 553.20746
Epoch 93: Val Loss 553.20331
Epoch 94: Val Loss 553.19916
Epoch 95: Val Loss 553.19501
Epoch 96: Val Loss 553.19092
Epoch 97: Val Loss 553.18683
Epoch 98: Val Loss 553.18268
Epoch 99: Val Loss 553.17853
{'MSE - mean': 553.1785329603805, 'MSE - std': 0.0, 'R2 - mean': -6.408480371593262, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 621.39545
Epoch 1: Val Loss 621.39069
Epoch 2: Val Loss 621.38599
Epoch 3: Val Loss 621.38123
Epoch 4: Val Loss 621.37653
Epoch 5: Val Loss 621.37177
Epoch 6: Val Loss 621.36700
Epoch 7: Val Loss 621.36230
Epoch 8: Val Loss 621.35760
Epoch 9: Val Loss 621.35291
Epoch 10: Val Loss 621.34821
Epoch 11: Val Loss 621.34344
Epoch 12: Val Loss 621.33868
Epoch 13: Val Loss 621.33405
Epoch 14: Val Loss 621.32935
Epoch 15: Val Loss 621.32465
Epoch 16: Val Loss 621.31989
Epoch 17: Val Loss 621.31512
Epoch 18: Val Loss 621.31042
Epoch 19: Val Loss 621.30566
Epoch 20: Val Loss 621.30084
Epoch 21: Val Loss 621.29608
Epoch 22: Val Loss 621.29138
Epoch 23: Val Loss 621.28668
Epoch 24: Val Loss 621.28204
Epoch 25: Val Loss 621.27728
Epoch 26: Val Loss 621.27258
Epoch 27: Val Loss 621.26794
Epoch 28: Val Loss 621.26324
Epoch 29: Val Loss 621.25861
Epoch 30: Val Loss 621.25385
Epoch 31: Val Loss 621.24921
Epoch 32: Val Loss 621.24451
Epoch 33: Val Loss 621.23987
Epoch 34: Val Loss 621.23517
Epoch 35: Val Loss 621.23047
Epoch 36: Val Loss 621.22589
Epoch 37: Val Loss 621.22131
Epoch 38: Val Loss 621.21674
Epoch 39: Val Loss 621.21210
Epoch 40: Val Loss 621.20740
Epoch 41: Val Loss 621.20282
Epoch 42: Val Loss 621.19812
Epoch 43: Val Loss 621.19342
Epoch 44: Val Loss 621.18878
Epoch 45: Val Loss 621.18414
Epoch 46: Val Loss 621.17944
Epoch 47: Val Loss 621.17474
Epoch 48: Val Loss 621.16998
Epoch 49: Val Loss 621.16528
Epoch 50: Val Loss 621.16064
Epoch 51: Val Loss 621.15594
Epoch 52: Val Loss 621.15137
Epoch 53: Val Loss 621.14673
Epoch 54: Val Loss 621.14209
Epoch 55: Val Loss 621.13751
Epoch 56: Val Loss 621.13287
Epoch 57: Val Loss 621.12830
Epoch 58: Val Loss 621.12366
Epoch 59: Val Loss 621.11914
Epoch 60: Val Loss 621.11450
Epoch 61: Val Loss 621.10980
Epoch 62: Val Loss 621.10522
Epoch 63: Val Loss 621.10065
Epoch 64: Val Loss 621.09601
Epoch 65: Val Loss 621.09125
Epoch 66: Val Loss 621.08661
Epoch 67: Val Loss 621.08197
Epoch 68: Val Loss 621.07739
Epoch 69: Val Loss 621.07288
Epoch 70: Val Loss 621.06824
Epoch 71: Val Loss 621.06360
Epoch 72: Val Loss 621.05908
Epoch 73: Val Loss 621.05444
Epoch 74: Val Loss 621.04987
Epoch 75: Val Loss 621.04529
Epoch 76: Val Loss 621.04077
Epoch 77: Val Loss 621.03613
Epoch 78: Val Loss 621.03156
Epoch 79: Val Loss 621.02704
Epoch 80: Val Loss 621.02246
Epoch 81: Val Loss 621.01794
Epoch 82: Val Loss 621.01331
Epoch 83: Val Loss 621.00873
Epoch 84: Val Loss 621.00415
Epoch 85: Val Loss 620.99963
Epoch 86: Val Loss 620.99506
Epoch 87: Val Loss 620.99054
Epoch 88: Val Loss 620.98596
Epoch 89: Val Loss 620.98145
Epoch 90: Val Loss 620.97699
Epoch 91: Val Loss 620.97235
Epoch 92: Val Loss 620.96783
Epoch 93: Val Loss 620.96332
Epoch 94: Val Loss 620.95880
Epoch 95: Val Loss 620.95422
Epoch 96: Val Loss 620.94971
Epoch 97: Val Loss 620.94513
Epoch 98: Val Loss 620.94061
Epoch 99: Val Loss 620.93604
{'MSE - mean': 587.0572959150968, 'MSE - std': 33.87876295471625, 'R2 - mean': -6.559064631477397, 'R2 - std': 0.15058425988413493} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 705.66333
Epoch 1: Val Loss 705.65924
Epoch 2: Val Loss 705.65515
Epoch 3: Val Loss 705.65112
Epoch 4: Val Loss 705.64709
Epoch 5: Val Loss 705.64301
Epoch 6: Val Loss 705.63898
Epoch 7: Val Loss 705.63477
Epoch 8: Val Loss 705.63055
Epoch 9: Val Loss 705.62640
Epoch 10: Val Loss 705.62219
Epoch 11: Val Loss 705.61804
Epoch 12: Val Loss 705.61383
Epoch 13: Val Loss 705.60974
Epoch 14: Val Loss 705.60565
Epoch 15: Val Loss 705.60156
Epoch 16: Val Loss 705.59729
Epoch 17: Val Loss 705.59302
Epoch 18: Val Loss 705.58893
Epoch 19: Val Loss 705.58472
Epoch 20: Val Loss 705.58044
Epoch 21: Val Loss 705.57623
Epoch 22: Val Loss 705.57214
Epoch 23: Val Loss 705.56793
Epoch 24: Val Loss 705.56378
Epoch 25: Val Loss 705.55969
Epoch 26: Val Loss 705.55536
Epoch 27: Val Loss 705.55115
Epoch 28: Val Loss 705.54688
Epoch 29: Val Loss 705.54254
Epoch 30: Val Loss 705.53827
Epoch 31: Val Loss 705.53394
Epoch 32: Val Loss 705.52966
Epoch 33: Val Loss 705.52539
Epoch 34: Val Loss 705.52087
Epoch 35: Val Loss 705.51654
Epoch 36: Val Loss 705.51215
Epoch 37: Val Loss 705.50775
Epoch 38: Val Loss 705.50348
Epoch 39: Val Loss 705.49908
Epoch 40: Val Loss 705.49475
Epoch 41: Val Loss 705.49042
Epoch 42: Val Loss 705.48608
Epoch 43: Val Loss 705.48175
Epoch 44: Val Loss 705.47748
Epoch 45: Val Loss 705.47308
Epoch 46: Val Loss 705.46881
Epoch 47: Val Loss 705.46448
Epoch 48: Val Loss 705.46002
Epoch 49: Val Loss 705.45557
Epoch 50: Val Loss 705.45123
Epoch 51: Val Loss 705.44690
Epoch 52: Val Loss 705.44244
Epoch 53: Val Loss 705.43793
Epoch 54: Val Loss 705.43347
Epoch 55: Val Loss 705.42889
Epoch 56: Val Loss 705.42432
Epoch 57: Val Loss 705.41980
Epoch 58: Val Loss 705.41522
Epoch 59: Val Loss 705.41071
Epoch 60: Val Loss 705.40619
Epoch 61: Val Loss 705.40161
Epoch 62: Val Loss 705.39722
Epoch 63: Val Loss 705.39264
Epoch 64: Val Loss 705.38824
Epoch 65: Val Loss 705.38379
Epoch 66: Val Loss 705.37933
Epoch 67: Val Loss 705.37500
Epoch 68: Val Loss 705.37048
Epoch 69: Val Loss 705.36603
Epoch 70: Val Loss 705.36145
Epoch 71: Val Loss 705.35706
Epoch 72: Val Loss 705.35254
Epoch 73: Val Loss 705.34790
Epoch 74: Val Loss 705.34344
Epoch 75: Val Loss 705.33893
Epoch 76: Val Loss 705.33435
Epoch 77: Val Loss 705.32996
Epoch 78: Val Loss 705.32550
Epoch 79: Val Loss 705.32098
Epoch 80: Val Loss 705.31653
Epoch 81: Val Loss 705.31201
Epoch 82: Val Loss 705.30743
Epoch 83: Val Loss 705.30298
Epoch 84: Val Loss 705.29846
Epoch 85: Val Loss 705.29395
Epoch 86: Val Loss 705.28937
Epoch 87: Val Loss 705.28497
Epoch 88: Val Loss 705.28046
Epoch 89: Val Loss 705.27600
Epoch 90: Val Loss 705.27167
Epoch 91: Val Loss 705.26715
Epoch 92: Val Loss 705.26270
Epoch 93: Val Loss 705.25818
Epoch 94: Val Loss 705.25372
Epoch 95: Val Loss 705.24921
Epoch 96: Val Loss 705.24475
Epoch 97: Val Loss 705.24023
Epoch 98: Val Loss 705.23578
Epoch 99: Val Loss 705.23126
{'MSE - mean': 626.448621576487, 'MSE - std': 62.19753581092509, 'R2 - mean': -6.502625448419565, 'R2 - std': 0.1465873197734805} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 528.23492
Epoch 1: Val Loss 528.23010
Epoch 2: Val Loss 528.22534
Epoch 3: Val Loss 528.22046
Epoch 4: Val Loss 528.21564
Epoch 5: Val Loss 528.21088
Epoch 6: Val Loss 528.20587
Epoch 7: Val Loss 528.20105
Epoch 8: Val Loss 528.19611
Epoch 9: Val Loss 528.19122
Epoch 10: Val Loss 528.18646
Epoch 11: Val Loss 528.18170
Epoch 12: Val Loss 528.17688
Epoch 13: Val Loss 528.17200
Epoch 14: Val Loss 528.16718
Epoch 15: Val Loss 528.16229
Epoch 16: Val Loss 528.15747
Epoch 17: Val Loss 528.15271
Epoch 18: Val Loss 528.14807
Epoch 19: Val Loss 528.14331
Epoch 20: Val Loss 528.13855
Epoch 21: Val Loss 528.13379
Epoch 22: Val Loss 528.12903
Epoch 23: Val Loss 528.12408
Epoch 24: Val Loss 528.11926
Epoch 25: Val Loss 528.11450
Epoch 26: Val Loss 528.10962
Epoch 27: Val Loss 528.10480
Epoch 28: Val Loss 528.10004
Epoch 29: Val Loss 528.09509
Epoch 30: Val Loss 528.09015
Epoch 31: Val Loss 528.08527
Epoch 32: Val Loss 528.08051
Epoch 33: Val Loss 528.07550
Epoch 34: Val Loss 528.07062
Epoch 35: Val Loss 528.06567
Epoch 36: Val Loss 528.06085
Epoch 37: Val Loss 528.05597
Epoch 38: Val Loss 528.05121
Epoch 39: Val Loss 528.04651
Epoch 40: Val Loss 528.04175
Epoch 41: Val Loss 528.03699
Epoch 42: Val Loss 528.03223
Epoch 43: Val Loss 528.02753
Epoch 44: Val Loss 528.02277
Epoch 45: Val Loss 528.01807
Epoch 46: Val Loss 528.01324
Epoch 47: Val Loss 528.00836
Epoch 48: Val Loss 528.00354
Epoch 49: Val Loss 527.99866
Epoch 50: Val Loss 527.99377
Epoch 51: Val Loss 527.98883
Epoch 52: Val Loss 527.98401
Epoch 53: Val Loss 527.97913
Epoch 54: Val Loss 527.97406
Epoch 55: Val Loss 527.96918
Epoch 56: Val Loss 527.96436
Epoch 57: Val Loss 527.95953
Epoch 58: Val Loss 527.95477
Epoch 59: Val Loss 527.95001
Epoch 60: Val Loss 527.94519
Epoch 61: Val Loss 527.94037
Epoch 62: Val Loss 527.93549
Epoch 63: Val Loss 527.93066
Epoch 64: Val Loss 527.92572
Epoch 65: Val Loss 527.92084
Epoch 66: Val Loss 527.91602
Epoch 67: Val Loss 527.91119
Epoch 68: Val Loss 527.90625
Epoch 69: Val Loss 527.90143
Epoch 70: Val Loss 527.89648
Epoch 71: Val Loss 527.89154
Epoch 72: Val Loss 527.88666
Epoch 73: Val Loss 527.88171
Epoch 74: Val Loss 527.87671
Epoch 75: Val Loss 527.87177
Epoch 76: Val Loss 527.86670
Epoch 77: Val Loss 527.86176
Epoch 78: Val Loss 527.85675
Epoch 79: Val Loss 527.85175
Epoch 80: Val Loss 527.84686
Epoch 81: Val Loss 527.84180
Epoch 82: Val Loss 527.83685
Epoch 83: Val Loss 527.83191
Epoch 84: Val Loss 527.82697
Epoch 85: Val Loss 527.82190
Epoch 86: Val Loss 527.81689
Epoch 87: Val Loss 527.81189
Epoch 88: Val Loss 527.80688
Epoch 89: Val Loss 527.80194
Epoch 90: Val Loss 527.79688
Epoch 91: Val Loss 527.79181
Epoch 92: Val Loss 527.78674
Epoch 93: Val Loss 527.78180
Epoch 94: Val Loss 527.77667
Epoch 95: Val Loss 527.77173
Epoch 96: Val Loss 527.76672
Epoch 97: Val Loss 527.76172
Epoch 98: Val Loss 527.75677
Epoch 99: Val Loss 527.75171
{'MSE - mean': 601.7743935956398, 'MSE - std': 68.75938245116767, 'R2 - mean': -6.375520489033527, 'R2 - std': 0.25413164667226756} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 586.12408
Epoch 1: Val Loss 586.11462
Epoch 2: Val Loss 586.10522
Epoch 3: Val Loss 586.09576
Epoch 4: Val Loss 586.08630
Epoch 5: Val Loss 586.07690
Epoch 6: Val Loss 586.06750
Epoch 7: Val Loss 586.05811
Epoch 8: Val Loss 586.04871
Epoch 9: Val Loss 586.03931
Epoch 10: Val Loss 586.02985
Epoch 11: Val Loss 586.02051
Epoch 12: Val Loss 586.01099
Epoch 13: Val Loss 586.00146
Epoch 14: Val Loss 585.99200
Epoch 15: Val Loss 585.98260
Epoch 16: Val Loss 585.97321
Epoch 17: Val Loss 585.96375
Epoch 18: Val Loss 585.95435
Epoch 19: Val Loss 585.94501
Epoch 20: Val Loss 585.93573
Epoch 21: Val Loss 585.92639
Epoch 22: Val Loss 585.91705
Epoch 23: Val Loss 585.90765
Epoch 24: Val Loss 585.89819
Epoch 25: Val Loss 585.88873
Epoch 26: Val Loss 585.87921
Epoch 27: Val Loss 585.86963
Epoch 28: Val Loss 585.86017
Epoch 29: Val Loss 585.85071
Epoch 30: Val Loss 585.84119
Epoch 31: Val Loss 585.83173
Epoch 32: Val Loss 585.82245
Epoch 33: Val Loss 585.81311
Epoch 34: Val Loss 585.80365
Epoch 35: Val Loss 585.79425
Epoch 36: Val Loss 585.78479
Epoch 37: Val Loss 585.77539
Epoch 38: Val Loss 585.76587
Epoch 39: Val Loss 585.75635
Epoch 40: Val Loss 585.74689
Epoch 41: Val Loss 585.73749
Epoch 42: Val Loss 585.72803
Epoch 43: Val Loss 585.71863
Epoch 44: Val Loss 585.70917
Epoch 45: Val Loss 585.69983
Epoch 46: Val Loss 585.69049
Epoch 47: Val Loss 585.68121
Epoch 48: Val Loss 585.67181
Epoch 49: Val Loss 585.66229
Epoch 50: Val Loss 585.65277
Epoch 51: Val Loss 585.64319
Epoch 52: Val Loss 585.63367
Epoch 53: Val Loss 585.62421
Epoch 54: Val Loss 585.61481
Epoch 55: Val Loss 585.60529
Epoch 56: Val Loss 585.59589
Epoch 57: Val Loss 585.58649
Epoch 58: Val Loss 585.57697
Epoch 59: Val Loss 585.56738
Epoch 60: Val Loss 585.55774
Epoch 61: Val Loss 585.54816
Epoch 62: Val Loss 585.53851
Epoch 63: Val Loss 585.52887
Epoch 64: Val Loss 585.51935
Epoch 65: Val Loss 585.50983
Epoch 66: Val Loss 585.50024
Epoch 67: Val Loss 585.49072
Epoch 68: Val Loss 585.48120
Epoch 69: Val Loss 585.47180
Epoch 70: Val Loss 585.46234
Epoch 71: Val Loss 585.45294
Epoch 72: Val Loss 585.44354
Epoch 73: Val Loss 585.43408
Epoch 74: Val Loss 585.42462
Epoch 75: Val Loss 585.41516
Epoch 76: Val Loss 585.40570
Epoch 77: Val Loss 585.39636
Epoch 78: Val Loss 585.38696
Epoch 79: Val Loss 585.37756
Epoch 80: Val Loss 585.36810
Epoch 81: Val Loss 585.35876
Epoch 82: Val Loss 585.34949
Epoch 83: Val Loss 585.34021
Epoch 84: Val Loss 585.33081
Epoch 85: Val Loss 585.32129
Epoch 86: Val Loss 585.31177
Epoch 87: Val Loss 585.30219
Epoch 88: Val Loss 585.29279
Epoch 89: Val Loss 585.28333
Epoch 90: Val Loss 585.27380
Epoch 91: Val Loss 585.26440
Epoch 92: Val Loss 585.25482
Epoch 93: Val Loss 585.24536
Epoch 94: Val Loss 585.23584
Epoch 95: Val Loss 585.22626
Epoch 96: Val Loss 585.21674
Epoch 97: Val Loss 585.20721
Epoch 98: Val Loss 585.19769
Epoch 99: Val Loss 585.18817
{'MSE - mean': 598.4571635923248, 'MSE - std': 61.85707881586658, 'R2 - mean': -6.182649097763471, 'R2 - std': 0.4477318498321027} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 598.4571635923248 and parameters: {'dim': 32, 'depth': 3, 'heads': 8, 'weight_decay': -3, 'learning_rate': -5, 'dropout': 0.1}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 566.06805
Epoch 1: Val Loss 565.70270
Epoch 2: Val Loss 565.35052
Epoch 3: Val Loss 565.01807
Epoch 4: Val Loss 564.71075
Epoch 5: Val Loss 564.41919
Epoch 6: Val Loss 564.13879
Epoch 7: Val Loss 563.87653
Epoch 8: Val Loss 563.62909
Epoch 9: Val Loss 563.38965
Epoch 10: Val Loss 563.15454
Epoch 11: Val Loss 562.92462
Epoch 12: Val Loss 562.69647
Epoch 13: Val Loss 562.47266
Epoch 14: Val Loss 562.25562
Epoch 15: Val Loss 562.04266
Epoch 16: Val Loss 561.83612
Epoch 17: Val Loss 561.63391
Epoch 18: Val Loss 561.43451
Epoch 19: Val Loss 561.23822
Epoch 20: Val Loss 561.04419
Epoch 21: Val Loss 560.85248
Epoch 22: Val Loss 560.66284
Epoch 23: Val Loss 560.47369
Epoch 24: Val Loss 560.28546
Epoch 25: Val Loss 560.09698
Epoch 26: Val Loss 559.90973
Epoch 27: Val Loss 559.72394
Epoch 28: Val Loss 559.53918
Epoch 29: Val Loss 559.35486
Epoch 30: Val Loss 559.17334
Epoch 31: Val Loss 558.99268
Epoch 32: Val Loss 558.81207
Epoch 33: Val Loss 558.63226
Epoch 34: Val Loss 558.45221
Epoch 35: Val Loss 558.27295
Epoch 36: Val Loss 558.09406
Epoch 37: Val Loss 557.91577
Epoch 38: Val Loss 557.73572
Epoch 39: Val Loss 557.55634
Epoch 40: Val Loss 557.37646
Epoch 41: Val Loss 557.19733
Epoch 42: Val Loss 557.02032
Epoch 43: Val Loss 556.84351
Epoch 44: Val Loss 556.66638
Epoch 45: Val Loss 556.48969
Epoch 46: Val Loss 556.31281
Epoch 47: Val Loss 556.13672
Epoch 48: Val Loss 555.96136
Epoch 49: Val Loss 555.78577
Epoch 50: Val Loss 555.60864
Epoch 51: Val Loss 555.43079
Epoch 52: Val Loss 555.25464
Epoch 53: Val Loss 555.07825
Epoch 54: Val Loss 554.90228
Epoch 55: Val Loss 554.72705
Epoch 56: Val Loss 554.55011
Epoch 57: Val Loss 554.37244
Epoch 58: Val Loss 554.19641
Epoch 59: Val Loss 554.02069
Epoch 60: Val Loss 553.84619
Epoch 61: Val Loss 553.66974
Epoch 62: Val Loss 553.49335
Epoch 63: Val Loss 553.31879
Epoch 64: Val Loss 553.14343
Epoch 65: Val Loss 552.97009
Epoch 66: Val Loss 552.79639
Epoch 67: Val Loss 552.62183
Epoch 68: Val Loss 552.44592
Epoch 69: Val Loss 552.27063
Epoch 70: Val Loss 552.09625
Epoch 71: Val Loss 551.92297
Epoch 72: Val Loss 551.74976
Epoch 73: Val Loss 551.57422
Epoch 74: Val Loss 551.39984
Epoch 75: Val Loss 551.22473
Epoch 76: Val Loss 551.05029
Epoch 77: Val Loss 550.87744
Epoch 78: Val Loss 550.70587
Epoch 79: Val Loss 550.53442
Epoch 80: Val Loss 550.36200
Epoch 81: Val Loss 550.18958
Epoch 82: Val Loss 550.01758
Epoch 83: Val Loss 549.84552
Epoch 84: Val Loss 549.67426
Epoch 85: Val Loss 549.50348
Epoch 86: Val Loss 549.33270
Epoch 87: Val Loss 549.16284
Epoch 88: Val Loss 548.99225
Epoch 89: Val Loss 548.82208
Epoch 90: Val Loss 548.64905
Epoch 91: Val Loss 548.47668
Epoch 92: Val Loss 548.30432
Epoch 93: Val Loss 548.13263
Epoch 94: Val Loss 547.96082
Epoch 95: Val Loss 547.79004
Epoch 96: Val Loss 547.61975
Epoch 97: Val Loss 547.44800
Epoch 98: Val Loss 547.27643
Epoch 99: Val Loss 547.10474
{'MSE - mean': 547.1047197731855, 'MSE - std': 0.0, 'R2 - mean': -6.3271364236688, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 624.57922
Epoch 1: Val Loss 624.02197
Epoch 2: Val Loss 623.49414
Epoch 3: Val Loss 622.99182
Epoch 4: Val Loss 622.51184
Epoch 5: Val Loss 622.04901
Epoch 6: Val Loss 621.59558
Epoch 7: Val Loss 621.16791
Epoch 8: Val Loss 620.78961
Epoch 9: Val Loss 620.45465
Epoch 10: Val Loss 620.14752
Epoch 11: Val Loss 619.85522
Epoch 12: Val Loss 619.57001
Epoch 13: Val Loss 619.29034
Epoch 14: Val Loss 619.01709
Epoch 15: Val Loss 618.75830
Epoch 16: Val Loss 618.51135
Epoch 17: Val Loss 618.27435
Epoch 18: Val Loss 618.03552
Epoch 19: Val Loss 617.80841
Epoch 20: Val Loss 617.59406
Epoch 21: Val Loss 617.38409
Epoch 22: Val Loss 617.18799
Epoch 23: Val Loss 616.99762
Epoch 24: Val Loss 616.80811
Epoch 25: Val Loss 616.61847
Epoch 26: Val Loss 616.42920
Epoch 27: Val Loss 616.24005
Epoch 28: Val Loss 616.05273
Epoch 29: Val Loss 615.86633
Epoch 30: Val Loss 615.67987
Epoch 31: Val Loss 615.49365
Epoch 32: Val Loss 615.30792
Epoch 33: Val Loss 615.12329
Epoch 34: Val Loss 614.93817
Epoch 35: Val Loss 614.75348
Epoch 36: Val Loss 614.56805
Epoch 37: Val Loss 614.38318
Epoch 38: Val Loss 614.19891
Epoch 39: Val Loss 614.01508
Epoch 40: Val Loss 613.83203
Epoch 41: Val Loss 613.64941
Epoch 42: Val Loss 613.46613
Epoch 43: Val Loss 613.28119
Epoch 44: Val Loss 613.09821
Epoch 45: Val Loss 612.91589
Epoch 46: Val Loss 612.73328
Epoch 47: Val Loss 612.55011
Epoch 48: Val Loss 612.36658
Epoch 49: Val Loss 612.18402
Epoch 50: Val Loss 612.00159
Epoch 51: Val Loss 611.81885
Epoch 52: Val Loss 611.63672
Epoch 53: Val Loss 611.45459
Epoch 54: Val Loss 611.27338
Epoch 55: Val Loss 611.09314
Epoch 56: Val Loss 610.91260
Epoch 57: Val Loss 610.73285
Epoch 58: Val Loss 610.55328
Epoch 59: Val Loss 610.37207
Epoch 60: Val Loss 610.19012
Epoch 61: Val Loss 610.00964
Epoch 62: Val Loss 609.82880
Epoch 63: Val Loss 609.64728
Epoch 64: Val Loss 609.46875
Epoch 65: Val Loss 609.29065
Epoch 66: Val Loss 609.11340
Epoch 67: Val Loss 608.93518
Epoch 68: Val Loss 608.75781
Epoch 69: Val Loss 608.57867
Epoch 70: Val Loss 608.39899
Epoch 71: Val Loss 608.22119
Epoch 72: Val Loss 608.04340
Epoch 73: Val Loss 607.86377
Epoch 74: Val Loss 607.68341
Epoch 75: Val Loss 607.50378
Epoch 76: Val Loss 607.32581
Epoch 77: Val Loss 607.14746
Epoch 78: Val Loss 606.96826
Epoch 79: Val Loss 606.79016
Epoch 80: Val Loss 606.61249
Epoch 81: Val Loss 606.43579
Epoch 82: Val Loss 606.26044
Epoch 83: Val Loss 606.08362
Epoch 84: Val Loss 605.90753
Epoch 85: Val Loss 605.73224
Epoch 86: Val Loss 605.55511
Epoch 87: Val Loss 605.37781
Epoch 88: Val Loss 605.20020
Epoch 89: Val Loss 605.02344
Epoch 90: Val Loss 604.84778
Epoch 91: Val Loss 604.67133
Epoch 92: Val Loss 604.49402
Epoch 93: Val Loss 604.31781
Epoch 94: Val Loss 604.14209
Epoch 95: Val Loss 603.96759
Epoch 96: Val Loss 603.79279
Epoch 97: Val Loss 603.61932
Epoch 98: Val Loss 603.44476
Epoch 99: Val Loss 603.27161
{'MSE - mean': 575.1881886329944, 'MSE - std': 28.08346885980893, 'R2 - mean': -6.408730534863837, 'R2 - std': 0.08159411119503712} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 655.79541
Epoch 1: Val Loss 655.51471
Epoch 2: Val Loss 655.28937
Epoch 3: Val Loss 655.08014
Epoch 4: Val Loss 654.87415
Epoch 5: Val Loss 654.66284
Epoch 6: Val Loss 654.44183
Epoch 7: Val Loss 654.20032
Epoch 8: Val Loss 653.90320
Epoch 9: Val Loss 653.54547
Epoch 10: Val Loss 653.14557
Epoch 11: Val Loss 652.68829
Epoch 12: Val Loss 652.15002
Epoch 13: Val Loss 651.48505
Epoch 14: Val Loss 650.70929
Epoch 15: Val Loss 649.81897
Epoch 16: Val Loss 648.77905
Epoch 17: Val Loss 647.57349
Epoch 18: Val Loss 646.10706
Epoch 19: Val Loss 644.45557
Epoch 20: Val Loss 642.65247
Epoch 21: Val Loss 640.69940
Epoch 22: Val Loss 638.66174
Epoch 23: Val Loss 636.49561
Epoch 24: Val Loss 634.18359
Epoch 25: Val Loss 631.75354
Epoch 26: Val Loss 629.21356
Epoch 27: Val Loss 626.60205
Epoch 28: Val Loss 623.83710
Epoch 29: Val Loss 620.92993
Epoch 30: Val Loss 617.86328
Epoch 31: Val Loss 614.65948
Epoch 32: Val Loss 611.36456
Epoch 33: Val Loss 607.93610
Epoch 34: Val Loss 604.37567
Epoch 35: Val Loss 600.62408
Epoch 36: Val Loss 596.67926
Epoch 37: Val Loss 592.53088
Epoch 38: Val Loss 588.15448
Epoch 39: Val Loss 583.63147
Epoch 40: Val Loss 578.94342
Epoch 41: Val Loss 574.01929
Epoch 42: Val Loss 568.83539
Epoch 43: Val Loss 563.47235
Epoch 44: Val Loss 557.89606
Epoch 45: Val Loss 552.18018
Epoch 46: Val Loss 546.23663
Epoch 47: Val Loss 540.06677
Epoch 48: Val Loss 533.74524
Epoch 49: Val Loss 527.07318
Epoch 50: Val Loss 520.14880
Epoch 51: Val Loss 512.99707
Epoch 52: Val Loss 505.66443
Epoch 53: Val Loss 498.13245
Epoch 54: Val Loss 490.31729
Epoch 55: Val Loss 482.51794
Epoch 56: Val Loss 474.49487
Epoch 57: Val Loss 466.25525
Epoch 58: Val Loss 457.78705
Epoch 59: Val Loss 449.25165
Epoch 60: Val Loss 440.53125
Epoch 61: Val Loss 431.50934
Epoch 62: Val Loss 422.29102
Epoch 63: Val Loss 412.87296
Epoch 64: Val Loss 403.41513
Epoch 65: Val Loss 393.84149
Epoch 66: Val Loss 384.13992
Epoch 67: Val Loss 374.57571
Epoch 68: Val Loss 364.87183
Epoch 69: Val Loss 354.87402
Epoch 70: Val Loss 344.81561
Epoch 71: Val Loss 334.85696
Epoch 72: Val Loss 325.02219
Epoch 73: Val Loss 315.11536
Epoch 74: Val Loss 305.40152
Epoch 75: Val Loss 295.76642
Epoch 76: Val Loss 286.25909
Epoch 77: Val Loss 276.65533
Epoch 78: Val Loss 267.21729
Epoch 79: Val Loss 258.04205
Epoch 80: Val Loss 248.91769
Epoch 81: Val Loss 240.02644
Epoch 82: Val Loss 231.23824
Epoch 83: Val Loss 222.73973
Epoch 84: Val Loss 214.62851
Epoch 85: Val Loss 206.83376
Epoch 86: Val Loss 199.31596
Epoch 87: Val Loss 192.09679
Epoch 88: Val Loss 185.45755
Epoch 89: Val Loss 179.22180
Epoch 90: Val Loss 173.23251
Epoch 91: Val Loss 167.57211
Epoch 92: Val Loss 162.23111
Epoch 93: Val Loss 157.23683
Epoch 94: Val Loss 152.55229
Epoch 95: Val Loss 148.15451
Epoch 96: Val Loss 143.99634
Epoch 97: Val Loss 140.15715
Epoch 98: Val Loss 136.53699
Epoch 99: Val Loss 133.12318
{'MSE - mean': 427.8331855855509, 'MSE - std': 209.6491863288704, 'R2 - mean': -4.404129585066702, 'R2 - std': 2.8357165470434964} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 568.04224
Epoch 1: Val Loss 567.73010
Epoch 2: Val Loss 567.41760
Epoch 3: Val Loss 567.09015
Epoch 4: Val Loss 566.74945
Epoch 5: Val Loss 566.40344
Epoch 6: Val Loss 566.04669
Epoch 7: Val Loss 565.67291
Epoch 8: Val Loss 565.26343
Epoch 9: Val Loss 564.82404
Epoch 10: Val Loss 564.31702
Epoch 11: Val Loss 563.67749
Epoch 12: Val Loss 562.94354
Epoch 13: Val Loss 562.06134
Epoch 14: Val Loss 561.00391
Epoch 15: Val Loss 559.67389
Epoch 16: Val Loss 557.95343
Epoch 17: Val Loss 555.93390
Epoch 18: Val Loss 553.57501
Epoch 19: Val Loss 550.78961
Epoch 20: Val Loss 547.75720
Epoch 21: Val Loss 544.43958
Epoch 22: Val Loss 540.82086
Epoch 23: Val Loss 537.01099
Epoch 24: Val Loss 532.91492
Epoch 25: Val Loss 528.60181
Epoch 26: Val Loss 524.15601
Epoch 27: Val Loss 519.56580
Epoch 28: Val Loss 514.74518
Epoch 29: Val Loss 509.72424
Epoch 30: Val Loss 504.46432
Epoch 31: Val Loss 499.04526
Epoch 32: Val Loss 493.39587
Epoch 33: Val Loss 487.44925
Epoch 34: Val Loss 481.29013
Epoch 35: Val Loss 474.81531
Epoch 36: Val Loss 468.07043
Epoch 37: Val Loss 461.03168
Epoch 38: Val Loss 453.79507
Epoch 39: Val Loss 446.29590
Epoch 40: Val Loss 438.39673
Epoch 41: Val Loss 430.15140
Epoch 42: Val Loss 421.78476
Epoch 43: Val Loss 413.05640
Epoch 44: Val Loss 404.15494
Epoch 45: Val Loss 395.10870
Epoch 46: Val Loss 385.77097
Epoch 47: Val Loss 376.22070
Epoch 48: Val Loss 366.43906
Epoch 49: Val Loss 356.50504
Epoch 50: Val Loss 346.47168
Epoch 51: Val Loss 336.24573
Epoch 52: Val Loss 325.82385
Epoch 53: Val Loss 315.18301
Epoch 54: Val Loss 304.55835
Epoch 55: Val Loss 294.33130
Epoch 56: Val Loss 283.94965
Epoch 57: Val Loss 273.42960
Epoch 58: Val Loss 263.04974
Epoch 59: Val Loss 252.78421
Epoch 60: Val Loss 242.68195
Epoch 61: Val Loss 232.92586
Epoch 62: Val Loss 223.30724
Epoch 63: Val Loss 213.79860
Epoch 64: Val Loss 204.66017
Epoch 65: Val Loss 196.01404
Epoch 66: Val Loss 187.75285
Epoch 67: Val Loss 180.15936
Epoch 68: Val Loss 173.16417
Epoch 69: Val Loss 166.75368
Epoch 70: Val Loss 160.84323
Epoch 71: Val Loss 155.31068
Epoch 72: Val Loss 150.19563
Epoch 73: Val Loss 145.36609
Epoch 74: Val Loss 140.97525
Epoch 75: Val Loss 137.03380
Epoch 76: Val Loss 133.43037
Epoch 77: Val Loss 130.05949
Epoch 78: Val Loss 126.86465
Epoch 79: Val Loss 124.02971
Epoch 80: Val Loss 121.45641
Epoch 81: Val Loss 119.13145
Epoch 82: Val Loss 117.00058
Epoch 83: Val Loss 114.99870
Epoch 84: Val Loss 113.09282
Epoch 85: Val Loss 111.32855
Epoch 86: Val Loss 109.65097
Epoch 87: Val Loss 108.08706
Epoch 88: Val Loss 106.64375
Epoch 89: Val Loss 105.29356
Epoch 90: Val Loss 103.96597
Epoch 91: Val Loss 102.61134
Epoch 92: Val Loss 101.22704
Epoch 93: Val Loss 99.82534
Epoch 94: Val Loss 98.41259
Epoch 95: Val Loss 97.02850
Epoch 96: Val Loss 95.56792
Epoch 97: Val Loss 94.18612
Epoch 98: Val Loss 92.79656
Epoch 99: Val Loss 91.38671
{'MSE - mean': 343.72156742112924, 'MSE - std': 232.78504876168589, 'R2 - mean': -3.355880414618315, 'R2 - std': 3.0540866427492546} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 578.21338
Epoch 1: Val Loss 577.92633
Epoch 2: Val Loss 577.65387
Epoch 3: Val Loss 577.39862
Epoch 4: Val Loss 577.14868
Epoch 5: Val Loss 576.90460
Epoch 6: Val Loss 576.66730
Epoch 7: Val Loss 576.43750
Epoch 8: Val Loss 576.21478
Epoch 9: Val Loss 575.99567
Epoch 10: Val Loss 575.78149
Epoch 11: Val Loss 575.57306
Epoch 12: Val Loss 575.36694
Epoch 13: Val Loss 575.15869
Epoch 14: Val Loss 574.94727
Epoch 15: Val Loss 574.73383
Epoch 16: Val Loss 574.51306
Epoch 17: Val Loss 574.28644
Epoch 18: Val Loss 574.04810
Epoch 19: Val Loss 573.79651
Epoch 20: Val Loss 573.53369
Epoch 21: Val Loss 573.25140
Epoch 22: Val Loss 572.95123
Epoch 23: Val Loss 572.63318
Epoch 24: Val Loss 572.29059
Epoch 25: Val Loss 571.91388
Epoch 26: Val Loss 571.48981
Epoch 27: Val Loss 571.01831
Epoch 28: Val Loss 570.50098
Epoch 29: Val Loss 569.93646
Epoch 30: Val Loss 569.30292
Epoch 31: Val Loss 568.58807
Epoch 32: Val Loss 567.81195
Epoch 33: Val Loss 566.97559
Epoch 34: Val Loss 566.04639
Epoch 35: Val Loss 565.03314
Epoch 36: Val Loss 563.92188
Epoch 37: Val Loss 562.72382
Epoch 38: Val Loss 561.45020
Epoch 39: Val Loss 560.09198
Epoch 40: Val Loss 558.63361
Epoch 41: Val Loss 557.08057
Epoch 42: Val Loss 555.40393
Epoch 43: Val Loss 553.60004
Epoch 44: Val Loss 551.63354
Epoch 45: Val Loss 549.54901
Epoch 46: Val Loss 547.30560
Epoch 47: Val Loss 544.86578
Epoch 48: Val Loss 542.25879
Epoch 49: Val Loss 539.46454
Epoch 50: Val Loss 536.47546
Epoch 51: Val Loss 533.35010
Epoch 52: Val Loss 530.06836
Epoch 53: Val Loss 526.56549
Epoch 54: Val Loss 522.81335
Epoch 55: Val Loss 518.87744
Epoch 56: Val Loss 514.72968
Epoch 57: Val Loss 510.28760
Epoch 58: Val Loss 505.67078
Epoch 59: Val Loss 500.86108
Epoch 60: Val Loss 495.88605
Epoch 61: Val Loss 490.57812
Epoch 62: Val Loss 485.03839
Epoch 63: Val Loss 479.25186
Epoch 64: Val Loss 473.27722
Epoch 65: Val Loss 467.05496
Epoch 66: Val Loss 460.64169
Epoch 67: Val Loss 453.97412
Epoch 68: Val Loss 447.06210
Epoch 69: Val Loss 440.01932
Epoch 70: Val Loss 432.83746
Epoch 71: Val Loss 425.34088
Epoch 72: Val Loss 417.71951
Epoch 73: Val Loss 409.96469
Epoch 74: Val Loss 402.03766
Epoch 75: Val Loss 394.02829
Epoch 76: Val Loss 385.82959
Epoch 77: Val Loss 377.61774
Epoch 78: Val Loss 369.31601
Epoch 79: Val Loss 360.81021
Epoch 80: Val Loss 352.16861
Epoch 81: Val Loss 343.55469
Epoch 82: Val Loss 334.89923
Epoch 83: Val Loss 326.23660
Epoch 84: Val Loss 317.44058
Epoch 85: Val Loss 308.51288
Epoch 86: Val Loss 299.60779
Epoch 87: Val Loss 290.67642
Epoch 88: Val Loss 282.06018
Epoch 89: Val Loss 273.71844
Epoch 90: Val Loss 265.65790
Epoch 91: Val Loss 257.79965
Epoch 92: Val Loss 250.17206
Epoch 93: Val Loss 242.70219
Epoch 94: Val Loss 235.57646
Epoch 95: Val Loss 228.81712
Epoch 96: Val Loss 222.37733
Epoch 97: Val Loss 216.26326
Epoch 98: Val Loss 210.41008
Epoch 99: Val Loss 204.79343
{'MSE - mean': 315.93593676651864, 'MSE - std': 215.49772208791936, 'R2 - mean': -2.933436541518156, 'R2 - std': 2.8593340974455894} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 315.93593676651864 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 4 with value: 247.74844253395108.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 509.66339
Epoch 1: Val Loss 508.56885
Epoch 2: Val Loss 507.45050
Epoch 3: Val Loss 506.29910
Epoch 4: Val Loss 505.11597
Epoch 5: Val Loss 503.89465
Epoch 6: Val Loss 502.64331
Epoch 7: Val Loss 501.34644
Epoch 8: Val Loss 499.99963
Epoch 9: Val Loss 498.59653
Epoch 10: Val Loss 497.12234
Epoch 11: Val Loss 495.58533
Epoch 12: Val Loss 493.97107
Epoch 13: Val Loss 492.26303
Epoch 14: Val Loss 490.48627
Epoch 15: Val Loss 488.62683
Epoch 16: Val Loss 486.68054
Epoch 17: Val Loss 484.63007
Epoch 18: Val Loss 482.47345
Epoch 19: Val Loss 480.21317
Epoch 20: Val Loss 477.84186
Epoch 21: Val Loss 475.31467
Epoch 22: Val Loss 472.63586
Epoch 23: Val Loss 469.80151
Epoch 24: Val Loss 466.84055
Epoch 25: Val Loss 463.76511
Epoch 26: Val Loss 460.54160
Epoch 27: Val Loss 457.13376
Epoch 28: Val Loss 453.59930
Epoch 29: Val Loss 449.87595
Epoch 30: Val Loss 445.93719
Epoch 31: Val Loss 441.82938
Epoch 32: Val Loss 437.55069
Epoch 33: Val Loss 433.11307
Epoch 34: Val Loss 428.49548
Epoch 35: Val Loss 423.68140
Epoch 36: Val Loss 418.60703
Epoch 37: Val Loss 413.28073
Epoch 38: Val Loss 407.71387
Epoch 39: Val Loss 401.93643
Epoch 40: Val Loss 396.03027
Epoch 41: Val Loss 389.87598
Epoch 42: Val Loss 383.51862
Epoch 43: Val Loss 376.84210
Epoch 44: Val Loss 369.94403
Epoch 45: Val Loss 362.85754
Epoch 46: Val Loss 355.64093
Epoch 47: Val Loss 348.23346
Epoch 48: Val Loss 340.64014
Epoch 49: Val Loss 332.91226
Epoch 50: Val Loss 324.98898
Epoch 51: Val Loss 316.90994
Epoch 52: Val Loss 308.70312
Epoch 53: Val Loss 300.29135
Epoch 54: Val Loss 291.67578
Epoch 55: Val Loss 282.84772
Epoch 56: Val Loss 274.01440
Epoch 57: Val Loss 265.13336
Epoch 58: Val Loss 256.26782
Epoch 59: Val Loss 247.44800
Epoch 60: Val Loss 238.53293
Epoch 61: Val Loss 229.74431
Epoch 62: Val Loss 221.11852
Epoch 63: Val Loss 212.51674
Epoch 64: Val Loss 204.04526
Epoch 65: Val Loss 195.69572
Epoch 66: Val Loss 187.63466
Epoch 67: Val Loss 179.85062
Epoch 68: Val Loss 172.43907
Epoch 69: Val Loss 165.40173
Epoch 70: Val Loss 158.66684
Epoch 71: Val Loss 152.40492
Epoch 72: Val Loss 146.57927
Epoch 73: Val Loss 141.04570
Epoch 74: Val Loss 135.83232
Epoch 75: Val Loss 131.00984
Epoch 76: Val Loss 126.45985
Epoch 77: Val Loss 122.36879
Epoch 78: Val Loss 118.67509
Epoch 79: Val Loss 115.33089
Epoch 80: Val Loss 112.28339
Epoch 81: Val Loss 109.51823
Epoch 82: Val Loss 106.92974
Epoch 83: Val Loss 104.54017
Epoch 84: Val Loss 102.43434
Epoch 85: Val Loss 100.55599
Epoch 86: Val Loss 98.77511
Epoch 87: Val Loss 97.10120
Epoch 88: Val Loss 95.61740
Epoch 89: Val Loss 94.23418
Epoch 90: Val Loss 92.95914
Epoch 91: Val Loss 91.69616
Epoch 92: Val Loss 90.48696
Epoch 93: Val Loss 89.30688
Epoch 94: Val Loss 88.15837
Epoch 95: Val Loss 87.06854
Epoch 96: Val Loss 86.01580
Epoch 97: Val Loss 85.01016
Epoch 98: Val Loss 84.05099
Epoch 99: Val Loss 83.10294
{'MSE - mean': 83.10295212461696, 'MSE - std': 0.0, 'R2 - mean': -0.11296182507641572, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 587.33881
Epoch 1: Val Loss 586.98651
Epoch 2: Val Loss 586.64215
Epoch 3: Val Loss 586.29651
Epoch 4: Val Loss 585.94891
Epoch 5: Val Loss 585.58954
Epoch 6: Val Loss 585.21960
Epoch 7: Val Loss 584.83240
Epoch 8: Val Loss 584.43048
Epoch 9: Val Loss 584.00983
Epoch 10: Val Loss 583.57190
Epoch 11: Val Loss 583.11224
Epoch 12: Val Loss 582.62524
Epoch 13: Val Loss 582.11194
Epoch 14: Val Loss 581.56360
Epoch 15: Val Loss 580.98151
Epoch 16: Val Loss 580.36914
Epoch 17: Val Loss 579.71295
Epoch 18: Val Loss 579.01050
Epoch 19: Val Loss 578.25574
Epoch 20: Val Loss 577.44733
Epoch 21: Val Loss 576.58051
Epoch 22: Val Loss 575.63721
Epoch 23: Val Loss 574.62860
Epoch 24: Val Loss 573.54913
Epoch 25: Val Loss 572.39899
Epoch 26: Val Loss 571.15271
Epoch 27: Val Loss 569.78558
Epoch 28: Val Loss 568.30908
Epoch 29: Val Loss 566.68835
Epoch 30: Val Loss 564.94635
Epoch 31: Val Loss 563.07526
Epoch 32: Val Loss 561.06696
Epoch 33: Val Loss 558.88696
Epoch 34: Val Loss 556.52850
Epoch 35: Val Loss 554.02124
Epoch 36: Val Loss 551.30573
Epoch 37: Val Loss 548.35760
Epoch 38: Val Loss 545.29578
Epoch 39: Val Loss 542.04596
Epoch 40: Val Loss 538.56360
Epoch 41: Val Loss 534.81683
Epoch 42: Val Loss 530.81720
Epoch 43: Val Loss 526.56775
Epoch 44: Val Loss 522.05908
Epoch 45: Val Loss 517.28149
Epoch 46: Val Loss 512.28070
Epoch 47: Val Loss 507.02798
Epoch 48: Val Loss 501.44980
Epoch 49: Val Loss 495.62949
Epoch 50: Val Loss 489.54703
Epoch 51: Val Loss 483.11075
Epoch 52: Val Loss 476.33322
Epoch 53: Val Loss 469.24002
Epoch 54: Val Loss 461.82455
Epoch 55: Val Loss 454.13025
Epoch 56: Val Loss 446.17291
Epoch 57: Val Loss 438.03629
Epoch 58: Val Loss 429.59045
Epoch 59: Val Loss 420.84708
Epoch 60: Val Loss 411.91101
Epoch 61: Val Loss 402.78809
Epoch 62: Val Loss 393.53311
Epoch 63: Val Loss 383.99326
Epoch 64: Val Loss 374.05109
Epoch 65: Val Loss 363.93115
Epoch 66: Val Loss 353.86731
Epoch 67: Val Loss 343.82043
Epoch 68: Val Loss 333.64456
Epoch 69: Val Loss 323.48425
Epoch 70: Val Loss 313.51242
Epoch 71: Val Loss 303.71371
Epoch 72: Val Loss 293.63858
Epoch 73: Val Loss 283.55609
Epoch 74: Val Loss 273.58716
Epoch 75: Val Loss 263.83551
Epoch 76: Val Loss 254.17986
Epoch 77: Val Loss 244.86885
Epoch 78: Val Loss 235.57220
Epoch 79: Val Loss 226.51703
Epoch 80: Val Loss 217.76801
Epoch 81: Val Loss 209.60835
Epoch 82: Val Loss 201.73170
Epoch 83: Val Loss 193.97215
Epoch 84: Val Loss 186.64256
Epoch 85: Val Loss 179.76181
Epoch 86: Val Loss 173.19318
Epoch 87: Val Loss 167.00356
Epoch 88: Val Loss 161.29529
Epoch 89: Val Loss 156.02364
Epoch 90: Val Loss 151.28233
Epoch 91: Val Loss 146.83179
Epoch 92: Val Loss 142.85579
Epoch 93: Val Loss 139.17577
Epoch 94: Val Loss 135.74486
Epoch 95: Val Loss 132.51218
Epoch 96: Val Loss 129.52823
Epoch 97: Val Loss 126.64520
Epoch 98: Val Loss 124.15459
Epoch 99: Val Loss 121.81869
{'MSE - mean': 102.4608197464487, 'MSE - std': 19.357867621831744, 'R2 - mean': -0.3127417985023865, 'R2 - std': 0.19977997342597076} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 682.25446
Epoch 1: Val Loss 681.94293
Epoch 2: Val Loss 681.61310
Epoch 3: Val Loss 681.26630
Epoch 4: Val Loss 680.89960
Epoch 5: Val Loss 680.51166
Epoch 6: Val Loss 680.09949
Epoch 7: Val Loss 679.66058
Epoch 8: Val Loss 679.19385
Epoch 9: Val Loss 678.69525
Epoch 10: Val Loss 678.16980
Epoch 11: Val Loss 677.60846
Epoch 12: Val Loss 677.00385
Epoch 13: Val Loss 676.35779
Epoch 14: Val Loss 675.65485
Epoch 15: Val Loss 674.89984
Epoch 16: Val Loss 674.08038
Epoch 17: Val Loss 673.19153
Epoch 18: Val Loss 672.22742
Epoch 19: Val Loss 671.18988
Epoch 20: Val Loss 670.06592
Epoch 21: Val Loss 668.85980
Epoch 22: Val Loss 667.54669
Epoch 23: Val Loss 666.07123
Epoch 24: Val Loss 664.41333
Epoch 25: Val Loss 662.56909
Epoch 26: Val Loss 660.54718
Epoch 27: Val Loss 658.33691
Epoch 28: Val Loss 655.96826
Epoch 29: Val Loss 653.44873
Epoch 30: Val Loss 650.76111
Epoch 31: Val Loss 647.90833
Epoch 32: Val Loss 644.84686
Epoch 33: Val Loss 641.56525
Epoch 34: Val Loss 638.06482
Epoch 35: Val Loss 634.29016
Epoch 36: Val Loss 630.33923
Epoch 37: Val Loss 626.16681
Epoch 38: Val Loss 621.77264
Epoch 39: Val Loss 617.10992
Epoch 40: Val Loss 612.14520
Epoch 41: Val Loss 606.88892
Epoch 42: Val Loss 601.38397
Epoch 43: Val Loss 595.58716
Epoch 44: Val Loss 589.51215
Epoch 45: Val Loss 583.17108
Epoch 46: Val Loss 576.44556
Epoch 47: Val Loss 569.38641
Epoch 48: Val Loss 561.94647
Epoch 49: Val Loss 554.29730
Epoch 50: Val Loss 546.31940
Epoch 51: Val Loss 537.93237
Epoch 52: Val Loss 529.29761
Epoch 53: Val Loss 520.36694
Epoch 54: Val Loss 511.25711
Epoch 55: Val Loss 502.03070
Epoch 56: Val Loss 492.49609
Epoch 57: Val Loss 482.52304
Epoch 58: Val Loss 472.29013
Epoch 59: Val Loss 461.65439
Epoch 60: Val Loss 450.77475
Epoch 61: Val Loss 439.53384
Epoch 62: Val Loss 428.14703
Epoch 63: Val Loss 416.54044
Epoch 64: Val Loss 404.61984
Epoch 65: Val Loss 392.61893
Epoch 66: Val Loss 380.51633
Epoch 67: Val Loss 368.34705
Epoch 68: Val Loss 356.05618
Epoch 69: Val Loss 343.75488
Epoch 70: Val Loss 331.44693
Epoch 71: Val Loss 319.14285
Epoch 72: Val Loss 306.80832
Epoch 73: Val Loss 294.46829
Epoch 74: Val Loss 282.32596
Epoch 75: Val Loss 270.63116
Epoch 76: Val Loss 259.49814
Epoch 77: Val Loss 248.59761
Epoch 78: Val Loss 237.88509
Epoch 79: Val Loss 227.39441
Epoch 80: Val Loss 217.03577
Epoch 81: Val Loss 207.00774
Epoch 82: Val Loss 197.35210
Epoch 83: Val Loss 188.34824
Epoch 84: Val Loss 179.62239
Epoch 85: Val Loss 171.18753
Epoch 86: Val Loss 163.27432
Epoch 87: Val Loss 155.94475
Epoch 88: Val Loss 149.10776
Epoch 89: Val Loss 142.48141
Epoch 90: Val Loss 136.53717
Epoch 91: Val Loss 131.03018
Epoch 92: Val Loss 126.01125
Epoch 93: Val Loss 121.42363
Epoch 94: Val Loss 117.25721
Epoch 95: Val Loss 113.33305
Epoch 96: Val Loss 109.82824
Epoch 97: Val Loss 106.46818
Epoch 98: Val Loss 103.51249
Epoch 99: Val Loss 100.75937
{'MSE - mean': 101.89366939622533, 'MSE - std': 15.82597058474113, 'R2 - mean': -0.22709598476974224, 'R2 - std': 0.20317095266771767} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 557.68140
Epoch 1: Val Loss 557.12274
Epoch 2: Val Loss 556.55206
Epoch 3: Val Loss 555.97717
Epoch 4: Val Loss 555.41431
Epoch 5: Val Loss 554.85223
Epoch 6: Val Loss 554.29608
Epoch 7: Val Loss 553.75867
Epoch 8: Val Loss 553.26270
Epoch 9: Val Loss 552.79999
Epoch 10: Val Loss 552.37408
Epoch 11: Val Loss 551.96942
Epoch 12: Val Loss 551.58911
Epoch 13: Val Loss 551.23895
Epoch 14: Val Loss 550.91620
Epoch 15: Val Loss 550.61078
Epoch 16: Val Loss 550.32062
Epoch 17: Val Loss 550.04163
Epoch 18: Val Loss 549.77930
Epoch 19: Val Loss 549.52948
Epoch 20: Val Loss 549.28973
Epoch 21: Val Loss 549.06097
Epoch 22: Val Loss 548.83978
Epoch 23: Val Loss 548.62335
Epoch 24: Val Loss 548.41443
Epoch 25: Val Loss 548.20923
Epoch 26: Val Loss 548.00616
Epoch 27: Val Loss 547.80707
Epoch 28: Val Loss 547.61157
Epoch 29: Val Loss 547.41931
Epoch 30: Val Loss 547.22839
Epoch 31: Val Loss 547.03931
Epoch 32: Val Loss 546.85223
Epoch 33: Val Loss 546.66583
Epoch 34: Val Loss 546.47925
Epoch 35: Val Loss 546.29364
Epoch 36: Val Loss 546.10828
Epoch 37: Val Loss 545.92371
Epoch 38: Val Loss 545.74139
Epoch 39: Val Loss 545.56000
Epoch 40: Val Loss 545.37885
Epoch 41: Val Loss 545.19824
Epoch 42: Val Loss 545.01837
Epoch 43: Val Loss 544.83881
Epoch 44: Val Loss 544.65906
Epoch 45: Val Loss 544.48010
Epoch 46: Val Loss 544.30109
Epoch 47: Val Loss 544.12207
Epoch 48: Val Loss 543.94531
Epoch 49: Val Loss 543.77051
Epoch 50: Val Loss 543.59460
Epoch 51: Val Loss 543.41809
Epoch 52: Val Loss 543.24261
Epoch 53: Val Loss 543.06757
Epoch 54: Val Loss 542.89130
Epoch 55: Val Loss 542.71582
Epoch 56: Val Loss 542.54065
Epoch 57: Val Loss 542.36450
Epoch 58: Val Loss 542.18671
Epoch 59: Val Loss 542.01068
Epoch 60: Val Loss 541.83630
Epoch 61: Val Loss 541.66309
Epoch 62: Val Loss 541.48987
Epoch 63: Val Loss 541.31653
Epoch 64: Val Loss 541.14264
Epoch 65: Val Loss 540.96906
Epoch 66: Val Loss 540.79681
Epoch 67: Val Loss 540.62427
Epoch 68: Val Loss 540.45160
Epoch 69: Val Loss 540.27753
Epoch 70: Val Loss 540.10394
Epoch 71: Val Loss 539.93335
Epoch 72: Val Loss 539.76300
Epoch 73: Val Loss 539.59259
Epoch 74: Val Loss 539.42078
Epoch 75: Val Loss 539.24957
Epoch 76: Val Loss 539.07922
Epoch 77: Val Loss 538.90869
Epoch 78: Val Loss 538.73926
Epoch 79: Val Loss 538.57129
Epoch 80: Val Loss 538.40247
Epoch 81: Val Loss 538.23218
Epoch 82: Val Loss 538.06134
Epoch 83: Val Loss 537.89117
Epoch 84: Val Loss 537.72205
Epoch 85: Val Loss 537.55316
Epoch 86: Val Loss 537.38556
Epoch 87: Val Loss 537.21759
Epoch 88: Val Loss 537.05042
Epoch 89: Val Loss 536.88367
Epoch 90: Val Loss 536.71588
Epoch 91: Val Loss 536.54724
Epoch 92: Val Loss 536.37994
Epoch 93: Val Loss 536.21503
Epoch 94: Val Loss 536.04987
Epoch 95: Val Loss 535.88495
Epoch 96: Val Loss 535.71667
Epoch 97: Val Loss 535.54828
Epoch 98: Val Loss 535.38013
Epoch 99: Val Loss 535.21191
{'MSE - mean': 210.2232334720518, 'MSE - std': 188.13221246582836, 'R2 - mean': -1.6935906418402662, 'R2 - std': 2.5461301154212665} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 542.47174
Epoch 1: Val Loss 541.80829
Epoch 2: Val Loss 541.14044
Epoch 3: Val Loss 540.45203
Epoch 4: Val Loss 539.71564
Epoch 5: Val Loss 538.94501
Epoch 6: Val Loss 538.13147
Epoch 7: Val Loss 537.26880
Epoch 8: Val Loss 536.35132
Epoch 9: Val Loss 535.38055
Epoch 10: Val Loss 534.35413
Epoch 11: Val Loss 533.28864
Epoch 12: Val Loss 532.15625
Epoch 13: Val Loss 530.95062
Epoch 14: Val Loss 529.67139
Epoch 15: Val Loss 528.32422
Epoch 16: Val Loss 526.90051
Epoch 17: Val Loss 525.38525
Epoch 18: Val Loss 523.79144
Epoch 19: Val Loss 522.08173
Epoch 20: Val Loss 520.26654
Epoch 21: Val Loss 518.36157
Epoch 22: Val Loss 516.35510
Epoch 23: Val Loss 514.23730
Epoch 24: Val Loss 511.98822
Epoch 25: Val Loss 509.57147
Epoch 26: Val Loss 507.04184
Epoch 27: Val Loss 504.32062
Epoch 28: Val Loss 501.38965
Epoch 29: Val Loss 498.30383
Epoch 30: Val Loss 495.06311
Epoch 31: Val Loss 491.68231
Epoch 32: Val Loss 488.16846
Epoch 33: Val Loss 484.43277
Epoch 34: Val Loss 480.51474
Epoch 35: Val Loss 476.40494
Epoch 36: Val Loss 472.03751
Epoch 37: Val Loss 467.42407
Epoch 38: Val Loss 462.65198
Epoch 39: Val Loss 457.61417
Epoch 40: Val Loss 452.44611
Epoch 41: Val Loss 447.09238
Epoch 42: Val Loss 441.46426
Epoch 43: Val Loss 435.52899
Epoch 44: Val Loss 429.46991
Epoch 45: Val Loss 423.21048
Epoch 46: Val Loss 416.77200
Epoch 47: Val Loss 410.11224
Epoch 48: Val Loss 403.37982
Epoch 49: Val Loss 396.45868
Epoch 50: Val Loss 389.33337
Epoch 51: Val Loss 382.00412
Epoch 52: Val Loss 374.58987
Epoch 53: Val Loss 366.89267
Epoch 54: Val Loss 359.06482
Epoch 55: Val Loss 351.16760
Epoch 56: Val Loss 343.08728
Epoch 57: Val Loss 334.98755
Epoch 58: Val Loss 326.86371
Epoch 59: Val Loss 318.67368
Epoch 60: Val Loss 310.45300
Epoch 61: Val Loss 302.22363
Epoch 62: Val Loss 294.06845
Epoch 63: Val Loss 285.91620
Epoch 64: Val Loss 277.78165
Epoch 65: Val Loss 269.74869
Epoch 66: Val Loss 261.94226
Epoch 67: Val Loss 254.32887
Epoch 68: Val Loss 246.83168
Epoch 69: Val Loss 239.45724
Epoch 70: Val Loss 232.24118
Epoch 71: Val Loss 225.28778
Epoch 72: Val Loss 218.63815
Epoch 73: Val Loss 212.19751
Epoch 74: Val Loss 206.05730
Epoch 75: Val Loss 200.26164
Epoch 76: Val Loss 194.68521
Epoch 77: Val Loss 189.33653
Epoch 78: Val Loss 184.36823
Epoch 79: Val Loss 179.77550
Epoch 80: Val Loss 175.48764
Epoch 81: Val Loss 171.51714
Epoch 82: Val Loss 167.84921
Epoch 83: Val Loss 164.36844
Epoch 84: Val Loss 161.23734
Epoch 85: Val Loss 158.37045
Epoch 86: Val Loss 155.72247
Epoch 87: Val Loss 153.15695
Epoch 88: Val Loss 150.77112
Epoch 89: Val Loss 148.67245
Epoch 90: Val Loss 146.79099
Epoch 91: Val Loss 144.99442
Epoch 92: Val Loss 143.32054
Epoch 93: Val Loss 141.73438
Epoch 94: Val Loss 140.31827
Epoch 95: Val Loss 138.92847
Epoch 96: Val Loss 137.63954
Epoch 97: Val Loss 136.38667
Epoch 98: Val Loss 135.18382
Epoch 99: Val Loss 134.10951
{'MSE - mean': 195.00048677562603, 'MSE - std': 171.00266536450312, 'R2 - mean': -1.4487259683752627, 'R2 - std': 2.3293899812973744} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 195.00048677562603 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 13 with value: 195.00048677562603.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 544.07709
Epoch 1: Val Loss 543.65137
Epoch 2: Val Loss 543.26294
Epoch 3: Val Loss 542.89954
Epoch 4: Val Loss 542.57843
Epoch 5: Val Loss 542.28632
Epoch 6: Val Loss 542.00354
Epoch 7: Val Loss 541.72729
Epoch 8: Val Loss 541.43207
Epoch 9: Val Loss 541.11664
Epoch 10: Val Loss 540.78546
Epoch 11: Val Loss 540.42926
Epoch 12: Val Loss 540.04504
Epoch 13: Val Loss 539.63464
Epoch 14: Val Loss 539.19525
Epoch 15: Val Loss 538.72839
Epoch 16: Val Loss 538.22668
Epoch 17: Val Loss 537.68097
Epoch 18: Val Loss 537.08374
Epoch 19: Val Loss 536.43750
Epoch 20: Val Loss 535.73846
Epoch 21: Val Loss 534.98645
Epoch 22: Val Loss 534.17249
Epoch 23: Val Loss 533.29889
Epoch 24: Val Loss 532.37231
Epoch 25: Val Loss 531.38098
Epoch 26: Val Loss 530.33038
Epoch 27: Val Loss 529.18262
Epoch 28: Val Loss 527.96906
Epoch 29: Val Loss 526.66650
Epoch 30: Val Loss 525.25531
Epoch 31: Val Loss 523.75635
Epoch 32: Val Loss 522.13391
Epoch 33: Val Loss 520.40460
Epoch 34: Val Loss 518.60712
Epoch 35: Val Loss 516.72864
Epoch 36: Val Loss 514.71991
Epoch 37: Val Loss 512.59406
Epoch 38: Val Loss 510.34637
Epoch 39: Val Loss 507.99893
Epoch 40: Val Loss 505.50565
Epoch 41: Val Loss 502.84763
Epoch 42: Val Loss 500.04398
Epoch 43: Val Loss 497.07538
Epoch 44: Val Loss 493.93588
Epoch 45: Val Loss 490.61047
Epoch 46: Val Loss 487.10242
Epoch 47: Val Loss 483.43835
Epoch 48: Val Loss 479.56543
Epoch 49: Val Loss 475.50125
Epoch 50: Val Loss 471.28265
Epoch 51: Val Loss 466.85486
Epoch 52: Val Loss 462.23547
Epoch 53: Val Loss 457.38510
Epoch 54: Val Loss 452.35101
Epoch 55: Val Loss 447.10117
Epoch 56: Val Loss 441.67319
Epoch 57: Val Loss 436.00476
Epoch 58: Val Loss 430.12891
Epoch 59: Val Loss 424.15051
Epoch 60: Val Loss 417.98993
Epoch 61: Val Loss 411.62543
Epoch 62: Val Loss 405.19971
Epoch 63: Val Loss 398.55963
Epoch 64: Val Loss 391.68671
Epoch 65: Val Loss 384.58932
Epoch 66: Val Loss 377.46094
Epoch 67: Val Loss 370.18219
Epoch 68: Val Loss 362.83112
Epoch 69: Val Loss 355.32803
Epoch 70: Val Loss 347.57684
Epoch 71: Val Loss 339.66678
Epoch 72: Val Loss 331.58224
Epoch 73: Val Loss 323.49814
Epoch 74: Val Loss 315.37405
Epoch 75: Val Loss 307.16864
Epoch 76: Val Loss 298.98798
Epoch 77: Val Loss 290.95999
Epoch 78: Val Loss 283.12366
Epoch 79: Val Loss 275.29764
Epoch 80: Val Loss 267.42441
Epoch 81: Val Loss 259.59036
Epoch 82: Val Loss 251.83025
Epoch 83: Val Loss 244.23212
Epoch 84: Val Loss 236.66231
Epoch 85: Val Loss 229.20392
Epoch 86: Val Loss 221.86789
Epoch 87: Val Loss 214.64880
Epoch 88: Val Loss 207.91916
Epoch 89: Val Loss 201.56879
Epoch 90: Val Loss 195.45807
Epoch 91: Val Loss 189.40843
Epoch 92: Val Loss 183.61511
Epoch 93: Val Loss 178.08582
Epoch 94: Val Loss 172.75586
Epoch 95: Val Loss 167.82733
Epoch 96: Val Loss 163.26364
Epoch 97: Val Loss 158.95851
Epoch 98: Val Loss 154.82381
Epoch 99: Val Loss 150.95612
{'MSE - mean': 150.95611541852983, 'MSE - std': 0.0, 'R2 - mean': -1.0216898368509963, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 616.28149
Epoch 1: Val Loss 615.64484
Epoch 2: Val Loss 614.96173
Epoch 3: Val Loss 614.23352
Epoch 4: Val Loss 613.45520
Epoch 5: Val Loss 612.62433
Epoch 6: Val Loss 611.74622
Epoch 7: Val Loss 610.79700
Epoch 8: Val Loss 609.78064
Epoch 9: Val Loss 608.68988
Epoch 10: Val Loss 607.51947
Epoch 11: Val Loss 606.26068
Epoch 12: Val Loss 604.90228
Epoch 13: Val Loss 603.43872
Epoch 14: Val Loss 601.83771
Epoch 15: Val Loss 600.10828
Epoch 16: Val Loss 598.23242
Epoch 17: Val Loss 596.23303
Epoch 18: Val Loss 594.08319
Epoch 19: Val Loss 591.79669
Epoch 20: Val Loss 589.36279
Epoch 21: Val Loss 586.77277
Epoch 22: Val Loss 584.02264
Epoch 23: Val Loss 581.13019
Epoch 24: Val Loss 578.07520
Epoch 25: Val Loss 574.84503
Epoch 26: Val Loss 571.45612
Epoch 27: Val Loss 567.84528
Epoch 28: Val Loss 564.02832
Epoch 29: Val Loss 560.00928
Epoch 30: Val Loss 555.80103
Epoch 31: Val Loss 551.37463
Epoch 32: Val Loss 546.71075
Epoch 33: Val Loss 541.85767
Epoch 34: Val Loss 536.79218
Epoch 35: Val Loss 531.50488
Epoch 36: Val Loss 525.95837
Epoch 37: Val Loss 520.10974
Epoch 38: Val Loss 513.98199
Epoch 39: Val Loss 507.63184
Epoch 40: Val Loss 501.02606
Epoch 41: Val Loss 494.14417
Epoch 42: Val Loss 486.96390
Epoch 43: Val Loss 479.48074
Epoch 44: Val Loss 471.75519
Epoch 45: Val Loss 463.76678
Epoch 46: Val Loss 455.52661
Epoch 47: Val Loss 446.99509
Epoch 48: Val Loss 438.30951
Epoch 49: Val Loss 429.42813
Epoch 50: Val Loss 420.25317
Epoch 51: Val Loss 410.74268
Epoch 52: Val Loss 400.95520
Epoch 53: Val Loss 391.10635
Epoch 54: Val Loss 381.20010
Epoch 55: Val Loss 371.16333
Epoch 56: Val Loss 360.90277
Epoch 57: Val Loss 350.51959
Epoch 58: Val Loss 339.96674
Epoch 59: Val Loss 329.41055
Epoch 60: Val Loss 318.75519
Epoch 61: Val Loss 308.02322
Epoch 62: Val Loss 297.37674
Epoch 63: Val Loss 286.88742
Epoch 64: Val Loss 276.51855
Epoch 65: Val Loss 266.28714
Epoch 66: Val Loss 256.11908
Epoch 67: Val Loss 246.20987
Epoch 68: Val Loss 236.62062
Epoch 69: Val Loss 227.32791
Epoch 70: Val Loss 218.39519
Epoch 71: Val Loss 209.65103
Epoch 72: Val Loss 201.26237
Epoch 73: Val Loss 193.15935
Epoch 74: Val Loss 185.52962
Epoch 75: Val Loss 178.16350
Epoch 76: Val Loss 171.11423
Epoch 77: Val Loss 164.54079
Epoch 78: Val Loss 158.22403
Epoch 79: Val Loss 152.32077
Epoch 80: Val Loss 146.82478
Epoch 81: Val Loss 141.72287
Epoch 82: Val Loss 137.01378
Epoch 83: Val Loss 132.64957
Epoch 84: Val Loss 128.57008
Epoch 85: Val Loss 124.81834
Epoch 86: Val Loss 121.38283
Epoch 87: Val Loss 118.21919
Epoch 88: Val Loss 115.23893
Epoch 89: Val Loss 112.54045
Epoch 90: Val Loss 110.11842
Epoch 91: Val Loss 107.74863
Epoch 92: Val Loss 105.50474
Epoch 93: Val Loss 103.35501
Epoch 94: Val Loss 101.37995
Epoch 95: Val Loss 99.45798
Epoch 96: Val Loss 97.58721
Epoch 97: Val Loss 95.77410
Epoch 98: Val Loss 94.18823
Epoch 99: Val Loss 92.76242
{'MSE - mean': 121.8592713429244, 'MSE - std': 29.096844075605432, 'R2 - mean': -0.5867220432321877, 'R2 - std': 0.43496779361880866} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 666.38739
Epoch 1: Val Loss 665.87830
Epoch 2: Val Loss 665.39966
Epoch 3: Val Loss 664.95209
Epoch 4: Val Loss 664.53082
Epoch 5: Val Loss 664.12628
Epoch 6: Val Loss 663.73126
Epoch 7: Val Loss 663.35480
Epoch 8: Val Loss 662.99646
Epoch 9: Val Loss 662.65173
Epoch 10: Val Loss 662.31561
Epoch 11: Val Loss 661.98712
Epoch 12: Val Loss 661.66833
Epoch 13: Val Loss 661.35687
Epoch 14: Val Loss 661.05988
Epoch 15: Val Loss 660.77539
Epoch 16: Val Loss 660.50732
Epoch 17: Val Loss 660.25702
Epoch 18: Val Loss 660.02094
Epoch 19: Val Loss 659.79456
Epoch 20: Val Loss 659.57642
Epoch 21: Val Loss 659.36401
Epoch 22: Val Loss 659.15582
Epoch 23: Val Loss 658.95166
Epoch 24: Val Loss 658.75232
Epoch 25: Val Loss 658.55243
Epoch 26: Val Loss 658.35382
Epoch 27: Val Loss 658.15466
Epoch 28: Val Loss 657.95581
Epoch 29: Val Loss 657.75726
Epoch 30: Val Loss 657.55975
Epoch 31: Val Loss 657.36163
Epoch 32: Val Loss 657.16382
Epoch 33: Val Loss 656.96802
Epoch 34: Val Loss 656.77271
Epoch 35: Val Loss 656.58075
Epoch 36: Val Loss 656.38892
Epoch 37: Val Loss 656.19727
Epoch 38: Val Loss 656.00403
Epoch 39: Val Loss 655.81091
Epoch 40: Val Loss 655.61816
Epoch 41: Val Loss 655.42651
Epoch 42: Val Loss 655.23468
Epoch 43: Val Loss 655.04364
Epoch 44: Val Loss 654.85309
Epoch 45: Val Loss 654.66406
Epoch 46: Val Loss 654.47485
Epoch 47: Val Loss 654.28571
Epoch 48: Val Loss 654.09589
Epoch 49: Val Loss 653.90649
Epoch 50: Val Loss 653.71741
Epoch 51: Val Loss 653.52698
Epoch 52: Val Loss 653.33685
Epoch 53: Val Loss 653.14850
Epoch 54: Val Loss 652.95959
Epoch 55: Val Loss 652.77106
Epoch 56: Val Loss 652.58289
Epoch 57: Val Loss 652.39490
Epoch 58: Val Loss 652.20691
Epoch 59: Val Loss 652.01819
Epoch 60: Val Loss 651.82867
Epoch 61: Val Loss 651.63892
Epoch 62: Val Loss 651.45001
Epoch 63: Val Loss 651.26141
Epoch 64: Val Loss 651.07361
Epoch 65: Val Loss 650.88556
Epoch 66: Val Loss 650.69769
Epoch 67: Val Loss 650.50861
Epoch 68: Val Loss 650.32062
Epoch 69: Val Loss 650.13293
Epoch 70: Val Loss 649.94446
Epoch 71: Val Loss 649.75861
Epoch 72: Val Loss 649.57330
Epoch 73: Val Loss 649.38831
Epoch 74: Val Loss 649.20526
Epoch 75: Val Loss 649.02167
Epoch 76: Val Loss 648.83813
Epoch 77: Val Loss 648.65479
Epoch 78: Val Loss 648.47217
Epoch 79: Val Loss 648.28864
Epoch 80: Val Loss 648.10400
Epoch 81: Val Loss 647.91925
Epoch 82: Val Loss 647.73456
Epoch 83: Val Loss 647.54883
Epoch 84: Val Loss 647.36438
Epoch 85: Val Loss 647.17847
Epoch 86: Val Loss 646.99298
Epoch 87: Val Loss 646.80841
Epoch 88: Val Loss 646.62183
Epoch 89: Val Loss 646.43579
Epoch 90: Val Loss 646.25037
Epoch 91: Val Loss 646.06494
Epoch 92: Val Loss 645.88007
Epoch 93: Val Loss 645.69391
Epoch 94: Val Loss 645.50745
Epoch 95: Val Loss 645.32111
Epoch 96: Val Loss 645.13666
Epoch 97: Val Loss 644.95374
Epoch 98: Val Loss 644.77087
Epoch 99: Val Loss 644.58569
{'MSE - mean': 296.1014202318174, 'MSE - std': 247.55821629590827, 'R2 - mean': -2.309239446489209, 'R2 - std': 2.461760290937571} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 552.57922
Epoch 1: Val Loss 551.49365
Epoch 2: Val Loss 550.35254
Epoch 3: Val Loss 549.11694
Epoch 4: Val Loss 547.76343
Epoch 5: Val Loss 546.29871
Epoch 6: Val Loss 544.77411
Epoch 7: Val Loss 543.13971
Epoch 8: Val Loss 541.35748
Epoch 9: Val Loss 539.45728
Epoch 10: Val Loss 537.46088
Epoch 11: Val Loss 535.30182
Epoch 12: Val Loss 532.97638
Epoch 13: Val Loss 530.44214
Epoch 14: Val Loss 527.68182
Epoch 15: Val Loss 524.79175
Epoch 16: Val Loss 521.78192
Epoch 17: Val Loss 518.54260
Epoch 18: Val Loss 515.12512
Epoch 19: Val Loss 511.55692
Epoch 20: Val Loss 507.75827
Epoch 21: Val Loss 503.77216
Epoch 22: Val Loss 499.59839
Epoch 23: Val Loss 495.24588
Epoch 24: Val Loss 490.67758
Epoch 25: Val Loss 485.85983
Epoch 26: Val Loss 480.90536
Epoch 27: Val Loss 475.80612
Epoch 28: Val Loss 470.49759
Epoch 29: Val Loss 464.96478
Epoch 30: Val Loss 459.15976
Epoch 31: Val Loss 453.12006
Epoch 32: Val Loss 446.81226
Epoch 33: Val Loss 440.29691
Epoch 34: Val Loss 433.46136
Epoch 35: Val Loss 426.39633
Epoch 36: Val Loss 419.10553
Epoch 37: Val Loss 411.63690
Epoch 38: Val Loss 403.94363
Epoch 39: Val Loss 395.94043
Epoch 40: Val Loss 387.80862
Epoch 41: Val Loss 379.60535
Epoch 42: Val Loss 371.13702
Epoch 43: Val Loss 362.49210
Epoch 44: Val Loss 353.71729
Epoch 45: Val Loss 344.74255
Epoch 46: Val Loss 335.40485
Epoch 47: Val Loss 325.88559
Epoch 48: Val Loss 316.24457
Epoch 49: Val Loss 306.54230
Epoch 50: Val Loss 296.57007
Epoch 51: Val Loss 286.56772
Epoch 52: Val Loss 276.73553
Epoch 53: Val Loss 266.91306
Epoch 54: Val Loss 257.03830
Epoch 55: Val Loss 247.08884
Epoch 56: Val Loss 237.03966
Epoch 57: Val Loss 227.11168
Epoch 58: Val Loss 217.50406
Epoch 59: Val Loss 208.21387
Epoch 60: Val Loss 199.04324
Epoch 61: Val Loss 190.00710
Epoch 62: Val Loss 181.28690
Epoch 63: Val Loss 172.83891
Epoch 64: Val Loss 164.72157
Epoch 65: Val Loss 157.07042
Epoch 66: Val Loss 149.79001
Epoch 67: Val Loss 143.00859
Epoch 68: Val Loss 136.67621
Epoch 69: Val Loss 130.65903
Epoch 70: Val Loss 125.02750
Epoch 71: Val Loss 119.90073
Epoch 72: Val Loss 115.16295
Epoch 73: Val Loss 110.77306
Epoch 74: Val Loss 106.79767
Epoch 75: Val Loss 103.20505
Epoch 76: Val Loss 99.85896
Epoch 77: Val Loss 96.85777
Epoch 78: Val Loss 94.13152
Epoch 79: Val Loss 91.60693
Epoch 80: Val Loss 89.23888
Epoch 81: Val Loss 87.06427
Epoch 82: Val Loss 84.97254
Epoch 83: Val Loss 82.96320
Epoch 84: Val Loss 81.01284
Epoch 85: Val Loss 79.11639
Epoch 86: Val Loss 77.23074
Epoch 87: Val Loss 75.45565
Epoch 88: Val Loss 73.69600
Epoch 89: Val Loss 72.01752
Epoch 90: Val Loss 70.33398
Epoch 91: Val Loss 68.71651
Epoch 92: Val Loss 67.10583
Epoch 93: Val Loss 65.53426
Epoch 94: Val Loss 63.98137
Epoch 95: Val Loss 62.41888
Epoch 96: Val Loss 60.85169
Epoch 97: Val Loss 59.34145
Epoch 98: Val Loss 57.85612
Epoch 99: Val Loss 56.47371
{'MSE - mean': 236.19449287019862, 'MSE - std': 238.1812811272951, 'R2 - mean': -1.6690387583745732, 'R2 - std': 2.4030748137990074} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 548.80688
Epoch 1: Val Loss 548.24585
Epoch 2: Val Loss 547.70337
Epoch 3: Val Loss 547.19354
Epoch 4: Val Loss 546.71423
Epoch 5: Val Loss 546.27155
Epoch 6: Val Loss 545.88477
Epoch 7: Val Loss 545.53223
Epoch 8: Val Loss 545.21002
Epoch 9: Val Loss 544.91461
Epoch 10: Val Loss 544.63092
Epoch 11: Val Loss 544.34308
Epoch 12: Val Loss 544.05670
Epoch 13: Val Loss 543.77222
Epoch 14: Val Loss 543.48169
Epoch 15: Val Loss 543.18793
Epoch 16: Val Loss 542.89325
Epoch 17: Val Loss 542.59833
Epoch 18: Val Loss 542.30212
Epoch 19: Val Loss 542.00555
Epoch 20: Val Loss 541.70941
Epoch 21: Val Loss 541.40503
Epoch 22: Val Loss 541.08710
Epoch 23: Val Loss 540.75275
Epoch 24: Val Loss 540.40417
Epoch 25: Val Loss 540.03485
Epoch 26: Val Loss 539.64288
Epoch 27: Val Loss 539.22736
Epoch 28: Val Loss 538.78455
Epoch 29: Val Loss 538.31006
Epoch 30: Val Loss 537.79291
Epoch 31: Val Loss 537.22742
Epoch 32: Val Loss 536.62024
Epoch 33: Val Loss 535.96002
Epoch 34: Val Loss 535.24811
Epoch 35: Val Loss 534.48303
Epoch 36: Val Loss 533.66547
Epoch 37: Val Loss 532.77979
Epoch 38: Val Loss 531.82465
Epoch 39: Val Loss 530.78876
Epoch 40: Val Loss 529.68903
Epoch 41: Val Loss 528.49902
Epoch 42: Val Loss 527.22302
Epoch 43: Val Loss 525.83588
Epoch 44: Val Loss 524.34167
Epoch 45: Val Loss 522.75146
Epoch 46: Val Loss 521.03754
Epoch 47: Val Loss 519.21295
Epoch 48: Val Loss 517.24603
Epoch 49: Val Loss 515.12555
Epoch 50: Val Loss 512.84589
Epoch 51: Val Loss 510.44818
Epoch 52: Val Loss 507.88730
Epoch 53: Val Loss 505.13980
Epoch 54: Val Loss 502.19016
Epoch 55: Val Loss 499.03156
Epoch 56: Val Loss 495.64185
Epoch 57: Val Loss 492.03387
Epoch 58: Val Loss 488.13895
Epoch 59: Val Loss 484.05688
Epoch 60: Val Loss 479.74088
Epoch 61: Val Loss 475.17380
Epoch 62: Val Loss 470.32553
Epoch 63: Val Loss 465.25671
Epoch 64: Val Loss 459.92136
Epoch 65: Val Loss 454.36218
Epoch 66: Val Loss 448.43402
Epoch 67: Val Loss 442.16406
Epoch 68: Val Loss 435.66425
Epoch 69: Val Loss 428.91553
Epoch 70: Val Loss 421.95691
Epoch 71: Val Loss 414.69144
Epoch 72: Val Loss 407.16275
Epoch 73: Val Loss 399.37384
Epoch 74: Val Loss 391.22540
Epoch 75: Val Loss 382.88068
Epoch 76: Val Loss 374.32697
Epoch 77: Val Loss 365.61661
Epoch 78: Val Loss 356.67389
Epoch 79: Val Loss 347.43182
Epoch 80: Val Loss 338.07584
Epoch 81: Val Loss 328.60806
Epoch 82: Val Loss 319.00119
Epoch 83: Val Loss 309.39853
Epoch 84: Val Loss 299.78122
Epoch 85: Val Loss 290.14984
Epoch 86: Val Loss 280.45377
Epoch 87: Val Loss 270.60953
Epoch 88: Val Loss 260.79211
Epoch 89: Val Loss 251.26248
Epoch 90: Val Loss 241.68640
Epoch 91: Val Loss 232.25893
Epoch 92: Val Loss 222.97932
Epoch 93: Val Loss 213.91003
Epoch 94: Val Loss 204.95215
Epoch 95: Val Loss 196.24474
Epoch 96: Val Loss 187.88318
Epoch 97: Val Loss 179.79683
Epoch 98: Val Loss 172.08379
Epoch 99: Val Loss 164.82538
{'MSE - mean': 221.92066944778276, 'MSE - std': 214.94005276804248, 'R2 - mean': -1.4963874208676045, 'R2 - std': 2.176935641203912} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 221.92066944778276 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 13 with value: 195.00048677562603.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 541.78113
Epoch 1: Val Loss 540.61884
Epoch 2: Val Loss 539.45428
Epoch 3: Val Loss 538.28937
Epoch 4: Val Loss 537.11859
Epoch 5: Val Loss 535.95056
Epoch 6: Val Loss 534.76123
Epoch 7: Val Loss 533.54266
Epoch 8: Val Loss 532.31378
Epoch 9: Val Loss 531.06805
Epoch 10: Val Loss 529.80151
Epoch 11: Val Loss 528.50848
Epoch 12: Val Loss 527.18958
Epoch 13: Val Loss 525.82904
Epoch 14: Val Loss 524.40857
Epoch 15: Val Loss 522.93665
Epoch 16: Val Loss 521.41199
Epoch 17: Val Loss 519.81653
Epoch 18: Val Loss 518.15588
Epoch 19: Val Loss 516.43317
Epoch 20: Val Loss 514.63928
Epoch 21: Val Loss 512.77716
Epoch 22: Val Loss 510.85641
Epoch 23: Val Loss 508.86093
Epoch 24: Val Loss 506.76437
Epoch 25: Val Loss 504.55896
Epoch 26: Val Loss 502.26569
Epoch 27: Val Loss 499.88187
Epoch 28: Val Loss 497.40976
Epoch 29: Val Loss 494.85840
Epoch 30: Val Loss 492.18399
Epoch 31: Val Loss 489.38980
Epoch 32: Val Loss 486.46014
Epoch 33: Val Loss 483.40704
Epoch 34: Val Loss 480.26373
Epoch 35: Val Loss 476.97873
Epoch 36: Val Loss 473.53503
Epoch 37: Val Loss 470.00381
Epoch 38: Val Loss 466.34702
Epoch 39: Val Loss 462.56995
Epoch 40: Val Loss 458.61575
Epoch 41: Val Loss 454.54272
Epoch 42: Val Loss 450.23615
Epoch 43: Val Loss 445.81421
Epoch 44: Val Loss 441.30045
Epoch 45: Val Loss 436.62073
Epoch 46: Val Loss 431.77762
Epoch 47: Val Loss 426.79083
Epoch 48: Val Loss 421.66956
Epoch 49: Val Loss 416.35269
Epoch 50: Val Loss 410.88995
Epoch 51: Val Loss 405.26944
Epoch 52: Val Loss 399.50806
Epoch 53: Val Loss 393.59561
Epoch 54: Val Loss 387.44815
Epoch 55: Val Loss 381.13766
Epoch 56: Val Loss 374.69476
Epoch 57: Val Loss 368.07700
Epoch 58: Val Loss 361.27637
Epoch 59: Val Loss 354.30017
Epoch 60: Val Loss 347.13516
Epoch 61: Val Loss 339.85815
Epoch 62: Val Loss 332.46854
Epoch 63: Val Loss 324.95889
Epoch 64: Val Loss 317.30841
Epoch 65: Val Loss 309.48288
Epoch 66: Val Loss 301.58850
Epoch 67: Val Loss 293.60037
Epoch 68: Val Loss 285.50552
Epoch 69: Val Loss 277.18149
Epoch 70: Val Loss 268.75629
Epoch 71: Val Loss 260.34381
Epoch 72: Val Loss 251.96286
Epoch 73: Val Loss 243.52533
Epoch 74: Val Loss 235.10410
Epoch 75: Val Loss 226.68655
Epoch 76: Val Loss 218.38321
Epoch 77: Val Loss 210.14111
Epoch 78: Val Loss 201.87469
Epoch 79: Val Loss 193.59749
Epoch 80: Val Loss 185.53368
Epoch 81: Val Loss 177.69057
Epoch 82: Val Loss 169.90710
Epoch 83: Val Loss 162.31810
Epoch 84: Val Loss 154.84846
Epoch 85: Val Loss 147.45882
Epoch 86: Val Loss 140.30420
Epoch 87: Val Loss 133.45236
Epoch 88: Val Loss 126.72297
Epoch 89: Val Loss 120.30309
Epoch 90: Val Loss 114.26340
Epoch 91: Val Loss 108.56939
Epoch 92: Val Loss 103.17613
Epoch 93: Val Loss 98.08998
Epoch 94: Val Loss 93.38415
Epoch 95: Val Loss 88.93929
Epoch 96: Val Loss 84.72585
Epoch 97: Val Loss 80.86703
Epoch 98: Val Loss 77.20286
Epoch 99: Val Loss 73.74851
{'MSE - mean': 73.74850288501209, 'MSE - std': 0.0, 'R2 - mean': 0.012318259831799128, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 620.27356
Epoch 1: Val Loss 619.95050
Epoch 2: Val Loss 619.60632
Epoch 3: Val Loss 619.24170
Epoch 4: Val Loss 618.85858
Epoch 5: Val Loss 618.45026
Epoch 6: Val Loss 618.01794
Epoch 7: Val Loss 617.55524
Epoch 8: Val Loss 617.06390
Epoch 9: Val Loss 616.53961
Epoch 10: Val Loss 615.97443
Epoch 11: Val Loss 615.37177
Epoch 12: Val Loss 614.72260
Epoch 13: Val Loss 614.02960
Epoch 14: Val Loss 613.28674
Epoch 15: Val Loss 612.50140
Epoch 16: Val Loss 611.67200
Epoch 17: Val Loss 610.79456
Epoch 18: Val Loss 609.86047
Epoch 19: Val Loss 608.86993
Epoch 20: Val Loss 607.82458
Epoch 21: Val Loss 606.71765
Epoch 22: Val Loss 605.53229
Epoch 23: Val Loss 604.26630
Epoch 24: Val Loss 602.91516
Epoch 25: Val Loss 601.47339
Epoch 26: Val Loss 599.95422
Epoch 27: Val Loss 598.34723
Epoch 28: Val Loss 596.64465
Epoch 29: Val Loss 594.84473
Epoch 30: Val Loss 592.93402
Epoch 31: Val Loss 590.92694
Epoch 32: Val Loss 588.80725
Epoch 33: Val Loss 586.58453
Epoch 34: Val Loss 584.23865
Epoch 35: Val Loss 581.76489
Epoch 36: Val Loss 579.12628
Epoch 37: Val Loss 576.31866
Epoch 38: Val Loss 573.36920
Epoch 39: Val Loss 570.28729
Epoch 40: Val Loss 567.03687
Epoch 41: Val Loss 563.61993
Epoch 42: Val Loss 560.06097
Epoch 43: Val Loss 556.35400
Epoch 44: Val Loss 552.41730
Epoch 45: Val Loss 548.28943
Epoch 46: Val Loss 544.00848
Epoch 47: Val Loss 539.50293
Epoch 48: Val Loss 534.77307
Epoch 49: Val Loss 529.80475
Epoch 50: Val Loss 524.61371
Epoch 51: Val Loss 519.28381
Epoch 52: Val Loss 513.68872
Epoch 53: Val Loss 507.85071
Epoch 54: Val Loss 501.67944
Epoch 55: Val Loss 495.31891
Epoch 56: Val Loss 488.67496
Epoch 57: Val Loss 481.85040
Epoch 58: Val Loss 474.72995
Epoch 59: Val Loss 467.39948
Epoch 60: Val Loss 459.89902
Epoch 61: Val Loss 452.16437
Epoch 62: Val Loss 444.15778
Epoch 63: Val Loss 435.97134
Epoch 64: Val Loss 427.55383
Epoch 65: Val Loss 418.90540
Epoch 66: Val Loss 410.17990
Epoch 67: Val Loss 401.31494
Epoch 68: Val Loss 392.32639
Epoch 69: Val Loss 383.15710
Epoch 70: Val Loss 373.55612
Epoch 71: Val Loss 363.68073
Epoch 72: Val Loss 353.68185
Epoch 73: Val Loss 343.71829
Epoch 74: Val Loss 333.71582
Epoch 75: Val Loss 323.80814
Epoch 76: Val Loss 313.81461
Epoch 77: Val Loss 303.52487
Epoch 78: Val Loss 293.12265
Epoch 79: Val Loss 282.85934
Epoch 80: Val Loss 272.57883
Epoch 81: Val Loss 262.18365
Epoch 82: Val Loss 251.93953
Epoch 83: Val Loss 241.78667
Epoch 84: Val Loss 231.90245
Epoch 85: Val Loss 222.24277
Epoch 86: Val Loss 212.94038
Epoch 87: Val Loss 203.88374
Epoch 88: Val Loss 195.03294
Epoch 89: Val Loss 186.41638
Epoch 90: Val Loss 178.16255
Epoch 91: Val Loss 170.09070
Epoch 92: Val Loss 162.34444
Epoch 93: Val Loss 154.80675
Epoch 94: Val Loss 147.55176
Epoch 95: Val Loss 140.77649
Epoch 96: Val Loss 134.30673
Epoch 97: Val Loss 128.00076
Epoch 98: Val Loss 121.89778
Epoch 99: Val Loss 116.18317
{'MSE - mean': 94.96584080319275, 'MSE - std': 21.217337918180654, 'R2 - mean': -0.2151160337897215, 'R2 - std': 0.22743429362152062} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 684.53271
Epoch 1: Val Loss 683.99463
Epoch 2: Val Loss 683.44684
Epoch 3: Val Loss 682.88971
Epoch 4: Val Loss 682.31677
Epoch 5: Val Loss 681.73236
Epoch 6: Val Loss 681.12238
Epoch 7: Val Loss 680.48560
Epoch 8: Val Loss 679.82214
Epoch 9: Val Loss 679.13568
Epoch 10: Val Loss 678.42303
Epoch 11: Val Loss 677.67926
Epoch 12: Val Loss 676.89606
Epoch 13: Val Loss 676.08630
Epoch 14: Val Loss 675.24146
Epoch 15: Val Loss 674.35315
Epoch 16: Val Loss 673.41498
Epoch 17: Val Loss 672.41956
Epoch 18: Val Loss 671.37183
Epoch 19: Val Loss 670.26587
Epoch 20: Val Loss 669.10913
Epoch 21: Val Loss 667.89014
Epoch 22: Val Loss 666.59174
Epoch 23: Val Loss 665.22681
Epoch 24: Val Loss 663.78650
Epoch 25: Val Loss 662.25818
Epoch 26: Val Loss 660.62744
Epoch 27: Val Loss 658.90216
Epoch 28: Val Loss 657.04193
Epoch 29: Val Loss 655.06329
Epoch 30: Val Loss 652.95105
Epoch 31: Val Loss 650.69598
Epoch 32: Val Loss 648.34180
Epoch 33: Val Loss 645.82690
Epoch 34: Val Loss 643.09863
Epoch 35: Val Loss 640.19385
Epoch 36: Val Loss 637.12817
Epoch 37: Val Loss 633.93445
Epoch 38: Val Loss 630.60175
Epoch 39: Val Loss 627.13000
Epoch 40: Val Loss 623.44940
Epoch 41: Val Loss 619.49139
Epoch 42: Val Loss 615.32117
Epoch 43: Val Loss 610.96881
Epoch 44: Val Loss 606.46521
Epoch 45: Val Loss 601.67401
Epoch 46: Val Loss 596.57086
Epoch 47: Val Loss 591.15344
Epoch 48: Val Loss 585.44409
Epoch 49: Val Loss 579.37366
Epoch 50: Val Loss 572.94775
Epoch 51: Val Loss 566.17615
Epoch 52: Val Loss 559.02277
Epoch 53: Val Loss 551.60303
Epoch 54: Val Loss 543.84271
Epoch 55: Val Loss 535.72266
Epoch 56: Val Loss 527.27625
Epoch 57: Val Loss 518.49646
Epoch 58: Val Loss 509.35367
Epoch 59: Val Loss 499.90964
Epoch 60: Val Loss 490.02695
Epoch 61: Val Loss 479.67563
Epoch 62: Val Loss 468.96722
Epoch 63: Val Loss 457.82385
Epoch 64: Val Loss 446.64548
Epoch 65: Val Loss 435.01358
Epoch 66: Val Loss 422.99280
Epoch 67: Val Loss 410.78287
Epoch 68: Val Loss 398.31604
Epoch 69: Val Loss 385.96930
Epoch 70: Val Loss 373.58438
Epoch 71: Val Loss 361.22891
Epoch 72: Val Loss 348.51913
Epoch 73: Val Loss 335.67911
Epoch 74: Val Loss 322.98541
Epoch 75: Val Loss 310.45572
Epoch 76: Val Loss 297.93271
Epoch 77: Val Loss 285.58633
Epoch 78: Val Loss 273.56888
Epoch 79: Val Loss 261.87329
Epoch 80: Val Loss 250.53062
Epoch 81: Val Loss 239.37253
Epoch 82: Val Loss 228.48880
Epoch 83: Val Loss 218.03271
Epoch 84: Val Loss 208.04257
Epoch 85: Val Loss 198.65211
Epoch 86: Val Loss 189.75508
Epoch 87: Val Loss 181.40527
Epoch 88: Val Loss 173.67781
Epoch 89: Val Loss 166.74271
Epoch 90: Val Loss 160.41685
Epoch 91: Val Loss 154.62589
Epoch 92: Val Loss 149.28777
Epoch 93: Val Loss 144.45828
Epoch 94: Val Loss 140.18358
Epoch 95: Val Loss 136.28793
Epoch 96: Val Loss 132.68800
Epoch 97: Val Loss 129.39769
Epoch 98: Val Loss 126.46140
Epoch 99: Val Loss 123.78722
{'MSE - mean': 104.57296297424331, 'MSE - std': 22.01614282842994, 'R2 - mean': -0.2424443392144907, 'R2 - std': 0.18967844148679358} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 544.82349
Epoch 1: Val Loss 544.12842
Epoch 2: Val Loss 543.38763
Epoch 3: Val Loss 542.59064
Epoch 4: Val Loss 541.72131
Epoch 5: Val Loss 540.78339
Epoch 6: Val Loss 539.77655
Epoch 7: Val Loss 538.67847
Epoch 8: Val Loss 537.49054
Epoch 9: Val Loss 536.21332
Epoch 10: Val Loss 534.88953
Epoch 11: Val Loss 533.51801
Epoch 12: Val Loss 532.11108
Epoch 13: Val Loss 530.68011
Epoch 14: Val Loss 529.21185
Epoch 15: Val Loss 527.69482
Epoch 16: Val Loss 526.11841
Epoch 17: Val Loss 524.49341
Epoch 18: Val Loss 522.83887
Epoch 19: Val Loss 521.13013
Epoch 20: Val Loss 519.34985
Epoch 21: Val Loss 517.50525
Epoch 22: Val Loss 515.57819
Epoch 23: Val Loss 513.54858
Epoch 24: Val Loss 511.45273
Epoch 25: Val Loss 509.28201
Epoch 26: Val Loss 507.00507
Epoch 27: Val Loss 504.61819
Epoch 28: Val Loss 502.13977
Epoch 29: Val Loss 499.57797
Epoch 30: Val Loss 496.93430
Epoch 31: Val Loss 494.21216
Epoch 32: Val Loss 491.33679
Epoch 33: Val Loss 488.35062
Epoch 34: Val Loss 485.21823
Epoch 35: Val Loss 481.95465
Epoch 36: Val Loss 478.55673
Epoch 37: Val Loss 475.08362
Epoch 38: Val Loss 471.50668
Epoch 39: Val Loss 467.84232
Epoch 40: Val Loss 464.01755
Epoch 41: Val Loss 460.07947
Epoch 42: Val Loss 455.99094
Epoch 43: Val Loss 451.76593
Epoch 44: Val Loss 447.34985
Epoch 45: Val Loss 442.79965
Epoch 46: Val Loss 438.14487
Epoch 47: Val Loss 433.38846
Epoch 48: Val Loss 428.46713
Epoch 49: Val Loss 423.37592
Epoch 50: Val Loss 418.13214
Epoch 51: Val Loss 412.75055
Epoch 52: Val Loss 407.25455
Epoch 53: Val Loss 401.61246
Epoch 54: Val Loss 395.85565
Epoch 55: Val Loss 389.96945
Epoch 56: Val Loss 383.95700
Epoch 57: Val Loss 377.73456
Epoch 58: Val Loss 371.36441
Epoch 59: Val Loss 364.80042
Epoch 60: Val Loss 358.13675
Epoch 61: Val Loss 351.41196
Epoch 62: Val Loss 344.53494
Epoch 63: Val Loss 337.53378
Epoch 64: Val Loss 330.39392
Epoch 65: Val Loss 323.12933
Epoch 66: Val Loss 315.75708
Epoch 67: Val Loss 308.26987
Epoch 68: Val Loss 300.65283
Epoch 69: Val Loss 292.95847
Epoch 70: Val Loss 285.23187
Epoch 71: Val Loss 277.41101
Epoch 72: Val Loss 269.46930
Epoch 73: Val Loss 261.59668
Epoch 74: Val Loss 253.74834
Epoch 75: Val Loss 245.97205
Epoch 76: Val Loss 238.20451
Epoch 77: Val Loss 230.34471
Epoch 78: Val Loss 222.40369
Epoch 79: Val Loss 214.64177
Epoch 80: Val Loss 207.07722
Epoch 81: Val Loss 199.71298
Epoch 82: Val Loss 192.56177
Epoch 83: Val Loss 185.54289
Epoch 84: Val Loss 178.69209
Epoch 85: Val Loss 171.90776
Epoch 86: Val Loss 165.36653
Epoch 87: Val Loss 159.16066
Epoch 88: Val Loss 153.09645
Epoch 89: Val Loss 147.28432
Epoch 90: Val Loss 141.70407
Epoch 91: Val Loss 136.37999
Epoch 92: Val Loss 131.26353
Epoch 93: Val Loss 126.50520
Epoch 94: Val Loss 122.17077
Epoch 95: Val Loss 118.21947
Epoch 96: Val Loss 114.43683
Epoch 97: Val Loss 110.91092
Epoch 98: Val Loss 107.68327
Epoch 99: Val Loss 104.71544
{'MSE - mean': 104.60858392716963, 'MSE - std': 19.06663880546942, 'R2 - mean': -0.27877735333999765, 'R2 - std': 0.17590820651685668} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 553.09082
Epoch 1: Val Loss 552.31427
Epoch 2: Val Loss 551.59369
Epoch 3: Val Loss 550.93805
Epoch 4: Val Loss 550.34039
Epoch 5: Val Loss 549.79712
Epoch 6: Val Loss 549.28802
Epoch 7: Val Loss 548.80237
Epoch 8: Val Loss 548.31592
Epoch 9: Val Loss 547.83862
Epoch 10: Val Loss 547.37738
Epoch 11: Val Loss 546.92627
Epoch 12: Val Loss 546.46942
Epoch 13: Val Loss 546.02032
Epoch 14: Val Loss 545.58289
Epoch 15: Val Loss 545.13757
Epoch 16: Val Loss 544.68713
Epoch 17: Val Loss 544.22021
Epoch 18: Val Loss 543.74939
Epoch 19: Val Loss 543.25592
Epoch 20: Val Loss 542.72003
Epoch 21: Val Loss 542.16254
Epoch 22: Val Loss 541.58374
Epoch 23: Val Loss 541.00610
Epoch 24: Val Loss 540.43005
Epoch 25: Val Loss 539.85516
Epoch 26: Val Loss 539.27924
Epoch 27: Val Loss 538.70422
Epoch 28: Val Loss 538.13086
Epoch 29: Val Loss 537.55017
Epoch 30: Val Loss 536.98438
Epoch 31: Val Loss 536.42664
Epoch 32: Val Loss 535.89160
Epoch 33: Val Loss 535.36206
Epoch 34: Val Loss 534.83240
Epoch 35: Val Loss 534.30914
Epoch 36: Val Loss 533.78540
Epoch 37: Val Loss 533.26398
Epoch 38: Val Loss 532.74854
Epoch 39: Val Loss 532.23541
Epoch 40: Val Loss 531.72980
Epoch 41: Val Loss 531.22699
Epoch 42: Val Loss 530.72668
Epoch 43: Val Loss 530.22852
Epoch 44: Val Loss 529.72668
Epoch 45: Val Loss 529.22626
Epoch 46: Val Loss 528.72162
Epoch 47: Val Loss 528.21320
Epoch 48: Val Loss 527.70551
Epoch 49: Val Loss 527.19861
Epoch 50: Val Loss 526.68793
Epoch 51: Val Loss 526.17346
Epoch 52: Val Loss 525.65472
Epoch 53: Val Loss 525.13275
Epoch 54: Val Loss 524.60309
Epoch 55: Val Loss 524.06061
Epoch 56: Val Loss 523.50800
Epoch 57: Val Loss 522.94135
Epoch 58: Val Loss 522.36206
Epoch 59: Val Loss 521.77081
Epoch 60: Val Loss 521.16559
Epoch 61: Val Loss 520.53790
Epoch 62: Val Loss 519.89587
Epoch 63: Val Loss 519.24127
Epoch 64: Val Loss 518.56909
Epoch 65: Val Loss 517.86841
Epoch 66: Val Loss 517.14166
Epoch 67: Val Loss 516.39185
Epoch 68: Val Loss 515.61993
Epoch 69: Val Loss 514.81506
Epoch 70: Val Loss 513.96826
Epoch 71: Val Loss 513.09412
Epoch 72: Val Loss 512.18921
Epoch 73: Val Loss 511.22168
Epoch 74: Val Loss 510.19531
Epoch 75: Val Loss 509.13098
Epoch 76: Val Loss 508.02551
Epoch 77: Val Loss 506.88730
Epoch 78: Val Loss 505.68756
Epoch 79: Val Loss 504.44205
Epoch 80: Val Loss 503.14139
Epoch 81: Val Loss 501.75449
Epoch 82: Val Loss 500.29126
Epoch 83: Val Loss 498.72021
Epoch 84: Val Loss 497.05151
Epoch 85: Val Loss 495.31552
Epoch 86: Val Loss 493.52289
Epoch 87: Val Loss 491.63281
Epoch 88: Val Loss 489.62643
Epoch 89: Val Loss 487.49133
Epoch 90: Val Loss 485.23892
Epoch 91: Val Loss 482.85495
Epoch 92: Val Loss 480.32803
Epoch 93: Val Loss 477.65463
Epoch 94: Val Loss 474.78860
Epoch 95: Val Loss 471.71643
Epoch 96: Val Loss 468.52313
Epoch 97: Val Loss 465.13309
Epoch 98: Val Loss 461.51447
Epoch 99: Val Loss 457.69489
{'MSE - mean': 175.22584408759803, 'MSE - std': 142.26039189645553, 'R2 - mean': -1.0258980635485755, 'R2 - std': 1.502502039994043} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 175.22584408759803 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 527.01416
Epoch 1: Val Loss 526.97949
Epoch 2: Val Loss 526.94501
Epoch 3: Val Loss 526.91058
Epoch 4: Val Loss 526.87634
Epoch 5: Val Loss 526.84216
Epoch 6: Val Loss 526.80817
Epoch 7: Val Loss 526.77411
Epoch 8: Val Loss 526.74023
Epoch 9: Val Loss 526.70648
Epoch 10: Val Loss 526.67279
Epoch 11: Val Loss 526.63940
Epoch 12: Val Loss 526.60620
Epoch 13: Val Loss 526.57318
Epoch 14: Val Loss 526.54016
Epoch 15: Val Loss 526.50720
Epoch 16: Val Loss 526.47467
Epoch 17: Val Loss 526.44226
Epoch 18: Val Loss 526.41034
Epoch 19: Val Loss 526.37830
Epoch 20: Val Loss 526.34631
Epoch 21: Val Loss 526.31439
Epoch 22: Val Loss 526.28271
Epoch 23: Val Loss 526.25092
Epoch 24: Val Loss 526.21918
Epoch 25: Val Loss 526.18762
Epoch 26: Val Loss 526.15619
Epoch 27: Val Loss 526.12500
Epoch 28: Val Loss 526.09399
Epoch 29: Val Loss 526.06323
Epoch 30: Val Loss 526.03241
Epoch 31: Val Loss 526.00177
Epoch 32: Val Loss 525.97101
Epoch 33: Val Loss 525.94031
Epoch 34: Val Loss 525.90985
Epoch 35: Val Loss 525.87964
Epoch 36: Val Loss 525.84937
Epoch 37: Val Loss 525.81934
Epoch 38: Val Loss 525.78949
Epoch 39: Val Loss 525.75983
Epoch 40: Val Loss 525.73035
Epoch 41: Val Loss 525.70087
Epoch 42: Val Loss 525.67139
Epoch 43: Val Loss 525.64209
Epoch 44: Val Loss 525.61298
Epoch 45: Val Loss 525.58380
Epoch 46: Val Loss 525.55499
Epoch 47: Val Loss 525.52600
Epoch 48: Val Loss 525.49731
Epoch 49: Val Loss 525.46863
Epoch 50: Val Loss 525.44006
Epoch 51: Val Loss 525.41193
Epoch 52: Val Loss 525.38354
Epoch 53: Val Loss 525.35547
Epoch 54: Val Loss 525.32721
Epoch 55: Val Loss 525.29901
Epoch 56: Val Loss 525.27087
Epoch 57: Val Loss 525.24292
Epoch 58: Val Loss 525.21515
Epoch 59: Val Loss 525.18738
Epoch 60: Val Loss 525.15997
Epoch 61: Val Loss 525.13269
Epoch 62: Val Loss 525.10565
Epoch 63: Val Loss 525.07861
Epoch 64: Val Loss 525.05170
Epoch 65: Val Loss 525.02466
Epoch 66: Val Loss 524.99780
Epoch 67: Val Loss 524.97089
Epoch 68: Val Loss 524.94397
Epoch 69: Val Loss 524.91730
Epoch 70: Val Loss 524.89081
Epoch 71: Val Loss 524.86444
Epoch 72: Val Loss 524.83801
Epoch 73: Val Loss 524.81158
Epoch 74: Val Loss 524.78522
Epoch 75: Val Loss 524.75922
Epoch 76: Val Loss 524.73334
Epoch 77: Val Loss 524.70758
Epoch 78: Val Loss 524.68182
Epoch 79: Val Loss 524.65613
Epoch 80: Val Loss 524.63025
Epoch 81: Val Loss 524.60431
Epoch 82: Val Loss 524.57861
Epoch 83: Val Loss 524.55280
Epoch 84: Val Loss 524.52722
Epoch 85: Val Loss 524.50177
Epoch 86: Val Loss 524.47644
Epoch 87: Val Loss 524.45105
Epoch 88: Val Loss 524.42584
Epoch 89: Val Loss 524.40082
Epoch 90: Val Loss 524.37579
Epoch 91: Val Loss 524.35077
Epoch 92: Val Loss 524.32587
Epoch 93: Val Loss 524.30090
Epoch 94: Val Loss 524.27637
Epoch 95: Val Loss 524.25214
Epoch 96: Val Loss 524.22784
Epoch 97: Val Loss 524.20374
Epoch 98: Val Loss 524.17981
Epoch 99: Val Loss 524.15582
{'MSE - mean': 524.1557945001517, 'MSE - std': 0.0, 'R2 - mean': -6.019791412421577, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 621.47803
Epoch 1: Val Loss 621.45758
Epoch 2: Val Loss 621.43713
Epoch 3: Val Loss 621.41663
Epoch 4: Val Loss 621.39606
Epoch 5: Val Loss 621.37543
Epoch 6: Val Loss 621.35498
Epoch 7: Val Loss 621.33417
Epoch 8: Val Loss 621.31293
Epoch 9: Val Loss 621.29144
Epoch 10: Val Loss 621.26996
Epoch 11: Val Loss 621.24811
Epoch 12: Val Loss 621.22595
Epoch 13: Val Loss 621.20349
Epoch 14: Val Loss 621.18115
Epoch 15: Val Loss 621.15881
Epoch 16: Val Loss 621.13654
Epoch 17: Val Loss 621.11407
Epoch 18: Val Loss 621.09143
Epoch 19: Val Loss 621.06836
Epoch 20: Val Loss 621.04510
Epoch 21: Val Loss 621.02167
Epoch 22: Val Loss 620.99823
Epoch 23: Val Loss 620.97479
Epoch 24: Val Loss 620.95129
Epoch 25: Val Loss 620.92773
Epoch 26: Val Loss 620.90363
Epoch 27: Val Loss 620.87903
Epoch 28: Val Loss 620.85425
Epoch 29: Val Loss 620.82928
Epoch 30: Val Loss 620.80377
Epoch 31: Val Loss 620.77777
Epoch 32: Val Loss 620.75159
Epoch 33: Val Loss 620.72528
Epoch 34: Val Loss 620.69824
Epoch 35: Val Loss 620.67065
Epoch 36: Val Loss 620.64264
Epoch 37: Val Loss 620.61322
Epoch 38: Val Loss 620.58344
Epoch 39: Val Loss 620.55292
Epoch 40: Val Loss 620.52136
Epoch 41: Val Loss 620.48907
Epoch 42: Val Loss 620.45624
Epoch 43: Val Loss 620.42267
Epoch 44: Val Loss 620.38831
Epoch 45: Val Loss 620.35413
Epoch 46: Val Loss 620.31799
Epoch 47: Val Loss 620.28094
Epoch 48: Val Loss 620.24316
Epoch 49: Val Loss 620.20441
Epoch 50: Val Loss 620.16321
Epoch 51: Val Loss 620.12079
Epoch 52: Val Loss 620.07794
Epoch 53: Val Loss 620.03479
Epoch 54: Val Loss 619.99054
Epoch 55: Val Loss 619.94562
Epoch 56: Val Loss 619.89996
Epoch 57: Val Loss 619.85468
Epoch 58: Val Loss 619.80829
Epoch 59: Val Loss 619.76056
Epoch 60: Val Loss 619.71216
Epoch 61: Val Loss 619.66351
Epoch 62: Val Loss 619.61481
Epoch 63: Val Loss 619.56488
Epoch 64: Val Loss 619.51331
Epoch 65: Val Loss 619.46106
Epoch 66: Val Loss 619.40820
Epoch 67: Val Loss 619.35431
Epoch 68: Val Loss 619.30005
Epoch 69: Val Loss 619.24634
Epoch 70: Val Loss 619.19269
Epoch 71: Val Loss 619.13788
Epoch 72: Val Loss 619.08270
Epoch 73: Val Loss 619.02820
Epoch 74: Val Loss 618.97278
Epoch 75: Val Loss 618.91644
Epoch 76: Val Loss 618.85913
Epoch 77: Val Loss 618.80225
Epoch 78: Val Loss 618.74481
Epoch 79: Val Loss 618.68671
Epoch 80: Val Loss 618.62775
Epoch 81: Val Loss 618.56818
Epoch 82: Val Loss 618.50861
Epoch 83: Val Loss 618.44757
Epoch 84: Val Loss 618.38654
Epoch 85: Val Loss 618.32373
Epoch 86: Val Loss 618.25873
Epoch 87: Val Loss 618.19147
Epoch 88: Val Loss 618.12329
Epoch 89: Val Loss 618.05353
Epoch 90: Val Loss 617.98334
Epoch 91: Val Loss 617.91174
Epoch 92: Val Loss 617.84070
Epoch 93: Val Loss 617.76569
Epoch 94: Val Loss 617.68866
Epoch 95: Val Loss 617.61017
Epoch 96: Val Loss 617.52838
Epoch 97: Val Loss 617.44470
Epoch 98: Val Loss 617.35699
Epoch 99: Val Loss 617.26599
{'MSE - mean': 570.7109072465826, 'MSE - std': 46.55511274643078, 'R2 - mean': -6.341936234970001, 'R2 - std': 0.3221448225484247} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 670.35913
Epoch 1: Val Loss 670.29333
Epoch 2: Val Loss 670.22711
Epoch 3: Val Loss 670.16290
Epoch 4: Val Loss 670.09882
Epoch 5: Val Loss 670.03528
Epoch 6: Val Loss 669.97308
Epoch 7: Val Loss 669.91229
Epoch 8: Val Loss 669.85083
Epoch 9: Val Loss 669.78851
Epoch 10: Val Loss 669.72827
Epoch 11: Val Loss 669.66870
Epoch 12: Val Loss 669.60968
Epoch 13: Val Loss 669.55029
Epoch 14: Val Loss 669.49194
Epoch 15: Val Loss 669.43286
Epoch 16: Val Loss 669.37677
Epoch 17: Val Loss 669.32007
Epoch 18: Val Loss 669.26483
Epoch 19: Val Loss 669.21021
Epoch 20: Val Loss 669.15686
Epoch 21: Val Loss 669.10327
Epoch 22: Val Loss 669.04895
Epoch 23: Val Loss 668.99506
Epoch 24: Val Loss 668.94128
Epoch 25: Val Loss 668.88831
Epoch 26: Val Loss 668.83508
Epoch 27: Val Loss 668.78223
Epoch 28: Val Loss 668.72968
Epoch 29: Val Loss 668.67761
Epoch 30: Val Loss 668.62799
Epoch 31: Val Loss 668.57874
Epoch 32: Val Loss 668.52875
Epoch 33: Val Loss 668.47894
Epoch 34: Val Loss 668.42926
Epoch 35: Val Loss 668.37946
Epoch 36: Val Loss 668.32983
Epoch 37: Val Loss 668.28040
Epoch 38: Val Loss 668.23114
Epoch 39: Val Loss 668.18237
Epoch 40: Val Loss 668.13397
Epoch 41: Val Loss 668.08600
Epoch 42: Val Loss 668.03796
Epoch 43: Val Loss 667.99042
Epoch 44: Val Loss 667.94336
Epoch 45: Val Loss 667.89697
Epoch 46: Val Loss 667.85107
Epoch 47: Val Loss 667.80493
Epoch 48: Val Loss 667.75787
Epoch 49: Val Loss 667.71069
Epoch 50: Val Loss 667.66388
Epoch 51: Val Loss 667.61743
Epoch 52: Val Loss 667.57068
Epoch 53: Val Loss 667.52539
Epoch 54: Val Loss 667.48126
Epoch 55: Val Loss 667.43744
Epoch 56: Val Loss 667.39435
Epoch 57: Val Loss 667.35211
Epoch 58: Val Loss 667.31049
Epoch 59: Val Loss 667.26892
Epoch 60: Val Loss 667.22784
Epoch 61: Val Loss 667.18805
Epoch 62: Val Loss 667.14844
Epoch 63: Val Loss 667.10919
Epoch 64: Val Loss 667.07068
Epoch 65: Val Loss 667.03223
Epoch 66: Val Loss 666.99475
Epoch 67: Val Loss 666.95715
Epoch 68: Val Loss 666.92017
Epoch 69: Val Loss 666.88275
Epoch 70: Val Loss 666.84515
Epoch 71: Val Loss 666.80756
Epoch 72: Val Loss 666.77020
Epoch 73: Val Loss 666.73297
Epoch 74: Val Loss 666.69513
Epoch 75: Val Loss 666.65808
Epoch 76: Val Loss 666.62067
Epoch 77: Val Loss 666.58423
Epoch 78: Val Loss 666.54810
Epoch 79: Val Loss 666.51208
Epoch 80: Val Loss 666.47577
Epoch 81: Val Loss 666.43927
Epoch 82: Val Loss 666.40332
Epoch 83: Val Loss 666.36774
Epoch 84: Val Loss 666.33221
Epoch 85: Val Loss 666.29700
Epoch 86: Val Loss 666.26190
Epoch 87: Val Loss 666.22711
Epoch 88: Val Loss 666.19226
Epoch 89: Val Loss 666.15747
Epoch 90: Val Loss 666.12299
Epoch 91: Val Loss 666.08807
Epoch 92: Val Loss 666.05322
Epoch 93: Val Loss 666.01904
Epoch 94: Val Loss 665.98468
Epoch 95: Val Loss 665.95032
Epoch 96: Val Loss 665.91583
Epoch 97: Val Loss 665.88055
Epoch 98: Val Loss 665.84497
Epoch 99: Val Loss 665.81000
{'MSE - mean': 602.4106173178055, 'MSE - std': 58.776374951345105, 'R2 - mean': -6.220181729395487, 'R2 - std': 0.3143774436118033} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 553.06348
Epoch 1: Val Loss 552.96960
Epoch 2: Val Loss 552.87567
Epoch 3: Val Loss 552.78156
Epoch 4: Val Loss 552.68866
Epoch 5: Val Loss 552.59564
Epoch 6: Val Loss 552.50293
Epoch 7: Val Loss 552.41144
Epoch 8: Val Loss 552.31940
Epoch 9: Val Loss 552.22766
Epoch 10: Val Loss 552.13525
Epoch 11: Val Loss 552.04401
Epoch 12: Val Loss 551.95551
Epoch 13: Val Loss 551.86780
Epoch 14: Val Loss 551.78070
Epoch 15: Val Loss 551.69476
Epoch 16: Val Loss 551.61005
Epoch 17: Val Loss 551.52686
Epoch 18: Val Loss 551.44269
Epoch 19: Val Loss 551.35846
Epoch 20: Val Loss 551.27338
Epoch 21: Val Loss 551.18860
Epoch 22: Val Loss 551.10577
Epoch 23: Val Loss 551.02374
Epoch 24: Val Loss 550.94116
Epoch 25: Val Loss 550.85687
Epoch 26: Val Loss 550.77264
Epoch 27: Val Loss 550.68945
Epoch 28: Val Loss 550.60614
Epoch 29: Val Loss 550.52386
Epoch 30: Val Loss 550.44189
Epoch 31: Val Loss 550.36121
Epoch 32: Val Loss 550.28223
Epoch 33: Val Loss 550.20441
Epoch 34: Val Loss 550.12604
Epoch 35: Val Loss 550.04681
Epoch 36: Val Loss 549.96985
Epoch 37: Val Loss 549.89337
Epoch 38: Val Loss 549.81635
Epoch 39: Val Loss 549.74194
Epoch 40: Val Loss 549.66901
Epoch 41: Val Loss 549.59631
Epoch 42: Val Loss 549.52521
Epoch 43: Val Loss 549.45557
Epoch 44: Val Loss 549.38580
Epoch 45: Val Loss 549.31610
Epoch 46: Val Loss 549.24530
Epoch 47: Val Loss 549.17578
Epoch 48: Val Loss 549.10730
Epoch 49: Val Loss 549.03900
Epoch 50: Val Loss 548.96973
Epoch 51: Val Loss 548.90088
Epoch 52: Val Loss 548.83337
Epoch 53: Val Loss 548.76672
Epoch 54: Val Loss 548.70105
Epoch 55: Val Loss 548.63605
Epoch 56: Val Loss 548.57294
Epoch 57: Val Loss 548.51001
Epoch 58: Val Loss 548.44659
Epoch 59: Val Loss 548.38361
Epoch 60: Val Loss 548.32239
Epoch 61: Val Loss 548.26154
Epoch 62: Val Loss 548.20044
Epoch 63: Val Loss 548.14014
Epoch 64: Val Loss 548.08057
Epoch 65: Val Loss 548.01996
Epoch 66: Val Loss 547.96002
Epoch 67: Val Loss 547.90112
Epoch 68: Val Loss 547.84412
Epoch 69: Val Loss 547.78967
Epoch 70: Val Loss 547.73608
Epoch 71: Val Loss 547.68268
Epoch 72: Val Loss 547.62976
Epoch 73: Val Loss 547.57721
Epoch 74: Val Loss 547.52661
Epoch 75: Val Loss 547.47705
Epoch 76: Val Loss 547.42896
Epoch 77: Val Loss 547.38324
Epoch 78: Val Loss 547.33789
Epoch 79: Val Loss 547.29456
Epoch 80: Val Loss 547.25262
Epoch 81: Val Loss 547.21338
Epoch 82: Val Loss 547.17542
Epoch 83: Val Loss 547.13715
Epoch 84: Val Loss 547.09924
Epoch 85: Val Loss 547.06219
Epoch 86: Val Loss 547.02539
Epoch 87: Val Loss 546.98883
Epoch 88: Val Loss 546.95319
Epoch 89: Val Loss 546.91895
Epoch 90: Val Loss 546.88495
Epoch 91: Val Loss 546.85236
Epoch 92: Val Loss 546.82056
Epoch 93: Val Loss 546.78900
Epoch 94: Val Loss 546.75781
Epoch 95: Val Loss 546.72668
Epoch 96: Val Loss 546.69580
Epoch 97: Val Loss 546.66431
Epoch 98: Val Loss 546.63287
Epoch 99: Val Loss 546.60248
{'MSE - mean': 588.4585814579793, 'MSE - std': 56.34691298746945, 'R2 - mean': -6.226144210328583, 'R2 - std': 0.27245465003473884} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 587.38068
Epoch 1: Val Loss 587.30109
Epoch 2: Val Loss 587.22266
Epoch 3: Val Loss 587.14532
Epoch 4: Val Loss 587.06897
Epoch 5: Val Loss 586.99402
Epoch 6: Val Loss 586.91870
Epoch 7: Val Loss 586.84369
Epoch 8: Val Loss 586.76819
Epoch 9: Val Loss 586.69458
Epoch 10: Val Loss 586.62207
Epoch 11: Val Loss 586.55017
Epoch 12: Val Loss 586.48004
Epoch 13: Val Loss 586.41052
Epoch 14: Val Loss 586.34259
Epoch 15: Val Loss 586.27484
Epoch 16: Val Loss 586.20856
Epoch 17: Val Loss 586.14197
Epoch 18: Val Loss 586.07532
Epoch 19: Val Loss 586.00922
Epoch 20: Val Loss 585.94342
Epoch 21: Val Loss 585.87762
Epoch 22: Val Loss 585.81262
Epoch 23: Val Loss 585.74811
Epoch 24: Val Loss 585.68396
Epoch 25: Val Loss 585.62024
Epoch 26: Val Loss 585.55646
Epoch 27: Val Loss 585.49146
Epoch 28: Val Loss 585.42743
Epoch 29: Val Loss 585.36414
Epoch 30: Val Loss 585.30145
Epoch 31: Val Loss 585.23907
Epoch 32: Val Loss 585.17743
Epoch 33: Val Loss 585.11621
Epoch 34: Val Loss 585.05579
Epoch 35: Val Loss 584.99677
Epoch 36: Val Loss 584.93842
Epoch 37: Val Loss 584.88019
Epoch 38: Val Loss 584.82245
Epoch 39: Val Loss 584.76501
Epoch 40: Val Loss 584.70831
Epoch 41: Val Loss 584.65179
Epoch 42: Val Loss 584.59546
Epoch 43: Val Loss 584.53949
Epoch 44: Val Loss 584.48450
Epoch 45: Val Loss 584.43066
Epoch 46: Val Loss 584.37787
Epoch 47: Val Loss 584.32556
Epoch 48: Val Loss 584.27301
Epoch 49: Val Loss 584.22046
Epoch 50: Val Loss 584.16876
Epoch 51: Val Loss 584.11774
Epoch 52: Val Loss 584.06653
Epoch 53: Val Loss 584.01544
Epoch 54: Val Loss 583.96472
Epoch 55: Val Loss 583.91492
Epoch 56: Val Loss 583.86456
Epoch 57: Val Loss 583.81445
Epoch 58: Val Loss 583.76471
Epoch 59: Val Loss 583.71564
Epoch 60: Val Loss 583.66638
Epoch 61: Val Loss 583.61761
Epoch 62: Val Loss 583.56927
Epoch 63: Val Loss 583.52173
Epoch 64: Val Loss 583.47406
Epoch 65: Val Loss 583.42688
Epoch 66: Val Loss 583.37952
Epoch 67: Val Loss 583.33185
Epoch 68: Val Loss 583.28400
Epoch 69: Val Loss 583.23688
Epoch 70: Val Loss 583.19025
Epoch 71: Val Loss 583.14380
Epoch 72: Val Loss 583.09808
Epoch 73: Val Loss 583.05280
Epoch 74: Val Loss 583.00842
Epoch 75: Val Loss 582.96362
Epoch 76: Val Loss 582.91840
Epoch 77: Val Loss 582.87433
Epoch 78: Val Loss 582.83038
Epoch 79: Val Loss 582.78638
Epoch 80: Val Loss 582.74274
Epoch 81: Val Loss 582.69965
Epoch 82: Val Loss 582.65631
Epoch 83: Val Loss 582.61255
Epoch 84: Val Loss 582.56824
Epoch 85: Val Loss 582.52466
Epoch 86: Val Loss 582.48181
Epoch 87: Val Loss 582.43903
Epoch 88: Val Loss 582.39630
Epoch 89: Val Loss 582.35400
Epoch 90: Val Loss 582.31201
Epoch 91: Val Loss 582.27063
Epoch 92: Val Loss 582.23010
Epoch 93: Val Loss 582.18958
Epoch 94: Val Loss 582.14911
Epoch 95: Val Loss 582.10828
Epoch 96: Val Loss 582.06750
Epoch 97: Val Loss 582.02777
Epoch 98: Val Loss 581.98810
Epoch 99: Val Loss 581.94843
{'MSE - mean': 587.1565508603153, 'MSE - std': 50.46544181202595, 'R2 - mean': -6.056049167837536, 'R2 - std': 0.4184668720228235} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 587.1565508603153 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 561.24982
Epoch 1: Val Loss 561.15576
Epoch 2: Val Loss 561.06299
Epoch 3: Val Loss 560.97137
Epoch 4: Val Loss 560.87860
Epoch 5: Val Loss 560.78479
Epoch 6: Val Loss 560.68896
Epoch 7: Val Loss 560.59247
Epoch 8: Val Loss 560.49298
Epoch 9: Val Loss 560.39368
Epoch 10: Val Loss 560.29401
Epoch 11: Val Loss 560.19275
Epoch 12: Val Loss 560.09045
Epoch 13: Val Loss 559.98853
Epoch 14: Val Loss 559.88782
Epoch 15: Val Loss 559.78638
Epoch 16: Val Loss 559.68359
Epoch 17: Val Loss 559.57928
Epoch 18: Val Loss 559.47247
Epoch 19: Val Loss 559.36395
Epoch 20: Val Loss 559.25458
Epoch 21: Val Loss 559.14490
Epoch 22: Val Loss 559.03564
Epoch 23: Val Loss 558.92639
Epoch 24: Val Loss 558.81665
Epoch 25: Val Loss 558.70624
Epoch 26: Val Loss 558.59528
Epoch 27: Val Loss 558.48407
Epoch 28: Val Loss 558.37671
Epoch 29: Val Loss 558.26904
Epoch 30: Val Loss 558.16168
Epoch 31: Val Loss 558.05334
Epoch 32: Val Loss 557.94324
Epoch 33: Val Loss 557.83026
Epoch 34: Val Loss 557.71417
Epoch 35: Val Loss 557.59515
Epoch 36: Val Loss 557.47369
Epoch 37: Val Loss 557.35126
Epoch 38: Val Loss 557.22882
Epoch 39: Val Loss 557.10809
Epoch 40: Val Loss 556.98492
Epoch 41: Val Loss 556.86133
Epoch 42: Val Loss 556.73828
Epoch 43: Val Loss 556.61395
Epoch 44: Val Loss 556.48932
Epoch 45: Val Loss 556.36182
Epoch 46: Val Loss 556.23315
Epoch 47: Val Loss 556.10321
Epoch 48: Val Loss 555.97540
Epoch 49: Val Loss 555.84698
Epoch 50: Val Loss 555.71967
Epoch 51: Val Loss 555.59045
Epoch 52: Val Loss 555.45837
Epoch 53: Val Loss 555.32080
Epoch 54: Val Loss 555.17999
Epoch 55: Val Loss 555.03827
Epoch 56: Val Loss 554.89465
Epoch 57: Val Loss 554.74774
Epoch 58: Val Loss 554.59998
Epoch 59: Val Loss 554.45245
Epoch 60: Val Loss 554.30133
Epoch 61: Val Loss 554.15271
Epoch 62: Val Loss 554.00446
Epoch 63: Val Loss 553.85388
Epoch 64: Val Loss 553.70105
Epoch 65: Val Loss 553.54358
Epoch 66: Val Loss 553.38135
Epoch 67: Val Loss 553.22235
Epoch 68: Val Loss 553.06342
Epoch 69: Val Loss 552.90210
Epoch 70: Val Loss 552.73889
Epoch 71: Val Loss 552.57465
Epoch 72: Val Loss 552.40607
Epoch 73: Val Loss 552.23505
Epoch 74: Val Loss 552.05939
Epoch 75: Val Loss 551.88544
Epoch 76: Val Loss 551.71216
Epoch 77: Val Loss 551.53668
Epoch 78: Val Loss 551.36041
Epoch 79: Val Loss 551.18359
Epoch 80: Val Loss 551.00745
Epoch 81: Val Loss 550.82678
Epoch 82: Val Loss 550.64148
Epoch 83: Val Loss 550.45746
Epoch 84: Val Loss 550.27032
Epoch 85: Val Loss 550.08124
Epoch 86: Val Loss 549.89166
Epoch 87: Val Loss 549.70184
Epoch 88: Val Loss 549.51349
Epoch 89: Val Loss 549.32092
Epoch 90: Val Loss 549.12994
Epoch 91: Val Loss 548.93475
Epoch 92: Val Loss 548.73090
Epoch 93: Val Loss 548.52167
Epoch 94: Val Loss 548.31628
Epoch 95: Val Loss 548.11298
Epoch 96: Val Loss 547.90796
Epoch 97: Val Loss 547.70087
Epoch 98: Val Loss 547.48907
Epoch 99: Val Loss 547.27618
{'MSE - mean': 547.2761675942611, 'MSE - std': 0.0, 'R2 - mean': -6.32943255003942, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 599.88892
Epoch 1: Val Loss 599.83063
Epoch 2: Val Loss 599.77167
Epoch 3: Val Loss 599.71118
Epoch 4: Val Loss 599.65021
Epoch 5: Val Loss 599.58899
Epoch 6: Val Loss 599.52747
Epoch 7: Val Loss 599.46564
Epoch 8: Val Loss 599.40411
Epoch 9: Val Loss 599.34192
Epoch 10: Val Loss 599.27777
Epoch 11: Val Loss 599.21124
Epoch 12: Val Loss 599.14215
Epoch 13: Val Loss 599.07098
Epoch 14: Val Loss 598.99927
Epoch 15: Val Loss 598.92590
Epoch 16: Val Loss 598.85181
Epoch 17: Val Loss 598.77698
Epoch 18: Val Loss 598.70178
Epoch 19: Val Loss 598.62659
Epoch 20: Val Loss 598.55133
Epoch 21: Val Loss 598.47449
Epoch 22: Val Loss 598.39612
Epoch 23: Val Loss 598.31653
Epoch 24: Val Loss 598.23633
Epoch 25: Val Loss 598.15533
Epoch 26: Val Loss 598.07178
Epoch 27: Val Loss 597.98669
Epoch 28: Val Loss 597.90057
Epoch 29: Val Loss 597.81146
Epoch 30: Val Loss 597.71887
Epoch 31: Val Loss 597.62463
Epoch 32: Val Loss 597.52832
Epoch 33: Val Loss 597.43005
Epoch 34: Val Loss 597.32727
Epoch 35: Val Loss 597.22101
Epoch 36: Val Loss 597.11237
Epoch 37: Val Loss 597.00281
Epoch 38: Val Loss 596.89197
Epoch 39: Val Loss 596.77985
Epoch 40: Val Loss 596.66833
Epoch 41: Val Loss 596.55432
Epoch 42: Val Loss 596.43835
Epoch 43: Val Loss 596.31995
Epoch 44: Val Loss 596.20337
Epoch 45: Val Loss 596.08600
Epoch 46: Val Loss 595.96686
Epoch 47: Val Loss 595.84534
Epoch 48: Val Loss 595.72229
Epoch 49: Val Loss 595.59668
Epoch 50: Val Loss 595.46967
Epoch 51: Val Loss 595.34210
Epoch 52: Val Loss 595.21497
Epoch 53: Val Loss 595.08600
Epoch 54: Val Loss 594.95428
Epoch 55: Val Loss 594.82037
Epoch 56: Val Loss 594.68567
Epoch 57: Val Loss 594.55066
Epoch 58: Val Loss 594.41516
Epoch 59: Val Loss 594.28009
Epoch 60: Val Loss 594.14380
Epoch 61: Val Loss 594.00616
Epoch 62: Val Loss 593.86877
Epoch 63: Val Loss 593.73071
Epoch 64: Val Loss 593.59235
Epoch 65: Val Loss 593.45264
Epoch 66: Val Loss 593.31226
Epoch 67: Val Loss 593.17084
Epoch 68: Val Loss 593.02753
Epoch 69: Val Loss 592.88214
Epoch 70: Val Loss 592.73627
Epoch 71: Val Loss 592.59064
Epoch 72: Val Loss 592.44403
Epoch 73: Val Loss 592.29724
Epoch 74: Val Loss 592.14923
Epoch 75: Val Loss 591.99994
Epoch 76: Val Loss 591.84955
Epoch 77: Val Loss 591.69800
Epoch 78: Val Loss 591.54498
Epoch 79: Val Loss 591.39001
Epoch 80: Val Loss 591.23547
Epoch 81: Val Loss 591.08112
Epoch 82: Val Loss 590.92505
Epoch 83: Val Loss 590.76746
Epoch 84: Val Loss 590.60791
Epoch 85: Val Loss 590.44885
Epoch 86: Val Loss 590.29065
Epoch 87: Val Loss 590.13202
Epoch 88: Val Loss 589.97247
Epoch 89: Val Loss 589.81329
Epoch 90: Val Loss 589.65332
Epoch 91: Val Loss 589.49158
Epoch 92: Val Loss 589.32758
Epoch 93: Val Loss 589.16174
Epoch 94: Val Loss 588.99377
Epoch 95: Val Loss 588.82471
Epoch 96: Val Loss 588.65430
Epoch 97: Val Loss 588.48193
Epoch 98: Val Loss 588.30768
Epoch 99: Val Loss 588.13361
{'MSE - mean': 567.7048754596814, 'MSE - std': 20.42870786542028, 'R2 - mean': -6.315900131943639, 'R2 - std': 0.013532418095781207} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 673.76593
Epoch 1: Val Loss 673.72583
Epoch 2: Val Loss 673.68610
Epoch 3: Val Loss 673.64667
Epoch 4: Val Loss 673.60748
Epoch 5: Val Loss 673.56854
Epoch 6: Val Loss 673.53003
Epoch 7: Val Loss 673.49164
Epoch 8: Val Loss 673.45312
Epoch 9: Val Loss 673.41461
Epoch 10: Val Loss 673.37598
Epoch 11: Val Loss 673.33722
Epoch 12: Val Loss 673.29865
Epoch 13: Val Loss 673.26099
Epoch 14: Val Loss 673.22339
Epoch 15: Val Loss 673.18549
Epoch 16: Val Loss 673.14771
Epoch 17: Val Loss 673.10950
Epoch 18: Val Loss 673.07129
Epoch 19: Val Loss 673.03308
Epoch 20: Val Loss 672.99536
Epoch 21: Val Loss 672.95758
Epoch 22: Val Loss 672.92047
Epoch 23: Val Loss 672.88397
Epoch 24: Val Loss 672.84778
Epoch 25: Val Loss 672.81104
Epoch 26: Val Loss 672.77472
Epoch 27: Val Loss 672.73822
Epoch 28: Val Loss 672.70178
Epoch 29: Val Loss 672.66602
Epoch 30: Val Loss 672.63055
Epoch 31: Val Loss 672.59528
Epoch 32: Val Loss 672.55988
Epoch 33: Val Loss 672.52496
Epoch 34: Val Loss 672.49030
Epoch 35: Val Loss 672.45587
Epoch 36: Val Loss 672.42188
Epoch 37: Val Loss 672.38782
Epoch 38: Val Loss 672.35370
Epoch 39: Val Loss 672.31915
Epoch 40: Val Loss 672.28381
Epoch 41: Val Loss 672.24860
Epoch 42: Val Loss 672.21356
Epoch 43: Val Loss 672.17822
Epoch 44: Val Loss 672.14294
Epoch 45: Val Loss 672.10822
Epoch 46: Val Loss 672.07349
Epoch 47: Val Loss 672.03833
Epoch 48: Val Loss 672.00354
Epoch 49: Val Loss 671.96887
Epoch 50: Val Loss 671.93378
Epoch 51: Val Loss 671.89819
Epoch 52: Val Loss 671.86212
Epoch 53: Val Loss 671.82654
Epoch 54: Val Loss 671.79144
Epoch 55: Val Loss 671.75665
Epoch 56: Val Loss 671.72192
Epoch 57: Val Loss 671.68677
Epoch 58: Val Loss 671.65186
Epoch 59: Val Loss 671.61676
Epoch 60: Val Loss 671.58136
Epoch 61: Val Loss 671.54572
Epoch 62: Val Loss 671.50995
Epoch 63: Val Loss 671.47430
Epoch 64: Val Loss 671.43878
Epoch 65: Val Loss 671.40302
Epoch 66: Val Loss 671.36688
Epoch 67: Val Loss 671.33099
Epoch 68: Val Loss 671.29529
Epoch 69: Val Loss 671.25989
Epoch 70: Val Loss 671.22424
Epoch 71: Val Loss 671.18860
Epoch 72: Val Loss 671.15314
Epoch 73: Val Loss 671.11725
Epoch 74: Val Loss 671.08105
Epoch 75: Val Loss 671.04486
Epoch 76: Val Loss 671.00842
Epoch 77: Val Loss 670.97174
Epoch 78: Val Loss 670.93561
Epoch 79: Val Loss 670.89899
Epoch 80: Val Loss 670.86206
Epoch 81: Val Loss 670.82556
Epoch 82: Val Loss 670.78857
Epoch 83: Val Loss 670.75171
Epoch 84: Val Loss 670.71454
Epoch 85: Val Loss 670.67743
Epoch 86: Val Loss 670.64032
Epoch 87: Val Loss 670.60297
Epoch 88: Val Loss 670.56543
Epoch 89: Val Loss 670.52789
Epoch 90: Val Loss 670.49127
Epoch 91: Val Loss 670.45465
Epoch 92: Val Loss 670.41766
Epoch 93: Val Loss 670.38074
Epoch 94: Val Loss 670.34332
Epoch 95: Val Loss 670.30627
Epoch 96: Val Loss 670.26953
Epoch 97: Val Loss 670.23328
Epoch 98: Val Loss 670.19659
Epoch 99: Val Loss 670.15955
{'MSE - mean': 601.8564340805254, 'MSE - std': 51.096764240341194, 'R2 - mean': -6.218016415706959, 'R2 - std': 0.13886874390931817} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 522.19580
Epoch 1: Val Loss 522.14246
Epoch 2: Val Loss 522.08893
Epoch 3: Val Loss 522.03583
Epoch 4: Val Loss 521.98303
Epoch 5: Val Loss 521.93054
Epoch 6: Val Loss 521.87836
Epoch 7: Val Loss 521.82605
Epoch 8: Val Loss 521.77386
Epoch 9: Val Loss 521.72235
Epoch 10: Val Loss 521.67053
Epoch 11: Val Loss 521.61920
Epoch 12: Val Loss 521.56818
Epoch 13: Val Loss 521.51733
Epoch 14: Val Loss 521.46686
Epoch 15: Val Loss 521.41614
Epoch 16: Val Loss 521.36530
Epoch 17: Val Loss 521.31433
Epoch 18: Val Loss 521.26331
Epoch 19: Val Loss 521.21246
Epoch 20: Val Loss 521.16187
Epoch 21: Val Loss 521.11151
Epoch 22: Val Loss 521.06122
Epoch 23: Val Loss 521.01068
Epoch 24: Val Loss 520.96033
Epoch 25: Val Loss 520.90997
Epoch 26: Val Loss 520.86005
Epoch 27: Val Loss 520.81018
Epoch 28: Val Loss 520.76080
Epoch 29: Val Loss 520.71149
Epoch 30: Val Loss 520.66223
Epoch 31: Val Loss 520.61298
Epoch 32: Val Loss 520.56409
Epoch 33: Val Loss 520.51556
Epoch 34: Val Loss 520.46698
Epoch 35: Val Loss 520.41840
Epoch 36: Val Loss 520.36975
Epoch 37: Val Loss 520.32117
Epoch 38: Val Loss 520.27252
Epoch 39: Val Loss 520.22363
Epoch 40: Val Loss 520.17487
Epoch 41: Val Loss 520.12616
Epoch 42: Val Loss 520.07764
Epoch 43: Val Loss 520.02911
Epoch 44: Val Loss 519.98059
Epoch 45: Val Loss 519.93213
Epoch 46: Val Loss 519.88391
Epoch 47: Val Loss 519.83588
Epoch 48: Val Loss 519.78784
Epoch 49: Val Loss 519.73962
Epoch 50: Val Loss 519.69128
Epoch 51: Val Loss 519.64307
Epoch 52: Val Loss 519.59503
Epoch 53: Val Loss 519.54669
Epoch 54: Val Loss 519.49866
Epoch 55: Val Loss 519.45074
Epoch 56: Val Loss 519.40289
Epoch 57: Val Loss 519.35529
Epoch 58: Val Loss 519.30756
Epoch 59: Val Loss 519.25958
Epoch 60: Val Loss 519.21136
Epoch 61: Val Loss 519.16315
Epoch 62: Val Loss 519.11523
Epoch 63: Val Loss 519.06720
Epoch 64: Val Loss 519.01892
Epoch 65: Val Loss 518.97021
Epoch 66: Val Loss 518.92169
Epoch 67: Val Loss 518.87323
Epoch 68: Val Loss 518.82477
Epoch 69: Val Loss 518.77655
Epoch 70: Val Loss 518.72803
Epoch 71: Val Loss 518.67950
Epoch 72: Val Loss 518.63104
Epoch 73: Val Loss 518.58246
Epoch 74: Val Loss 518.53339
Epoch 75: Val Loss 518.48431
Epoch 76: Val Loss 518.43549
Epoch 77: Val Loss 518.38702
Epoch 78: Val Loss 518.33832
Epoch 79: Val Loss 518.28937
Epoch 80: Val Loss 518.24023
Epoch 81: Val Loss 518.19092
Epoch 82: Val Loss 518.14209
Epoch 83: Val Loss 518.09326
Epoch 84: Val Loss 518.04425
Epoch 85: Val Loss 517.99548
Epoch 86: Val Loss 517.94653
Epoch 87: Val Loss 517.89789
Epoch 88: Val Loss 517.84967
Epoch 89: Val Loss 517.80127
Epoch 90: Val Loss 517.75275
Epoch 91: Val Loss 517.70410
Epoch 92: Val Loss 517.65533
Epoch 93: Val Loss 517.60596
Epoch 94: Val Loss 517.55682
Epoch 95: Val Loss 517.50775
Epoch 96: Val Loss 517.45898
Epoch 97: Val Loss 517.41028
Epoch 98: Val Loss 517.36115
Epoch 99: Val Loss 517.31262
{'MSE - mean': 580.7204866986182, 'MSE - std': 57.43121365657361, 'R2 - mean': -6.1274769162184395, 'R2 - std': 0.1976248945075205} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 593.63727
Epoch 1: Val Loss 593.55530
Epoch 2: Val Loss 593.47205
Epoch 3: Val Loss 593.38873
Epoch 4: Val Loss 593.30670
Epoch 5: Val Loss 593.22540
Epoch 6: Val Loss 593.14355
Epoch 7: Val Loss 593.06226
Epoch 8: Val Loss 592.98029
Epoch 9: Val Loss 592.89624
Epoch 10: Val Loss 592.81073
Epoch 11: Val Loss 592.72491
Epoch 12: Val Loss 592.64014
Epoch 13: Val Loss 592.55493
Epoch 14: Val Loss 592.47046
Epoch 15: Val Loss 592.38605
Epoch 16: Val Loss 592.30048
Epoch 17: Val Loss 592.21423
Epoch 18: Val Loss 592.12811
Epoch 19: Val Loss 592.04059
Epoch 20: Val Loss 591.95074
Epoch 21: Val Loss 591.85968
Epoch 22: Val Loss 591.76917
Epoch 23: Val Loss 591.67981
Epoch 24: Val Loss 591.58978
Epoch 25: Val Loss 591.50043
Epoch 26: Val Loss 591.41266
Epoch 27: Val Loss 591.32574
Epoch 28: Val Loss 591.23926
Epoch 29: Val Loss 591.15302
Epoch 30: Val Loss 591.06738
Epoch 31: Val Loss 590.98145
Epoch 32: Val Loss 590.89423
Epoch 33: Val Loss 590.80731
Epoch 34: Val Loss 590.72034
Epoch 35: Val Loss 590.63202
Epoch 36: Val Loss 590.54468
Epoch 37: Val Loss 590.45667
Epoch 38: Val Loss 590.36719
Epoch 39: Val Loss 590.27679
Epoch 40: Val Loss 590.18585
Epoch 41: Val Loss 590.09509
Epoch 42: Val Loss 590.00604
Epoch 43: Val Loss 589.91724
Epoch 44: Val Loss 589.82788
Epoch 45: Val Loss 589.73846
Epoch 46: Val Loss 589.65063
Epoch 47: Val Loss 589.56256
Epoch 48: Val Loss 589.47437
Epoch 49: Val Loss 589.38458
Epoch 50: Val Loss 589.29498
Epoch 51: Val Loss 589.20532
Epoch 52: Val Loss 589.11670
Epoch 53: Val Loss 589.02734
Epoch 54: Val Loss 588.93695
Epoch 55: Val Loss 588.84686
Epoch 56: Val Loss 588.75720
Epoch 57: Val Loss 588.66705
Epoch 58: Val Loss 588.57214
Epoch 59: Val Loss 588.47827
Epoch 60: Val Loss 588.38422
Epoch 61: Val Loss 588.29034
Epoch 62: Val Loss 588.19537
Epoch 63: Val Loss 588.09991
Epoch 64: Val Loss 588.00397
Epoch 65: Val Loss 587.90735
Epoch 66: Val Loss 587.80951
Epoch 67: Val Loss 587.71301
Epoch 68: Val Loss 587.61615
Epoch 69: Val Loss 587.52051
Epoch 70: Val Loss 587.42444
Epoch 71: Val Loss 587.32739
Epoch 72: Val Loss 587.23102
Epoch 73: Val Loss 587.13269
Epoch 74: Val Loss 587.03357
Epoch 75: Val Loss 586.93481
Epoch 76: Val Loss 586.83459
Epoch 77: Val Loss 586.73523
Epoch 78: Val Loss 586.63568
Epoch 79: Val Loss 586.53455
Epoch 80: Val Loss 586.43402
Epoch 81: Val Loss 586.33228
Epoch 82: Val Loss 586.23096
Epoch 83: Val Loss 586.12988
Epoch 84: Val Loss 586.02905
Epoch 85: Val Loss 585.92682
Epoch 86: Val Loss 585.82318
Epoch 87: Val Loss 585.71838
Epoch 88: Val Loss 585.61304
Epoch 89: Val Loss 585.50922
Epoch 90: Val Loss 585.40607
Epoch 91: Val Loss 585.30231
Epoch 92: Val Loss 585.19757
Epoch 93: Val Loss 585.09125
Epoch 94: Val Loss 584.98450
Epoch 95: Val Loss 584.87653
Epoch 96: Val Loss 584.76862
Epoch 97: Val Loss 584.66144
Epoch 98: Val Loss 584.55432
Epoch 99: Val Loss 584.44586
{'MSE - mean': 581.4655611052128, 'MSE - std': 51.3896486185599, 'R2 - mean': -5.9825875661414, 'R2 - std': 0.3394350809893907} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 581.4655611052128 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 520.83978
Epoch 1: Val Loss 520.80743
Epoch 2: Val Loss 520.77454
Epoch 3: Val Loss 520.74042
Epoch 4: Val Loss 520.70612
Epoch 5: Val Loss 520.67188
Epoch 6: Val Loss 520.63751
Epoch 7: Val Loss 520.60291
Epoch 8: Val Loss 520.56573
Epoch 9: Val Loss 520.52722
Epoch 10: Val Loss 520.49017
Epoch 11: Val Loss 520.45325
Epoch 12: Val Loss 520.41626
Epoch 13: Val Loss 520.37964
Epoch 14: Val Loss 520.34259
Epoch 15: Val Loss 520.30438
Epoch 16: Val Loss 520.26514
Epoch 17: Val Loss 520.22430
Epoch 18: Val Loss 520.18292
Epoch 19: Val Loss 520.14105
Epoch 20: Val Loss 520.09784
Epoch 21: Val Loss 520.05365
Epoch 22: Val Loss 520.00940
Epoch 23: Val Loss 519.96344
Epoch 24: Val Loss 519.91418
Epoch 25: Val Loss 519.86438
Epoch 26: Val Loss 519.81647
Epoch 27: Val Loss 519.76904
Epoch 28: Val Loss 519.71875
Epoch 29: Val Loss 519.66742
Epoch 30: Val Loss 519.61511
Epoch 31: Val Loss 519.56293
Epoch 32: Val Loss 519.51025
Epoch 33: Val Loss 519.45667
Epoch 34: Val Loss 519.40076
Epoch 35: Val Loss 519.34509
Epoch 36: Val Loss 519.28955
Epoch 37: Val Loss 519.23193
Epoch 38: Val Loss 519.17462
Epoch 39: Val Loss 519.11670
Epoch 40: Val Loss 519.05908
Epoch 41: Val Loss 519.00092
Epoch 42: Val Loss 518.94373
Epoch 43: Val Loss 518.88556
Epoch 44: Val Loss 518.82599
Epoch 45: Val Loss 518.76697
Epoch 46: Val Loss 518.70776
Epoch 47: Val Loss 518.64948
Epoch 48: Val Loss 518.59003
Epoch 49: Val Loss 518.52960
Epoch 50: Val Loss 518.46552
Epoch 51: Val Loss 518.40137
Epoch 52: Val Loss 518.33655
Epoch 53: Val Loss 518.27136
Epoch 54: Val Loss 518.20587
Epoch 55: Val Loss 518.14160
Epoch 56: Val Loss 518.07581
Epoch 57: Val Loss 518.01031
Epoch 58: Val Loss 517.94427
Epoch 59: Val Loss 517.87604
Epoch 60: Val Loss 517.80481
Epoch 61: Val Loss 517.73071
Epoch 62: Val Loss 517.65533
Epoch 63: Val Loss 517.58081
Epoch 64: Val Loss 517.50488
Epoch 65: Val Loss 517.42676
Epoch 66: Val Loss 517.34644
Epoch 67: Val Loss 517.26776
Epoch 68: Val Loss 517.18927
Epoch 69: Val Loss 517.10828
Epoch 70: Val Loss 517.02521
Epoch 71: Val Loss 516.93762
Epoch 72: Val Loss 516.85010
Epoch 73: Val Loss 516.76190
Epoch 74: Val Loss 516.67456
Epoch 75: Val Loss 516.58466
Epoch 76: Val Loss 516.49292
Epoch 77: Val Loss 516.40118
Epoch 78: Val Loss 516.30701
Epoch 79: Val Loss 516.21136
Epoch 80: Val Loss 516.11761
Epoch 81: Val Loss 516.02240
Epoch 82: Val Loss 515.92725
Epoch 83: Val Loss 515.83051
Epoch 84: Val Loss 515.73199
Epoch 85: Val Loss 515.62964
Epoch 86: Val Loss 515.52570
Epoch 87: Val Loss 515.41992
Epoch 88: Val Loss 515.31451
Epoch 89: Val Loss 515.20868
Epoch 90: Val Loss 515.10089
Epoch 91: Val Loss 514.99219
Epoch 92: Val Loss 514.88373
Epoch 93: Val Loss 514.77496
Epoch 94: Val Loss 514.66589
Epoch 95: Val Loss 514.55609
Epoch 96: Val Loss 514.44391
Epoch 97: Val Loss 514.33264
Epoch 98: Val Loss 514.21844
Epoch 99: Val Loss 514.10065
{'MSE - mean': 514.1006592069868, 'MSE - std': 0.0, 'R2 - mean': -5.885127342840876, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 592.62366
Epoch 1: Val Loss 592.54657
Epoch 2: Val Loss 592.47113
Epoch 3: Val Loss 592.39624
Epoch 4: Val Loss 592.32056
Epoch 5: Val Loss 592.24414
Epoch 6: Val Loss 592.16797
Epoch 7: Val Loss 592.09253
Epoch 8: Val Loss 592.01727
Epoch 9: Val Loss 591.94244
Epoch 10: Val Loss 591.86835
Epoch 11: Val Loss 591.79395
Epoch 12: Val Loss 591.71826
Epoch 13: Val Loss 591.64301
Epoch 14: Val Loss 591.56671
Epoch 15: Val Loss 591.49158
Epoch 16: Val Loss 591.41730
Epoch 17: Val Loss 591.34351
Epoch 18: Val Loss 591.26947
Epoch 19: Val Loss 591.19415
Epoch 20: Val Loss 591.11859
Epoch 21: Val Loss 591.04346
Epoch 22: Val Loss 590.96802
Epoch 23: Val Loss 590.89142
Epoch 24: Val Loss 590.81409
Epoch 25: Val Loss 590.73633
Epoch 26: Val Loss 590.65869
Epoch 27: Val Loss 590.57977
Epoch 28: Val Loss 590.50067
Epoch 29: Val Loss 590.42322
Epoch 30: Val Loss 590.34515
Epoch 31: Val Loss 590.26770
Epoch 32: Val Loss 590.19067
Epoch 33: Val Loss 590.11322
Epoch 34: Val Loss 590.03497
Epoch 35: Val Loss 589.95697
Epoch 36: Val Loss 589.87781
Epoch 37: Val Loss 589.79791
Epoch 38: Val Loss 589.71582
Epoch 39: Val Loss 589.63489
Epoch 40: Val Loss 589.55292
Epoch 41: Val Loss 589.47119
Epoch 42: Val Loss 589.38953
Epoch 43: Val Loss 589.30743
Epoch 44: Val Loss 589.22540
Epoch 45: Val Loss 589.14301
Epoch 46: Val Loss 589.06079
Epoch 47: Val Loss 588.97998
Epoch 48: Val Loss 588.89923
Epoch 49: Val Loss 588.81946
Epoch 50: Val Loss 588.73816
Epoch 51: Val Loss 588.65686
Epoch 52: Val Loss 588.57483
Epoch 53: Val Loss 588.49164
Epoch 54: Val Loss 588.40961
Epoch 55: Val Loss 588.32672
Epoch 56: Val Loss 588.24371
Epoch 57: Val Loss 588.15979
Epoch 58: Val Loss 588.07428
Epoch 59: Val Loss 587.98761
Epoch 60: Val Loss 587.90045
Epoch 61: Val Loss 587.81421
Epoch 62: Val Loss 587.72839
Epoch 63: Val Loss 587.64215
Epoch 64: Val Loss 587.55621
Epoch 65: Val Loss 587.46960
Epoch 66: Val Loss 587.38147
Epoch 67: Val Loss 587.29156
Epoch 68: Val Loss 587.20007
Epoch 69: Val Loss 587.10846
Epoch 70: Val Loss 587.01660
Epoch 71: Val Loss 586.92542
Epoch 72: Val Loss 586.83405
Epoch 73: Val Loss 586.74207
Epoch 74: Val Loss 586.65106
Epoch 75: Val Loss 586.55804
Epoch 76: Val Loss 586.46399
Epoch 77: Val Loss 586.36957
Epoch 78: Val Loss 586.27472
Epoch 79: Val Loss 586.17853
Epoch 80: Val Loss 586.08319
Epoch 81: Val Loss 585.98755
Epoch 82: Val Loss 585.89069
Epoch 83: Val Loss 585.79382
Epoch 84: Val Loss 585.69586
Epoch 85: Val Loss 585.59808
Epoch 86: Val Loss 585.50061
Epoch 87: Val Loss 585.40155
Epoch 88: Val Loss 585.29974
Epoch 89: Val Loss 585.19885
Epoch 90: Val Loss 585.09808
Epoch 91: Val Loss 584.99652
Epoch 92: Val Loss 584.89490
Epoch 93: Val Loss 584.79315
Epoch 94: Val Loss 584.69153
Epoch 95: Val Loss 584.58862
Epoch 96: Val Loss 584.48328
Epoch 97: Val Loss 584.37714
Epoch 98: Val Loss 584.27289
Epoch 99: Val Loss 584.16699
{'MSE - mean': 549.1338334203383, 'MSE - std': 35.033174213351515, 'R2 - mean': -6.069122685379492, 'R2 - std': 0.18399534253861605} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 644.55182
Epoch 1: Val Loss 644.48297
Epoch 2: Val Loss 644.41449
Epoch 3: Val Loss 644.34521
Epoch 4: Val Loss 644.27655
Epoch 5: Val Loss 644.20886
Epoch 6: Val Loss 644.14020
Epoch 7: Val Loss 644.07050
Epoch 8: Val Loss 644.00079
Epoch 9: Val Loss 643.92993
Epoch 10: Val Loss 643.85834
Epoch 11: Val Loss 643.78760
Epoch 12: Val Loss 643.71729
Epoch 13: Val Loss 643.64532
Epoch 14: Val Loss 643.57343
Epoch 15: Val Loss 643.50104
Epoch 16: Val Loss 643.42810
Epoch 17: Val Loss 643.35492
Epoch 18: Val Loss 643.28082
Epoch 19: Val Loss 643.20721
Epoch 20: Val Loss 643.13251
Epoch 21: Val Loss 643.05811
Epoch 22: Val Loss 642.98212
Epoch 23: Val Loss 642.90594
Epoch 24: Val Loss 642.82855
Epoch 25: Val Loss 642.75159
Epoch 26: Val Loss 642.67310
Epoch 27: Val Loss 642.59308
Epoch 28: Val Loss 642.51318
Epoch 29: Val Loss 642.43384
Epoch 30: Val Loss 642.35303
Epoch 31: Val Loss 642.27222
Epoch 32: Val Loss 642.19080
Epoch 33: Val Loss 642.10925
Epoch 34: Val Loss 642.02649
Epoch 35: Val Loss 641.94440
Epoch 36: Val Loss 641.86224
Epoch 37: Val Loss 641.77991
Epoch 38: Val Loss 641.69788
Epoch 39: Val Loss 641.61365
Epoch 40: Val Loss 641.52838
Epoch 41: Val Loss 641.44336
Epoch 42: Val Loss 641.35852
Epoch 43: Val Loss 641.27319
Epoch 44: Val Loss 641.18604
Epoch 45: Val Loss 641.09833
Epoch 46: Val Loss 641.01050
Epoch 47: Val Loss 640.92230
Epoch 48: Val Loss 640.83362
Epoch 49: Val Loss 640.74487
Epoch 50: Val Loss 640.65552
Epoch 51: Val Loss 640.56506
Epoch 52: Val Loss 640.47351
Epoch 53: Val Loss 640.38104
Epoch 54: Val Loss 640.28656
Epoch 55: Val Loss 640.19055
Epoch 56: Val Loss 640.09546
Epoch 57: Val Loss 640.00012
Epoch 58: Val Loss 639.90570
Epoch 59: Val Loss 639.81165
Epoch 60: Val Loss 639.71716
Epoch 61: Val Loss 639.62177
Epoch 62: Val Loss 639.52411
Epoch 63: Val Loss 639.42706
Epoch 64: Val Loss 639.32922
Epoch 65: Val Loss 639.23047
Epoch 66: Val Loss 639.13263
Epoch 67: Val Loss 639.03424
Epoch 68: Val Loss 638.93530
Epoch 69: Val Loss 638.83527
Epoch 70: Val Loss 638.73486
Epoch 71: Val Loss 638.63336
Epoch 72: Val Loss 638.53149
Epoch 73: Val Loss 638.42853
Epoch 74: Val Loss 638.32471
Epoch 75: Val Loss 638.22070
Epoch 76: Val Loss 638.11560
Epoch 77: Val Loss 638.01068
Epoch 78: Val Loss 637.90497
Epoch 79: Val Loss 637.80042
Epoch 80: Val Loss 637.69519
Epoch 81: Val Loss 637.58850
Epoch 82: Val Loss 637.48199
Epoch 83: Val Loss 637.37555
Epoch 84: Val Loss 637.26880
Epoch 85: Val Loss 637.16119
Epoch 86: Val Loss 637.05231
Epoch 87: Val Loss 636.94086
Epoch 88: Val Loss 636.82965
Epoch 89: Val Loss 636.71869
Epoch 90: Val Loss 636.60773
Epoch 91: Val Loss 636.49475
Epoch 92: Val Loss 636.38110
Epoch 93: Val Loss 636.26758
Epoch 94: Val Loss 636.15247
Epoch 95: Val Loss 636.03711
Epoch 96: Val Loss 635.92004
Epoch 97: Val Loss 635.80176
Epoch 98: Val Loss 635.68237
Epoch 99: Val Loss 635.56317
{'MSE - mean': 577.9436113215289, 'MSE - std': 49.78174500497703, 'R2 - mean': -5.932659021655586, 'R2 - std': 0.24456939114391368} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 553.16724
Epoch 1: Val Loss 553.03119
Epoch 2: Val Loss 552.89813
Epoch 3: Val Loss 552.76428
Epoch 4: Val Loss 552.62775
Epoch 5: Val Loss 552.49023
Epoch 6: Val Loss 552.35315
Epoch 7: Val Loss 552.21619
Epoch 8: Val Loss 552.07886
Epoch 9: Val Loss 551.94464
Epoch 10: Val Loss 551.81097
Epoch 11: Val Loss 551.67767
Epoch 12: Val Loss 551.54529
Epoch 13: Val Loss 551.41504
Epoch 14: Val Loss 551.28455
Epoch 15: Val Loss 551.15405
Epoch 16: Val Loss 551.02588
Epoch 17: Val Loss 550.89813
Epoch 18: Val Loss 550.76886
Epoch 19: Val Loss 550.63928
Epoch 20: Val Loss 550.50861
Epoch 21: Val Loss 550.37677
Epoch 22: Val Loss 550.24426
Epoch 23: Val Loss 550.11108
Epoch 24: Val Loss 549.98120
Epoch 25: Val Loss 549.85193
Epoch 26: Val Loss 549.72266
Epoch 27: Val Loss 549.59381
Epoch 28: Val Loss 549.46570
Epoch 29: Val Loss 549.33582
Epoch 30: Val Loss 549.20599
Epoch 31: Val Loss 549.07831
Epoch 32: Val Loss 548.95105
Epoch 33: Val Loss 548.82385
Epoch 34: Val Loss 548.69672
Epoch 35: Val Loss 548.56750
Epoch 36: Val Loss 548.43597
Epoch 37: Val Loss 548.30597
Epoch 38: Val Loss 548.17877
Epoch 39: Val Loss 548.04883
Epoch 40: Val Loss 547.91821
Epoch 41: Val Loss 547.78931
Epoch 42: Val Loss 547.65997
Epoch 43: Val Loss 547.53107
Epoch 44: Val Loss 547.40216
Epoch 45: Val Loss 547.27460
Epoch 46: Val Loss 547.14728
Epoch 47: Val Loss 547.01819
Epoch 48: Val Loss 546.88940
Epoch 49: Val Loss 546.76105
Epoch 50: Val Loss 546.63177
Epoch 51: Val Loss 546.50299
Epoch 52: Val Loss 546.37799
Epoch 53: Val Loss 546.25208
Epoch 54: Val Loss 546.12677
Epoch 55: Val Loss 545.99915
Epoch 56: Val Loss 545.87244
Epoch 57: Val Loss 545.74854
Epoch 58: Val Loss 545.62415
Epoch 59: Val Loss 545.49908
Epoch 60: Val Loss 545.37384
Epoch 61: Val Loss 545.24896
Epoch 62: Val Loss 545.12268
Epoch 63: Val Loss 544.99750
Epoch 64: Val Loss 544.87787
Epoch 65: Val Loss 544.75690
Epoch 66: Val Loss 544.63440
Epoch 67: Val Loss 544.51324
Epoch 68: Val Loss 544.39105
Epoch 69: Val Loss 544.26978
Epoch 70: Val Loss 544.14960
Epoch 71: Val Loss 544.02728
Epoch 72: Val Loss 543.90222
Epoch 73: Val Loss 543.78088
Epoch 74: Val Loss 543.66498
Epoch 75: Val Loss 543.55017
Epoch 76: Val Loss 543.43469
Epoch 77: Val Loss 543.32013
Epoch 78: Val Loss 543.20630
Epoch 79: Val Loss 543.09674
Epoch 80: Val Loss 542.98334
Epoch 81: Val Loss 542.87231
Epoch 82: Val Loss 542.76337
Epoch 83: Val Loss 542.65656
Epoch 84: Val Loss 542.55017
Epoch 85: Val Loss 542.44727
Epoch 86: Val Loss 542.34540
Epoch 87: Val Loss 542.24359
Epoch 88: Val Loss 542.14301
Epoch 89: Val Loss 542.04285
Epoch 90: Val Loss 541.94080
Epoch 91: Val Loss 541.83911
Epoch 92: Val Loss 541.74249
Epoch 93: Val Loss 541.64673
Epoch 94: Val Loss 541.55103
Epoch 95: Val Loss 541.45392
Epoch 96: Val Loss 541.35785
Epoch 97: Val Loss 541.26599
Epoch 98: Val Loss 541.17773
Epoch 99: Val Loss 541.09009
{'MSE - mean': 568.7302335855899, 'MSE - std': 45.97092113195808, 'R2 - mean': -5.992238537282983, 'R2 - std': 0.23560517048775495} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 601.48334
Epoch 1: Val Loss 601.45367
Epoch 2: Val Loss 601.42456
Epoch 3: Val Loss 601.39508
Epoch 4: Val Loss 601.36511
Epoch 5: Val Loss 601.33459
Epoch 6: Val Loss 601.30359
Epoch 7: Val Loss 601.27252
Epoch 8: Val Loss 601.24146
Epoch 9: Val Loss 601.21045
Epoch 10: Val Loss 601.17975
Epoch 11: Val Loss 601.14868
Epoch 12: Val Loss 601.11646
Epoch 13: Val Loss 601.08344
Epoch 14: Val Loss 601.05029
Epoch 15: Val Loss 601.01703
Epoch 16: Val Loss 600.98346
Epoch 17: Val Loss 600.94934
Epoch 18: Val Loss 600.91473
Epoch 19: Val Loss 600.87939
Epoch 20: Val Loss 600.84357
Epoch 21: Val Loss 600.80725
Epoch 22: Val Loss 600.77081
Epoch 23: Val Loss 600.73407
Epoch 24: Val Loss 600.69702
Epoch 25: Val Loss 600.65918
Epoch 26: Val Loss 600.62097
Epoch 27: Val Loss 600.58130
Epoch 28: Val Loss 600.54163
Epoch 29: Val Loss 600.50153
Epoch 30: Val Loss 600.46075
Epoch 31: Val Loss 600.41998
Epoch 32: Val Loss 600.37762
Epoch 33: Val Loss 600.33447
Epoch 34: Val Loss 600.29150
Epoch 35: Val Loss 600.24762
Epoch 36: Val Loss 600.20343
Epoch 37: Val Loss 600.15912
Epoch 38: Val Loss 600.11450
Epoch 39: Val Loss 600.06921
Epoch 40: Val Loss 600.02271
Epoch 41: Val Loss 599.97601
Epoch 42: Val Loss 599.92914
Epoch 43: Val Loss 599.88141
Epoch 44: Val Loss 599.83331
Epoch 45: Val Loss 599.78613
Epoch 46: Val Loss 599.73785
Epoch 47: Val Loss 599.68854
Epoch 48: Val Loss 599.63959
Epoch 49: Val Loss 599.59021
Epoch 50: Val Loss 599.53979
Epoch 51: Val Loss 599.48785
Epoch 52: Val Loss 599.43512
Epoch 53: Val Loss 599.38293
Epoch 54: Val Loss 599.33154
Epoch 55: Val Loss 599.28046
Epoch 56: Val Loss 599.22931
Epoch 57: Val Loss 599.17682
Epoch 58: Val Loss 599.12439
Epoch 59: Val Loss 599.07025
Epoch 60: Val Loss 599.01526
Epoch 61: Val Loss 598.95807
Epoch 62: Val Loss 598.90082
Epoch 63: Val Loss 598.84369
Epoch 64: Val Loss 598.78699
Epoch 65: Val Loss 598.73029
Epoch 66: Val Loss 598.67303
Epoch 67: Val Loss 598.61432
Epoch 68: Val Loss 598.55548
Epoch 69: Val Loss 598.49725
Epoch 70: Val Loss 598.43890
Epoch 71: Val Loss 598.38007
Epoch 72: Val Loss 598.32031
Epoch 73: Val Loss 598.25940
Epoch 74: Val Loss 598.19824
Epoch 75: Val Loss 598.13684
Epoch 76: Val Loss 598.07562
Epoch 77: Val Loss 598.01404
Epoch 78: Val Loss 597.95221
Epoch 79: Val Loss 597.88989
Epoch 80: Val Loss 597.82623
Epoch 81: Val Loss 597.76129
Epoch 82: Val Loss 597.69519
Epoch 83: Val Loss 597.62946
Epoch 84: Val Loss 597.56274
Epoch 85: Val Loss 597.49493
Epoch 86: Val Loss 597.42712
Epoch 87: Val Loss 597.35901
Epoch 88: Val Loss 597.29175
Epoch 89: Val Loss 597.22424
Epoch 90: Val Loss 597.15448
Epoch 91: Val Loss 597.08398
Epoch 92: Val Loss 597.01288
Epoch 93: Val Loss 596.94208
Epoch 94: Val Loss 596.87042
Epoch 95: Val Loss 596.79767
Epoch 96: Val Loss 596.72394
Epoch 97: Val Loss 596.64941
Epoch 98: Val Loss 596.57349
Epoch 99: Val Loss 596.49854
{'MSE - mean': 574.2838984596375, 'MSE - std': 42.591469168244096, 'R2 - mean': -5.900806083131916, 'R2 - std': 0.27901149040281353} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 574.2838984596375 and parameters: {'dim': 256, 'depth': 2, 'heads': 2, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 560.96051
Epoch 1: Val Loss 560.44885
Epoch 2: Val Loss 559.92383
Epoch 3: Val Loss 559.38745
Epoch 4: Val Loss 558.84406
Epoch 5: Val Loss 558.30060
Epoch 6: Val Loss 557.74219
Epoch 7: Val Loss 557.16333
Epoch 8: Val Loss 556.56055
Epoch 9: Val Loss 555.92841
Epoch 10: Val Loss 555.26825
Epoch 11: Val Loss 554.57654
Epoch 12: Val Loss 553.85486
Epoch 13: Val Loss 553.10309
Epoch 14: Val Loss 552.31531
Epoch 15: Val Loss 551.48853
Epoch 16: Val Loss 550.62347
Epoch 17: Val Loss 549.71893
Epoch 18: Val Loss 548.76556
Epoch 19: Val Loss 547.76709
Epoch 20: Val Loss 546.71899
Epoch 21: Val Loss 545.61151
Epoch 22: Val Loss 544.45367
Epoch 23: Val Loss 543.23230
Epoch 24: Val Loss 541.94922
Epoch 25: Val Loss 540.59784
Epoch 26: Val Loss 539.17169
Epoch 27: Val Loss 537.66876
Epoch 28: Val Loss 536.09155
Epoch 29: Val Loss 534.41553
Epoch 30: Val Loss 532.63269
Epoch 31: Val Loss 530.74603
Epoch 32: Val Loss 528.74939
Epoch 33: Val Loss 526.64392
Epoch 34: Val Loss 524.40710
Epoch 35: Val Loss 522.06036
Epoch 36: Val Loss 519.56958
Epoch 37: Val Loss 516.93689
Epoch 38: Val Loss 514.15625
Epoch 39: Val Loss 511.22925
Epoch 40: Val Loss 508.13443
Epoch 41: Val Loss 504.86325
Epoch 42: Val Loss 501.39850
Epoch 43: Val Loss 497.72519
Epoch 44: Val Loss 493.82355
Epoch 45: Val Loss 489.71194
Epoch 46: Val Loss 485.34973
Epoch 47: Val Loss 480.71341
Epoch 48: Val Loss 475.81494
Epoch 49: Val Loss 470.59860
Epoch 50: Val Loss 465.10019
Epoch 51: Val Loss 459.33401
Epoch 52: Val Loss 453.31635
Epoch 53: Val Loss 446.99237
Epoch 54: Val Loss 440.39639
Epoch 55: Val Loss 433.51749
Epoch 56: Val Loss 426.29837
Epoch 57: Val Loss 418.70230
Epoch 58: Val Loss 410.75699
Epoch 59: Val Loss 402.59207
Epoch 60: Val Loss 394.13574
Epoch 61: Val Loss 385.39087
Epoch 62: Val Loss 376.27881
Epoch 63: Val Loss 366.83267
Epoch 64: Val Loss 356.98517
Epoch 65: Val Loss 346.76471
Epoch 66: Val Loss 336.25436
Epoch 67: Val Loss 325.49280
Epoch 68: Val Loss 314.54532
Epoch 69: Val Loss 303.43820
Epoch 70: Val Loss 291.96729
Epoch 71: Val Loss 280.19586
Epoch 72: Val Loss 268.35434
Epoch 73: Val Loss 256.17548
Epoch 74: Val Loss 244.05420
Epoch 75: Val Loss 231.97110
Epoch 76: Val Loss 219.76291
Epoch 77: Val Loss 207.74817
Epoch 78: Val Loss 195.78055
Epoch 79: Val Loss 184.26176
Epoch 80: Val Loss 173.43161
Epoch 81: Val Loss 162.95049
Epoch 82: Val Loss 153.00479
Epoch 83: Val Loss 143.79417
Epoch 84: Val Loss 135.10558
Epoch 85: Val Loss 126.97376
Epoch 86: Val Loss 119.27128
Epoch 87: Val Loss 112.36826
Epoch 88: Val Loss 106.13512
Epoch 89: Val Loss 100.55873
Epoch 90: Val Loss 95.68312
Epoch 91: Val Loss 91.44481
Epoch 92: Val Loss 87.67875
Epoch 93: Val Loss 84.41805
Epoch 94: Val Loss 81.58387
Epoch 95: Val Loss 79.13464
Epoch 96: Val Loss 77.19497
Epoch 97: Val Loss 75.51734
Epoch 98: Val Loss 73.93797
Epoch 99: Val Loss 72.63300
{'MSE - mean': 72.6329879800563, 'MSE - std': 0.0, 'R2 - mean': 0.027257867544625225, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 641.86096
Epoch 1: Val Loss 641.51459
Epoch 2: Val Loss 641.18585
Epoch 3: Val Loss 640.88525
Epoch 4: Val Loss 640.59717
Epoch 5: Val Loss 640.31891
Epoch 6: Val Loss 640.05450
Epoch 7: Val Loss 639.80377
Epoch 8: Val Loss 639.56720
Epoch 9: Val Loss 639.34174
Epoch 10: Val Loss 639.12592
Epoch 11: Val Loss 638.91864
Epoch 12: Val Loss 638.71582
Epoch 13: Val Loss 638.51520
Epoch 14: Val Loss 638.31781
Epoch 15: Val Loss 638.12360
Epoch 16: Val Loss 637.93164
Epoch 17: Val Loss 637.73932
Epoch 18: Val Loss 637.54822
Epoch 19: Val Loss 637.35712
Epoch 20: Val Loss 637.16754
Epoch 21: Val Loss 636.97858
Epoch 22: Val Loss 636.79065
Epoch 23: Val Loss 636.60272
Epoch 24: Val Loss 636.41479
Epoch 25: Val Loss 636.22656
Epoch 26: Val Loss 636.03876
Epoch 27: Val Loss 635.85040
Epoch 28: Val Loss 635.66083
Epoch 29: Val Loss 635.47198
Epoch 30: Val Loss 635.28302
Epoch 31: Val Loss 635.09521
Epoch 32: Val Loss 634.90564
Epoch 33: Val Loss 634.71625
Epoch 34: Val Loss 634.52551
Epoch 35: Val Loss 634.33514
Epoch 36: Val Loss 634.14716
Epoch 37: Val Loss 633.95917
Epoch 38: Val Loss 633.77081
Epoch 39: Val Loss 633.58331
Epoch 40: Val Loss 633.39648
Epoch 41: Val Loss 633.20874
Epoch 42: Val Loss 633.02222
Epoch 43: Val Loss 632.83600
Epoch 44: Val Loss 632.64923
Epoch 45: Val Loss 632.46204
Epoch 46: Val Loss 632.27295
Epoch 47: Val Loss 632.08423
Epoch 48: Val Loss 631.89514
Epoch 49: Val Loss 631.70575
Epoch 50: Val Loss 631.51562
Epoch 51: Val Loss 631.32477
Epoch 52: Val Loss 631.13483
Epoch 53: Val Loss 630.94507
Epoch 54: Val Loss 630.75629
Epoch 55: Val Loss 630.56775
Epoch 56: Val Loss 630.37878
Epoch 57: Val Loss 630.18927
Epoch 58: Val Loss 630.00195
Epoch 59: Val Loss 629.81433
Epoch 60: Val Loss 629.62628
Epoch 61: Val Loss 629.43744
Epoch 62: Val Loss 629.25012
Epoch 63: Val Loss 629.06372
Epoch 64: Val Loss 628.87738
Epoch 65: Val Loss 628.69141
Epoch 66: Val Loss 628.50531
Epoch 67: Val Loss 628.32123
Epoch 68: Val Loss 628.13593
Epoch 69: Val Loss 627.95203
Epoch 70: Val Loss 627.76831
Epoch 71: Val Loss 627.58423
Epoch 72: Val Loss 627.39911
Epoch 73: Val Loss 627.21222
Epoch 74: Val Loss 627.02417
Epoch 75: Val Loss 626.83545
Epoch 76: Val Loss 626.64557
Epoch 77: Val Loss 626.45520
Epoch 78: Val Loss 626.26605
Epoch 79: Val Loss 626.07739
Epoch 80: Val Loss 625.88898
Epoch 81: Val Loss 625.70123
Epoch 82: Val Loss 625.51489
Epoch 83: Val Loss 625.32849
Epoch 84: Val Loss 625.14252
Epoch 85: Val Loss 624.95459
Epoch 86: Val Loss 624.76752
Epoch 87: Val Loss 624.58081
Epoch 88: Val Loss 624.39606
Epoch 89: Val Loss 624.21246
Epoch 90: Val Loss 624.02759
Epoch 91: Val Loss 623.84186
Epoch 92: Val Loss 623.65765
Epoch 93: Val Loss 623.47418
Epoch 94: Val Loss 623.28986
Epoch 95: Val Loss 623.10468
Epoch 96: Val Loss 622.91931
Epoch 97: Val Loss 622.73340
Epoch 98: Val Loss 622.54852
Epoch 99: Val Loss 622.36426
{'MSE - mean': 347.4985799603799, 'MSE - std': 274.86559198032364, 'R2 - mean': -3.350061360651212, 'R2 - std': 3.3773192281958373} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 684.78021
Epoch 1: Val Loss 684.07501
Epoch 2: Val Loss 683.37244
Epoch 3: Val Loss 682.68085
Epoch 4: Val Loss 681.99921
Epoch 5: Val Loss 681.36029
Epoch 6: Val Loss 680.70972
Epoch 7: Val Loss 680.04871
Epoch 8: Val Loss 679.38519
Epoch 9: Val Loss 678.67157
Epoch 10: Val Loss 677.91302
Epoch 11: Val Loss 677.10706
Epoch 12: Val Loss 676.24811
Epoch 13: Val Loss 675.36499
Epoch 14: Val Loss 674.48468
Epoch 15: Val Loss 673.58417
Epoch 16: Val Loss 672.68036
Epoch 17: Val Loss 671.77692
Epoch 18: Val Loss 670.89276
Epoch 19: Val Loss 669.98175
Epoch 20: Val Loss 669.05768
Epoch 21: Val Loss 668.10254
Epoch 22: Val Loss 667.12048
Epoch 23: Val Loss 666.10315
Epoch 24: Val Loss 665.05670
Epoch 25: Val Loss 663.99042
Epoch 26: Val Loss 662.86835
Epoch 27: Val Loss 661.71442
Epoch 28: Val Loss 660.54523
Epoch 29: Val Loss 659.32721
Epoch 30: Val Loss 658.06512
Epoch 31: Val Loss 656.73682
Epoch 32: Val Loss 655.36017
Epoch 33: Val Loss 653.92102
Epoch 34: Val Loss 652.40704
Epoch 35: Val Loss 650.76642
Epoch 36: Val Loss 649.03699
Epoch 37: Val Loss 647.26447
Epoch 38: Val Loss 645.42792
Epoch 39: Val Loss 643.45929
Epoch 40: Val Loss 641.38232
Epoch 41: Val Loss 639.23639
Epoch 42: Val Loss 636.95264
Epoch 43: Val Loss 634.54608
Epoch 44: Val Loss 632.04468
Epoch 45: Val Loss 629.43231
Epoch 46: Val Loss 626.66510
Epoch 47: Val Loss 623.79395
Epoch 48: Val Loss 620.79858
Epoch 49: Val Loss 617.68665
Epoch 50: Val Loss 614.42615
Epoch 51: Val Loss 610.99347
Epoch 52: Val Loss 607.46045
Epoch 53: Val Loss 603.82520
Epoch 54: Val Loss 600.08801
Epoch 55: Val Loss 596.16931
Epoch 56: Val Loss 592.13049
Epoch 57: Val Loss 587.92749
Epoch 58: Val Loss 583.56494
Epoch 59: Val Loss 579.06293
Epoch 60: Val Loss 574.41760
Epoch 61: Val Loss 569.59369
Epoch 62: Val Loss 564.56073
Epoch 63: Val Loss 559.44135
Epoch 64: Val Loss 554.18433
Epoch 65: Val Loss 548.75958
Epoch 66: Val Loss 543.03485
Epoch 67: Val Loss 537.10168
Epoch 68: Val Loss 531.00476
Epoch 69: Val Loss 524.69775
Epoch 70: Val Loss 518.03241
Epoch 71: Val Loss 511.15207
Epoch 72: Val Loss 504.09219
Epoch 73: Val Loss 496.84564
Epoch 74: Val Loss 489.33081
Epoch 75: Val Loss 481.53934
Epoch 76: Val Loss 473.38666
Epoch 77: Val Loss 465.09854
Epoch 78: Val Loss 456.59686
Epoch 79: Val Loss 447.78519
Epoch 80: Val Loss 438.75143
Epoch 81: Val Loss 429.50784
Epoch 82: Val Loss 420.08353
Epoch 83: Val Loss 410.47763
Epoch 84: Val Loss 400.77631
Epoch 85: Val Loss 390.84937
Epoch 86: Val Loss 380.68872
Epoch 87: Val Loss 370.46124
Epoch 88: Val Loss 360.08200
Epoch 89: Val Loss 349.52676
Epoch 90: Val Loss 338.82053
Epoch 91: Val Loss 327.96814
Epoch 92: Val Loss 317.01706
Epoch 93: Val Loss 305.93445
Epoch 94: Val Loss 295.00720
Epoch 95: Val Loss 284.17288
Epoch 96: Val Loss 273.39258
Epoch 97: Val Loss 262.79053
Epoch 98: Val Loss 252.29892
Epoch 99: Val Loss 241.97702
{'MSE - mean': 312.3247308704409, 'MSE - std': 229.87343276115976, 'R2 - mean': -2.745224201349991, 'R2 - std': 2.8871865355920505} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 545.70526
Epoch 1: Val Loss 544.88116
Epoch 2: Val Loss 544.00500
Epoch 3: Val Loss 543.09845
Epoch 4: Val Loss 542.16272
Epoch 5: Val Loss 541.18707
Epoch 6: Val Loss 540.17725
Epoch 7: Val Loss 539.11365
Epoch 8: Val Loss 537.99585
Epoch 9: Val Loss 536.80994
Epoch 10: Val Loss 535.54425
Epoch 11: Val Loss 534.19183
Epoch 12: Val Loss 532.73682
Epoch 13: Val Loss 531.19928
Epoch 14: Val Loss 529.57587
Epoch 15: Val Loss 527.87354
Epoch 16: Val Loss 526.08807
Epoch 17: Val Loss 524.16113
Epoch 18: Val Loss 522.09027
Epoch 19: Val Loss 519.84760
Epoch 20: Val Loss 517.48438
Epoch 21: Val Loss 514.96381
Epoch 22: Val Loss 512.29382
Epoch 23: Val Loss 509.46750
Epoch 24: Val Loss 506.48373
Epoch 25: Val Loss 503.34726
Epoch 26: Val Loss 499.97949
Epoch 27: Val Loss 496.40002
Epoch 28: Val Loss 492.69864
Epoch 29: Val Loss 488.77945
Epoch 30: Val Loss 484.62646
Epoch 31: Val Loss 480.26309
Epoch 32: Val Loss 475.70087
Epoch 33: Val Loss 470.90930
Epoch 34: Val Loss 465.87320
Epoch 35: Val Loss 460.68372
Epoch 36: Val Loss 455.23026
Epoch 37: Val Loss 449.45932
Epoch 38: Val Loss 443.40198
Epoch 39: Val Loss 437.10797
Epoch 40: Val Loss 430.47498
Epoch 41: Val Loss 423.57596
Epoch 42: Val Loss 416.34787
Epoch 43: Val Loss 408.76425
Epoch 44: Val Loss 400.84195
Epoch 45: Val Loss 392.52704
Epoch 46: Val Loss 383.79214
Epoch 47: Val Loss 374.60214
Epoch 48: Val Loss 365.02148
Epoch 49: Val Loss 355.11942
Epoch 50: Val Loss 344.86606
Epoch 51: Val Loss 334.31247
Epoch 52: Val Loss 323.46548
Epoch 53: Val Loss 312.40790
Epoch 54: Val Loss 301.21939
Epoch 55: Val Loss 289.88437
Epoch 56: Val Loss 278.65189
Epoch 57: Val Loss 267.42279
Epoch 58: Val Loss 256.04474
Epoch 59: Val Loss 244.51425
Epoch 60: Val Loss 233.00897
Epoch 61: Val Loss 221.44101
Epoch 62: Val Loss 210.17065
Epoch 63: Val Loss 199.30037
Epoch 64: Val Loss 188.63277
Epoch 65: Val Loss 178.35078
Epoch 66: Val Loss 168.42195
Epoch 67: Val Loss 159.09558
Epoch 68: Val Loss 150.24208
Epoch 69: Val Loss 141.88672
Epoch 70: Val Loss 134.23219
Epoch 71: Val Loss 127.09801
Epoch 72: Val Loss 120.69044
Epoch 73: Val Loss 114.90575
Epoch 74: Val Loss 109.70364
Epoch 75: Val Loss 104.89370
Epoch 76: Val Loss 100.73856
Epoch 77: Val Loss 97.03603
Epoch 78: Val Loss 93.90730
Epoch 79: Val Loss 91.16411
Epoch 80: Val Loss 88.76148
Epoch 81: Val Loss 86.68185
Epoch 82: Val Loss 84.81312
Epoch 83: Val Loss 83.12609
Epoch 84: Val Loss 81.66312
Epoch 85: Val Loss 80.32306
Epoch 86: Val Loss 79.11003
Epoch 87: Val Loss 77.99821
Epoch 88: Val Loss 76.89808
Epoch 89: Val Loss 75.85733
Epoch 90: Val Loss 74.89027
Epoch 91: Val Loss 73.89127
Epoch 92: Val Loss 72.91882
Epoch 93: Val Loss 72.00179
Epoch 94: Val Loss 71.10326
Epoch 95: Val Loss 70.28650
Epoch 96: Val Loss 69.47890
Epoch 97: Val Loss 68.68274
Epoch 98: Val Loss 67.86671
Epoch 99: Val Loss 67.02476
{'MSE - mean': 250.9997373957514, 'MSE - std': 225.64044582268866, 'R2 - mean': -2.030985139477129, 'R2 - std': 2.789676841677741} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 584.54065
Epoch 1: Val Loss 583.67285
Epoch 2: Val Loss 582.77991
Epoch 3: Val Loss 581.86816
Epoch 4: Val Loss 580.92834
Epoch 5: Val Loss 579.94458
Epoch 6: Val Loss 578.92267
Epoch 7: Val Loss 577.85358
Epoch 8: Val Loss 576.72351
Epoch 9: Val Loss 575.53046
Epoch 10: Val Loss 574.27728
Epoch 11: Val Loss 572.96771
Epoch 12: Val Loss 571.58441
Epoch 13: Val Loss 570.13141
Epoch 14: Val Loss 568.59528
Epoch 15: Val Loss 566.98944
Epoch 16: Val Loss 565.29803
Epoch 17: Val Loss 563.52020
Epoch 18: Val Loss 561.65393
Epoch 19: Val Loss 559.70087
Epoch 20: Val Loss 557.62079
Epoch 21: Val Loss 555.40594
Epoch 22: Val Loss 553.05591
Epoch 23: Val Loss 550.57153
Epoch 24: Val Loss 547.92493
Epoch 25: Val Loss 545.13855
Epoch 26: Val Loss 542.19153
Epoch 27: Val Loss 539.03461
Epoch 28: Val Loss 535.71466
Epoch 29: Val Loss 532.22595
Epoch 30: Val Loss 528.53613
Epoch 31: Val Loss 524.66547
Epoch 32: Val Loss 520.58514
Epoch 33: Val Loss 516.26221
Epoch 34: Val Loss 511.71094
Epoch 35: Val Loss 506.92807
Epoch 36: Val Loss 501.93274
Epoch 37: Val Loss 496.70599
Epoch 38: Val Loss 491.28287
Epoch 39: Val Loss 485.64236
Epoch 40: Val Loss 479.82379
Epoch 41: Val Loss 473.73453
Epoch 42: Val Loss 467.31055
Epoch 43: Val Loss 460.59229
Epoch 44: Val Loss 453.68451
Epoch 45: Val Loss 446.59668
Epoch 46: Val Loss 439.31836
Epoch 47: Val Loss 431.85696
Epoch 48: Val Loss 424.08447
Epoch 49: Val Loss 416.19626
Epoch 50: Val Loss 407.98062
Epoch 51: Val Loss 399.45209
Epoch 52: Val Loss 390.57858
Epoch 53: Val Loss 381.38080
Epoch 54: Val Loss 371.78937
Epoch 55: Val Loss 361.89835
Epoch 56: Val Loss 351.60916
Epoch 57: Val Loss 341.00452
Epoch 58: Val Loss 330.23624
Epoch 59: Val Loss 319.33844
Epoch 60: Val Loss 308.23074
Epoch 61: Val Loss 296.96298
Epoch 62: Val Loss 285.94998
Epoch 63: Val Loss 274.90192
Epoch 64: Val Loss 263.83948
Epoch 65: Val Loss 252.81720
Epoch 66: Val Loss 241.85818
Epoch 67: Val Loss 231.07523
Epoch 68: Val Loss 220.61465
Epoch 69: Val Loss 210.43831
Epoch 70: Val Loss 200.67116
Epoch 71: Val Loss 191.41956
Epoch 72: Val Loss 182.60040
Epoch 73: Val Loss 174.04633
Epoch 74: Val Loss 165.95854
Epoch 75: Val Loss 158.33435
Epoch 76: Val Loss 151.32750
Epoch 77: Val Loss 144.86200
Epoch 78: Val Loss 138.86742
Epoch 79: Val Loss 133.47919
Epoch 80: Val Loss 128.51906
Epoch 81: Val Loss 124.04695
Epoch 82: Val Loss 120.04772
Epoch 83: Val Loss 116.45693
Epoch 84: Val Loss 113.11788
Epoch 85: Val Loss 110.01302
Epoch 86: Val Loss 107.19106
Epoch 87: Val Loss 104.60170
Epoch 88: Val Loss 102.15165
Epoch 89: Val Loss 99.95119
Epoch 90: Val Loss 97.85674
Epoch 91: Val Loss 95.73472
Epoch 92: Val Loss 93.60282
Epoch 93: Val Loss 91.70765
Epoch 94: Val Loss 89.88778
Epoch 95: Val Loss 88.12812
Epoch 96: Val Loss 86.45488
Epoch 97: Val Loss 84.94079
Epoch 98: Val Loss 83.51986
Epoch 99: Val Loss 82.05987
{'MSE - mean': 217.21176409946946, 'MSE - std': 212.83185191772964, 'R2 - mean': -1.6045932452887826, 'R2 - std': 2.6368689187201895} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 217.21176409946946 and parameters: {'dim': 128, 'depth': 12, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 551.48962
Epoch 1: Val Loss 551.44250
Epoch 2: Val Loss 551.39526
Epoch 3: Val Loss 551.34766
Epoch 4: Val Loss 551.29987
Epoch 5: Val Loss 551.25226
Epoch 6: Val Loss 551.20477
Epoch 7: Val Loss 551.15717
Epoch 8: Val Loss 551.10950
Epoch 9: Val Loss 551.06189
Epoch 10: Val Loss 551.01410
Epoch 11: Val Loss 550.96619
Epoch 12: Val Loss 550.91809
Epoch 13: Val Loss 550.86993
Epoch 14: Val Loss 550.82220
Epoch 15: Val Loss 550.77454
Epoch 16: Val Loss 550.72693
Epoch 17: Val Loss 550.67914
Epoch 18: Val Loss 550.63116
Epoch 19: Val Loss 550.58295
Epoch 20: Val Loss 550.53467
Epoch 21: Val Loss 550.48633
Epoch 22: Val Loss 550.43744
Epoch 23: Val Loss 550.38855
Epoch 24: Val Loss 550.33936
Epoch 25: Val Loss 550.29010
Epoch 26: Val Loss 550.24017
Epoch 27: Val Loss 550.19019
Epoch 28: Val Loss 550.14044
Epoch 29: Val Loss 550.09064
Epoch 30: Val Loss 550.04065
Epoch 31: Val Loss 549.99060
Epoch 32: Val Loss 549.94049
Epoch 33: Val Loss 549.89050
Epoch 34: Val Loss 549.84064
Epoch 35: Val Loss 549.79041
Epoch 36: Val Loss 549.74030
Epoch 37: Val Loss 549.69012
Epoch 38: Val Loss 549.63983
Epoch 39: Val Loss 549.58923
Epoch 40: Val Loss 549.53839
Epoch 41: Val Loss 549.48700
Epoch 42: Val Loss 549.43604
Epoch 43: Val Loss 549.38507
Epoch 44: Val Loss 549.33380
Epoch 45: Val Loss 549.28241
Epoch 46: Val Loss 549.23035
Epoch 47: Val Loss 549.17810
Epoch 48: Val Loss 549.12604
Epoch 49: Val Loss 549.07343
Epoch 50: Val Loss 549.02002
Epoch 51: Val Loss 548.96667
Epoch 52: Val Loss 548.91290
Epoch 53: Val Loss 548.85968
Epoch 54: Val Loss 548.80621
Epoch 55: Val Loss 548.75287
Epoch 56: Val Loss 548.69885
Epoch 57: Val Loss 548.64478
Epoch 58: Val Loss 548.59070
Epoch 59: Val Loss 548.53680
Epoch 60: Val Loss 548.48285
Epoch 61: Val Loss 548.42896
Epoch 62: Val Loss 548.37500
Epoch 63: Val Loss 548.32050
Epoch 64: Val Loss 548.26605
Epoch 65: Val Loss 548.21155
Epoch 66: Val Loss 548.15649
Epoch 67: Val Loss 548.10138
Epoch 68: Val Loss 548.04553
Epoch 69: Val Loss 547.98975
Epoch 70: Val Loss 547.93475
Epoch 71: Val Loss 547.88007
Epoch 72: Val Loss 547.82495
Epoch 73: Val Loss 547.77002
Epoch 74: Val Loss 547.71490
Epoch 75: Val Loss 547.65967
Epoch 76: Val Loss 547.60382
Epoch 77: Val Loss 547.54797
Epoch 78: Val Loss 547.49268
Epoch 79: Val Loss 547.43701
Epoch 80: Val Loss 547.38074
Epoch 81: Val Loss 547.32446
Epoch 82: Val Loss 547.26837
Epoch 83: Val Loss 547.21259
Epoch 84: Val Loss 547.15717
Epoch 85: Val Loss 547.10144
Epoch 86: Val Loss 547.04572
Epoch 87: Val Loss 546.98969
Epoch 88: Val Loss 546.93323
Epoch 89: Val Loss 546.87585
Epoch 90: Val Loss 546.81848
Epoch 91: Val Loss 546.76123
Epoch 92: Val Loss 546.70447
Epoch 93: Val Loss 546.64795
Epoch 94: Val Loss 546.59131
Epoch 95: Val Loss 546.53406
Epoch 96: Val Loss 546.47656
Epoch 97: Val Loss 546.41901
Epoch 98: Val Loss 546.36139
Epoch 99: Val Loss 546.30432
{'MSE - mean': 546.3043112169822, 'MSE - std': 0.0, 'R2 - mean': -6.316416898733238, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 593.91406
Epoch 1: Val Loss 593.89526
Epoch 2: Val Loss 593.87659
Epoch 3: Val Loss 593.85797
Epoch 4: Val Loss 593.83936
Epoch 5: Val Loss 593.82080
Epoch 6: Val Loss 593.80231
Epoch 7: Val Loss 593.78357
Epoch 8: Val Loss 593.76483
Epoch 9: Val Loss 593.74609
Epoch 10: Val Loss 593.72754
Epoch 11: Val Loss 593.70905
Epoch 12: Val Loss 593.69061
Epoch 13: Val Loss 593.67212
Epoch 14: Val Loss 593.65350
Epoch 15: Val Loss 593.63513
Epoch 16: Val Loss 593.61658
Epoch 17: Val Loss 593.59790
Epoch 18: Val Loss 593.57922
Epoch 19: Val Loss 593.56061
Epoch 20: Val Loss 593.54218
Epoch 21: Val Loss 593.52374
Epoch 22: Val Loss 593.50531
Epoch 23: Val Loss 593.48688
Epoch 24: Val Loss 593.46863
Epoch 25: Val Loss 593.45038
Epoch 26: Val Loss 593.43201
Epoch 27: Val Loss 593.41357
Epoch 28: Val Loss 593.39502
Epoch 29: Val Loss 593.37646
Epoch 30: Val Loss 593.35791
Epoch 31: Val Loss 593.33929
Epoch 32: Val Loss 593.32037
Epoch 33: Val Loss 593.30145
Epoch 34: Val Loss 593.28253
Epoch 35: Val Loss 593.26373
Epoch 36: Val Loss 593.24481
Epoch 37: Val Loss 593.22601
Epoch 38: Val Loss 593.20697
Epoch 39: Val Loss 593.18805
Epoch 40: Val Loss 593.16888
Epoch 41: Val Loss 593.14960
Epoch 42: Val Loss 593.13025
Epoch 43: Val Loss 593.11102
Epoch 44: Val Loss 593.09180
Epoch 45: Val Loss 593.07233
Epoch 46: Val Loss 593.05292
Epoch 47: Val Loss 593.03351
Epoch 48: Val Loss 593.01410
Epoch 49: Val Loss 592.99451
Epoch 50: Val Loss 592.97473
Epoch 51: Val Loss 592.95465
Epoch 52: Val Loss 592.93457
Epoch 53: Val Loss 592.91431
Epoch 54: Val Loss 592.89374
Epoch 55: Val Loss 592.87323
Epoch 56: Val Loss 592.85254
Epoch 57: Val Loss 592.83191
Epoch 58: Val Loss 592.81140
Epoch 59: Val Loss 592.79059
Epoch 60: Val Loss 592.76959
Epoch 61: Val Loss 592.74854
Epoch 62: Val Loss 592.72723
Epoch 63: Val Loss 592.70569
Epoch 64: Val Loss 592.68378
Epoch 65: Val Loss 592.66156
Epoch 66: Val Loss 592.63959
Epoch 67: Val Loss 592.61743
Epoch 68: Val Loss 592.59497
Epoch 69: Val Loss 592.57233
Epoch 70: Val Loss 592.54950
Epoch 71: Val Loss 592.52618
Epoch 72: Val Loss 592.50226
Epoch 73: Val Loss 592.47839
Epoch 74: Val Loss 592.45435
Epoch 75: Val Loss 592.42993
Epoch 76: Val Loss 592.40533
Epoch 77: Val Loss 592.38019
Epoch 78: Val Loss 592.35486
Epoch 79: Val Loss 592.32855
Epoch 80: Val Loss 592.30243
Epoch 81: Val Loss 592.27594
Epoch 82: Val Loss 592.24908
Epoch 83: Val Loss 592.22198
Epoch 84: Val Loss 592.19409
Epoch 85: Val Loss 592.16571
Epoch 86: Val Loss 592.13715
Epoch 87: Val Loss 592.10809
Epoch 88: Val Loss 592.07874
Epoch 89: Val Loss 592.04913
Epoch 90: Val Loss 592.01941
Epoch 91: Val Loss 591.98914
Epoch 92: Val Loss 591.95789
Epoch 93: Val Loss 591.92572
Epoch 94: Val Loss 591.89319
Epoch 95: Val Loss 591.85986
Epoch 96: Val Loss 591.82562
Epoch 97: Val Loss 591.79022
Epoch 98: Val Loss 591.75409
Epoch 99: Val Loss 591.71759
{'MSE - mean': 569.0109395469899, 'MSE - std': 22.70662833000773, 'R2 - mean': -6.3316419905635755, 'R2 - std': 0.015225091830336712} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 677.89136
Epoch 1: Val Loss 677.81573
Epoch 2: Val Loss 677.74054
Epoch 3: Val Loss 677.66559
Epoch 4: Val Loss 677.59113
Epoch 5: Val Loss 677.51593
Epoch 6: Val Loss 677.44061
Epoch 7: Val Loss 677.36584
Epoch 8: Val Loss 677.29266
Epoch 9: Val Loss 677.22119
Epoch 10: Val Loss 677.14960
Epoch 11: Val Loss 677.07758
Epoch 12: Val Loss 677.00562
Epoch 13: Val Loss 676.93433
Epoch 14: Val Loss 676.86328
Epoch 15: Val Loss 676.79352
Epoch 16: Val Loss 676.72449
Epoch 17: Val Loss 676.65533
Epoch 18: Val Loss 676.58600
Epoch 19: Val Loss 676.51678
Epoch 20: Val Loss 676.44763
Epoch 21: Val Loss 676.37872
Epoch 22: Val Loss 676.30957
Epoch 23: Val Loss 676.24011
Epoch 24: Val Loss 676.17078
Epoch 25: Val Loss 676.10303
Epoch 26: Val Loss 676.03503
Epoch 27: Val Loss 675.96753
Epoch 28: Val Loss 675.90088
Epoch 29: Val Loss 675.83447
Epoch 30: Val Loss 675.76947
Epoch 31: Val Loss 675.70422
Epoch 32: Val Loss 675.63843
Epoch 33: Val Loss 675.57288
Epoch 34: Val Loss 675.50635
Epoch 35: Val Loss 675.43921
Epoch 36: Val Loss 675.37213
Epoch 37: Val Loss 675.30615
Epoch 38: Val Loss 675.24017
Epoch 39: Val Loss 675.17474
Epoch 40: Val Loss 675.10907
Epoch 41: Val Loss 675.04407
Epoch 42: Val Loss 674.97943
Epoch 43: Val Loss 674.91467
Epoch 44: Val Loss 674.85101
Epoch 45: Val Loss 674.78741
Epoch 46: Val Loss 674.72382
Epoch 47: Val Loss 674.66187
Epoch 48: Val Loss 674.60010
Epoch 49: Val Loss 674.53851
Epoch 50: Val Loss 674.47778
Epoch 51: Val Loss 674.41705
Epoch 52: Val Loss 674.35706
Epoch 53: Val Loss 674.29688
Epoch 54: Val Loss 674.23639
Epoch 55: Val Loss 674.17603
Epoch 56: Val Loss 674.11578
Epoch 57: Val Loss 674.05560
Epoch 58: Val Loss 673.99548
Epoch 59: Val Loss 673.93628
Epoch 60: Val Loss 673.87726
Epoch 61: Val Loss 673.81830
Epoch 62: Val Loss 673.75867
Epoch 63: Val Loss 673.69879
Epoch 64: Val Loss 673.63898
Epoch 65: Val Loss 673.58069
Epoch 66: Val Loss 673.52240
Epoch 67: Val Loss 673.46411
Epoch 68: Val Loss 673.40741
Epoch 69: Val Loss 673.35132
Epoch 70: Val Loss 673.29529
Epoch 71: Val Loss 673.23956
Epoch 72: Val Loss 673.18439
Epoch 73: Val Loss 673.13043
Epoch 74: Val Loss 673.07581
Epoch 75: Val Loss 673.02240
Epoch 76: Val Loss 672.96857
Epoch 77: Val Loss 672.91492
Epoch 78: Val Loss 672.86151
Epoch 79: Val Loss 672.80670
Epoch 80: Val Loss 672.75201
Epoch 81: Val Loss 672.69763
Epoch 82: Val Loss 672.64435
Epoch 83: Val Loss 672.59094
Epoch 84: Val Loss 672.53802
Epoch 85: Val Loss 672.48474
Epoch 86: Val Loss 672.43298
Epoch 87: Val Loss 672.38287
Epoch 88: Val Loss 672.33289
Epoch 89: Val Loss 672.28339
Epoch 90: Val Loss 672.23407
Epoch 91: Val Loss 672.18433
Epoch 92: Val Loss 672.13538
Epoch 93: Val Loss 672.08618
Epoch 94: Val Loss 672.03772
Epoch 95: Val Loss 671.98932
Epoch 96: Val Loss 671.94196
Epoch 97: Val Loss 671.89569
Epoch 98: Val Loss 671.84961
Epoch 99: Val Loss 671.80267
{'MSE - mean': 603.2748483913027, 'MSE - std': 51.88215697129564, 'R2 - mean': -6.234250099507449, 'R2 - std': 0.13829279265796238} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 536.64410
Epoch 1: Val Loss 536.56213
Epoch 2: Val Loss 536.48065
Epoch 3: Val Loss 536.39954
Epoch 4: Val Loss 536.31909
Epoch 5: Val Loss 536.23883
Epoch 6: Val Loss 536.15936
Epoch 7: Val Loss 536.08246
Epoch 8: Val Loss 536.00690
Epoch 9: Val Loss 535.93085
Epoch 10: Val Loss 535.85437
Epoch 11: Val Loss 535.77716
Epoch 12: Val Loss 535.70105
Epoch 13: Val Loss 535.62469
Epoch 14: Val Loss 535.54828
Epoch 15: Val Loss 535.47351
Epoch 16: Val Loss 535.40082
Epoch 17: Val Loss 535.32941
Epoch 18: Val Loss 535.25940
Epoch 19: Val Loss 535.18787
Epoch 20: Val Loss 535.11749
Epoch 21: Val Loss 535.04755
Epoch 22: Val Loss 534.97870
Epoch 23: Val Loss 534.90887
Epoch 24: Val Loss 534.83936
Epoch 25: Val Loss 534.76807
Epoch 26: Val Loss 534.69794
Epoch 27: Val Loss 534.62915
Epoch 28: Val Loss 534.56183
Epoch 29: Val Loss 534.49609
Epoch 30: Val Loss 534.42938
Epoch 31: Val Loss 534.36346
Epoch 32: Val Loss 534.29755
Epoch 33: Val Loss 534.23169
Epoch 34: Val Loss 534.16608
Epoch 35: Val Loss 534.09967
Epoch 36: Val Loss 534.03412
Epoch 37: Val Loss 533.96924
Epoch 38: Val Loss 533.90582
Epoch 39: Val Loss 533.84326
Epoch 40: Val Loss 533.78101
Epoch 41: Val Loss 533.71912
Epoch 42: Val Loss 533.65613
Epoch 43: Val Loss 533.59375
Epoch 44: Val Loss 533.53119
Epoch 45: Val Loss 533.46893
Epoch 46: Val Loss 533.40656
Epoch 47: Val Loss 533.34546
Epoch 48: Val Loss 533.28473
Epoch 49: Val Loss 533.22491
Epoch 50: Val Loss 533.16541
Epoch 51: Val Loss 533.10687
Epoch 52: Val Loss 533.04907
Epoch 53: Val Loss 532.99133
Epoch 54: Val Loss 532.93414
Epoch 55: Val Loss 532.87677
Epoch 56: Val Loss 532.81885
Epoch 57: Val Loss 532.76135
Epoch 58: Val Loss 532.70435
Epoch 59: Val Loss 532.64703
Epoch 60: Val Loss 532.59021
Epoch 61: Val Loss 532.53369
Epoch 62: Val Loss 532.47803
Epoch 63: Val Loss 532.42328
Epoch 64: Val Loss 532.36829
Epoch 65: Val Loss 532.31378
Epoch 66: Val Loss 532.25934
Epoch 67: Val Loss 532.20538
Epoch 68: Val Loss 532.15167
Epoch 69: Val Loss 532.09845
Epoch 70: Val Loss 532.04358
Epoch 71: Val Loss 531.98914
Epoch 72: Val Loss 531.93372
Epoch 73: Val Loss 531.87860
Epoch 74: Val Loss 531.82465
Epoch 75: Val Loss 531.77148
Epoch 76: Val Loss 531.71747
Epoch 77: Val Loss 531.66370
Epoch 78: Val Loss 531.61029
Epoch 79: Val Loss 531.55756
Epoch 80: Val Loss 531.50446
Epoch 81: Val Loss 531.45087
Epoch 82: Val Loss 531.39832
Epoch 83: Val Loss 531.34662
Epoch 84: Val Loss 531.29456
Epoch 85: Val Loss 531.24249
Epoch 86: Val Loss 531.19092
Epoch 87: Val Loss 531.13947
Epoch 88: Val Loss 531.08765
Epoch 89: Val Loss 531.03705
Epoch 90: Val Loss 530.98700
Epoch 91: Val Loss 530.93793
Epoch 92: Val Loss 530.88831
Epoch 93: Val Loss 530.83820
Epoch 94: Val Loss 530.78870
Epoch 95: Val Loss 530.73950
Epoch 96: Val Loss 530.68951
Epoch 97: Val Loss 530.64001
Epoch 98: Val Loss 530.58954
Epoch 99: Val Loss 530.53833
{'MSE - mean': 585.0907176539591, 'MSE - std': 54.87081638734778, 'R2 - mean': -6.183471616215925, 'R2 - std': 0.1485901594050432} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 553.60052
Epoch 1: Val Loss 553.57660
Epoch 2: Val Loss 553.55280
Epoch 3: Val Loss 553.52911
Epoch 4: Val Loss 553.50580
Epoch 5: Val Loss 553.48279
Epoch 6: Val Loss 553.46008
Epoch 7: Val Loss 553.43738
Epoch 8: Val Loss 553.41449
Epoch 9: Val Loss 553.39172
Epoch 10: Val Loss 553.36945
Epoch 11: Val Loss 553.34760
Epoch 12: Val Loss 553.32581
Epoch 13: Val Loss 553.30396
Epoch 14: Val Loss 553.28223
Epoch 15: Val Loss 553.26074
Epoch 16: Val Loss 553.23926
Epoch 17: Val Loss 553.21802
Epoch 18: Val Loss 553.19678
Epoch 19: Val Loss 553.17560
Epoch 20: Val Loss 553.15430
Epoch 21: Val Loss 553.13293
Epoch 22: Val Loss 553.11169
Epoch 23: Val Loss 553.09052
Epoch 24: Val Loss 553.06952
Epoch 25: Val Loss 553.04883
Epoch 26: Val Loss 553.02814
Epoch 27: Val Loss 553.00757
Epoch 28: Val Loss 552.98706
Epoch 29: Val Loss 552.96667
Epoch 30: Val Loss 552.94629
Epoch 31: Val Loss 552.92590
Epoch 32: Val Loss 552.90552
Epoch 33: Val Loss 552.88507
Epoch 34: Val Loss 552.86469
Epoch 35: Val Loss 552.84442
Epoch 36: Val Loss 552.82434
Epoch 37: Val Loss 552.80420
Epoch 38: Val Loss 552.78436
Epoch 39: Val Loss 552.76459
Epoch 40: Val Loss 552.74493
Epoch 41: Val Loss 552.72516
Epoch 42: Val Loss 552.70575
Epoch 43: Val Loss 552.68634
Epoch 44: Val Loss 552.66699
Epoch 45: Val Loss 552.64764
Epoch 46: Val Loss 552.62830
Epoch 47: Val Loss 552.60889
Epoch 48: Val Loss 552.58942
Epoch 49: Val Loss 552.57013
Epoch 50: Val Loss 552.55096
Epoch 51: Val Loss 552.53180
Epoch 52: Val Loss 552.51251
Epoch 53: Val Loss 552.49329
Epoch 54: Val Loss 552.47418
Epoch 55: Val Loss 552.45520
Epoch 56: Val Loss 552.43616
Epoch 57: Val Loss 552.41699
Epoch 58: Val Loss 552.39783
Epoch 59: Val Loss 552.37872
Epoch 60: Val Loss 552.35962
Epoch 61: Val Loss 552.34082
Epoch 62: Val Loss 552.32202
Epoch 63: Val Loss 552.30334
Epoch 64: Val Loss 552.28467
Epoch 65: Val Loss 552.26630
Epoch 66: Val Loss 552.24780
Epoch 67: Val Loss 552.22949
Epoch 68: Val Loss 552.21100
Epoch 69: Val Loss 552.19263
Epoch 70: Val Loss 552.17419
Epoch 71: Val Loss 552.15607
Epoch 72: Val Loss 552.13794
Epoch 73: Val Loss 552.11993
Epoch 74: Val Loss 552.10223
Epoch 75: Val Loss 552.08447
Epoch 76: Val Loss 552.06683
Epoch 77: Val Loss 552.04913
Epoch 78: Val Loss 552.03119
Epoch 79: Val Loss 552.01337
Epoch 80: Val Loss 551.99561
Epoch 81: Val Loss 551.97797
Epoch 82: Val Loss 551.96014
Epoch 83: Val Loss 551.94238
Epoch 84: Val Loss 551.92462
Epoch 85: Val Loss 551.90704
Epoch 86: Val Loss 551.88940
Epoch 87: Val Loss 551.87189
Epoch 88: Val Loss 551.85443
Epoch 89: Val Loss 551.83704
Epoch 90: Val Loss 551.81958
Epoch 91: Val Loss 551.80176
Epoch 92: Val Loss 551.78400
Epoch 93: Val Loss 551.76624
Epoch 94: Val Loss 551.74866
Epoch 95: Val Loss 551.73102
Epoch 96: Val Loss 551.71362
Epoch 97: Val Loss 551.69623
Epoch 98: Val Loss 551.67896
Epoch 99: Val Loss 551.66156
{'MSE - mean': 578.4048901845712, 'MSE - std': 50.86694750634163, 'R2 - mean': -5.955548195277758, 'R2 - std': 0.4748258329390277} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 578.4048901845712 and parameters: {'dim': 256, 'depth': 1, 'heads': 2, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 586.95477
Epoch 1: Val Loss 586.27557
Epoch 2: Val Loss 585.63263
Epoch 3: Val Loss 585.03149
Epoch 4: Val Loss 584.46362
Epoch 5: Val Loss 583.92126
Epoch 6: Val Loss 583.40869
Epoch 7: Val Loss 582.92694
Epoch 8: Val Loss 582.46539
Epoch 9: Val Loss 582.02612
Epoch 10: Val Loss 581.60370
Epoch 11: Val Loss 581.19397
Epoch 12: Val Loss 580.80194
Epoch 13: Val Loss 580.41467
Epoch 14: Val Loss 580.02484
Epoch 15: Val Loss 579.63818
Epoch 16: Val Loss 579.25812
Epoch 17: Val Loss 578.87878
Epoch 18: Val Loss 578.49542
Epoch 19: Val Loss 578.08612
Epoch 20: Val Loss 577.64270
Epoch 21: Val Loss 577.16046
Epoch 22: Val Loss 576.64081
Epoch 23: Val Loss 576.06934
Epoch 24: Val Loss 575.43646
Epoch 25: Val Loss 574.74298
Epoch 26: Val Loss 573.94678
Epoch 27: Val Loss 573.05219
Epoch 28: Val Loss 572.07178
Epoch 29: Val Loss 570.99463
Epoch 30: Val Loss 569.82153
Epoch 31: Val Loss 568.54382
Epoch 32: Val Loss 567.16650
Epoch 33: Val Loss 565.69440
Epoch 34: Val Loss 564.07996
Epoch 35: Val Loss 562.32556
Epoch 36: Val Loss 560.44061
Epoch 37: Val Loss 558.41754
Epoch 38: Val Loss 556.28241
Epoch 39: Val Loss 553.97131
Epoch 40: Val Loss 551.48993
Epoch 41: Val Loss 548.84039
Epoch 42: Val Loss 546.04285
Epoch 43: Val Loss 543.10382
Epoch 44: Val Loss 539.99481
Epoch 45: Val Loss 536.71350
Epoch 46: Val Loss 533.21088
Epoch 47: Val Loss 529.52393
Epoch 48: Val Loss 525.62830
Epoch 49: Val Loss 521.60968
Epoch 50: Val Loss 517.35278
Epoch 51: Val Loss 512.82361
Epoch 52: Val Loss 508.02200
Epoch 53: Val Loss 503.03439
Epoch 54: Val Loss 497.82907
Epoch 55: Val Loss 492.40314
Epoch 56: Val Loss 486.70651
Epoch 57: Val Loss 480.71884
Epoch 58: Val Loss 474.48685
Epoch 59: Val Loss 467.87830
Epoch 60: Val Loss 461.08035
Epoch 61: Val Loss 454.05658
Epoch 62: Val Loss 446.80563
Epoch 63: Val Loss 439.22925
Epoch 64: Val Loss 431.34622
Epoch 65: Val Loss 423.24344
Epoch 66: Val Loss 414.96506
Epoch 67: Val Loss 406.44931
Epoch 68: Val Loss 397.73141
Epoch 69: Val Loss 388.88544
Epoch 70: Val Loss 379.85089
Epoch 71: Val Loss 370.69571
Epoch 72: Val Loss 361.36942
Epoch 73: Val Loss 351.85916
Epoch 74: Val Loss 342.10590
Epoch 75: Val Loss 332.15405
Epoch 76: Val Loss 322.12552
Epoch 77: Val Loss 312.23224
Epoch 78: Val Loss 302.45514
Epoch 79: Val Loss 292.71771
Epoch 80: Val Loss 283.08658
Epoch 81: Val Loss 273.51486
Epoch 82: Val Loss 264.03479
Epoch 83: Val Loss 254.80264
Epoch 84: Val Loss 245.85725
Epoch 85: Val Loss 237.12616
Epoch 86: Val Loss 228.54262
Epoch 87: Val Loss 220.17740
Epoch 88: Val Loss 212.05516
Epoch 89: Val Loss 204.22235
Epoch 90: Val Loss 196.73068
Epoch 91: Val Loss 189.63805
Epoch 92: Val Loss 183.09851
Epoch 93: Val Loss 176.90163
Epoch 94: Val Loss 170.88557
Epoch 95: Val Loss 165.27184
Epoch 96: Val Loss 159.95993
Epoch 97: Val Loss 155.06645
Epoch 98: Val Loss 150.26331
Epoch 99: Val Loss 145.86115
{'MSE - mean': 145.8611306531466, 'MSE - std': 0.0, 'R2 - mean': -0.9534549138038075, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 648.46063
Epoch 1: Val Loss 648.12256
Epoch 2: Val Loss 647.81128
Epoch 3: Val Loss 647.52197
Epoch 4: Val Loss 647.24890
Epoch 5: Val Loss 646.99255
Epoch 6: Val Loss 646.73724
Epoch 7: Val Loss 646.46686
Epoch 8: Val Loss 646.15302
Epoch 9: Val Loss 645.80597
Epoch 10: Val Loss 645.43121
Epoch 11: Val Loss 645.01996
Epoch 12: Val Loss 644.57538
Epoch 13: Val Loss 644.05591
Epoch 14: Val Loss 643.44922
Epoch 15: Val Loss 642.67535
Epoch 16: Val Loss 641.79407
Epoch 17: Val Loss 640.88611
Epoch 18: Val Loss 639.94775
Epoch 19: Val Loss 638.98230
Epoch 20: Val Loss 637.96460
Epoch 21: Val Loss 636.87823
Epoch 22: Val Loss 635.72028
Epoch 23: Val Loss 634.51282
Epoch 24: Val Loss 633.26880
Epoch 25: Val Loss 631.97333
Epoch 26: Val Loss 630.60864
Epoch 27: Val Loss 629.15540
Epoch 28: Val Loss 627.61072
Epoch 29: Val Loss 625.96088
Epoch 30: Val Loss 624.20844
Epoch 31: Val Loss 622.33447
Epoch 32: Val Loss 620.33276
Epoch 33: Val Loss 618.25690
Epoch 34: Val Loss 616.11395
Epoch 35: Val Loss 613.91791
Epoch 36: Val Loss 611.60498
Epoch 37: Val Loss 609.22266
Epoch 38: Val Loss 606.73560
Epoch 39: Val Loss 604.14960
Epoch 40: Val Loss 601.45703
Epoch 41: Val Loss 598.65320
Epoch 42: Val Loss 595.69678
Epoch 43: Val Loss 592.60864
Epoch 44: Val Loss 589.39600
Epoch 45: Val Loss 586.05157
Epoch 46: Val Loss 582.62561
Epoch 47: Val Loss 579.05408
Epoch 48: Val Loss 575.28949
Epoch 49: Val Loss 571.37195
Epoch 50: Val Loss 567.32245
Epoch 51: Val Loss 563.16290
Epoch 52: Val Loss 558.88013
Epoch 53: Val Loss 554.43207
Epoch 54: Val Loss 549.87268
Epoch 55: Val Loss 545.15277
Epoch 56: Val Loss 540.32233
Epoch 57: Val Loss 535.32794
Epoch 58: Val Loss 530.06158
Epoch 59: Val Loss 524.57141
Epoch 60: Val Loss 518.88110
Epoch 61: Val Loss 513.06647
Epoch 62: Val Loss 507.14529
Epoch 63: Val Loss 501.13528
Epoch 64: Val Loss 494.95013
Epoch 65: Val Loss 488.54431
Epoch 66: Val Loss 481.89819
Epoch 67: Val Loss 475.03232
Epoch 68: Val Loss 468.03549
Epoch 69: Val Loss 460.98355
Epoch 70: Val Loss 453.69794
Epoch 71: Val Loss 446.23102
Epoch 72: Val Loss 438.67215
Epoch 73: Val Loss 431.00931
Epoch 74: Val Loss 423.17380
Epoch 75: Val Loss 415.24921
Epoch 76: Val Loss 407.19235
Epoch 77: Val Loss 398.98599
Epoch 78: Val Loss 390.49124
Epoch 79: Val Loss 381.94144
Epoch 80: Val Loss 373.29825
Epoch 81: Val Loss 364.54910
Epoch 82: Val Loss 355.64133
Epoch 83: Val Loss 346.64023
Epoch 84: Val Loss 337.72308
Epoch 85: Val Loss 328.81946
Epoch 86: Val Loss 319.90198
Epoch 87: Val Loss 310.85251
Epoch 88: Val Loss 301.73502
Epoch 89: Val Loss 292.72247
Epoch 90: Val Loss 283.66483
Epoch 91: Val Loss 274.49066
Epoch 92: Val Loss 265.46106
Epoch 93: Val Loss 256.58716
Epoch 94: Val Loss 248.03186
Epoch 95: Val Loss 239.64546
Epoch 96: Val Loss 231.26927
Epoch 97: Val Loss 223.06999
Epoch 98: Val Loss 215.10741
Epoch 99: Val Loss 207.19980
{'MSE - mean': 176.53046972515364, 'MSE - std': 30.669339072007034, 'R2 - mean': -1.2630416863339184, 'R2 - std': 0.3095867725301109} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 716.42145
Epoch 1: Val Loss 715.88379
Epoch 2: Val Loss 715.36432
Epoch 3: Val Loss 714.85950
Epoch 4: Val Loss 714.37360
Epoch 5: Val Loss 713.90112
Epoch 6: Val Loss 713.44165
Epoch 7: Val Loss 712.99194
Epoch 8: Val Loss 712.55389
Epoch 9: Val Loss 712.12329
Epoch 10: Val Loss 711.70453
Epoch 11: Val Loss 711.29578
Epoch 12: Val Loss 710.89618
Epoch 13: Val Loss 710.50262
Epoch 14: Val Loss 710.11414
Epoch 15: Val Loss 709.73413
Epoch 16: Val Loss 709.36371
Epoch 17: Val Loss 708.99921
Epoch 18: Val Loss 708.64221
Epoch 19: Val Loss 708.29309
Epoch 20: Val Loss 707.94971
Epoch 21: Val Loss 707.61340
Epoch 22: Val Loss 707.27979
Epoch 23: Val Loss 706.95123
Epoch 24: Val Loss 706.63135
Epoch 25: Val Loss 706.31927
Epoch 26: Val Loss 706.01862
Epoch 27: Val Loss 705.72369
Epoch 28: Val Loss 705.43549
Epoch 29: Val Loss 705.15399
Epoch 30: Val Loss 704.87994
Epoch 31: Val Loss 704.60846
Epoch 32: Val Loss 704.34296
Epoch 33: Val Loss 704.07904
Epoch 34: Val Loss 703.81781
Epoch 35: Val Loss 703.55695
Epoch 36: Val Loss 703.29578
Epoch 37: Val Loss 703.03064
Epoch 38: Val Loss 702.75720
Epoch 39: Val Loss 702.47583
Epoch 40: Val Loss 702.18994
Epoch 41: Val Loss 701.89636
Epoch 42: Val Loss 701.59448
Epoch 43: Val Loss 701.27759
Epoch 44: Val Loss 700.93671
Epoch 45: Val Loss 700.57501
Epoch 46: Val Loss 700.18994
Epoch 47: Val Loss 699.77777
Epoch 48: Val Loss 699.32635
Epoch 49: Val Loss 698.83276
Epoch 50: Val Loss 698.29541
Epoch 51: Val Loss 697.70856
Epoch 52: Val Loss 697.06525
Epoch 53: Val Loss 696.36804
Epoch 54: Val Loss 695.60217
Epoch 55: Val Loss 694.75757
Epoch 56: Val Loss 693.83112
Epoch 57: Val Loss 692.79956
Epoch 58: Val Loss 691.65436
Epoch 59: Val Loss 690.39014
Epoch 60: Val Loss 689.00244
Epoch 61: Val Loss 687.51447
Epoch 62: Val Loss 685.88672
Epoch 63: Val Loss 684.09174
Epoch 64: Val Loss 682.14789
Epoch 65: Val Loss 680.05243
Epoch 66: Val Loss 677.78107
Epoch 67: Val Loss 675.30988
Epoch 68: Val Loss 672.62012
Epoch 69: Val Loss 669.69489
Epoch 70: Val Loss 666.53571
Epoch 71: Val Loss 663.15344
Epoch 72: Val Loss 659.52264
Epoch 73: Val Loss 655.67810
Epoch 74: Val Loss 651.51471
Epoch 75: Val Loss 646.98248
Epoch 76: Val Loss 642.06116
Epoch 77: Val Loss 636.68518
Epoch 78: Val Loss 630.79230
Epoch 79: Val Loss 624.36072
Epoch 80: Val Loss 617.41534
Epoch 81: Val Loss 609.92969
Epoch 82: Val Loss 601.86096
Epoch 83: Val Loss 593.32214
Epoch 84: Val Loss 584.35236
Epoch 85: Val Loss 574.84198
Epoch 86: Val Loss 564.70868
Epoch 87: Val Loss 554.00323
Epoch 88: Val Loss 542.97174
Epoch 89: Val Loss 531.56824
Epoch 90: Val Loss 519.65808
Epoch 91: Val Loss 507.18552
Epoch 92: Val Loss 494.26013
Epoch 93: Val Loss 480.77029
Epoch 94: Val Loss 467.08014
Epoch 95: Val Loss 453.22601
Epoch 96: Val Loss 439.10504
Epoch 97: Val Loss 424.71014
Epoch 98: Val Loss 410.27432
Epoch 99: Val Loss 395.71185
{'MSE - mean': 249.59093372610187, 'MSE - std': 106.31432189256938, 'R2 - mean': -1.8908465202950404, 'R2 - std': 0.9231326009338948} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 529.36182
Epoch 1: Val Loss 529.14587
Epoch 2: Val Loss 528.91876
Epoch 3: Val Loss 528.67731
Epoch 4: Val Loss 528.41852
Epoch 5: Val Loss 528.13989
Epoch 6: Val Loss 527.84363
Epoch 7: Val Loss 527.52087
Epoch 8: Val Loss 527.16864
Epoch 9: Val Loss 526.78278
Epoch 10: Val Loss 526.36194
Epoch 11: Val Loss 525.89166
Epoch 12: Val Loss 525.37445
Epoch 13: Val Loss 524.80438
Epoch 14: Val Loss 524.17651
Epoch 15: Val Loss 523.46381
Epoch 16: Val Loss 522.67242
Epoch 17: Val Loss 521.80334
Epoch 18: Val Loss 520.85272
Epoch 19: Val Loss 519.78595
Epoch 20: Val Loss 518.61877
Epoch 21: Val Loss 517.34460
Epoch 22: Val Loss 515.96332
Epoch 23: Val Loss 514.47931
Epoch 24: Val Loss 512.88409
Epoch 25: Val Loss 511.18201
Epoch 26: Val Loss 509.31387
Epoch 27: Val Loss 507.29593
Epoch 28: Val Loss 505.12354
Epoch 29: Val Loss 502.80423
Epoch 30: Val Loss 500.30719
Epoch 31: Val Loss 497.57819
Epoch 32: Val Loss 494.62561
Epoch 33: Val Loss 491.42807
Epoch 34: Val Loss 487.78549
Epoch 35: Val Loss 483.62970
Epoch 36: Val Loss 479.11624
Epoch 37: Val Loss 474.29327
Epoch 38: Val Loss 469.16537
Epoch 39: Val Loss 463.72183
Epoch 40: Val Loss 457.99988
Epoch 41: Val Loss 452.02213
Epoch 42: Val Loss 445.73715
Epoch 43: Val Loss 439.14233
Epoch 44: Val Loss 432.24854
Epoch 45: Val Loss 425.03790
Epoch 46: Val Loss 417.47949
Epoch 47: Val Loss 409.58902
Epoch 48: Val Loss 401.47278
Epoch 49: Val Loss 393.05878
Epoch 50: Val Loss 384.29733
Epoch 51: Val Loss 375.27551
Epoch 52: Val Loss 365.97687
Epoch 53: Val Loss 356.33221
Epoch 54: Val Loss 346.53564
Epoch 55: Val Loss 336.53391
Epoch 56: Val Loss 326.31604
Epoch 57: Val Loss 315.99274
Epoch 58: Val Loss 305.48956
Epoch 59: Val Loss 294.95877
Epoch 60: Val Loss 284.30139
Epoch 61: Val Loss 273.50546
Epoch 62: Val Loss 262.69797
Epoch 63: Val Loss 252.04755
Epoch 64: Val Loss 241.30362
Epoch 65: Val Loss 230.70609
Epoch 66: Val Loss 220.40694
Epoch 67: Val Loss 210.23570
Epoch 68: Val Loss 200.12036
Epoch 69: Val Loss 190.35474
Epoch 70: Val Loss 180.84962
Epoch 71: Val Loss 171.57501
Epoch 72: Val Loss 162.70801
Epoch 73: Val Loss 154.30580
Epoch 74: Val Loss 146.47112
Epoch 75: Val Loss 139.06805
Epoch 76: Val Loss 132.10075
Epoch 77: Val Loss 125.69110
Epoch 78: Val Loss 119.89262
Epoch 79: Val Loss 114.66660
Epoch 80: Val Loss 110.14666
Epoch 81: Val Loss 106.14493
Epoch 82: Val Loss 102.65443
Epoch 83: Val Loss 99.56716
Epoch 84: Val Loss 96.88112
Epoch 85: Val Loss 94.52141
Epoch 86: Val Loss 92.43448
Epoch 87: Val Loss 90.52207
Epoch 88: Val Loss 88.85716
Epoch 89: Val Loss 87.36727
Epoch 90: Val Loss 86.00159
Epoch 91: Val Loss 84.73692
Epoch 92: Val Loss 83.57082
Epoch 93: Val Loss 82.50596
Epoch 94: Val Loss 81.47868
Epoch 95: Val Loss 80.55588
Epoch 96: Val Loss 79.66499
Epoch 97: Val Loss 78.77170
Epoch 98: Val Loss 77.82355
Epoch 99: Val Loss 76.85395
{'MSE - mean': 206.40668800742432, 'MSE - std': 118.62414810855756, 'R2 - mean': -1.4227680465099057, 'R2 - std': 1.1386056042560548} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 571.73578
Epoch 1: Val Loss 570.50995
Epoch 2: Val Loss 569.26520
Epoch 3: Val Loss 567.95715
Epoch 4: Val Loss 566.56854
Epoch 5: Val Loss 565.13098
Epoch 6: Val Loss 563.62592
Epoch 7: Val Loss 562.02435
Epoch 8: Val Loss 560.35223
Epoch 9: Val Loss 558.60187
Epoch 10: Val Loss 556.75726
Epoch 11: Val Loss 554.84033
Epoch 12: Val Loss 552.84619
Epoch 13: Val Loss 550.74780
Epoch 14: Val Loss 548.57330
Epoch 15: Val Loss 546.31946
Epoch 16: Val Loss 543.97723
Epoch 17: Val Loss 541.54437
Epoch 18: Val Loss 539.04480
Epoch 19: Val Loss 536.45465
Epoch 20: Val Loss 533.76849
Epoch 21: Val Loss 530.99768
Epoch 22: Val Loss 528.11713
Epoch 23: Val Loss 525.15680
Epoch 24: Val Loss 522.11163
Epoch 25: Val Loss 518.93011
Epoch 26: Val Loss 515.67816
Epoch 27: Val Loss 512.36523
Epoch 28: Val Loss 508.96643
Epoch 29: Val Loss 505.41217
Epoch 30: Val Loss 501.70255
Epoch 31: Val Loss 497.90308
Epoch 32: Val Loss 493.93842
Epoch 33: Val Loss 489.84915
Epoch 34: Val Loss 485.62909
Epoch 35: Val Loss 481.29492
Epoch 36: Val Loss 476.83087
Epoch 37: Val Loss 472.23608
Epoch 38: Val Loss 467.56461
Epoch 39: Val Loss 462.80014
Epoch 40: Val Loss 457.88922
Epoch 41: Val Loss 452.91669
Epoch 42: Val Loss 447.64426
Epoch 43: Val Loss 442.26285
Epoch 44: Val Loss 436.71347
Epoch 45: Val Loss 430.99854
Epoch 46: Val Loss 425.18713
Epoch 47: Val Loss 419.30862
Epoch 48: Val Loss 413.29459
Epoch 49: Val Loss 407.26755
Epoch 50: Val Loss 401.16769
Epoch 51: Val Loss 394.99426
Epoch 52: Val Loss 388.62289
Epoch 53: Val Loss 382.11096
Epoch 54: Val Loss 375.44864
Epoch 55: Val Loss 368.73944
Epoch 56: Val Loss 362.00470
Epoch 57: Val Loss 355.20187
Epoch 58: Val Loss 348.31601
Epoch 59: Val Loss 341.28983
Epoch 60: Val Loss 334.27444
Epoch 61: Val Loss 327.17722
Epoch 62: Val Loss 320.12955
Epoch 63: Val Loss 313.03766
Epoch 64: Val Loss 306.01056
Epoch 65: Val Loss 299.03207
Epoch 66: Val Loss 292.07666
Epoch 67: Val Loss 285.10748
Epoch 68: Val Loss 278.17569
Epoch 69: Val Loss 271.39102
Epoch 70: Val Loss 264.67194
Epoch 71: Val Loss 257.96930
Epoch 72: Val Loss 251.56625
Epoch 73: Val Loss 245.18465
Epoch 74: Val Loss 238.98613
Epoch 75: Val Loss 232.85143
Epoch 76: Val Loss 227.04739
Epoch 77: Val Loss 221.41248
Epoch 78: Val Loss 215.96245
Epoch 79: Val Loss 210.76964
Epoch 80: Val Loss 205.65462
Epoch 81: Val Loss 200.58409
Epoch 82: Val Loss 195.55035
Epoch 83: Val Loss 190.78067
Epoch 84: Val Loss 186.21616
Epoch 85: Val Loss 181.70976
Epoch 86: Val Loss 177.48955
Epoch 87: Val Loss 173.49742
Epoch 88: Val Loss 169.77519
Epoch 89: Val Loss 166.29155
Epoch 90: Val Loss 163.03572
Epoch 91: Val Loss 159.99512
Epoch 92: Val Loss 157.21426
Epoch 93: Val Loss 154.65379
Epoch 94: Val Loss 152.28671
Epoch 95: Val Loss 150.20853
Epoch 96: Val Loss 148.22348
Epoch 97: Val Loss 146.48958
Epoch 98: Val Loss 144.92816
Epoch 99: Val Loss 143.44193
{'MSE - mean': 193.8137382150706, 'MSE - std': 109.04898142337584, 'R2 - mean': -1.2525166258651477, 'R2 - std': 1.0738157954513754} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 193.8137382150706 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 531.76886
Epoch 1: Val Loss 531.11719
Epoch 2: Val Loss 530.47461
Epoch 3: Val Loss 529.84088
Epoch 4: Val Loss 529.21167
Epoch 5: Val Loss 528.58362
Epoch 6: Val Loss 527.96869
Epoch 7: Val Loss 527.35925
Epoch 8: Val Loss 526.74426
Epoch 9: Val Loss 526.12811
Epoch 10: Val Loss 525.50928
Epoch 11: Val Loss 524.88818
Epoch 12: Val Loss 524.25964
Epoch 13: Val Loss 523.63599
Epoch 14: Val Loss 523.05432
Epoch 15: Val Loss 522.56281
Epoch 16: Val Loss 522.13123
Epoch 17: Val Loss 521.73090
Epoch 18: Val Loss 521.34314
Epoch 19: Val Loss 520.96741
Epoch 20: Val Loss 520.59747
Epoch 21: Val Loss 520.24146
Epoch 22: Val Loss 519.90588
Epoch 23: Val Loss 519.59784
Epoch 24: Val Loss 519.30188
Epoch 25: Val Loss 519.01550
Epoch 26: Val Loss 518.73938
Epoch 27: Val Loss 518.47925
Epoch 28: Val Loss 518.22467
Epoch 29: Val Loss 517.97443
Epoch 30: Val Loss 517.72504
Epoch 31: Val Loss 517.47607
Epoch 32: Val Loss 517.23303
Epoch 33: Val Loss 516.99561
Epoch 34: Val Loss 516.75763
Epoch 35: Val Loss 516.52545
Epoch 36: Val Loss 516.29803
Epoch 37: Val Loss 516.07678
Epoch 38: Val Loss 515.86017
Epoch 39: Val Loss 515.64313
Epoch 40: Val Loss 515.42725
Epoch 41: Val Loss 515.21277
Epoch 42: Val Loss 514.99750
Epoch 43: Val Loss 514.78027
Epoch 44: Val Loss 514.56464
Epoch 45: Val Loss 514.35016
Epoch 46: Val Loss 514.13757
Epoch 47: Val Loss 513.92627
Epoch 48: Val Loss 513.71893
Epoch 49: Val Loss 513.51331
Epoch 50: Val Loss 513.31042
Epoch 51: Val Loss 513.10999
Epoch 52: Val Loss 512.91003
Epoch 53: Val Loss 512.71063
Epoch 54: Val Loss 512.51190
Epoch 55: Val Loss 512.31567
Epoch 56: Val Loss 512.12335
Epoch 57: Val Loss 511.93146
Epoch 58: Val Loss 511.73938
Epoch 59: Val Loss 511.54730
Epoch 60: Val Loss 511.35434
Epoch 61: Val Loss 511.16003
Epoch 62: Val Loss 510.96509
Epoch 63: Val Loss 510.77200
Epoch 64: Val Loss 510.58072
Epoch 65: Val Loss 510.38904
Epoch 66: Val Loss 510.19467
Epoch 67: Val Loss 509.99957
Epoch 68: Val Loss 509.80585
Epoch 69: Val Loss 509.61243
Epoch 70: Val Loss 509.41989
Epoch 71: Val Loss 509.22684
Epoch 72: Val Loss 509.03296
Epoch 73: Val Loss 508.83853
Epoch 74: Val Loss 508.64041
Epoch 75: Val Loss 508.44330
Epoch 76: Val Loss 508.24725
Epoch 77: Val Loss 508.05096
Epoch 78: Val Loss 507.85794
Epoch 79: Val Loss 507.66492
Epoch 80: Val Loss 507.47345
Epoch 81: Val Loss 507.28357
Epoch 82: Val Loss 507.09409
Epoch 83: Val Loss 506.91113
Epoch 84: Val Loss 506.73172
Epoch 85: Val Loss 506.55075
Epoch 86: Val Loss 506.37015
Epoch 87: Val Loss 506.19040
Epoch 88: Val Loss 506.01004
Epoch 89: Val Loss 505.83017
Epoch 90: Val Loss 505.65176
Epoch 91: Val Loss 505.47363
Epoch 92: Val Loss 505.29669
Epoch 93: Val Loss 505.12039
Epoch 94: Val Loss 504.94537
Epoch 95: Val Loss 504.77023
Epoch 96: Val Loss 504.59558
Epoch 97: Val Loss 504.42197
Epoch 98: Val Loss 504.24973
Epoch 99: Val Loss 504.07867
{'MSE - mean': 504.0786964302437, 'MSE - std': 0.0, 'R2 - mean': -5.750907538397276, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 614.89655
Epoch 1: Val Loss 614.07385
Epoch 2: Val Loss 613.22754
Epoch 3: Val Loss 612.34119
Epoch 4: Val Loss 611.42316
Epoch 5: Val Loss 610.46436
Epoch 6: Val Loss 609.45197
Epoch 7: Val Loss 608.39246
Epoch 8: Val Loss 607.28876
Epoch 9: Val Loss 606.10565
Epoch 10: Val Loss 604.85541
Epoch 11: Val Loss 603.54724
Epoch 12: Val Loss 602.16577
Epoch 13: Val Loss 600.72699
Epoch 14: Val Loss 599.24969
Epoch 15: Val Loss 597.71234
Epoch 16: Val Loss 596.12653
Epoch 17: Val Loss 594.45258
Epoch 18: Val Loss 592.72235
Epoch 19: Val Loss 590.92297
Epoch 20: Val Loss 589.03625
Epoch 21: Val Loss 587.04639
Epoch 22: Val Loss 585.00922
Epoch 23: Val Loss 582.88092
Epoch 24: Val Loss 580.67645
Epoch 25: Val Loss 578.33661
Epoch 26: Val Loss 575.89935
Epoch 27: Val Loss 573.32159
Epoch 28: Val Loss 570.62653
Epoch 29: Val Loss 567.81720
Epoch 30: Val Loss 564.83606
Epoch 31: Val Loss 561.68646
Epoch 32: Val Loss 558.40631
Epoch 33: Val Loss 554.94427
Epoch 34: Val Loss 551.37079
Epoch 35: Val Loss 547.67517
Epoch 36: Val Loss 543.80914
Epoch 37: Val Loss 539.81134
Epoch 38: Val Loss 535.68121
Epoch 39: Val Loss 531.33795
Epoch 40: Val Loss 526.77936
Epoch 41: Val Loss 521.97491
Epoch 42: Val Loss 517.04309
Epoch 43: Val Loss 511.94562
Epoch 44: Val Loss 506.66019
Epoch 45: Val Loss 501.20001
Epoch 46: Val Loss 495.49487
Epoch 47: Val Loss 489.47369
Epoch 48: Val Loss 483.27548
Epoch 49: Val Loss 476.90826
Epoch 50: Val Loss 470.47409
Epoch 51: Val Loss 463.86142
Epoch 52: Val Loss 457.22968
Epoch 53: Val Loss 450.47461
Epoch 54: Val Loss 443.46356
Epoch 55: Val Loss 436.30197
Epoch 56: Val Loss 428.99820
Epoch 57: Val Loss 421.53311
Epoch 58: Val Loss 413.87027
Epoch 59: Val Loss 406.01132
Epoch 60: Val Loss 398.05740
Epoch 61: Val Loss 390.04034
Epoch 62: Val Loss 381.90567
Epoch 63: Val Loss 373.61908
Epoch 64: Val Loss 365.27258
Epoch 65: Val Loss 356.85495
Epoch 66: Val Loss 348.37164
Epoch 67: Val Loss 339.88495
Epoch 68: Val Loss 331.33511
Epoch 69: Val Loss 322.74539
Epoch 70: Val Loss 314.25751
Epoch 71: Val Loss 305.75986
Epoch 72: Val Loss 297.28519
Epoch 73: Val Loss 288.75729
Epoch 74: Val Loss 280.11194
Epoch 75: Val Loss 271.50021
Epoch 76: Val Loss 262.86234
Epoch 77: Val Loss 254.39622
Epoch 78: Val Loss 246.25893
Epoch 79: Val Loss 238.36649
Epoch 80: Val Loss 230.53902
Epoch 81: Val Loss 222.87923
Epoch 82: Val Loss 215.51610
Epoch 83: Val Loss 208.31828
Epoch 84: Val Loss 201.34901
Epoch 85: Val Loss 194.65395
Epoch 86: Val Loss 188.22838
Epoch 87: Val Loss 182.11133
Epoch 88: Val Loss 176.15756
Epoch 89: Val Loss 170.37221
Epoch 90: Val Loss 164.76573
Epoch 91: Val Loss 159.40436
Epoch 92: Val Loss 154.35764
Epoch 93: Val Loss 149.69815
Epoch 94: Val Loss 145.29915
Epoch 95: Val Loss 141.16400
Epoch 96: Val Loss 137.19638
Epoch 97: Val Loss 133.52972
Epoch 98: Val Loss 130.05949
Epoch 99: Val Loss 126.80017
{'MSE - mean': 315.4394317888206, 'MSE - std': 188.63926464142313, 'R2 - mean': -3.1626401099723767, 'R2 - std': 2.588267428424899} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 707.33832
Epoch 1: Val Loss 706.74194
Epoch 2: Val Loss 706.19244
Epoch 3: Val Loss 705.67480
Epoch 4: Val Loss 705.16931
Epoch 5: Val Loss 704.70074
Epoch 6: Val Loss 704.26184
Epoch 7: Val Loss 703.84174
Epoch 8: Val Loss 703.44971
Epoch 9: Val Loss 703.06458
Epoch 10: Val Loss 702.70319
Epoch 11: Val Loss 702.38287
Epoch 12: Val Loss 702.08911
Epoch 13: Val Loss 701.81372
Epoch 14: Val Loss 701.56281
Epoch 15: Val Loss 701.32550
Epoch 16: Val Loss 701.10052
Epoch 17: Val Loss 700.88367
Epoch 18: Val Loss 700.67078
Epoch 19: Val Loss 700.46149
Epoch 20: Val Loss 700.25592
Epoch 21: Val Loss 700.05585
Epoch 22: Val Loss 699.85645
Epoch 23: Val Loss 699.65680
Epoch 24: Val Loss 699.46014
Epoch 25: Val Loss 699.26343
Epoch 26: Val Loss 699.06622
Epoch 27: Val Loss 698.86945
Epoch 28: Val Loss 698.67218
Epoch 29: Val Loss 698.47339
Epoch 30: Val Loss 698.27496
Epoch 31: Val Loss 698.07758
Epoch 32: Val Loss 697.88074
Epoch 33: Val Loss 697.68359
Epoch 34: Val Loss 697.48584
Epoch 35: Val Loss 697.28735
Epoch 36: Val Loss 697.08978
Epoch 37: Val Loss 696.89337
Epoch 38: Val Loss 696.69800
Epoch 39: Val Loss 696.50299
Epoch 40: Val Loss 696.30847
Epoch 41: Val Loss 696.11322
Epoch 42: Val Loss 695.91821
Epoch 43: Val Loss 695.72333
Epoch 44: Val Loss 695.52893
Epoch 45: Val Loss 695.33521
Epoch 46: Val Loss 695.14154
Epoch 47: Val Loss 694.94684
Epoch 48: Val Loss 694.75140
Epoch 49: Val Loss 694.55615
Epoch 50: Val Loss 694.36060
Epoch 51: Val Loss 694.16675
Epoch 52: Val Loss 693.97229
Epoch 53: Val Loss 693.77905
Epoch 54: Val Loss 693.58594
Epoch 55: Val Loss 693.39215
Epoch 56: Val Loss 693.19757
Epoch 57: Val Loss 693.00122
Epoch 58: Val Loss 692.80396
Epoch 59: Val Loss 692.60675
Epoch 60: Val Loss 692.41040
Epoch 61: Val Loss 692.21484
Epoch 62: Val Loss 692.01996
Epoch 63: Val Loss 691.82581
Epoch 64: Val Loss 691.63232
Epoch 65: Val Loss 691.43811
Epoch 66: Val Loss 691.24377
Epoch 67: Val Loss 691.04980
Epoch 68: Val Loss 690.85657
Epoch 69: Val Loss 690.66241
Epoch 70: Val Loss 690.47015
Epoch 71: Val Loss 690.27863
Epoch 72: Val Loss 690.08551
Epoch 73: Val Loss 689.89288
Epoch 74: Val Loss 689.70062
Epoch 75: Val Loss 689.50781
Epoch 76: Val Loss 689.31512
Epoch 77: Val Loss 689.12079
Epoch 78: Val Loss 688.92761
Epoch 79: Val Loss 688.73547
Epoch 80: Val Loss 688.54340
Epoch 81: Val Loss 688.35138
Epoch 82: Val Loss 688.15887
Epoch 83: Val Loss 687.96527
Epoch 84: Val Loss 687.77197
Epoch 85: Val Loss 687.57886
Epoch 86: Val Loss 687.38599
Epoch 87: Val Loss 687.19366
Epoch 88: Val Loss 687.00183
Epoch 89: Val Loss 686.80988
Epoch 90: Val Loss 686.61676
Epoch 91: Val Loss 686.42279
Epoch 92: Val Loss 686.22913
Epoch 93: Val Loss 686.03601
Epoch 94: Val Loss 685.84320
Epoch 95: Val Loss 685.64752
Epoch 96: Val Loss 685.45355
Epoch 97: Val Loss 685.26172
Epoch 98: Val Loss 685.07098
Epoch 99: Val Loss 684.87927
{'MSE - mean': 438.58601868699924, 'MSE - std': 232.49375297481126, 'R2 - mean': -4.167256190155179, 'R2 - std': 2.5464862964132644} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 517.74518
Epoch 1: Val Loss 516.71210
Epoch 2: Val Loss 515.62012
Epoch 3: Val Loss 514.51813
Epoch 4: Val Loss 513.44324
Epoch 5: Val Loss 512.38336
Epoch 6: Val Loss 511.32022
Epoch 7: Val Loss 510.25143
Epoch 8: Val Loss 509.19012
Epoch 9: Val Loss 508.11401
Epoch 10: Val Loss 507.01166
Epoch 11: Val Loss 505.89975
Epoch 12: Val Loss 504.78378
Epoch 13: Val Loss 503.63120
Epoch 14: Val Loss 502.44925
Epoch 15: Val Loss 501.20502
Epoch 16: Val Loss 499.91058
Epoch 17: Val Loss 498.56256
Epoch 18: Val Loss 497.17551
Epoch 19: Val Loss 495.76663
Epoch 20: Val Loss 494.29767
Epoch 21: Val Loss 492.77829
Epoch 22: Val Loss 491.18396
Epoch 23: Val Loss 489.50204
Epoch 24: Val Loss 487.76810
Epoch 25: Val Loss 485.96048
Epoch 26: Val Loss 484.06662
Epoch 27: Val Loss 482.06860
Epoch 28: Val Loss 479.95776
Epoch 29: Val Loss 477.77286
Epoch 30: Val Loss 475.45898
Epoch 31: Val Loss 472.99164
Epoch 32: Val Loss 470.41885
Epoch 33: Val Loss 467.72729
Epoch 34: Val Loss 464.95102
Epoch 35: Val Loss 462.01743
Epoch 36: Val Loss 458.95587
Epoch 37: Val Loss 455.70792
Epoch 38: Val Loss 452.33029
Epoch 39: Val Loss 448.74255
Epoch 40: Val Loss 445.01285
Epoch 41: Val Loss 441.12607
Epoch 42: Val Loss 437.01291
Epoch 43: Val Loss 432.77173
Epoch 44: Val Loss 428.37112
Epoch 45: Val Loss 423.79260
Epoch 46: Val Loss 419.06171
Epoch 47: Val Loss 414.14343
Epoch 48: Val Loss 409.08420
Epoch 49: Val Loss 403.90958
Epoch 50: Val Loss 398.52676
Epoch 51: Val Loss 393.05722
Epoch 52: Val Loss 387.34180
Epoch 53: Val Loss 381.48862
Epoch 54: Val Loss 375.47757
Epoch 55: Val Loss 369.36993
Epoch 56: Val Loss 363.17453
Epoch 57: Val Loss 356.80905
Epoch 58: Val Loss 350.29431
Epoch 59: Val Loss 343.47412
Epoch 60: Val Loss 336.62845
Epoch 61: Val Loss 329.76349
Epoch 62: Val Loss 322.77850
Epoch 63: Val Loss 315.63858
Epoch 64: Val Loss 308.30386
Epoch 65: Val Loss 300.99643
Epoch 66: Val Loss 293.54611
Epoch 67: Val Loss 285.89188
Epoch 68: Val Loss 278.33887
Epoch 69: Val Loss 270.73831
Epoch 70: Val Loss 263.12903
Epoch 71: Val Loss 255.48576
Epoch 72: Val Loss 247.70097
Epoch 73: Val Loss 239.95584
Epoch 74: Val Loss 232.14194
Epoch 75: Val Loss 224.32242
Epoch 76: Val Loss 216.65083
Epoch 77: Val Loss 209.24091
Epoch 78: Val Loss 202.13153
Epoch 79: Val Loss 195.10028
Epoch 80: Val Loss 188.25566
Epoch 81: Val Loss 181.59058
Epoch 82: Val Loss 175.00587
Epoch 83: Val Loss 168.67369
Epoch 84: Val Loss 162.60391
Epoch 85: Val Loss 156.70908
Epoch 86: Val Loss 151.05728
Epoch 87: Val Loss 145.67697
Epoch 88: Val Loss 140.52380
Epoch 89: Val Loss 135.67317
Epoch 90: Val Loss 131.16290
Epoch 91: Val Loss 127.03426
Epoch 92: Val Loss 123.25568
Epoch 93: Val Loss 119.72728
Epoch 94: Val Loss 116.43705
Epoch 95: Val Loss 113.39692
Epoch 96: Val Loss 110.61715
Epoch 97: Val Loss 108.00553
Epoch 98: Val Loss 105.56519
Epoch 99: Val Loss 103.40561
{'MSE - mean': 354.7909174726316, 'MSE - std': 248.20327524392738, 'R2 - mean': -3.218046491978846, 'R2 - std': 2.7507165424124356} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 596.59955
Epoch 1: Val Loss 595.71930
Epoch 2: Val Loss 594.79572
Epoch 3: Val Loss 593.83228
Epoch 4: Val Loss 592.82587
Epoch 5: Val Loss 591.75787
Epoch 6: Val Loss 590.63110
Epoch 7: Val Loss 589.45441
Epoch 8: Val Loss 588.20392
Epoch 9: Val Loss 586.87390
Epoch 10: Val Loss 585.44824
Epoch 11: Val Loss 583.95886
Epoch 12: Val Loss 582.39783
Epoch 13: Val Loss 580.76978
Epoch 14: Val Loss 579.06427
Epoch 15: Val Loss 577.23621
Epoch 16: Val Loss 575.29333
Epoch 17: Val Loss 573.26642
Epoch 18: Val Loss 571.13019
Epoch 19: Val Loss 568.85284
Epoch 20: Val Loss 566.44501
Epoch 21: Val Loss 563.87915
Epoch 22: Val Loss 561.15674
Epoch 23: Val Loss 558.33417
Epoch 24: Val Loss 555.32556
Epoch 25: Val Loss 552.13556
Epoch 26: Val Loss 548.76831
Epoch 27: Val Loss 545.18933
Epoch 28: Val Loss 541.48779
Epoch 29: Val Loss 537.65454
Epoch 30: Val Loss 533.63556
Epoch 31: Val Loss 529.37524
Epoch 32: Val Loss 524.89142
Epoch 33: Val Loss 520.14062
Epoch 34: Val Loss 515.03149
Epoch 35: Val Loss 509.55240
Epoch 36: Val Loss 503.61316
Epoch 37: Val Loss 496.93988
Epoch 38: Val Loss 489.70630
Epoch 39: Val Loss 481.95145
Epoch 40: Val Loss 473.73483
Epoch 41: Val Loss 465.09274
Epoch 42: Val Loss 456.07208
Epoch 43: Val Loss 446.88202
Epoch 44: Val Loss 437.38397
Epoch 45: Val Loss 427.54593
Epoch 46: Val Loss 417.45099
Epoch 47: Val Loss 407.05701
Epoch 48: Val Loss 396.52322
Epoch 49: Val Loss 385.73886
Epoch 50: Val Loss 374.81305
Epoch 51: Val Loss 363.75003
Epoch 52: Val Loss 352.49179
Epoch 53: Val Loss 341.03195
Epoch 54: Val Loss 329.45093
Epoch 55: Val Loss 317.77695
Epoch 56: Val Loss 306.10141
Epoch 57: Val Loss 294.66666
Epoch 58: Val Loss 283.31503
Epoch 59: Val Loss 271.74277
Epoch 60: Val Loss 260.37354
Epoch 61: Val Loss 249.30573
Epoch 62: Val Loss 238.61414
Epoch 63: Val Loss 228.12494
Epoch 64: Val Loss 218.22099
Epoch 65: Val Loss 208.60516
Epoch 66: Val Loss 199.46198
Epoch 67: Val Loss 191.01764
Epoch 68: Val Loss 183.10411
Epoch 69: Val Loss 175.59106
Epoch 70: Val Loss 168.43102
Epoch 71: Val Loss 161.51128
Epoch 72: Val Loss 155.15088
Epoch 73: Val Loss 149.24889
Epoch 74: Val Loss 143.80284
Epoch 75: Val Loss 138.91368
Epoch 76: Val Loss 134.46046
Epoch 77: Val Loss 130.38583
Epoch 78: Val Loss 126.68568
Epoch 79: Val Loss 123.30357
Epoch 80: Val Loss 120.27449
Epoch 81: Val Loss 117.52270
Epoch 82: Val Loss 114.95023
Epoch 83: Val Loss 112.55303
Epoch 84: Val Loss 110.27519
Epoch 85: Val Loss 108.17750
Epoch 86: Val Loss 106.08059
Epoch 87: Val Loss 104.10889
Epoch 88: Val Loss 102.26881
Epoch 89: Val Loss 100.59968
Epoch 90: Val Loss 99.04449
Epoch 91: Val Loss 97.53088
Epoch 92: Val Loss 95.98021
Epoch 93: Val Loss 94.40659
Epoch 94: Val Loss 92.95921
Epoch 95: Val Loss 91.58211
Epoch 96: Val Loss 90.31848
Epoch 97: Val Loss 89.10673
Epoch 98: Val Loss 87.90759
Epoch 99: Val Loss 86.70784
{'MSE - mean': 301.17430247184404, 'MSE - std': 246.54179826151372, 'R2 - mean': -2.5644267101984517, 'R2 - std': 2.786041721493011} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 301.17430247184404 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 544.19409
Epoch 1: Val Loss 543.13550
Epoch 2: Val Loss 541.99823
Epoch 3: Val Loss 540.76947
Epoch 4: Val Loss 539.46729
Epoch 5: Val Loss 538.08960
Epoch 6: Val Loss 536.63928
Epoch 7: Val Loss 535.10419
Epoch 8: Val Loss 533.47284
Epoch 9: Val Loss 531.71338
Epoch 10: Val Loss 529.79376
Epoch 11: Val Loss 527.75641
Epoch 12: Val Loss 525.55560
Epoch 13: Val Loss 523.18610
Epoch 14: Val Loss 520.61853
Epoch 15: Val Loss 517.89783
Epoch 16: Val Loss 514.98407
Epoch 17: Val Loss 511.85641
Epoch 18: Val Loss 508.62509
Epoch 19: Val Loss 505.28317
Epoch 20: Val Loss 501.82523
Epoch 21: Val Loss 498.12189
Epoch 22: Val Loss 494.11307
Epoch 23: Val Loss 489.87912
Epoch 24: Val Loss 485.48178
Epoch 25: Val Loss 480.89066
Epoch 26: Val Loss 475.99420
Epoch 27: Val Loss 470.79910
Epoch 28: Val Loss 465.37894
Epoch 29: Val Loss 459.66269
Epoch 30: Val Loss 453.56741
Epoch 31: Val Loss 447.26486
Epoch 32: Val Loss 440.74374
Epoch 33: Val Loss 433.94257
Epoch 34: Val Loss 427.01801
Epoch 35: Val Loss 419.81564
Epoch 36: Val Loss 412.53159
Epoch 37: Val Loss 405.09970
Epoch 38: Val Loss 397.50922
Epoch 39: Val Loss 389.58365
Epoch 40: Val Loss 381.35916
Epoch 41: Val Loss 372.95117
Epoch 42: Val Loss 364.42780
Epoch 43: Val Loss 355.78647
Epoch 44: Val Loss 347.05795
Epoch 45: Val Loss 338.24265
Epoch 46: Val Loss 329.24533
Epoch 47: Val Loss 319.87888
Epoch 48: Val Loss 310.57730
Epoch 49: Val Loss 301.27637
Epoch 50: Val Loss 291.92886
Epoch 51: Val Loss 282.84149
Epoch 52: Val Loss 273.72336
Epoch 53: Val Loss 264.81107
Epoch 54: Val Loss 256.12350
Epoch 55: Val Loss 247.59027
Epoch 56: Val Loss 239.38692
Epoch 57: Val Loss 231.55072
Epoch 58: Val Loss 224.13255
Epoch 59: Val Loss 216.86363
Epoch 60: Val Loss 209.80290
Epoch 61: Val Loss 203.17546
Epoch 62: Val Loss 196.93494
Epoch 63: Val Loss 190.97186
Epoch 64: Val Loss 185.33684
Epoch 65: Val Loss 180.11481
Epoch 66: Val Loss 175.35722
Epoch 67: Val Loss 170.92479
Epoch 68: Val Loss 166.90785
Epoch 69: Val Loss 163.21222
Epoch 70: Val Loss 159.84642
Epoch 71: Val Loss 156.80573
Epoch 72: Val Loss 154.00374
Epoch 73: Val Loss 151.53897
Epoch 74: Val Loss 149.46120
Epoch 75: Val Loss 147.66090
Epoch 76: Val Loss 146.08411
Epoch 77: Val Loss 144.70691
Epoch 78: Val Loss 143.41504
Epoch 79: Val Loss 142.26227
Epoch 80: Val Loss 141.23509
Epoch 81: Val Loss 140.29301
Epoch 82: Val Loss 139.43207
Epoch 83: Val Loss 138.63167
Epoch 84: Val Loss 137.85480
Epoch 85: Val Loss 137.11415
Epoch 86: Val Loss 136.44437
Epoch 87: Val Loss 135.82828
Epoch 88: Val Loss 135.23509
Epoch 89: Val Loss 134.63121
Epoch 90: Val Loss 134.05696
Epoch 91: Val Loss 133.53033
Epoch 92: Val Loss 133.04472
Epoch 93: Val Loss 132.54352
Epoch 94: Val Loss 132.07309
Epoch 95: Val Loss 131.61586
Epoch 96: Val Loss 131.09386
Epoch 97: Val Loss 130.54663
Epoch 98: Val Loss 130.00403
Epoch 99: Val Loss 129.44333
{'MSE - mean': 129.44331592596345, 'MSE - std': 0.0, 'R2 - mean': -0.7335782358353538, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 621.00989
Epoch 1: Val Loss 620.29871
Epoch 2: Val Loss 619.55597
Epoch 3: Val Loss 618.79065
Epoch 4: Val Loss 617.97552
Epoch 5: Val Loss 617.10712
Epoch 6: Val Loss 616.18713
Epoch 7: Val Loss 615.20856
Epoch 8: Val Loss 614.14380
Epoch 9: Val Loss 612.98273
Epoch 10: Val Loss 611.71057
Epoch 11: Val Loss 610.34723
Epoch 12: Val Loss 608.91864
Epoch 13: Val Loss 607.42682
Epoch 14: Val Loss 605.85052
Epoch 15: Val Loss 604.20898
Epoch 16: Val Loss 602.51526
Epoch 17: Val Loss 600.72485
Epoch 18: Val Loss 598.83380
Epoch 19: Val Loss 596.83276
Epoch 20: Val Loss 594.72870
Epoch 21: Val Loss 592.55048
Epoch 22: Val Loss 590.25275
Epoch 23: Val Loss 587.83032
Epoch 24: Val Loss 585.32239
Epoch 25: Val Loss 582.67706
Epoch 26: Val Loss 579.85559
Epoch 27: Val Loss 576.83862
Epoch 28: Val Loss 573.68311
Epoch 29: Val Loss 570.40851
Epoch 30: Val Loss 566.92157
Epoch 31: Val Loss 563.26416
Epoch 32: Val Loss 559.40735
Epoch 33: Val Loss 555.32343
Epoch 34: Val Loss 550.97607
Epoch 35: Val Loss 546.39496
Epoch 36: Val Loss 541.50665
Epoch 37: Val Loss 536.29718
Epoch 38: Val Loss 530.80127
Epoch 39: Val Loss 525.05780
Epoch 40: Val Loss 519.00720
Epoch 41: Val Loss 512.73877
Epoch 42: Val Loss 506.30298
Epoch 43: Val Loss 499.52618
Epoch 44: Val Loss 492.29547
Epoch 45: Val Loss 484.77304
Epoch 46: Val Loss 476.92294
Epoch 47: Val Loss 468.62878
Epoch 48: Val Loss 460.01147
Epoch 49: Val Loss 451.12204
Epoch 50: Val Loss 442.09058
Epoch 51: Val Loss 432.74234
Epoch 52: Val Loss 423.14047
Epoch 53: Val Loss 413.36765
Epoch 54: Val Loss 403.32047
Epoch 55: Val Loss 392.95551
Epoch 56: Val Loss 382.27994
Epoch 57: Val Loss 371.46555
Epoch 58: Val Loss 360.54608
Epoch 59: Val Loss 349.58316
Epoch 60: Val Loss 338.51511
Epoch 61: Val Loss 327.44028
Epoch 62: Val Loss 316.26056
Epoch 63: Val Loss 305.13641
Epoch 64: Val Loss 293.91089
Epoch 65: Val Loss 282.91122
Epoch 66: Val Loss 271.76154
Epoch 67: Val Loss 260.59851
Epoch 68: Val Loss 249.56102
Epoch 69: Val Loss 238.67206
Epoch 70: Val Loss 227.86429
Epoch 71: Val Loss 217.57378
Epoch 72: Val Loss 207.61649
Epoch 73: Val Loss 197.96341
Epoch 74: Val Loss 188.77672
Epoch 75: Val Loss 180.04362
Epoch 76: Val Loss 171.52150
Epoch 77: Val Loss 163.53922
Epoch 78: Val Loss 155.91208
Epoch 79: Val Loss 148.50993
Epoch 80: Val Loss 141.66562
Epoch 81: Val Loss 135.19997
Epoch 82: Val Loss 129.07352
Epoch 83: Val Loss 123.41711
Epoch 84: Val Loss 118.17608
Epoch 85: Val Loss 113.36527
Epoch 86: Val Loss 109.04179
Epoch 87: Val Loss 104.92818
Epoch 88: Val Loss 101.07913
Epoch 89: Val Loss 97.43404
Epoch 90: Val Loss 94.18851
Epoch 91: Val Loss 91.19832
Epoch 92: Val Loss 88.46030
Epoch 93: Val Loss 85.95556
Epoch 94: Val Loss 83.47729
Epoch 95: Val Loss 81.12560
Epoch 96: Val Loss 78.93528
Epoch 97: Val Loss 76.76754
Epoch 98: Val Loss 74.72404
Epoch 99: Val Loss 72.78156
{'MSE - mean': 101.11244094856406, 'MSE - std': 28.3308749773994, 'R2 - mean': -0.3186233377103397, 'R2 - std': 0.41495489812501407} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 690.99738
Epoch 1: Val Loss 690.63226
Epoch 2: Val Loss 690.28003
Epoch 3: Val Loss 689.93005
Epoch 4: Val Loss 689.58765
Epoch 5: Val Loss 689.25140
Epoch 6: Val Loss 688.92273
Epoch 7: Val Loss 688.60339
Epoch 8: Val Loss 688.28436
Epoch 9: Val Loss 687.96796
Epoch 10: Val Loss 687.64294
Epoch 11: Val Loss 687.31305
Epoch 12: Val Loss 686.98322
Epoch 13: Val Loss 686.66864
Epoch 14: Val Loss 686.37964
Epoch 15: Val Loss 686.10876
Epoch 16: Val Loss 685.84918
Epoch 17: Val Loss 685.60486
Epoch 18: Val Loss 685.37482
Epoch 19: Val Loss 685.15424
Epoch 20: Val Loss 684.93640
Epoch 21: Val Loss 684.72046
Epoch 22: Val Loss 684.50616
Epoch 23: Val Loss 684.29657
Epoch 24: Val Loss 684.08984
Epoch 25: Val Loss 683.88611
Epoch 26: Val Loss 683.68237
Epoch 27: Val Loss 683.48260
Epoch 28: Val Loss 683.28583
Epoch 29: Val Loss 683.08948
Epoch 30: Val Loss 682.89264
Epoch 31: Val Loss 682.69574
Epoch 32: Val Loss 682.50037
Epoch 33: Val Loss 682.30627
Epoch 34: Val Loss 682.11151
Epoch 35: Val Loss 681.91705
Epoch 36: Val Loss 681.72382
Epoch 37: Val Loss 681.52954
Epoch 38: Val Loss 681.33508
Epoch 39: Val Loss 681.14044
Epoch 40: Val Loss 680.94476
Epoch 41: Val Loss 680.75092
Epoch 42: Val Loss 680.55792
Epoch 43: Val Loss 680.36493
Epoch 44: Val Loss 680.16998
Epoch 45: Val Loss 679.97522
Epoch 46: Val Loss 679.78107
Epoch 47: Val Loss 679.58893
Epoch 48: Val Loss 679.39642
Epoch 49: Val Loss 679.20422
Epoch 50: Val Loss 679.01123
Epoch 51: Val Loss 678.81781
Epoch 52: Val Loss 678.62439
Epoch 53: Val Loss 678.42944
Epoch 54: Val Loss 678.23566
Epoch 55: Val Loss 678.04333
Epoch 56: Val Loss 677.85022
Epoch 57: Val Loss 677.65826
Epoch 58: Val Loss 677.46753
Epoch 59: Val Loss 677.27814
Epoch 60: Val Loss 677.08789
Epoch 61: Val Loss 676.89642
Epoch 62: Val Loss 676.70441
Epoch 63: Val Loss 676.51312
Epoch 64: Val Loss 676.32159
Epoch 65: Val Loss 676.13062
Epoch 66: Val Loss 675.94043
Epoch 67: Val Loss 675.75153
Epoch 68: Val Loss 675.56189
Epoch 69: Val Loss 675.37329
Epoch 70: Val Loss 675.18555
Epoch 71: Val Loss 674.99622
Epoch 72: Val Loss 674.80695
Epoch 73: Val Loss 674.61694
Epoch 74: Val Loss 674.42627
Epoch 75: Val Loss 674.23376
Epoch 76: Val Loss 674.04169
Epoch 77: Val Loss 673.85150
Epoch 78: Val Loss 673.66150
Epoch 79: Val Loss 673.47137
Epoch 80: Val Loss 673.28271
Epoch 81: Val Loss 673.09412
Epoch 82: Val Loss 672.90594
Epoch 83: Val Loss 672.71912
Epoch 84: Val Loss 672.53265
Epoch 85: Val Loss 672.34631
Epoch 86: Val Loss 672.15924
Epoch 87: Val Loss 671.97302
Epoch 88: Val Loss 671.78790
Epoch 89: Val Loss 671.60315
Epoch 90: Val Loss 671.41852
Epoch 91: Val Loss 671.23267
Epoch 92: Val Loss 671.04498
Epoch 93: Val Loss 670.85779
Epoch 94: Val Loss 670.67078
Epoch 95: Val Loss 670.48206
Epoch 96: Val Loss 670.29266
Epoch 97: Val Loss 670.10498
Epoch 98: Val Loss 669.91846
Epoch 99: Val Loss 669.73242
{'MSE - mean': 290.6524482019516, 'MSE - std': 269.0463176057581, 'R2 - mean': -2.2183401406698926, 'R2 - std': 2.7078847048103225} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 556.09381
Epoch 1: Val Loss 555.47375
Epoch 2: Val Loss 554.81909
Epoch 3: Val Loss 554.10535
Epoch 4: Val Loss 553.30798
Epoch 5: Val Loss 552.44409
Epoch 6: Val Loss 551.48303
Epoch 7: Val Loss 550.41620
Epoch 8: Val Loss 549.24445
Epoch 9: Val Loss 548.00067
Epoch 10: Val Loss 546.68378
Epoch 11: Val Loss 545.24341
Epoch 12: Val Loss 543.70697
Epoch 13: Val Loss 542.08325
Epoch 14: Val Loss 540.39038
Epoch 15: Val Loss 538.57208
Epoch 16: Val Loss 536.61169
Epoch 17: Val Loss 534.47913
Epoch 18: Val Loss 532.20703
Epoch 19: Val Loss 529.79700
Epoch 20: Val Loss 527.17792
Epoch 21: Val Loss 524.37006
Epoch 22: Val Loss 521.42981
Epoch 23: Val Loss 518.33337
Epoch 24: Val Loss 515.09723
Epoch 25: Val Loss 511.65106
Epoch 26: Val Loss 508.03802
Epoch 27: Val Loss 504.18518
Epoch 28: Val Loss 500.14182
Epoch 29: Val Loss 495.96146
Epoch 30: Val Loss 491.64233
Epoch 31: Val Loss 487.07526
Epoch 32: Val Loss 482.33911
Epoch 33: Val Loss 477.47693
Epoch 34: Val Loss 472.45560
Epoch 35: Val Loss 467.19446
Epoch 36: Val Loss 461.72647
Epoch 37: Val Loss 456.05707
Epoch 38: Val Loss 450.19833
Epoch 39: Val Loss 444.09537
Epoch 40: Val Loss 437.82153
Epoch 41: Val Loss 431.26477
Epoch 42: Val Loss 424.52713
Epoch 43: Val Loss 417.71658
Epoch 44: Val Loss 410.67230
Epoch 45: Val Loss 403.29300
Epoch 46: Val Loss 395.82065
Epoch 47: Val Loss 388.26657
Epoch 48: Val Loss 380.67953
Epoch 49: Val Loss 372.97540
Epoch 50: Val Loss 365.16312
Epoch 51: Val Loss 357.18478
Epoch 52: Val Loss 349.02344
Epoch 53: Val Loss 340.64618
Epoch 54: Val Loss 332.14133
Epoch 55: Val Loss 323.48349
Epoch 56: Val Loss 314.90005
Epoch 57: Val Loss 306.42279
Epoch 58: Val Loss 297.84750
Epoch 59: Val Loss 289.30612
Epoch 60: Val Loss 280.66171
Epoch 61: Val Loss 271.99252
Epoch 62: Val Loss 263.37033
Epoch 63: Val Loss 254.69171
Epoch 64: Val Loss 246.09946
Epoch 65: Val Loss 237.80453
Epoch 66: Val Loss 229.77882
Epoch 67: Val Loss 221.71902
Epoch 68: Val Loss 213.62341
Epoch 69: Val Loss 205.77682
Epoch 70: Val Loss 198.17110
Epoch 71: Val Loss 190.70003
Epoch 72: Val Loss 183.40184
Epoch 73: Val Loss 176.38991
Epoch 74: Val Loss 169.77034
Epoch 75: Val Loss 163.32011
Epoch 76: Val Loss 157.03851
Epoch 77: Val Loss 151.10852
Epoch 78: Val Loss 145.51685
Epoch 79: Val Loss 140.26924
Epoch 80: Val Loss 135.25150
Epoch 81: Val Loss 130.60400
Epoch 82: Val Loss 126.26640
Epoch 83: Val Loss 122.15727
Epoch 84: Val Loss 118.30776
Epoch 85: Val Loss 114.57154
Epoch 86: Val Loss 111.13145
Epoch 87: Val Loss 107.98449
Epoch 88: Val Loss 105.15843
Epoch 89: Val Loss 102.58076
Epoch 90: Val Loss 100.22944
Epoch 91: Val Loss 98.03498
Epoch 92: Val Loss 96.02558
Epoch 93: Val Loss 94.12813
Epoch 94: Val Loss 92.29070
Epoch 95: Val Loss 90.58501
Epoch 96: Val Loss 89.02653
Epoch 97: Val Loss 87.60258
Epoch 98: Val Loss 86.27130
Epoch 99: Val Loss 85.00511
{'MSE - mean': 239.24061404774164, 'MSE - std': 249.43730842881905, 'R2 - mean': -1.6953947560837908, 'R2 - std': 2.513940195527528} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 594.58960
Epoch 1: Val Loss 593.78882
Epoch 2: Val Loss 592.97174
Epoch 3: Val Loss 592.13708
Epoch 4: Val Loss 591.25659
Epoch 5: Val Loss 590.33386
Epoch 6: Val Loss 589.36328
Epoch 7: Val Loss 588.36060
Epoch 8: Val Loss 587.30536
Epoch 9: Val Loss 586.19922
Epoch 10: Val Loss 585.01764
Epoch 11: Val Loss 583.75385
Epoch 12: Val Loss 582.43323
Epoch 13: Val Loss 581.05963
Epoch 14: Val Loss 579.62585
Epoch 15: Val Loss 578.12720
Epoch 16: Val Loss 576.52307
Epoch 17: Val Loss 574.85327
Epoch 18: Val Loss 573.11267
Epoch 19: Val Loss 571.28394
Epoch 20: Val Loss 569.34833
Epoch 21: Val Loss 567.32428
Epoch 22: Val Loss 565.19727
Epoch 23: Val Loss 562.93848
Epoch 24: Val Loss 560.57416
Epoch 25: Val Loss 558.08264
Epoch 26: Val Loss 555.42694
Epoch 27: Val Loss 552.59753
Epoch 28: Val Loss 549.65985
Epoch 29: Val Loss 546.58038
Epoch 30: Val Loss 543.39093
Epoch 31: Val Loss 540.10504
Epoch 32: Val Loss 536.64972
Epoch 33: Val Loss 532.98553
Epoch 34: Val Loss 529.21979
Epoch 35: Val Loss 525.24017
Epoch 36: Val Loss 521.05896
Epoch 37: Val Loss 516.65118
Epoch 38: Val Loss 512.01807
Epoch 39: Val Loss 507.12549
Epoch 40: Val Loss 502.07108
Epoch 41: Val Loss 496.83557
Epoch 42: Val Loss 491.43423
Epoch 43: Val Loss 485.77155
Epoch 44: Val Loss 479.91510
Epoch 45: Val Loss 473.85010
Epoch 46: Val Loss 467.49072
Epoch 47: Val Loss 461.06985
Epoch 48: Val Loss 454.48886
Epoch 49: Val Loss 447.80746
Epoch 50: Val Loss 440.93130
Epoch 51: Val Loss 433.83398
Epoch 52: Val Loss 426.49237
Epoch 53: Val Loss 418.98511
Epoch 54: Val Loss 411.25772
Epoch 55: Val Loss 403.23608
Epoch 56: Val Loss 395.06195
Epoch 57: Val Loss 386.83173
Epoch 58: Val Loss 378.57208
Epoch 59: Val Loss 370.20996
Epoch 60: Val Loss 361.84128
Epoch 61: Val Loss 353.27521
Epoch 62: Val Loss 344.77103
Epoch 63: Val Loss 336.31180
Epoch 64: Val Loss 327.73734
Epoch 65: Val Loss 319.19104
Epoch 66: Val Loss 310.67230
Epoch 67: Val Loss 302.23935
Epoch 68: Val Loss 293.72498
Epoch 69: Val Loss 285.24762
Epoch 70: Val Loss 276.92810
Epoch 71: Val Loss 268.67743
Epoch 72: Val Loss 260.57761
Epoch 73: Val Loss 252.68385
Epoch 74: Val Loss 244.95387
Epoch 75: Val Loss 237.57805
Epoch 76: Val Loss 230.57773
Epoch 77: Val Loss 223.81775
Epoch 78: Val Loss 217.31137
Epoch 79: Val Loss 211.03110
Epoch 80: Val Loss 205.12146
Epoch 81: Val Loss 199.50993
Epoch 82: Val Loss 194.23618
Epoch 83: Val Loss 189.32126
Epoch 84: Val Loss 184.65993
Epoch 85: Val Loss 180.14946
Epoch 86: Val Loss 175.97327
Epoch 87: Val Loss 172.18431
Epoch 88: Val Loss 168.79298
Epoch 89: Val Loss 165.65678
Epoch 90: Val Loss 162.79161
Epoch 91: Val Loss 160.11395
Epoch 92: Val Loss 157.60358
Epoch 93: Val Loss 155.19098
Epoch 94: Val Loss 152.96765
Epoch 95: Val Loss 150.87167
Epoch 96: Val Loss 148.92741
Epoch 97: Val Loss 147.08134
Epoch 98: Val Loss 145.41110
Epoch 99: Val Loss 143.93538
{'MSE - mean': 220.1795692267574, 'MSE - std': 226.33707249949427, 'R2 - mean': -1.471699216338208, 'R2 - std': 2.292613142978871} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 220.1795692267574 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 15 with value: 175.22584408759803.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 535.66736
Epoch 1: Val Loss 534.51721
Epoch 2: Val Loss 533.36914
Epoch 3: Val Loss 532.23785
Epoch 4: Val Loss 531.13409
Epoch 5: Val Loss 530.04248
Epoch 6: Val Loss 528.95514
Epoch 7: Val Loss 527.85541
Epoch 8: Val Loss 526.75958
Epoch 9: Val Loss 525.66675
Epoch 10: Val Loss 524.56824
Epoch 11: Val Loss 523.46472
Epoch 12: Val Loss 522.35236
Epoch 13: Val Loss 521.22559
Epoch 14: Val Loss 520.07678
Epoch 15: Val Loss 518.91663
Epoch 16: Val Loss 517.73193
Epoch 17: Val Loss 516.52490
Epoch 18: Val Loss 515.30890
Epoch 19: Val Loss 514.07135
Epoch 20: Val Loss 512.80060
Epoch 21: Val Loss 511.48660
Epoch 22: Val Loss 510.14456
Epoch 23: Val Loss 508.76556
Epoch 24: Val Loss 507.33487
Epoch 25: Val Loss 505.86325
Epoch 26: Val Loss 504.35312
Epoch 27: Val Loss 502.84576
Epoch 28: Val Loss 501.27197
Epoch 29: Val Loss 499.63489
Epoch 30: Val Loss 497.93777
Epoch 31: Val Loss 496.20236
Epoch 32: Val Loss 494.42996
Epoch 33: Val Loss 492.59100
Epoch 34: Val Loss 490.70261
Epoch 35: Val Loss 488.74680
Epoch 36: Val Loss 486.71371
Epoch 37: Val Loss 484.59131
Epoch 38: Val Loss 482.39340
Epoch 39: Val Loss 480.13315
Epoch 40: Val Loss 477.83105
Epoch 41: Val Loss 475.42795
Epoch 42: Val Loss 472.91879
Epoch 43: Val Loss 470.35013
Epoch 44: Val Loss 467.70306
Epoch 45: Val Loss 464.95566
Epoch 46: Val Loss 462.10971
Epoch 47: Val Loss 459.14597
Epoch 48: Val Loss 456.07938
Epoch 49: Val Loss 452.95096
Epoch 50: Val Loss 449.70886
Epoch 51: Val Loss 446.33722
Epoch 52: Val Loss 442.86560
Epoch 53: Val Loss 439.24680
Epoch 54: Val Loss 435.59537
Epoch 55: Val Loss 431.83435
Epoch 56: Val Loss 427.99805
Epoch 57: Val Loss 424.04492
Epoch 58: Val Loss 420.02759
Epoch 59: Val Loss 415.90323
Epoch 60: Val Loss 411.65979
Epoch 61: Val Loss 407.29782
Epoch 62: Val Loss 402.86783
Epoch 63: Val Loss 398.29517
Epoch 64: Val Loss 393.58954
Epoch 65: Val Loss 388.85651
Epoch 66: Val Loss 384.05518
Epoch 67: Val Loss 379.20117
Epoch 68: Val Loss 374.06680
Epoch 69: Val Loss 368.90665
Epoch 70: Val Loss 363.67648
Epoch 71: Val Loss 358.36938
Epoch 72: Val Loss 352.97687
Epoch 73: Val Loss 347.51111
Epoch 74: Val Loss 341.96933
Epoch 75: Val Loss 336.35812
Epoch 76: Val Loss 330.65985
Epoch 77: Val Loss 324.78259
Epoch 78: Val Loss 318.95047
Epoch 79: Val Loss 313.05496
Epoch 80: Val Loss 307.12930
Epoch 81: Val Loss 301.10651
Epoch 82: Val Loss 295.15030
Epoch 83: Val Loss 289.32431
Epoch 84: Val Loss 283.49875
Epoch 85: Val Loss 277.72000
Epoch 86: Val Loss 272.00262
Epoch 87: Val Loss 266.31116
Epoch 88: Val Loss 260.67773
Epoch 89: Val Loss 255.04588
Epoch 90: Val Loss 249.41386
Epoch 91: Val Loss 243.81538
Epoch 92: Val Loss 238.43971
Epoch 93: Val Loss 233.13362
Epoch 94: Val Loss 227.79616
Epoch 95: Val Loss 222.53317
Epoch 96: Val Loss 217.32961
Epoch 97: Val Loss 212.22684
Epoch 98: Val Loss 207.23021
Epoch 99: Val Loss 202.39107
{'MSE - mean': 202.39106556104375, 'MSE - std': 0.0, 'R2 - mean': -1.7105358347342587, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 595.32562
Epoch 1: Val Loss 594.71045
Epoch 2: Val Loss 594.08698
Epoch 3: Val Loss 593.44989
Epoch 4: Val Loss 592.79401
Epoch 5: Val Loss 592.11780
Epoch 6: Val Loss 591.42499
Epoch 7: Val Loss 590.71149
Epoch 8: Val Loss 589.97241
Epoch 9: Val Loss 589.20074
Epoch 10: Val Loss 588.40259
Epoch 11: Val Loss 587.57587
Epoch 12: Val Loss 586.71857
Epoch 13: Val Loss 585.82507
Epoch 14: Val Loss 584.88611
Epoch 15: Val Loss 583.90143
Epoch 16: Val Loss 582.88068
Epoch 17: Val Loss 581.80682
Epoch 18: Val Loss 580.68182
Epoch 19: Val Loss 579.50647
Epoch 20: Val Loss 578.25922
Epoch 21: Val Loss 576.95239
Epoch 22: Val Loss 575.60736
Epoch 23: Val Loss 574.18011
Epoch 24: Val Loss 572.67859
Epoch 25: Val Loss 571.09100
Epoch 26: Val Loss 569.43140
Epoch 27: Val Loss 567.72260
Epoch 28: Val Loss 565.92371
Epoch 29: Val Loss 564.04535
Epoch 30: Val Loss 562.05371
Epoch 31: Val Loss 559.94702
Epoch 32: Val Loss 557.73401
Epoch 33: Val Loss 555.40216
Epoch 34: Val Loss 552.97461
Epoch 35: Val Loss 550.45471
Epoch 36: Val Loss 547.85553
Epoch 37: Val Loss 545.19397
Epoch 38: Val Loss 542.44623
Epoch 39: Val Loss 539.55511
Epoch 40: Val Loss 536.52856
Epoch 41: Val Loss 533.40753
Epoch 42: Val Loss 530.11426
Epoch 43: Val Loss 526.72168
Epoch 44: Val Loss 523.16260
Epoch 45: Val Loss 519.50299
Epoch 46: Val Loss 515.64722
Epoch 47: Val Loss 511.63025
Epoch 48: Val Loss 507.45218
Epoch 49: Val Loss 503.04849
Epoch 50: Val Loss 498.36069
Epoch 51: Val Loss 493.50580
Epoch 52: Val Loss 488.39227
Epoch 53: Val Loss 483.07947
Epoch 54: Val Loss 477.46179
Epoch 55: Val Loss 471.53009
Epoch 56: Val Loss 465.36789
Epoch 57: Val Loss 458.75888
Epoch 58: Val Loss 451.72849
Epoch 59: Val Loss 444.33322
Epoch 60: Val Loss 436.45920
Epoch 61: Val Loss 427.89700
Epoch 62: Val Loss 418.44992
Epoch 63: Val Loss 408.29922
Epoch 64: Val Loss 397.65381
Epoch 65: Val Loss 386.34653
Epoch 66: Val Loss 374.50711
Epoch 67: Val Loss 361.99109
Epoch 68: Val Loss 349.31845
Epoch 69: Val Loss 336.37854
Epoch 70: Val Loss 323.18320
Epoch 71: Val Loss 309.75433
Epoch 72: Val Loss 296.25467
Epoch 73: Val Loss 282.82693
Epoch 74: Val Loss 269.32639
Epoch 75: Val Loss 255.97606
Epoch 76: Val Loss 242.94547
Epoch 77: Val Loss 229.99138
Epoch 78: Val Loss 217.27937
Epoch 79: Val Loss 204.98465
Epoch 80: Val Loss 193.04254
Epoch 81: Val Loss 181.72652
Epoch 82: Val Loss 171.12190
Epoch 83: Val Loss 161.34233
Epoch 84: Val Loss 152.47316
Epoch 85: Val Loss 144.46144
Epoch 86: Val Loss 137.06752
Epoch 87: Val Loss 130.12392
Epoch 88: Val Loss 123.79950
Epoch 89: Val Loss 118.24258
Epoch 90: Val Loss 113.33273
Epoch 91: Val Loss 109.02467
Epoch 92: Val Loss 105.12438
Epoch 93: Val Loss 101.63044
Epoch 94: Val Loss 98.66515
Epoch 95: Val Loss 95.99174
Epoch 96: Val Loss 93.44347
Epoch 97: Val Loss 91.14873
Epoch 98: Val Loss 89.12786
Epoch 99: Val Loss 87.28877
{'MSE - mean': 144.83992009288374, 'MSE - std': 57.551145468160016, 'R2 - mean': -0.8971641359074859, 'R2 - std': 0.8133716988267728} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 708.96686
Epoch 1: Val Loss 708.55859
Epoch 2: Val Loss 708.12958
Epoch 3: Val Loss 707.69214
Epoch 4: Val Loss 707.24854
Epoch 5: Val Loss 706.77350
Epoch 6: Val Loss 706.28705
Epoch 7: Val Loss 705.78052
Epoch 8: Val Loss 705.21161
Epoch 9: Val Loss 704.59137
Epoch 10: Val Loss 703.94800
Epoch 11: Val Loss 703.27734
Epoch 12: Val Loss 702.54779
Epoch 13: Val Loss 701.76721
Epoch 14: Val Loss 700.92487
Epoch 15: Val Loss 700.00464
Epoch 16: Val Loss 698.97662
Epoch 17: Val Loss 697.83856
Epoch 18: Val Loss 696.59070
Epoch 19: Val Loss 695.20758
Epoch 20: Val Loss 693.66614
Epoch 21: Val Loss 692.00848
Epoch 22: Val Loss 690.20789
Epoch 23: Val Loss 688.26636
Epoch 24: Val Loss 686.17535
Epoch 25: Val Loss 683.93677
Epoch 26: Val Loss 681.49695
Epoch 27: Val Loss 678.86029
Epoch 28: Val Loss 676.08453
Epoch 29: Val Loss 673.08679
Epoch 30: Val Loss 669.85199
Epoch 31: Val Loss 666.34027
Epoch 32: Val Loss 662.57642
Epoch 33: Val Loss 658.56390
Epoch 34: Val Loss 654.32190
Epoch 35: Val Loss 649.82904
Epoch 36: Val Loss 645.09094
Epoch 37: Val Loss 640.04883
Epoch 38: Val Loss 634.64392
Epoch 39: Val Loss 628.95209
Epoch 40: Val Loss 622.91260
Epoch 41: Val Loss 616.56622
Epoch 42: Val Loss 609.89355
Epoch 43: Val Loss 602.90436
Epoch 44: Val Loss 595.50690
Epoch 45: Val Loss 587.84314
Epoch 46: Val Loss 579.77295
Epoch 47: Val Loss 571.37982
Epoch 48: Val Loss 562.54626
Epoch 49: Val Loss 553.40424
Epoch 50: Val Loss 544.03143
Epoch 51: Val Loss 534.21985
Epoch 52: Val Loss 524.09094
Epoch 53: Val Loss 513.88440
Epoch 54: Val Loss 503.62167
Epoch 55: Val Loss 493.10477
Epoch 56: Val Loss 482.25323
Epoch 57: Val Loss 471.05258
Epoch 58: Val Loss 459.68927
Epoch 59: Val Loss 448.07352
Epoch 60: Val Loss 436.24704
Epoch 61: Val Loss 424.21991
Epoch 62: Val Loss 412.20892
Epoch 63: Val Loss 400.25333
Epoch 64: Val Loss 388.43811
Epoch 65: Val Loss 376.63675
Epoch 66: Val Loss 364.96405
Epoch 67: Val Loss 353.26260
Epoch 68: Val Loss 341.46918
Epoch 69: Val Loss 329.91718
Epoch 70: Val Loss 318.66568
Epoch 71: Val Loss 307.52609
Epoch 72: Val Loss 296.72617
Epoch 73: Val Loss 286.40976
Epoch 74: Val Loss 276.62146
Epoch 75: Val Loss 267.07150
Epoch 76: Val Loss 258.10782
Epoch 77: Val Loss 249.53326
Epoch 78: Val Loss 241.41696
Epoch 79: Val Loss 233.66452
Epoch 80: Val Loss 226.34195
Epoch 81: Val Loss 219.52483
Epoch 82: Val Loss 213.22926
Epoch 83: Val Loss 207.48398
Epoch 84: Val Loss 202.26479
Epoch 85: Val Loss 197.49887
Epoch 86: Val Loss 193.04076
Epoch 87: Val Loss 188.87091
Epoch 88: Val Loss 185.13918
Epoch 89: Val Loss 181.74084
Epoch 90: Val Loss 178.63196
Epoch 91: Val Loss 175.78041
Epoch 92: Val Loss 173.05751
Epoch 93: Val Loss 170.46913
Epoch 94: Val Loss 168.08466
Epoch 95: Val Loss 165.77263
Epoch 96: Val Loss 163.72459
Epoch 97: Val Loss 161.83459
Epoch 98: Val Loss 159.95724
Epoch 99: Val Loss 158.25198
{'MSE - mean': 149.31060735547578, 'MSE - std': 47.41374961265296, 'R2 - mean': -0.8175224809551788, 'R2 - std': 0.6735982482474142} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 520.03485
Epoch 1: Val Loss 519.01367
Epoch 2: Val Loss 517.92059
Epoch 3: Val Loss 516.77246
Epoch 4: Val Loss 515.58765
Epoch 5: Val Loss 514.33691
Epoch 6: Val Loss 512.99182
Epoch 7: Val Loss 511.56857
Epoch 8: Val Loss 510.05112
Epoch 9: Val Loss 508.42395
Epoch 10: Val Loss 506.69431
Epoch 11: Val Loss 504.83313
Epoch 12: Val Loss 502.79489
Epoch 13: Val Loss 500.62347
Epoch 14: Val Loss 498.33624
Epoch 15: Val Loss 495.87411
Epoch 16: Val Loss 493.26624
Epoch 17: Val Loss 490.45190
Epoch 18: Val Loss 487.42557
Epoch 19: Val Loss 484.24234
Epoch 20: Val Loss 480.87891
Epoch 21: Val Loss 477.32840
Epoch 22: Val Loss 473.52567
Epoch 23: Val Loss 469.54208
Epoch 24: Val Loss 465.34476
Epoch 25: Val Loss 461.00977
Epoch 26: Val Loss 456.39908
Epoch 27: Val Loss 451.43353
Epoch 28: Val Loss 446.22858
Epoch 29: Val Loss 440.81586
Epoch 30: Val Loss 435.15308
Epoch 31: Val Loss 429.17084
Epoch 32: Val Loss 422.90393
Epoch 33: Val Loss 416.37125
Epoch 34: Val Loss 409.50812
Epoch 35: Val Loss 402.49847
Epoch 36: Val Loss 395.25714
Epoch 37: Val Loss 387.93750
Epoch 38: Val Loss 380.29593
Epoch 39: Val Loss 372.42273
Epoch 40: Val Loss 364.26376
Epoch 41: Val Loss 355.99506
Epoch 42: Val Loss 347.47147
Epoch 43: Val Loss 338.78397
Epoch 44: Val Loss 329.80707
Epoch 45: Val Loss 320.72839
Epoch 46: Val Loss 311.61475
Epoch 47: Val Loss 302.27954
Epoch 48: Val Loss 292.69727
Epoch 49: Val Loss 282.89206
Epoch 50: Val Loss 272.91104
Epoch 51: Val Loss 262.89026
Epoch 52: Val Loss 252.88162
Epoch 53: Val Loss 242.83478
Epoch 54: Val Loss 232.95813
Epoch 55: Val Loss 223.01376
Epoch 56: Val Loss 213.23215
Epoch 57: Val Loss 203.62944
Epoch 58: Val Loss 194.15799
Epoch 59: Val Loss 184.87115
Epoch 60: Val Loss 175.56331
Epoch 61: Val Loss 166.42644
Epoch 62: Val Loss 157.62849
Epoch 63: Val Loss 149.15533
Epoch 64: Val Loss 141.01685
Epoch 65: Val Loss 133.09093
Epoch 66: Val Loss 125.55018
Epoch 67: Val Loss 118.35961
Epoch 68: Val Loss 111.76690
Epoch 69: Val Loss 105.61948
Epoch 70: Val Loss 99.94126
Epoch 71: Val Loss 94.73446
Epoch 72: Val Loss 89.90936
Epoch 73: Val Loss 85.55369
Epoch 74: Val Loss 81.69473
Epoch 75: Val Loss 78.24335
Epoch 76: Val Loss 75.19496
Epoch 77: Val Loss 72.51068
Epoch 78: Val Loss 70.13674
Epoch 79: Val Loss 68.00837
Epoch 80: Val Loss 66.11087
Epoch 81: Val Loss 64.47295
Epoch 82: Val Loss 62.95882
Epoch 83: Val Loss 61.63112
Epoch 84: Val Loss 60.48089
Epoch 85: Val Loss 59.47032
Epoch 86: Val Loss 58.54841
Epoch 87: Val Loss 57.70099
Epoch 88: Val Loss 56.90229
Epoch 89: Val Loss 56.16203
Epoch 90: Val Loss 55.47029
Epoch 91: Val Loss 54.83536
Epoch 92: Val Loss 54.25607
Epoch 93: Val Loss 53.68291
Epoch 94: Val Loss 53.13095
Epoch 95: Val Loss 52.61558
Epoch 96: Val Loss 52.09963
Epoch 97: Val Loss 51.53381
Epoch 98: Val Loss 50.98812
Epoch 99: Val Loss 50.49815
{'MSE - mean': 124.60749396453929, 'MSE - std': 59.30243816964788, 'R2 - mean': -0.5304527685394624, 'R2 - std': 0.7665037569319639} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 568.08173
Epoch 1: Val Loss 567.50952
Epoch 2: Val Loss 566.93164
Epoch 3: Val Loss 566.33154
Epoch 4: Val Loss 565.71649
Epoch 5: Val Loss 565.05157
Epoch 6: Val Loss 564.30554
Epoch 7: Val Loss 563.48230
Epoch 8: Val Loss 562.49286
Epoch 9: Val Loss 561.36224
Epoch 10: Val Loss 560.15222
Epoch 11: Val Loss 558.86145
Epoch 12: Val Loss 557.48413
Epoch 13: Val Loss 555.97278
Epoch 14: Val Loss 554.26953
Epoch 15: Val Loss 552.42487
Epoch 16: Val Loss 550.41840
Epoch 17: Val Loss 548.26691
Epoch 18: Val Loss 545.97516
Epoch 19: Val Loss 543.54761
Epoch 20: Val Loss 540.93030
Epoch 21: Val Loss 538.10364
Epoch 22: Val Loss 535.08850
Epoch 23: Val Loss 531.87598
Epoch 24: Val Loss 528.41241
Epoch 25: Val Loss 524.68005
Epoch 26: Val Loss 520.68976
Epoch 27: Val Loss 516.39697
Epoch 28: Val Loss 511.76361
Epoch 29: Val Loss 506.84103
Epoch 30: Val Loss 501.64148
Epoch 31: Val Loss 496.16251
Epoch 32: Val Loss 490.40298
Epoch 33: Val Loss 484.23947
Epoch 34: Val Loss 477.69492
Epoch 35: Val Loss 470.84073
Epoch 36: Val Loss 463.67737
Epoch 37: Val Loss 456.21674
Epoch 38: Val Loss 448.58121
Epoch 39: Val Loss 440.65494
Epoch 40: Val Loss 432.39655
Epoch 41: Val Loss 423.93958
Epoch 42: Val Loss 415.15894
Epoch 43: Val Loss 406.19443
Epoch 44: Val Loss 397.05750
Epoch 45: Val Loss 387.67496
Epoch 46: Val Loss 378.06821
Epoch 47: Val Loss 368.36572
Epoch 48: Val Loss 358.76852
Epoch 49: Val Loss 349.07205
Epoch 50: Val Loss 339.34955
Epoch 51: Val Loss 329.60718
Epoch 52: Val Loss 319.75043
Epoch 53: Val Loss 309.72018
Epoch 54: Val Loss 299.62830
Epoch 55: Val Loss 289.69464
Epoch 56: Val Loss 280.00635
Epoch 57: Val Loss 270.65601
Epoch 58: Val Loss 261.47580
Epoch 59: Val Loss 252.62538
Epoch 60: Val Loss 244.00789
Epoch 61: Val Loss 235.78139
Epoch 62: Val Loss 227.96059
Epoch 63: Val Loss 220.62294
Epoch 64: Val Loss 213.77773
Epoch 65: Val Loss 207.20212
Epoch 66: Val Loss 201.03848
Epoch 67: Val Loss 195.39388
Epoch 68: Val Loss 190.28401
Epoch 69: Val Loss 185.61031
Epoch 70: Val Loss 181.43120
Epoch 71: Val Loss 177.66388
Epoch 72: Val Loss 174.22888
Epoch 73: Val Loss 171.10634
Epoch 74: Val Loss 168.18178
Epoch 75: Val Loss 165.50011
Epoch 76: Val Loss 163.03972
Epoch 77: Val Loss 160.95061
Epoch 78: Val Loss 159.11101
Epoch 79: Val Loss 157.43053
Epoch 80: Val Loss 155.79427
Epoch 81: Val Loss 154.29413
Epoch 82: Val Loss 153.00194
Epoch 83: Val Loss 151.73653
Epoch 84: Val Loss 150.51825
Epoch 85: Val Loss 149.35098
Epoch 86: Val Loss 148.15791
Epoch 87: Val Loss 147.06000
Epoch 88: Val Loss 146.03479
Epoch 89: Val Loss 145.07404
Epoch 90: Val Loss 144.18028
Epoch 91: Val Loss 143.27351
Epoch 92: Val Loss 142.43213
Epoch 93: Val Loss 141.61838
Epoch 94: Val Loss 140.84935
Epoch 95: Val Loss 140.07935
Epoch 96: Val Loss 139.30931
Epoch 97: Val Loss 138.55630
Epoch 98: Val Loss 137.76787
Epoch 99: Val Loss 136.98967
{'MSE - mean': 127.08393124253458, 'MSE - std': 53.27245352580737, 'R2 - mean': -0.5245265645701316, 'R2 - std': 0.685684247366524} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 127.08393124253458 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 555.59692
Epoch 1: Val Loss 555.55640
Epoch 2: Val Loss 555.51605
Epoch 3: Val Loss 555.47644
Epoch 4: Val Loss 555.43677
Epoch 5: Val Loss 555.39703
Epoch 6: Val Loss 555.35742
Epoch 7: Val Loss 555.31799
Epoch 8: Val Loss 555.27844
Epoch 9: Val Loss 555.23865
Epoch 10: Val Loss 555.19818
Epoch 11: Val Loss 555.15790
Epoch 12: Val Loss 555.11804
Epoch 13: Val Loss 555.07825
Epoch 14: Val Loss 555.03845
Epoch 15: Val Loss 554.99896
Epoch 16: Val Loss 554.95990
Epoch 17: Val Loss 554.92053
Epoch 18: Val Loss 554.88104
Epoch 19: Val Loss 554.84113
Epoch 20: Val Loss 554.80121
Epoch 21: Val Loss 554.76160
Epoch 22: Val Loss 554.72253
Epoch 23: Val Loss 554.68365
Epoch 24: Val Loss 554.64484
Epoch 25: Val Loss 554.60590
Epoch 26: Val Loss 554.56683
Epoch 27: Val Loss 554.52759
Epoch 28: Val Loss 554.48822
Epoch 29: Val Loss 554.44904
Epoch 30: Val Loss 554.40985
Epoch 31: Val Loss 554.37109
Epoch 32: Val Loss 554.33228
Epoch 33: Val Loss 554.29376
Epoch 34: Val Loss 554.25531
Epoch 35: Val Loss 554.21759
Epoch 36: Val Loss 554.17993
Epoch 37: Val Loss 554.14233
Epoch 38: Val Loss 554.10413
Epoch 39: Val Loss 554.06567
Epoch 40: Val Loss 554.02808
Epoch 41: Val Loss 553.99042
Epoch 42: Val Loss 553.95276
Epoch 43: Val Loss 553.91553
Epoch 44: Val Loss 553.87866
Epoch 45: Val Loss 553.84180
Epoch 46: Val Loss 553.80481
Epoch 47: Val Loss 553.76746
Epoch 48: Val Loss 553.72943
Epoch 49: Val Loss 553.69122
Epoch 50: Val Loss 553.65308
Epoch 51: Val Loss 553.61505
Epoch 52: Val Loss 553.57745
Epoch 53: Val Loss 553.54022
Epoch 54: Val Loss 553.50372
Epoch 55: Val Loss 553.46680
Epoch 56: Val Loss 553.42981
Epoch 57: Val Loss 553.39301
Epoch 58: Val Loss 553.35614
Epoch 59: Val Loss 553.31909
Epoch 60: Val Loss 553.28186
Epoch 61: Val Loss 553.24506
Epoch 62: Val Loss 553.20782
Epoch 63: Val Loss 553.17084
Epoch 64: Val Loss 553.13391
Epoch 65: Val Loss 553.09705
Epoch 66: Val Loss 553.05975
Epoch 67: Val Loss 553.02246
Epoch 68: Val Loss 552.98511
Epoch 69: Val Loss 552.94775
Epoch 70: Val Loss 552.91028
Epoch 71: Val Loss 552.87280
Epoch 72: Val Loss 552.83527
Epoch 73: Val Loss 552.79761
Epoch 74: Val Loss 552.76013
Epoch 75: Val Loss 552.72253
Epoch 76: Val Loss 552.68494
Epoch 77: Val Loss 552.64722
Epoch 78: Val Loss 552.60950
Epoch 79: Val Loss 552.57166
Epoch 80: Val Loss 552.53381
Epoch 81: Val Loss 552.49591
Epoch 82: Val Loss 552.45789
Epoch 83: Val Loss 552.42004
Epoch 84: Val Loss 552.38251
Epoch 85: Val Loss 552.34467
Epoch 86: Val Loss 552.30676
Epoch 87: Val Loss 552.26904
Epoch 88: Val Loss 552.23102
Epoch 89: Val Loss 552.19275
Epoch 90: Val Loss 552.15460
Epoch 91: Val Loss 552.11658
Epoch 92: Val Loss 552.07855
Epoch 93: Val Loss 552.04047
Epoch 94: Val Loss 552.00256
Epoch 95: Val Loss 551.96472
Epoch 96: Val Loss 551.92719
Epoch 97: Val Loss 551.88959
Epoch 98: Val Loss 551.85187
Epoch 99: Val Loss 551.81390
{'MSE - mean': 551.8138727221142, 'MSE - std': 0.0, 'R2 - mean': -6.390204068398729, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 609.46381
Epoch 1: Val Loss 609.29840
Epoch 2: Val Loss 609.13306
Epoch 3: Val Loss 608.96912
Epoch 4: Val Loss 608.80548
Epoch 5: Val Loss 608.64130
Epoch 6: Val Loss 608.47766
Epoch 7: Val Loss 608.31659
Epoch 8: Val Loss 608.15668
Epoch 9: Val Loss 607.99371
Epoch 10: Val Loss 607.82928
Epoch 11: Val Loss 607.66418
Epoch 12: Val Loss 607.49811
Epoch 13: Val Loss 607.33234
Epoch 14: Val Loss 607.16766
Epoch 15: Val Loss 607.00165
Epoch 16: Val Loss 606.83716
Epoch 17: Val Loss 606.67230
Epoch 18: Val Loss 606.50763
Epoch 19: Val Loss 606.34344
Epoch 20: Val Loss 606.17963
Epoch 21: Val Loss 606.01721
Epoch 22: Val Loss 605.85455
Epoch 23: Val Loss 605.68854
Epoch 24: Val Loss 605.52246
Epoch 25: Val Loss 605.35687
Epoch 26: Val Loss 605.19330
Epoch 27: Val Loss 605.03125
Epoch 28: Val Loss 604.86810
Epoch 29: Val Loss 604.70325
Epoch 30: Val Loss 604.53772
Epoch 31: Val Loss 604.37225
Epoch 32: Val Loss 604.20514
Epoch 33: Val Loss 604.03998
Epoch 34: Val Loss 603.87872
Epoch 35: Val Loss 603.71594
Epoch 36: Val Loss 603.55328
Epoch 37: Val Loss 603.38849
Epoch 38: Val Loss 603.22247
Epoch 39: Val Loss 603.05670
Epoch 40: Val Loss 602.88916
Epoch 41: Val Loss 602.71796
Epoch 42: Val Loss 602.54913
Epoch 43: Val Loss 602.38123
Epoch 44: Val Loss 602.21289
Epoch 45: Val Loss 602.04047
Epoch 46: Val Loss 601.87341
Epoch 47: Val Loss 601.70728
Epoch 48: Val Loss 601.54089
Epoch 49: Val Loss 601.37408
Epoch 50: Val Loss 601.20990
Epoch 51: Val Loss 601.04822
Epoch 52: Val Loss 600.88538
Epoch 53: Val Loss 600.72333
Epoch 54: Val Loss 600.56091
Epoch 55: Val Loss 600.40021
Epoch 56: Val Loss 600.24292
Epoch 57: Val Loss 600.08405
Epoch 58: Val Loss 599.92633
Epoch 59: Val Loss 599.76813
Epoch 60: Val Loss 599.61224
Epoch 61: Val Loss 599.45953
Epoch 62: Val Loss 599.30627
Epoch 63: Val Loss 599.15393
Epoch 64: Val Loss 599.00006
Epoch 65: Val Loss 598.84760
Epoch 66: Val Loss 598.69501
Epoch 67: Val Loss 598.54102
Epoch 68: Val Loss 598.38959
Epoch 69: Val Loss 598.23749
Epoch 70: Val Loss 598.08740
Epoch 71: Val Loss 597.93457
Epoch 72: Val Loss 597.78137
Epoch 73: Val Loss 597.62598
Epoch 74: Val Loss 597.46844
Epoch 75: Val Loss 597.31378
Epoch 76: Val Loss 597.16248
Epoch 77: Val Loss 597.01141
Epoch 78: Val Loss 596.86255
Epoch 79: Val Loss 596.71509
Epoch 80: Val Loss 596.56842
Epoch 81: Val Loss 596.42194
Epoch 82: Val Loss 596.27747
Epoch 83: Val Loss 596.13110
Epoch 84: Val Loss 595.98474
Epoch 85: Val Loss 595.83966
Epoch 86: Val Loss 595.69458
Epoch 87: Val Loss 595.54846
Epoch 88: Val Loss 595.40448
Epoch 89: Val Loss 595.26111
Epoch 90: Val Loss 595.11835
Epoch 91: Val Loss 594.97290
Epoch 92: Val Loss 594.82745
Epoch 93: Val Loss 594.68585
Epoch 94: Val Loss 594.54535
Epoch 95: Val Loss 594.40424
Epoch 96: Val Loss 594.26562
Epoch 97: Val Loss 594.12616
Epoch 98: Val Loss 593.98608
Epoch 99: Val Loss 593.84619
{'MSE - mean': 572.8300461833892, 'MSE - std': 21.01617346127506, 'R2 - mean': -6.381750428605647, 'R2 - std': 0.008453639793081447} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 692.91785
Epoch 1: Val Loss 692.89203
Epoch 2: Val Loss 692.86615
Epoch 3: Val Loss 692.84033
Epoch 4: Val Loss 692.81451
Epoch 5: Val Loss 692.78845
Epoch 6: Val Loss 692.76221
Epoch 7: Val Loss 692.73584
Epoch 8: Val Loss 692.70947
Epoch 9: Val Loss 692.68298
Epoch 10: Val Loss 692.65662
Epoch 11: Val Loss 692.63025
Epoch 12: Val Loss 692.60400
Epoch 13: Val Loss 692.57751
Epoch 14: Val Loss 692.55072
Epoch 15: Val Loss 692.52411
Epoch 16: Val Loss 692.49725
Epoch 17: Val Loss 692.47046
Epoch 18: Val Loss 692.44336
Epoch 19: Val Loss 692.41632
Epoch 20: Val Loss 692.38904
Epoch 21: Val Loss 692.36182
Epoch 22: Val Loss 692.33447
Epoch 23: Val Loss 692.30725
Epoch 24: Val Loss 692.28003
Epoch 25: Val Loss 692.25287
Epoch 26: Val Loss 692.22540
Epoch 27: Val Loss 692.19794
Epoch 28: Val Loss 692.16998
Epoch 29: Val Loss 692.14185
Epoch 30: Val Loss 692.11353
Epoch 31: Val Loss 692.08533
Epoch 32: Val Loss 692.05725
Epoch 33: Val Loss 692.02893
Epoch 34: Val Loss 692.00061
Epoch 35: Val Loss 691.97223
Epoch 36: Val Loss 691.94360
Epoch 37: Val Loss 691.91461
Epoch 38: Val Loss 691.88574
Epoch 39: Val Loss 691.85657
Epoch 40: Val Loss 691.82721
Epoch 41: Val Loss 691.79755
Epoch 42: Val Loss 691.76776
Epoch 43: Val Loss 691.73798
Epoch 44: Val Loss 691.70807
Epoch 45: Val Loss 691.67822
Epoch 46: Val Loss 691.64838
Epoch 47: Val Loss 691.61804
Epoch 48: Val Loss 691.58771
Epoch 49: Val Loss 691.55707
Epoch 50: Val Loss 691.52612
Epoch 51: Val Loss 691.49493
Epoch 52: Val Loss 691.46393
Epoch 53: Val Loss 691.43280
Epoch 54: Val Loss 691.40161
Epoch 55: Val Loss 691.37018
Epoch 56: Val Loss 691.33881
Epoch 57: Val Loss 691.30756
Epoch 58: Val Loss 691.27612
Epoch 59: Val Loss 691.24445
Epoch 60: Val Loss 691.21222
Epoch 61: Val Loss 691.18024
Epoch 62: Val Loss 691.14801
Epoch 63: Val Loss 691.11578
Epoch 64: Val Loss 691.08356
Epoch 65: Val Loss 691.05121
Epoch 66: Val Loss 691.01874
Epoch 67: Val Loss 690.98645
Epoch 68: Val Loss 690.95380
Epoch 69: Val Loss 690.92108
Epoch 70: Val Loss 690.88831
Epoch 71: Val Loss 690.85535
Epoch 72: Val Loss 690.82245
Epoch 73: Val Loss 690.78961
Epoch 74: Val Loss 690.75659
Epoch 75: Val Loss 690.72333
Epoch 76: Val Loss 690.68951
Epoch 77: Val Loss 690.65503
Epoch 78: Val Loss 690.62091
Epoch 79: Val Loss 690.58679
Epoch 80: Val Loss 690.55243
Epoch 81: Val Loss 690.51801
Epoch 82: Val Loss 690.48334
Epoch 83: Val Loss 690.44873
Epoch 84: Val Loss 690.41400
Epoch 85: Val Loss 690.37885
Epoch 86: Val Loss 690.34344
Epoch 87: Val Loss 690.30768
Epoch 88: Val Loss 690.27142
Epoch 89: Val Loss 690.23529
Epoch 90: Val Loss 690.19891
Epoch 91: Val Loss 690.16241
Epoch 92: Val Loss 690.12598
Epoch 93: Val Loss 690.08893
Epoch 94: Val Loss 690.05182
Epoch 95: Val Loss 690.01422
Epoch 96: Val Loss 689.97681
Epoch 97: Val Loss 689.93927
Epoch 98: Val Loss 689.90186
Epoch 99: Val Loss 689.86401
{'MSE - mean': 611.8413718896162, 'MSE - std': 57.77733204309089, 'R2 - mean': -6.330740874834867, 'R2 - std': 0.07246786760780444} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 522.92078
Epoch 1: Val Loss 522.86530
Epoch 2: Val Loss 522.80994
Epoch 3: Val Loss 522.75458
Epoch 4: Val Loss 522.69965
Epoch 5: Val Loss 522.64520
Epoch 6: Val Loss 522.59106
Epoch 7: Val Loss 522.53735
Epoch 8: Val Loss 522.48383
Epoch 9: Val Loss 522.43066
Epoch 10: Val Loss 522.37775
Epoch 11: Val Loss 522.32495
Epoch 12: Val Loss 522.27283
Epoch 13: Val Loss 522.22083
Epoch 14: Val Loss 522.16882
Epoch 15: Val Loss 522.11725
Epoch 16: Val Loss 522.06555
Epoch 17: Val Loss 522.01385
Epoch 18: Val Loss 521.96216
Epoch 19: Val Loss 521.91138
Epoch 20: Val Loss 521.86096
Epoch 21: Val Loss 521.81036
Epoch 22: Val Loss 521.75909
Epoch 23: Val Loss 521.70825
Epoch 24: Val Loss 521.65857
Epoch 25: Val Loss 521.60956
Epoch 26: Val Loss 521.56079
Epoch 27: Val Loss 521.51172
Epoch 28: Val Loss 521.46216
Epoch 29: Val Loss 521.41266
Epoch 30: Val Loss 521.36261
Epoch 31: Val Loss 521.31293
Epoch 32: Val Loss 521.26331
Epoch 33: Val Loss 521.21320
Epoch 34: Val Loss 521.16321
Epoch 35: Val Loss 521.11438
Epoch 36: Val Loss 521.06708
Epoch 37: Val Loss 521.02014
Epoch 38: Val Loss 520.97272
Epoch 39: Val Loss 520.92480
Epoch 40: Val Loss 520.87732
Epoch 41: Val Loss 520.82971
Epoch 42: Val Loss 520.78333
Epoch 43: Val Loss 520.73669
Epoch 44: Val Loss 520.68958
Epoch 45: Val Loss 520.64294
Epoch 46: Val Loss 520.59583
Epoch 47: Val Loss 520.54926
Epoch 48: Val Loss 520.50208
Epoch 49: Val Loss 520.45428
Epoch 50: Val Loss 520.40656
Epoch 51: Val Loss 520.35919
Epoch 52: Val Loss 520.31165
Epoch 53: Val Loss 520.26367
Epoch 54: Val Loss 520.21582
Epoch 55: Val Loss 520.16846
Epoch 56: Val Loss 520.12256
Epoch 57: Val Loss 520.07727
Epoch 58: Val Loss 520.03223
Epoch 59: Val Loss 519.98761
Epoch 60: Val Loss 519.94330
Epoch 61: Val Loss 519.89966
Epoch 62: Val Loss 519.85754
Epoch 63: Val Loss 519.81592
Epoch 64: Val Loss 519.77448
Epoch 65: Val Loss 519.73285
Epoch 66: Val Loss 519.69073
Epoch 67: Val Loss 519.64911
Epoch 68: Val Loss 519.60883
Epoch 69: Val Loss 519.56940
Epoch 70: Val Loss 519.53009
Epoch 71: Val Loss 519.49139
Epoch 72: Val Loss 519.45264
Epoch 73: Val Loss 519.41418
Epoch 74: Val Loss 519.37579
Epoch 75: Val Loss 519.33734
Epoch 76: Val Loss 519.29944
Epoch 77: Val Loss 519.26208
Epoch 78: Val Loss 519.22565
Epoch 79: Val Loss 519.18964
Epoch 80: Val Loss 519.15369
Epoch 81: Val Loss 519.11755
Epoch 82: Val Loss 519.08173
Epoch 83: Val Loss 519.04565
Epoch 84: Val Loss 519.00946
Epoch 85: Val Loss 518.97369
Epoch 86: Val Loss 518.93848
Epoch 87: Val Loss 518.90540
Epoch 88: Val Loss 518.87280
Epoch 89: Val Loss 518.84064
Epoch 90: Val Loss 518.80933
Epoch 91: Val Loss 518.77802
Epoch 92: Val Loss 518.74658
Epoch 93: Val Loss 518.71533
Epoch 94: Val Loss 518.68457
Epoch 95: Val Loss 518.65411
Epoch 96: Val Loss 518.62396
Epoch 97: Val Loss 518.59381
Epoch 98: Val Loss 518.56342
Epoch 99: Val Loss 518.53320
{'MSE - mean': 588.5143291206678, 'MSE - std': 64.31265697004156, 'R2 - mean': -6.216064217832727, 'R2 - std': 0.20830482678103424} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 572.85327
Epoch 1: Val Loss 572.77600
Epoch 2: Val Loss 572.69885
Epoch 3: Val Loss 572.62177
Epoch 4: Val Loss 572.54401
Epoch 5: Val Loss 572.46643
Epoch 6: Val Loss 572.38812
Epoch 7: Val Loss 572.30939
Epoch 8: Val Loss 572.23047
Epoch 9: Val Loss 572.15247
Epoch 10: Val Loss 572.07434
Epoch 11: Val Loss 571.99512
Epoch 12: Val Loss 571.91534
Epoch 13: Val Loss 571.83545
Epoch 14: Val Loss 571.75592
Epoch 15: Val Loss 571.67645
Epoch 16: Val Loss 571.59650
Epoch 17: Val Loss 571.51556
Epoch 18: Val Loss 571.43481
Epoch 19: Val Loss 571.35394
Epoch 20: Val Loss 571.27197
Epoch 21: Val Loss 571.18994
Epoch 22: Val Loss 571.10748
Epoch 23: Val Loss 571.02521
Epoch 24: Val Loss 570.94257
Epoch 25: Val Loss 570.85919
Epoch 26: Val Loss 570.77612
Epoch 27: Val Loss 570.69354
Epoch 28: Val Loss 570.61053
Epoch 29: Val Loss 570.52753
Epoch 30: Val Loss 570.44507
Epoch 31: Val Loss 570.36206
Epoch 32: Val Loss 570.27850
Epoch 33: Val Loss 570.19531
Epoch 34: Val Loss 570.11212
Epoch 35: Val Loss 570.02930
Epoch 36: Val Loss 569.94574
Epoch 37: Val Loss 569.86182
Epoch 38: Val Loss 569.77734
Epoch 39: Val Loss 569.69183
Epoch 40: Val Loss 569.60645
Epoch 41: Val Loss 569.52051
Epoch 42: Val Loss 569.43451
Epoch 43: Val Loss 569.34839
Epoch 44: Val Loss 569.26208
Epoch 45: Val Loss 569.17487
Epoch 46: Val Loss 569.08710
Epoch 47: Val Loss 568.99896
Epoch 48: Val Loss 568.90924
Epoch 49: Val Loss 568.81921
Epoch 50: Val Loss 568.72833
Epoch 51: Val Loss 568.63812
Epoch 52: Val Loss 568.54871
Epoch 53: Val Loss 568.46021
Epoch 54: Val Loss 568.37054
Epoch 55: Val Loss 568.28107
Epoch 56: Val Loss 568.19067
Epoch 57: Val Loss 568.09918
Epoch 58: Val Loss 568.00806
Epoch 59: Val Loss 567.91699
Epoch 60: Val Loss 567.82404
Epoch 61: Val Loss 567.73108
Epoch 62: Val Loss 567.63806
Epoch 63: Val Loss 567.54474
Epoch 64: Val Loss 567.45038
Epoch 65: Val Loss 567.35480
Epoch 66: Val Loss 567.25879
Epoch 67: Val Loss 567.16333
Epoch 68: Val Loss 567.06726
Epoch 69: Val Loss 566.97076
Epoch 70: Val Loss 566.87421
Epoch 71: Val Loss 566.77753
Epoch 72: Val Loss 566.68213
Epoch 73: Val Loss 566.58740
Epoch 74: Val Loss 566.49152
Epoch 75: Val Loss 566.39380
Epoch 76: Val Loss 566.29584
Epoch 77: Val Loss 566.19745
Epoch 78: Val Loss 566.09961
Epoch 79: Val Loss 566.00140
Epoch 80: Val Loss 565.90283
Epoch 81: Val Loss 565.80389
Epoch 82: Val Loss 565.70349
Epoch 83: Val Loss 565.60181
Epoch 84: Val Loss 565.49878
Epoch 85: Val Loss 565.39551
Epoch 86: Val Loss 565.29169
Epoch 87: Val Loss 565.18750
Epoch 88: Val Loss 565.08240
Epoch 89: Val Loss 564.97449
Epoch 90: Val Loss 564.86493
Epoch 91: Val Loss 564.75543
Epoch 92: Val Loss 564.64740
Epoch 93: Val Loss 564.54047
Epoch 94: Val Loss 564.43414
Epoch 95: Val Loss 564.32825
Epoch 96: Val Loss 564.22070
Epoch 97: Val Loss 564.11279
Epoch 98: Val Loss 564.00269
Epoch 99: Val Loss 563.89130
{'MSE - mean': 583.5897250376399, 'MSE - std': 58.36009920086911, 'R2 - mean': -6.0084193934249495, 'R2 - std': 0.4551683349065152} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 583.5897250376399 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 559.73401
Epoch 1: Val Loss 559.36951
Epoch 2: Val Loss 558.97052
Epoch 3: Val Loss 558.54578
Epoch 4: Val Loss 558.08521
Epoch 5: Val Loss 557.59161
Epoch 6: Val Loss 557.04132
Epoch 7: Val Loss 556.44580
Epoch 8: Val Loss 555.79089
Epoch 9: Val Loss 555.05194
Epoch 10: Val Loss 554.23871
Epoch 11: Val Loss 553.34436
Epoch 12: Val Loss 552.36615
Epoch 13: Val Loss 551.30341
Epoch 14: Val Loss 550.13855
Epoch 15: Val Loss 548.89929
Epoch 16: Val Loss 547.55524
Epoch 17: Val Loss 546.10632
Epoch 18: Val Loss 544.53436
Epoch 19: Val Loss 542.82965
Epoch 20: Val Loss 541.00580
Epoch 21: Val Loss 539.05499
Epoch 22: Val Loss 536.94824
Epoch 23: Val Loss 534.69336
Epoch 24: Val Loss 532.27148
Epoch 25: Val Loss 529.68146
Epoch 26: Val Loss 526.91064
Epoch 27: Val Loss 523.95929
Epoch 28: Val Loss 520.79407
Epoch 29: Val Loss 517.39502
Epoch 30: Val Loss 513.79370
Epoch 31: Val Loss 509.97260
Epoch 32: Val Loss 505.87448
Epoch 33: Val Loss 501.51843
Epoch 34: Val Loss 496.84500
Epoch 35: Val Loss 491.86215
Epoch 36: Val Loss 486.46115
Epoch 37: Val Loss 480.66293
Epoch 38: Val Loss 474.42212
Epoch 39: Val Loss 467.81406
Epoch 40: Val Loss 460.81259
Epoch 41: Val Loss 453.29398
Epoch 42: Val Loss 445.36072
Epoch 43: Val Loss 437.00613
Epoch 44: Val Loss 428.21384
Epoch 45: Val Loss 419.06226
Epoch 46: Val Loss 409.62494
Epoch 47: Val Loss 399.79456
Epoch 48: Val Loss 389.83771
Epoch 49: Val Loss 379.62741
Epoch 50: Val Loss 369.06671
Epoch 51: Val Loss 358.46042
Epoch 52: Val Loss 347.72656
Epoch 53: Val Loss 336.76770
Epoch 54: Val Loss 325.76227
Epoch 55: Val Loss 314.28732
Epoch 56: Val Loss 302.81906
Epoch 57: Val Loss 291.37732
Epoch 58: Val Loss 279.90552
Epoch 59: Val Loss 268.32169
Epoch 60: Val Loss 256.98700
Epoch 61: Val Loss 245.87283
Epoch 62: Val Loss 234.96138
Epoch 63: Val Loss 224.29933
Epoch 64: Val Loss 214.05063
Epoch 65: Val Loss 204.10840
Epoch 66: Val Loss 194.65955
Epoch 67: Val Loss 185.69504
Epoch 68: Val Loss 176.95581
Epoch 69: Val Loss 168.70911
Epoch 70: Val Loss 161.01746
Epoch 71: Val Loss 153.91968
Epoch 72: Val Loss 147.31735
Epoch 73: Val Loss 141.30986
Epoch 74: Val Loss 135.58560
Epoch 75: Val Loss 130.29875
Epoch 76: Val Loss 125.44344
Epoch 77: Val Loss 121.10081
Epoch 78: Val Loss 117.22771
Epoch 79: Val Loss 113.66131
Epoch 80: Val Loss 110.49191
Epoch 81: Val Loss 107.64261
Epoch 82: Val Loss 104.96848
Epoch 83: Val Loss 102.47816
Epoch 84: Val Loss 100.11079
Epoch 85: Val Loss 97.95229
Epoch 86: Val Loss 95.96189
Epoch 87: Val Loss 94.10387
Epoch 88: Val Loss 92.31313
Epoch 89: Val Loss 90.62469
Epoch 90: Val Loss 88.99414
Epoch 91: Val Loss 87.45931
Epoch 92: Val Loss 85.99237
Epoch 93: Val Loss 84.63990
Epoch 94: Val Loss 83.30164
Epoch 95: Val Loss 81.97578
Epoch 96: Val Loss 80.57743
Epoch 97: Val Loss 79.26895
Epoch 98: Val Loss 78.08293
Epoch 99: Val Loss 76.99126
{'MSE - mean': 76.99126221653441, 'MSE - std': 0.0, 'R2 - mean': -0.03111061064301479, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 587.92773
Epoch 1: Val Loss 587.15295
Epoch 2: Val Loss 586.41168
Epoch 3: Val Loss 585.69843
Epoch 4: Val Loss 585.01025
Epoch 5: Val Loss 584.34869
Epoch 6: Val Loss 583.71722
Epoch 7: Val Loss 583.09705
Epoch 8: Val Loss 582.49213
Epoch 9: Val Loss 581.90521
Epoch 10: Val Loss 581.34100
Epoch 11: Val Loss 580.78369
Epoch 12: Val Loss 580.23621
Epoch 13: Val Loss 579.69366
Epoch 14: Val Loss 579.15192
Epoch 15: Val Loss 578.60150
Epoch 16: Val Loss 578.04309
Epoch 17: Val Loss 577.47015
Epoch 18: Val Loss 576.88196
Epoch 19: Val Loss 576.27130
Epoch 20: Val Loss 575.63593
Epoch 21: Val Loss 575.00635
Epoch 22: Val Loss 574.38873
Epoch 23: Val Loss 573.77783
Epoch 24: Val Loss 573.15570
Epoch 25: Val Loss 572.52032
Epoch 26: Val Loss 571.87311
Epoch 27: Val Loss 571.21277
Epoch 28: Val Loss 570.54108
Epoch 29: Val Loss 569.83984
Epoch 30: Val Loss 569.11481
Epoch 31: Val Loss 568.34283
Epoch 32: Val Loss 567.52893
Epoch 33: Val Loss 566.67542
Epoch 34: Val Loss 565.77600
Epoch 35: Val Loss 564.82330
Epoch 36: Val Loss 563.81586
Epoch 37: Val Loss 562.76440
Epoch 38: Val Loss 561.65167
Epoch 39: Val Loss 560.49811
Epoch 40: Val Loss 559.28796
Epoch 41: Val Loss 558.00635
Epoch 42: Val Loss 556.63873
Epoch 43: Val Loss 555.19189
Epoch 44: Val Loss 553.66388
Epoch 45: Val Loss 552.04626
Epoch 46: Val Loss 550.37292
Epoch 47: Val Loss 548.64319
Epoch 48: Val Loss 546.79962
Epoch 49: Val Loss 544.85529
Epoch 50: Val Loss 542.81134
Epoch 51: Val Loss 540.67908
Epoch 52: Val Loss 538.45203
Epoch 53: Val Loss 536.09821
Epoch 54: Val Loss 533.64594
Epoch 55: Val Loss 531.02307
Epoch 56: Val Loss 528.26324
Epoch 57: Val Loss 525.38312
Epoch 58: Val Loss 522.35211
Epoch 59: Val Loss 519.22400
Epoch 60: Val Loss 515.99628
Epoch 61: Val Loss 512.60315
Epoch 62: Val Loss 509.04657
Epoch 63: Val Loss 505.34436
Epoch 64: Val Loss 501.42838
Epoch 65: Val Loss 497.38635
Epoch 66: Val Loss 493.18765
Epoch 67: Val Loss 488.83298
Epoch 68: Val Loss 484.29916
Epoch 69: Val Loss 479.68216
Epoch 70: Val Loss 474.91321
Epoch 71: Val Loss 469.97910
Epoch 72: Val Loss 464.89258
Epoch 73: Val Loss 459.56458
Epoch 74: Val Loss 454.05762
Epoch 75: Val Loss 448.34604
Epoch 76: Val Loss 442.53125
Epoch 77: Val Loss 436.50204
Epoch 78: Val Loss 430.32785
Epoch 79: Val Loss 424.04153
Epoch 80: Val Loss 417.66110
Epoch 81: Val Loss 411.08926
Epoch 82: Val Loss 404.38229
Epoch 83: Val Loss 397.52930
Epoch 84: Val Loss 390.57751
Epoch 85: Val Loss 383.43594
Epoch 86: Val Loss 376.21906
Epoch 87: Val Loss 369.00882
Epoch 88: Val Loss 361.56400
Epoch 89: Val Loss 354.06744
Epoch 90: Val Loss 346.59879
Epoch 91: Val Loss 339.11517
Epoch 92: Val Loss 331.72046
Epoch 93: Val Loss 324.20184
Epoch 94: Val Loss 316.69217
Epoch 95: Val Loss 309.13757
Epoch 96: Val Loss 301.54599
Epoch 97: Val Loss 294.02084
Epoch 98: Val Loss 286.50089
Epoch 99: Val Loss 278.93106
{'MSE - mean': 177.96116633512503, 'MSE - std': 100.96990411859059, 'R2 - mean': -1.247183375048929, 'R2 - std': 1.2160727644059142} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 677.58472
Epoch 1: Val Loss 676.96442
Epoch 2: Val Loss 676.28790
Epoch 3: Val Loss 675.56128
Epoch 4: Val Loss 674.76965
Epoch 5: Val Loss 673.91235
Epoch 6: Val Loss 673.00470
Epoch 7: Val Loss 672.03845
Epoch 8: Val Loss 671.04236
Epoch 9: Val Loss 669.98712
Epoch 10: Val Loss 668.87067
Epoch 11: Val Loss 667.67365
Epoch 12: Val Loss 666.43353
Epoch 13: Val Loss 665.13110
Epoch 14: Val Loss 663.76825
Epoch 15: Val Loss 662.32098
Epoch 16: Val Loss 660.79395
Epoch 17: Val Loss 659.20343
Epoch 18: Val Loss 657.50214
Epoch 19: Val Loss 655.73364
Epoch 20: Val Loss 653.87408
Epoch 21: Val Loss 651.91431
Epoch 22: Val Loss 649.82690
Epoch 23: Val Loss 647.63434
Epoch 24: Val Loss 645.30847
Epoch 25: Val Loss 642.81940
Epoch 26: Val Loss 640.17694
Epoch 27: Val Loss 637.36688
Epoch 28: Val Loss 634.38342
Epoch 29: Val Loss 631.26447
Epoch 30: Val Loss 627.97229
Epoch 31: Val Loss 624.56055
Epoch 32: Val Loss 621.04950
Epoch 33: Val Loss 617.37280
Epoch 34: Val Loss 613.56073
Epoch 35: Val Loss 609.58990
Epoch 36: Val Loss 605.53180
Epoch 37: Val Loss 601.32593
Epoch 38: Val Loss 596.91791
Epoch 39: Val Loss 592.23694
Epoch 40: Val Loss 587.36218
Epoch 41: Val Loss 582.22766
Epoch 42: Val Loss 576.83191
Epoch 43: Val Loss 571.07617
Epoch 44: Val Loss 565.18805
Epoch 45: Val Loss 559.08350
Epoch 46: Val Loss 552.72192
Epoch 47: Val Loss 546.17792
Epoch 48: Val Loss 539.53485
Epoch 49: Val Loss 532.69727
Epoch 50: Val Loss 525.78583
Epoch 51: Val Loss 518.56879
Epoch 52: Val Loss 511.11841
Epoch 53: Val Loss 503.61676
Epoch 54: Val Loss 495.92007
Epoch 55: Val Loss 488.03326
Epoch 56: Val Loss 480.07318
Epoch 57: Val Loss 471.90521
Epoch 58: Val Loss 463.53876
Epoch 59: Val Loss 454.82516
Epoch 60: Val Loss 446.07889
Epoch 61: Val Loss 437.21185
Epoch 62: Val Loss 428.22510
Epoch 63: Val Loss 419.04443
Epoch 64: Val Loss 409.74191
Epoch 65: Val Loss 400.43420
Epoch 66: Val Loss 391.25787
Epoch 67: Val Loss 382.09665
Epoch 68: Val Loss 372.70853
Epoch 69: Val Loss 363.24838
Epoch 70: Val Loss 353.75665
Epoch 71: Val Loss 344.14471
Epoch 72: Val Loss 334.81036
Epoch 73: Val Loss 325.58423
Epoch 74: Val Loss 316.53854
Epoch 75: Val Loss 307.69199
Epoch 76: Val Loss 298.97061
Epoch 77: Val Loss 290.38220
Epoch 78: Val Loss 281.82684
Epoch 79: Val Loss 273.46091
Epoch 80: Val Loss 265.14816
Epoch 81: Val Loss 257.09677
Epoch 82: Val Loss 249.34839
Epoch 83: Val Loss 241.81383
Epoch 84: Val Loss 234.68607
Epoch 85: Val Loss 227.90807
Epoch 86: Val Loss 221.45786
Epoch 87: Val Loss 215.31215
Epoch 88: Val Loss 209.44980
Epoch 89: Val Loss 203.82033
Epoch 90: Val Loss 198.57312
Epoch 91: Val Loss 193.63153
Epoch 92: Val Loss 188.91731
Epoch 93: Val Loss 184.51012
Epoch 94: Val Loss 180.27451
Epoch 95: Val Loss 176.29807
Epoch 96: Val Loss 172.67616
Epoch 97: Val Loss 169.25975
Epoch 98: Val Loss 166.07596
Epoch 99: Val Loss 163.09738
{'MSE - mean': 173.00657020570875, 'MSE - std': 82.7388083313357, 'R2 - mean': -1.0677927581953597, 'R2 - std': 1.0248173653795276} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 568.71381
Epoch 1: Val Loss 568.11249
Epoch 2: Val Loss 567.53033
Epoch 3: Val Loss 566.95789
Epoch 4: Val Loss 566.39612
Epoch 5: Val Loss 565.82849
Epoch 6: Val Loss 565.25909
Epoch 7: Val Loss 564.68207
Epoch 8: Val Loss 564.09088
Epoch 9: Val Loss 563.48077
Epoch 10: Val Loss 562.84631
Epoch 11: Val Loss 562.17804
Epoch 12: Val Loss 561.47723
Epoch 13: Val Loss 560.74121
Epoch 14: Val Loss 559.96814
Epoch 15: Val Loss 559.15393
Epoch 16: Val Loss 558.28821
Epoch 17: Val Loss 557.36346
Epoch 18: Val Loss 556.39679
Epoch 19: Val Loss 555.36548
Epoch 20: Val Loss 554.25714
Epoch 21: Val Loss 553.08276
Epoch 22: Val Loss 551.85010
Epoch 23: Val Loss 550.54205
Epoch 24: Val Loss 549.15143
Epoch 25: Val Loss 547.65228
Epoch 26: Val Loss 546.07043
Epoch 27: Val Loss 544.39392
Epoch 28: Val Loss 542.61517
Epoch 29: Val Loss 540.76038
Epoch 30: Val Loss 538.80640
Epoch 31: Val Loss 536.75629
Epoch 32: Val Loss 534.58331
Epoch 33: Val Loss 532.27350
Epoch 34: Val Loss 529.80176
Epoch 35: Val Loss 527.18860
Epoch 36: Val Loss 524.47736
Epoch 37: Val Loss 521.62701
Epoch 38: Val Loss 518.62750
Epoch 39: Val Loss 515.47272
Epoch 40: Val Loss 512.17786
Epoch 41: Val Loss 508.65903
Epoch 42: Val Loss 504.94452
Epoch 43: Val Loss 501.03574
Epoch 44: Val Loss 496.91672
Epoch 45: Val Loss 492.59274
Epoch 46: Val Loss 488.01886
Epoch 47: Val Loss 483.19632
Epoch 48: Val Loss 478.12793
Epoch 49: Val Loss 472.82166
Epoch 50: Val Loss 467.25839
Epoch 51: Val Loss 461.41901
Epoch 52: Val Loss 455.31784
Epoch 53: Val Loss 448.92432
Epoch 54: Val Loss 442.27029
Epoch 55: Val Loss 435.37082
Epoch 56: Val Loss 428.18689
Epoch 57: Val Loss 420.63721
Epoch 58: Val Loss 412.79779
Epoch 59: Val Loss 404.71744
Epoch 60: Val Loss 396.42770
Epoch 61: Val Loss 387.84604
Epoch 62: Val Loss 378.93958
Epoch 63: Val Loss 369.82608
Epoch 64: Val Loss 360.42596
Epoch 65: Val Loss 350.80807
Epoch 66: Val Loss 340.79385
Epoch 67: Val Loss 330.49023
Epoch 68: Val Loss 320.09946
Epoch 69: Val Loss 309.53198
Epoch 70: Val Loss 298.70923
Epoch 71: Val Loss 287.71432
Epoch 72: Val Loss 276.62589
Epoch 73: Val Loss 265.41046
Epoch 74: Val Loss 254.09294
Epoch 75: Val Loss 242.89894
Epoch 76: Val Loss 231.85336
Epoch 77: Val Loss 220.89832
Epoch 78: Val Loss 209.92009
Epoch 79: Val Loss 199.29839
Epoch 80: Val Loss 188.90431
Epoch 81: Val Loss 178.88142
Epoch 82: Val Loss 169.20586
Epoch 83: Val Loss 159.88382
Epoch 84: Val Loss 150.93758
Epoch 85: Val Loss 142.52841
Epoch 86: Val Loss 134.66217
Epoch 87: Val Loss 127.14770
Epoch 88: Val Loss 119.99461
Epoch 89: Val Loss 113.52100
Epoch 90: Val Loss 107.54182
Epoch 91: Val Loss 102.03374
Epoch 92: Val Loss 97.00934
Epoch 93: Val Loss 92.46992
Epoch 94: Val Loss 88.33091
Epoch 95: Val Loss 84.57127
Epoch 96: Val Loss 81.29966
Epoch 97: Val Loss 78.25834
Epoch 98: Val Loss 75.77510
Epoch 99: Val Loss 73.64559
{'MSE - mean': 148.1663264011898, 'MSE - std': 83.5786811326367, 'R2 - mean': -0.7948477733056484, 'R2 - std': 1.0055768834937044} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 580.28564
Epoch 1: Val Loss 579.50604
Epoch 2: Val Loss 578.72333
Epoch 3: Val Loss 577.95587
Epoch 4: Val Loss 577.20923
Epoch 5: Val Loss 576.46796
Epoch 6: Val Loss 575.71649
Epoch 7: Val Loss 574.96234
Epoch 8: Val Loss 574.21844
Epoch 9: Val Loss 573.47095
Epoch 10: Val Loss 572.71191
Epoch 11: Val Loss 571.94629
Epoch 12: Val Loss 571.18005
Epoch 13: Val Loss 570.40436
Epoch 14: Val Loss 569.62140
Epoch 15: Val Loss 568.82660
Epoch 16: Val Loss 568.00330
Epoch 17: Val Loss 567.15973
Epoch 18: Val Loss 566.27930
Epoch 19: Val Loss 565.37506
Epoch 20: Val Loss 564.44824
Epoch 21: Val Loss 563.49463
Epoch 22: Val Loss 562.51251
Epoch 23: Val Loss 561.50696
Epoch 24: Val Loss 560.46539
Epoch 25: Val Loss 559.37738
Epoch 26: Val Loss 558.26196
Epoch 27: Val Loss 557.11572
Epoch 28: Val Loss 555.92694
Epoch 29: Val Loss 554.69019
Epoch 30: Val Loss 553.40704
Epoch 31: Val Loss 552.06830
Epoch 32: Val Loss 550.66315
Epoch 33: Val Loss 549.21600
Epoch 34: Val Loss 547.68964
Epoch 35: Val Loss 546.12976
Epoch 36: Val Loss 544.52490
Epoch 37: Val Loss 542.85052
Epoch 38: Val Loss 541.11334
Epoch 39: Val Loss 539.31268
Epoch 40: Val Loss 537.42847
Epoch 41: Val Loss 535.48224
Epoch 42: Val Loss 533.43909
Epoch 43: Val Loss 531.34991
Epoch 44: Val Loss 529.14929
Epoch 45: Val Loss 526.89539
Epoch 46: Val Loss 524.59149
Epoch 47: Val Loss 522.18781
Epoch 48: Val Loss 519.69312
Epoch 49: Val Loss 517.09760
Epoch 50: Val Loss 514.42169
Epoch 51: Val Loss 511.66074
Epoch 52: Val Loss 508.75455
Epoch 53: Val Loss 505.68549
Epoch 54: Val Loss 502.53085
Epoch 55: Val Loss 499.21448
Epoch 56: Val Loss 495.81891
Epoch 57: Val Loss 492.34018
Epoch 58: Val Loss 488.74704
Epoch 59: Val Loss 485.00870
Epoch 60: Val Loss 481.14917
Epoch 61: Val Loss 477.29666
Epoch 62: Val Loss 473.36697
Epoch 63: Val Loss 469.26141
Epoch 64: Val Loss 464.98395
Epoch 65: Val Loss 460.49420
Epoch 66: Val Loss 455.85944
Epoch 67: Val Loss 451.12823
Epoch 68: Val Loss 446.33630
Epoch 69: Val Loss 441.42743
Epoch 70: Val Loss 436.30121
Epoch 71: Val Loss 431.07849
Epoch 72: Val Loss 425.70953
Epoch 73: Val Loss 420.31427
Epoch 74: Val Loss 414.81152
Epoch 75: Val Loss 409.14697
Epoch 76: Val Loss 403.31729
Epoch 77: Val Loss 397.38034
Epoch 78: Val Loss 391.30258
Epoch 79: Val Loss 385.26505
Epoch 80: Val Loss 379.16165
Epoch 81: Val Loss 372.97238
Epoch 82: Val Loss 366.60294
Epoch 83: Val Loss 360.22980
Epoch 84: Val Loss 353.80228
Epoch 85: Val Loss 347.33475
Epoch 86: Val Loss 340.76532
Epoch 87: Val Loss 334.26700
Epoch 88: Val Loss 327.77457
Epoch 89: Val Loss 321.25159
Epoch 90: Val Loss 314.87521
Epoch 91: Val Loss 308.55591
Epoch 92: Val Loss 302.07175
Epoch 93: Val Loss 295.63403
Epoch 94: Val Loss 289.17294
Epoch 95: Val Loss 282.68170
Epoch 96: Val Loss 276.29224
Epoch 97: Val Loss 269.97952
Epoch 98: Val Loss 263.78577
Epoch 99: Val Loss 257.53085
{'MSE - mean': 170.03922748312547, 'MSE - std': 86.61415564939338, 'R2 - mean': -1.0001657946728106, 'R2 - std': 0.9887213229651896} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 170.03922748312547 and parameters: {'dim': 128, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 558.67365
Epoch 1: Val Loss 558.65594
Epoch 2: Val Loss 558.63837
Epoch 3: Val Loss 558.62079
Epoch 4: Val Loss 558.60333
Epoch 5: Val Loss 558.58582
Epoch 6: Val Loss 558.56830
Epoch 7: Val Loss 558.55090
Epoch 8: Val Loss 558.53339
Epoch 9: Val Loss 558.51593
Epoch 10: Val Loss 558.49854
Epoch 11: Val Loss 558.48096
Epoch 12: Val Loss 558.46326
Epoch 13: Val Loss 558.44550
Epoch 14: Val Loss 558.42773
Epoch 15: Val Loss 558.41034
Epoch 16: Val Loss 558.39276
Epoch 17: Val Loss 558.37518
Epoch 18: Val Loss 558.35748
Epoch 19: Val Loss 558.33984
Epoch 20: Val Loss 558.32233
Epoch 21: Val Loss 558.30469
Epoch 22: Val Loss 558.28711
Epoch 23: Val Loss 558.26935
Epoch 24: Val Loss 558.25171
Epoch 25: Val Loss 558.23407
Epoch 26: Val Loss 558.21631
Epoch 27: Val Loss 558.19855
Epoch 28: Val Loss 558.18079
Epoch 29: Val Loss 558.16302
Epoch 30: Val Loss 558.14514
Epoch 31: Val Loss 558.12708
Epoch 32: Val Loss 558.10870
Epoch 33: Val Loss 558.09039
Epoch 34: Val Loss 558.07239
Epoch 35: Val Loss 558.05426
Epoch 36: Val Loss 558.03607
Epoch 37: Val Loss 558.01794
Epoch 38: Val Loss 557.99939
Epoch 39: Val Loss 557.98059
Epoch 40: Val Loss 557.96106
Epoch 41: Val Loss 557.94098
Epoch 42: Val Loss 557.92029
Epoch 43: Val Loss 557.89874
Epoch 44: Val Loss 557.87683
Epoch 45: Val Loss 557.85455
Epoch 46: Val Loss 557.82990
Epoch 47: Val Loss 557.80463
Epoch 48: Val Loss 557.77948
Epoch 49: Val Loss 557.75446
Epoch 50: Val Loss 557.72852
Epoch 51: Val Loss 557.70300
Epoch 52: Val Loss 557.67755
Epoch 53: Val Loss 557.65216
Epoch 54: Val Loss 557.62646
Epoch 55: Val Loss 557.60052
Epoch 56: Val Loss 557.57452
Epoch 57: Val Loss 557.54681
Epoch 58: Val Loss 557.51862
Epoch 59: Val Loss 557.49121
Epoch 60: Val Loss 557.46338
Epoch 61: Val Loss 557.43494
Epoch 62: Val Loss 557.40607
Epoch 63: Val Loss 557.37714
Epoch 64: Val Loss 557.34814
Epoch 65: Val Loss 557.31793
Epoch 66: Val Loss 557.28656
Epoch 67: Val Loss 557.25482
Epoch 68: Val Loss 557.22272
Epoch 69: Val Loss 557.18951
Epoch 70: Val Loss 557.15491
Epoch 71: Val Loss 557.11969
Epoch 72: Val Loss 557.08319
Epoch 73: Val Loss 557.04401
Epoch 74: Val Loss 557.00415
Epoch 75: Val Loss 556.96246
Epoch 76: Val Loss 556.91809
Epoch 77: Val Loss 556.87347
Epoch 78: Val Loss 556.82794
Epoch 79: Val Loss 556.78076
Epoch 80: Val Loss 556.73242
Epoch 81: Val Loss 556.68262
Epoch 82: Val Loss 556.63147
Epoch 83: Val Loss 556.57965
Epoch 84: Val Loss 556.52576
Epoch 85: Val Loss 556.46979
Epoch 86: Val Loss 556.41058
Epoch 87: Val Loss 556.35040
Epoch 88: Val Loss 556.28809
Epoch 89: Val Loss 556.22449
Epoch 90: Val Loss 556.15863
Epoch 91: Val Loss 556.08954
Epoch 92: Val Loss 556.02045
Epoch 93: Val Loss 555.94965
Epoch 94: Val Loss 555.87799
Epoch 95: Val Loss 555.80548
Epoch 96: Val Loss 555.73254
Epoch 97: Val Loss 555.65778
Epoch 98: Val Loss 555.58167
Epoch 99: Val Loss 555.50574
{'MSE - mean': 555.5057281583123, 'MSE - std': 0.0, 'R2 - mean': -6.4396474883873225, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 591.30579
Epoch 1: Val Loss 591.23022
Epoch 2: Val Loss 591.15582
Epoch 3: Val Loss 591.08185
Epoch 4: Val Loss 591.00836
Epoch 5: Val Loss 590.93665
Epoch 6: Val Loss 590.86542
Epoch 7: Val Loss 590.79657
Epoch 8: Val Loss 590.72906
Epoch 9: Val Loss 590.66156
Epoch 10: Val Loss 590.59497
Epoch 11: Val Loss 590.52850
Epoch 12: Val Loss 590.46185
Epoch 13: Val Loss 590.39740
Epoch 14: Val Loss 590.33392
Epoch 15: Val Loss 590.27039
Epoch 16: Val Loss 590.20758
Epoch 17: Val Loss 590.14563
Epoch 18: Val Loss 590.08368
Epoch 19: Val Loss 590.02319
Epoch 20: Val Loss 589.96307
Epoch 21: Val Loss 589.90271
Epoch 22: Val Loss 589.84320
Epoch 23: Val Loss 589.78406
Epoch 24: Val Loss 589.72479
Epoch 25: Val Loss 589.66516
Epoch 26: Val Loss 589.60474
Epoch 27: Val Loss 589.54504
Epoch 28: Val Loss 589.48566
Epoch 29: Val Loss 589.42657
Epoch 30: Val Loss 589.36829
Epoch 31: Val Loss 589.31036
Epoch 32: Val Loss 589.25153
Epoch 33: Val Loss 589.19452
Epoch 34: Val Loss 589.13898
Epoch 35: Val Loss 589.08301
Epoch 36: Val Loss 589.02850
Epoch 37: Val Loss 588.97430
Epoch 38: Val Loss 588.91949
Epoch 39: Val Loss 588.86481
Epoch 40: Val Loss 588.80994
Epoch 41: Val Loss 588.75507
Epoch 42: Val Loss 588.70105
Epoch 43: Val Loss 588.64771
Epoch 44: Val Loss 588.59540
Epoch 45: Val Loss 588.54266
Epoch 46: Val Loss 588.49023
Epoch 47: Val Loss 588.43817
Epoch 48: Val Loss 588.38641
Epoch 49: Val Loss 588.33466
Epoch 50: Val Loss 588.28320
Epoch 51: Val Loss 588.23236
Epoch 52: Val Loss 588.18073
Epoch 53: Val Loss 588.12891
Epoch 54: Val Loss 588.07709
Epoch 55: Val Loss 588.02454
Epoch 56: Val Loss 587.97150
Epoch 57: Val Loss 587.91895
Epoch 58: Val Loss 587.86707
Epoch 59: Val Loss 587.81653
Epoch 60: Val Loss 587.76575
Epoch 61: Val Loss 587.71466
Epoch 62: Val Loss 587.66425
Epoch 63: Val Loss 587.61469
Epoch 64: Val Loss 587.56653
Epoch 65: Val Loss 587.51904
Epoch 66: Val Loss 587.47351
Epoch 67: Val Loss 587.42773
Epoch 68: Val Loss 587.38226
Epoch 69: Val Loss 587.33710
Epoch 70: Val Loss 587.29236
Epoch 71: Val Loss 587.24768
Epoch 72: Val Loss 587.20428
Epoch 73: Val Loss 587.16144
Epoch 74: Val Loss 587.11847
Epoch 75: Val Loss 587.07538
Epoch 76: Val Loss 587.03223
Epoch 77: Val Loss 586.98907
Epoch 78: Val Loss 586.94592
Epoch 79: Val Loss 586.90375
Epoch 80: Val Loss 586.86212
Epoch 81: Val Loss 586.82159
Epoch 82: Val Loss 586.78052
Epoch 83: Val Loss 586.73999
Epoch 84: Val Loss 586.69934
Epoch 85: Val Loss 586.65808
Epoch 86: Val Loss 586.61694
Epoch 87: Val Loss 586.57574
Epoch 88: Val Loss 586.53595
Epoch 89: Val Loss 586.49603
Epoch 90: Val Loss 586.45660
Epoch 91: Val Loss 586.41663
Epoch 92: Val Loss 586.37732
Epoch 93: Val Loss 586.33801
Epoch 94: Val Loss 586.29846
Epoch 95: Val Loss 586.25897
Epoch 96: Val Loss 586.21942
Epoch 97: Val Loss 586.18073
Epoch 98: Val Loss 586.14264
Epoch 99: Val Loss 586.10510
{'MSE - mean': 570.8054281455466, 'MSE - std': 15.299699987234249, 'R2 - mean': -6.358414776863578, 'R2 - std': 0.08123271152374478} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 683.29297
Epoch 1: Val Loss 683.17621
Epoch 2: Val Loss 683.06000
Epoch 3: Val Loss 682.94336
Epoch 4: Val Loss 682.82501
Epoch 5: Val Loss 682.70624
Epoch 6: Val Loss 682.58685
Epoch 7: Val Loss 682.46680
Epoch 8: Val Loss 682.34692
Epoch 9: Val Loss 682.22491
Epoch 10: Val Loss 682.10223
Epoch 11: Val Loss 681.97870
Epoch 12: Val Loss 681.85443
Epoch 13: Val Loss 681.72833
Epoch 14: Val Loss 681.60315
Epoch 15: Val Loss 681.47571
Epoch 16: Val Loss 681.34637
Epoch 17: Val Loss 681.21533
Epoch 18: Val Loss 681.08386
Epoch 19: Val Loss 680.95154
Epoch 20: Val Loss 680.81873
Epoch 21: Val Loss 680.68518
Epoch 22: Val Loss 680.54871
Epoch 23: Val Loss 680.41418
Epoch 24: Val Loss 680.27667
Epoch 25: Val Loss 680.13611
Epoch 26: Val Loss 679.99573
Epoch 27: Val Loss 679.85699
Epoch 28: Val Loss 679.71857
Epoch 29: Val Loss 679.57959
Epoch 30: Val Loss 679.43994
Epoch 31: Val Loss 679.29883
Epoch 32: Val Loss 679.15454
Epoch 33: Val Loss 679.00665
Epoch 34: Val Loss 678.85968
Epoch 35: Val Loss 678.71167
Epoch 36: Val Loss 678.56580
Epoch 37: Val Loss 678.41998
Epoch 38: Val Loss 678.27026
Epoch 39: Val Loss 678.11615
Epoch 40: Val Loss 677.95905
Epoch 41: Val Loss 677.80042
Epoch 42: Val Loss 677.64178
Epoch 43: Val Loss 677.48114
Epoch 44: Val Loss 677.31805
Epoch 45: Val Loss 677.15344
Epoch 46: Val Loss 676.99115
Epoch 47: Val Loss 676.82935
Epoch 48: Val Loss 676.66766
Epoch 49: Val Loss 676.50494
Epoch 50: Val Loss 676.34302
Epoch 51: Val Loss 676.18054
Epoch 52: Val Loss 676.02002
Epoch 53: Val Loss 675.85919
Epoch 54: Val Loss 675.69507
Epoch 55: Val Loss 675.52783
Epoch 56: Val Loss 675.35846
Epoch 57: Val Loss 675.19073
Epoch 58: Val Loss 675.02118
Epoch 59: Val Loss 674.84924
Epoch 60: Val Loss 674.67926
Epoch 61: Val Loss 674.50775
Epoch 62: Val Loss 674.33154
Epoch 63: Val Loss 674.15637
Epoch 64: Val Loss 673.97852
Epoch 65: Val Loss 673.79846
Epoch 66: Val Loss 673.61816
Epoch 67: Val Loss 673.43518
Epoch 68: Val Loss 673.25427
Epoch 69: Val Loss 673.07269
Epoch 70: Val Loss 672.88947
Epoch 71: Val Loss 672.70410
Epoch 72: Val Loss 672.52087
Epoch 73: Val Loss 672.33398
Epoch 74: Val Loss 672.14539
Epoch 75: Val Loss 671.95673
Epoch 76: Val Loss 671.76947
Epoch 77: Val Loss 671.57874
Epoch 78: Val Loss 671.38434
Epoch 79: Val Loss 671.18823
Epoch 80: Val Loss 670.99377
Epoch 81: Val Loss 670.80066
Epoch 82: Val Loss 670.60571
Epoch 83: Val Loss 670.41010
Epoch 84: Val Loss 670.21088
Epoch 85: Val Loss 670.01080
Epoch 86: Val Loss 669.80927
Epoch 87: Val Loss 669.60681
Epoch 88: Val Loss 669.40393
Epoch 89: Val Loss 669.20270
Epoch 90: Val Loss 668.99799
Epoch 91: Val Loss 668.79218
Epoch 92: Val Loss 668.58026
Epoch 93: Val Loss 668.37146
Epoch 94: Val Loss 668.15796
Epoch 95: Val Loss 667.94458
Epoch 96: Val Loss 667.73145
Epoch 97: Val Loss 667.50989
Epoch 98: Val Loss 667.28680
Epoch 99: Val Loss 667.06482
{'MSE - mean': 602.8918908789229, 'MSE - std': 47.06523197945901, 'R2 - mean': -6.235550146351859, 'R2 - std': 0.18598549354513996} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 504.97208
Epoch 1: Val Loss 504.88766
Epoch 2: Val Loss 504.80325
Epoch 3: Val Loss 504.71860
Epoch 4: Val Loss 504.63187
Epoch 5: Val Loss 504.54416
Epoch 6: Val Loss 504.45590
Epoch 7: Val Loss 504.36771
Epoch 8: Val Loss 504.27994
Epoch 9: Val Loss 504.19147
Epoch 10: Val Loss 504.10275
Epoch 11: Val Loss 504.01331
Epoch 12: Val Loss 503.92303
Epoch 13: Val Loss 503.83353
Epoch 14: Val Loss 503.74295
Epoch 15: Val Loss 503.65192
Epoch 16: Val Loss 503.56061
Epoch 17: Val Loss 503.46881
Epoch 18: Val Loss 503.37476
Epoch 19: Val Loss 503.28125
Epoch 20: Val Loss 503.18811
Epoch 21: Val Loss 503.09396
Epoch 22: Val Loss 502.99887
Epoch 23: Val Loss 502.90280
Epoch 24: Val Loss 502.80563
Epoch 25: Val Loss 502.70636
Epoch 26: Val Loss 502.60742
Epoch 27: Val Loss 502.50735
Epoch 28: Val Loss 502.40616
Epoch 29: Val Loss 502.30463
Epoch 30: Val Loss 502.20306
Epoch 31: Val Loss 502.10135
Epoch 32: Val Loss 502.00076
Epoch 33: Val Loss 501.90192
Epoch 34: Val Loss 501.80099
Epoch 35: Val Loss 501.69769
Epoch 36: Val Loss 501.59164
Epoch 37: Val Loss 501.48453
Epoch 38: Val Loss 501.37891
Epoch 39: Val Loss 501.27264
Epoch 40: Val Loss 501.16742
Epoch 41: Val Loss 501.06210
Epoch 42: Val Loss 500.95667
Epoch 43: Val Loss 500.84924
Epoch 44: Val Loss 500.74106
Epoch 45: Val Loss 500.63193
Epoch 46: Val Loss 500.52280
Epoch 47: Val Loss 500.41394
Epoch 48: Val Loss 500.30283
Epoch 49: Val Loss 500.19083
Epoch 50: Val Loss 500.08002
Epoch 51: Val Loss 499.96805
Epoch 52: Val Loss 499.85303
Epoch 53: Val Loss 499.73770
Epoch 54: Val Loss 499.62122
Epoch 55: Val Loss 499.50528
Epoch 56: Val Loss 499.38904
Epoch 57: Val Loss 499.27103
Epoch 58: Val Loss 499.15338
Epoch 59: Val Loss 499.03412
Epoch 60: Val Loss 498.91324
Epoch 61: Val Loss 498.79013
Epoch 62: Val Loss 498.66675
Epoch 63: Val Loss 498.54285
Epoch 64: Val Loss 498.41904
Epoch 65: Val Loss 498.29718
Epoch 66: Val Loss 498.17456
Epoch 67: Val Loss 498.05121
Epoch 68: Val Loss 497.92889
Epoch 69: Val Loss 497.80615
Epoch 70: Val Loss 497.68439
Epoch 71: Val Loss 497.55762
Epoch 72: Val Loss 497.42807
Epoch 73: Val Loss 497.29733
Epoch 74: Val Loss 497.16577
Epoch 75: Val Loss 497.03543
Epoch 76: Val Loss 496.90295
Epoch 77: Val Loss 496.76682
Epoch 78: Val Loss 496.63156
Epoch 79: Val Loss 496.49536
Epoch 80: Val Loss 496.35779
Epoch 81: Val Loss 496.21704
Epoch 82: Val Loss 496.07889
Epoch 83: Val Loss 495.94135
Epoch 84: Val Loss 495.80368
Epoch 85: Val Loss 495.66534
Epoch 86: Val Loss 495.52451
Epoch 87: Val Loss 495.38266
Epoch 88: Val Loss 495.24109
Epoch 89: Val Loss 495.09961
Epoch 90: Val Loss 494.95798
Epoch 91: Val Loss 494.81387
Epoch 92: Val Loss 494.66745
Epoch 93: Val Loss 494.51849
Epoch 94: Val Loss 494.36823
Epoch 95: Val Loss 494.21765
Epoch 96: Val Loss 494.06821
Epoch 97: Val Loss 493.91745
Epoch 98: Val Loss 493.76517
Epoch 99: Val Loss 493.61130
{'MSE - mean': 575.5717451396599, 'MSE - std': 62.45416828045944, 'R2 - mean': -6.062099746352255, 'R2 - std': 0.3408783898063798} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 566.17828
Epoch 1: Val Loss 566.10529
Epoch 2: Val Loss 566.03149
Epoch 3: Val Loss 565.95740
Epoch 4: Val Loss 565.88348
Epoch 5: Val Loss 565.80939
Epoch 6: Val Loss 565.73364
Epoch 7: Val Loss 565.65820
Epoch 8: Val Loss 565.58374
Epoch 9: Val Loss 565.50958
Epoch 10: Val Loss 565.43488
Epoch 11: Val Loss 565.35974
Epoch 12: Val Loss 565.28326
Epoch 13: Val Loss 565.20502
Epoch 14: Val Loss 565.12610
Epoch 15: Val Loss 565.04712
Epoch 16: Val Loss 564.96606
Epoch 17: Val Loss 564.88379
Epoch 18: Val Loss 564.80066
Epoch 19: Val Loss 564.71521
Epoch 20: Val Loss 564.62891
Epoch 21: Val Loss 564.54175
Epoch 22: Val Loss 564.45532
Epoch 23: Val Loss 564.36957
Epoch 24: Val Loss 564.28387
Epoch 25: Val Loss 564.19861
Epoch 26: Val Loss 564.11237
Epoch 27: Val Loss 564.02490
Epoch 28: Val Loss 563.93701
Epoch 29: Val Loss 563.84863
Epoch 30: Val Loss 563.75958
Epoch 31: Val Loss 563.66986
Epoch 32: Val Loss 563.58014
Epoch 33: Val Loss 563.49017
Epoch 34: Val Loss 563.40033
Epoch 35: Val Loss 563.31012
Epoch 36: Val Loss 563.21857
Epoch 37: Val Loss 563.12579
Epoch 38: Val Loss 563.03247
Epoch 39: Val Loss 562.93860
Epoch 40: Val Loss 562.84406
Epoch 41: Val Loss 562.74591
Epoch 42: Val Loss 562.64563
Epoch 43: Val Loss 562.54449
Epoch 44: Val Loss 562.44385
Epoch 45: Val Loss 562.34296
Epoch 46: Val Loss 562.24231
Epoch 47: Val Loss 562.14148
Epoch 48: Val Loss 562.03949
Epoch 49: Val Loss 561.93439
Epoch 50: Val Loss 561.82812
Epoch 51: Val Loss 561.71991
Epoch 52: Val Loss 561.60992
Epoch 53: Val Loss 561.49963
Epoch 54: Val Loss 561.39008
Epoch 55: Val Loss 561.28094
Epoch 56: Val Loss 561.17188
Epoch 57: Val Loss 561.06226
Epoch 58: Val Loss 560.95380
Epoch 59: Val Loss 560.84515
Epoch 60: Val Loss 560.73462
Epoch 61: Val Loss 560.62384
Epoch 62: Val Loss 560.51355
Epoch 63: Val Loss 560.40222
Epoch 64: Val Loss 560.28888
Epoch 65: Val Loss 560.17444
Epoch 66: Val Loss 560.06110
Epoch 67: Val Loss 559.94635
Epoch 68: Val Loss 559.83105
Epoch 69: Val Loss 559.71564
Epoch 70: Val Loss 559.59991
Epoch 71: Val Loss 559.48322
Epoch 72: Val Loss 559.36609
Epoch 73: Val Loss 559.24829
Epoch 74: Val Loss 559.13159
Epoch 75: Val Loss 559.01385
Epoch 76: Val Loss 558.89569
Epoch 77: Val Loss 558.77582
Epoch 78: Val Loss 558.65619
Epoch 79: Val Loss 558.53668
Epoch 80: Val Loss 558.41650
Epoch 81: Val Loss 558.29596
Epoch 82: Val Loss 558.17627
Epoch 83: Val Loss 558.05554
Epoch 84: Val Loss 557.93457
Epoch 85: Val Loss 557.81348
Epoch 86: Val Loss 557.69226
Epoch 87: Val Loss 557.56970
Epoch 88: Val Loss 557.44635
Epoch 89: Val Loss 557.32275
Epoch 90: Val Loss 557.19989
Epoch 91: Val Loss 557.07635
Epoch 92: Val Loss 556.95276
Epoch 93: Val Loss 556.82892
Epoch 94: Val Loss 556.70422
Epoch 95: Val Loss 556.57794
Epoch 96: Val Loss 556.45135
Epoch 97: Val Loss 556.32391
Epoch 98: Val Loss 556.19666
Epoch 99: Val Loss 556.06976
{'MSE - mean': 571.6713443519823, 'MSE - std': 56.40275714627099, 'R2 - mean': -5.868109622197225, 'R2 - std': 0.4934441552884459} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 571.6713443519823 and parameters: {'dim': 128, 'depth': 12, 'heads': 4, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 558.35236
Epoch 1: Val Loss 557.94305
Epoch 2: Val Loss 557.54913
Epoch 3: Val Loss 557.17712
Epoch 4: Val Loss 556.82715
Epoch 5: Val Loss 556.49426
Epoch 6: Val Loss 556.17755
Epoch 7: Val Loss 555.88159
Epoch 8: Val Loss 555.60669
Epoch 9: Val Loss 555.34808
Epoch 10: Val Loss 555.10858
Epoch 11: Val Loss 554.89331
Epoch 12: Val Loss 554.70166
Epoch 13: Val Loss 554.52081
Epoch 14: Val Loss 554.34161
Epoch 15: Val Loss 554.16315
Epoch 16: Val Loss 553.98578
Epoch 17: Val Loss 553.81073
Epoch 18: Val Loss 553.63477
Epoch 19: Val Loss 553.45807
Epoch 20: Val Loss 553.28174
Epoch 21: Val Loss 553.10559
Epoch 22: Val Loss 552.92969
Epoch 23: Val Loss 552.75391
Epoch 24: Val Loss 552.57806
Epoch 25: Val Loss 552.40369
Epoch 26: Val Loss 552.22974
Epoch 27: Val Loss 552.05579
Epoch 28: Val Loss 551.88074
Epoch 29: Val Loss 551.70715
Epoch 30: Val Loss 551.53351
Epoch 31: Val Loss 551.36041
Epoch 32: Val Loss 551.18860
Epoch 33: Val Loss 551.01642
Epoch 34: Val Loss 550.84387
Epoch 35: Val Loss 550.67059
Epoch 36: Val Loss 550.49561
Epoch 37: Val Loss 550.32043
Epoch 38: Val Loss 550.14557
Epoch 39: Val Loss 549.96979
Epoch 40: Val Loss 549.79419
Epoch 41: Val Loss 549.61945
Epoch 42: Val Loss 549.44421
Epoch 43: Val Loss 549.26831
Epoch 44: Val Loss 549.09399
Epoch 45: Val Loss 548.92114
Epoch 46: Val Loss 548.74854
Epoch 47: Val Loss 548.57745
Epoch 48: Val Loss 548.40765
Epoch 49: Val Loss 548.23706
Epoch 50: Val Loss 548.06580
Epoch 51: Val Loss 547.89374
Epoch 52: Val Loss 547.72113
Epoch 53: Val Loss 547.54901
Epoch 54: Val Loss 547.37683
Epoch 55: Val Loss 547.20422
Epoch 56: Val Loss 547.03229
Epoch 57: Val Loss 546.85968
Epoch 58: Val Loss 546.68842
Epoch 59: Val Loss 546.51703
Epoch 60: Val Loss 546.34534
Epoch 61: Val Loss 546.17426
Epoch 62: Val Loss 546.00317
Epoch 63: Val Loss 545.83142
Epoch 64: Val Loss 545.65997
Epoch 65: Val Loss 545.48969
Epoch 66: Val Loss 545.31927
Epoch 67: Val Loss 545.14813
Epoch 68: Val Loss 544.97699
Epoch 69: Val Loss 544.80634
Epoch 70: Val Loss 544.63617
Epoch 71: Val Loss 544.46698
Epoch 72: Val Loss 544.29767
Epoch 73: Val Loss 544.12909
Epoch 74: Val Loss 543.96021
Epoch 75: Val Loss 543.79163
Epoch 76: Val Loss 543.62134
Epoch 77: Val Loss 543.45038
Epoch 78: Val Loss 543.28125
Epoch 79: Val Loss 543.11157
Epoch 80: Val Loss 542.94275
Epoch 81: Val Loss 542.77539
Epoch 82: Val Loss 542.60907
Epoch 83: Val Loss 542.44342
Epoch 84: Val Loss 542.27789
Epoch 85: Val Loss 542.11108
Epoch 86: Val Loss 541.94611
Epoch 87: Val Loss 541.78113
Epoch 88: Val Loss 541.61450
Epoch 89: Val Loss 541.44696
Epoch 90: Val Loss 541.28119
Epoch 91: Val Loss 541.11505
Epoch 92: Val Loss 540.94989
Epoch 93: Val Loss 540.78448
Epoch 94: Val Loss 540.61865
Epoch 95: Val Loss 540.45300
Epoch 96: Val Loss 540.28564
Epoch 97: Val Loss 540.11627
Epoch 98: Val Loss 539.94977
Epoch 99: Val Loss 539.78333
{'MSE - mean': 539.7833164094258, 'MSE - std': 0.0, 'R2 - mean': -6.229084041153778, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 613.02441
Epoch 1: Val Loss 612.44977
Epoch 2: Val Loss 611.87640
Epoch 3: Val Loss 611.30450
Epoch 4: Val Loss 610.73151
Epoch 5: Val Loss 610.15979
Epoch 6: Val Loss 609.58734
Epoch 7: Val Loss 608.99982
Epoch 8: Val Loss 608.39307
Epoch 9: Val Loss 607.75470
Epoch 10: Val Loss 607.06915
Epoch 11: Val Loss 606.31244
Epoch 12: Val Loss 605.48828
Epoch 13: Val Loss 604.55939
Epoch 14: Val Loss 603.50043
Epoch 15: Val Loss 602.31250
Epoch 16: Val Loss 600.98218
Epoch 17: Val Loss 599.48956
Epoch 18: Val Loss 597.86005
Epoch 19: Val Loss 596.08887
Epoch 20: Val Loss 594.16785
Epoch 21: Val Loss 592.07507
Epoch 22: Val Loss 589.83618
Epoch 23: Val Loss 587.43762
Epoch 24: Val Loss 584.89093
Epoch 25: Val Loss 582.17474
Epoch 26: Val Loss 579.32849
Epoch 27: Val Loss 576.27832
Epoch 28: Val Loss 573.02081
Epoch 29: Val Loss 569.56586
Epoch 30: Val Loss 565.96899
Epoch 31: Val Loss 562.20654
Epoch 32: Val Loss 558.15082
Epoch 33: Val Loss 553.88025
Epoch 34: Val Loss 549.28516
Epoch 35: Val Loss 544.46405
Epoch 36: Val Loss 539.44250
Epoch 37: Val Loss 534.20178
Epoch 38: Val Loss 528.67102
Epoch 39: Val Loss 522.80615
Epoch 40: Val Loss 516.63147
Epoch 41: Val Loss 510.14972
Epoch 42: Val Loss 503.21210
Epoch 43: Val Loss 496.00369
Epoch 44: Val Loss 488.52768
Epoch 45: Val Loss 480.87906
Epoch 46: Val Loss 472.99667
Epoch 47: Val Loss 464.75812
Epoch 48: Val Loss 456.19421
Epoch 49: Val Loss 447.23212
Epoch 50: Val Loss 438.12723
Epoch 51: Val Loss 428.77841
Epoch 52: Val Loss 419.08810
Epoch 53: Val Loss 409.20746
Epoch 54: Val Loss 399.10202
Epoch 55: Val Loss 388.93396
Epoch 56: Val Loss 378.60202
Epoch 57: Val Loss 368.24939
Epoch 58: Val Loss 357.77679
Epoch 59: Val Loss 347.24771
Epoch 60: Val Loss 336.58060
Epoch 61: Val Loss 325.71133
Epoch 62: Val Loss 315.07785
Epoch 63: Val Loss 304.50186
Epoch 64: Val Loss 293.88959
Epoch 65: Val Loss 283.02435
Epoch 66: Val Loss 272.09558
Epoch 67: Val Loss 261.12488
Epoch 68: Val Loss 250.39165
Epoch 69: Val Loss 240.10315
Epoch 70: Val Loss 230.04738
Epoch 71: Val Loss 220.33566
Epoch 72: Val Loss 210.89815
Epoch 73: Val Loss 202.01456
Epoch 74: Val Loss 193.57831
Epoch 75: Val Loss 185.49612
Epoch 76: Val Loss 177.74272
Epoch 77: Val Loss 170.44798
Epoch 78: Val Loss 163.82654
Epoch 79: Val Loss 157.73230
Epoch 80: Val Loss 152.03398
Epoch 81: Val Loss 147.05019
Epoch 82: Val Loss 142.54865
Epoch 83: Val Loss 138.44797
Epoch 84: Val Loss 134.69574
Epoch 85: Val Loss 131.38766
Epoch 86: Val Loss 128.39253
Epoch 87: Val Loss 125.84540
Epoch 88: Val Loss 123.63628
Epoch 89: Val Loss 121.60747
Epoch 90: Val Loss 119.70940
Epoch 91: Val Loss 117.89544
Epoch 92: Val Loss 116.24297
Epoch 93: Val Loss 114.72370
Epoch 94: Val Loss 113.30054
Epoch 95: Val Loss 112.00150
Epoch 96: Val Loss 110.74627
Epoch 97: Val Loss 109.56712
Epoch 98: Val Loss 108.46848
Epoch 99: Val Loss 107.44748
{'MSE - mean': 323.61539618088625, 'MSE - std': 216.16792022853951, 'R2 - mean': -3.2815851903385, 'R2 - std': 2.9474988508152777} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 658.83417
Epoch 1: Val Loss 658.38507
Epoch 2: Val Loss 657.92511
Epoch 3: Val Loss 657.44989
Epoch 4: Val Loss 656.94861
Epoch 5: Val Loss 656.43286
Epoch 6: Val Loss 655.89612
Epoch 7: Val Loss 655.33771
Epoch 8: Val Loss 654.74414
Epoch 9: Val Loss 654.11835
Epoch 10: Val Loss 653.45575
Epoch 11: Val Loss 652.75958
Epoch 12: Val Loss 652.02319
Epoch 13: Val Loss 651.22681
Epoch 14: Val Loss 650.38324
Epoch 15: Val Loss 649.47784
Epoch 16: Val Loss 648.50903
Epoch 17: Val Loss 647.48334
Epoch 18: Val Loss 646.37854
Epoch 19: Val Loss 645.18164
Epoch 20: Val Loss 643.90192
Epoch 21: Val Loss 642.51910
Epoch 22: Val Loss 641.01880
Epoch 23: Val Loss 639.44092
Epoch 24: Val Loss 637.75830
Epoch 25: Val Loss 635.96454
Epoch 26: Val Loss 634.05511
Epoch 27: Val Loss 631.97534
Epoch 28: Val Loss 629.72424
Epoch 29: Val Loss 627.31079
Epoch 30: Val Loss 624.71277
Epoch 31: Val Loss 621.95746
Epoch 32: Val Loss 619.01862
Epoch 33: Val Loss 615.86951
Epoch 34: Val Loss 612.50183
Epoch 35: Val Loss 608.98193
Epoch 36: Val Loss 605.23547
Epoch 37: Val Loss 601.19006
Epoch 38: Val Loss 596.99219
Epoch 39: Val Loss 592.53387
Epoch 40: Val Loss 587.80945
Epoch 41: Val Loss 582.90564
Epoch 42: Val Loss 577.72986
Epoch 43: Val Loss 572.22418
Epoch 44: Val Loss 566.48438
Epoch 45: Val Loss 560.62970
Epoch 46: Val Loss 554.48328
Epoch 47: Val Loss 548.04584
Epoch 48: Val Loss 541.26819
Epoch 49: Val Loss 534.04376
Epoch 50: Val Loss 526.64270
Epoch 51: Val Loss 518.90100
Epoch 52: Val Loss 510.82584
Epoch 53: Val Loss 502.53851
Epoch 54: Val Loss 493.87967
Epoch 55: Val Loss 484.84641
Epoch 56: Val Loss 475.52924
Epoch 57: Val Loss 466.15610
Epoch 58: Val Loss 456.68884
Epoch 59: Val Loss 447.01349
Epoch 60: Val Loss 437.25085
Epoch 61: Val Loss 427.49991
Epoch 62: Val Loss 417.52768
Epoch 63: Val Loss 407.21109
Epoch 64: Val Loss 396.66528
Epoch 65: Val Loss 385.70999
Epoch 66: Val Loss 374.79678
Epoch 67: Val Loss 363.93362
Epoch 68: Val Loss 353.08453
Epoch 69: Val Loss 342.33395
Epoch 70: Val Loss 331.72974
Epoch 71: Val Loss 321.22617
Epoch 72: Val Loss 310.61499
Epoch 73: Val Loss 300.40820
Epoch 74: Val Loss 290.23758
Epoch 75: Val Loss 280.19809
Epoch 76: Val Loss 270.50458
Epoch 77: Val Loss 260.96515
Epoch 78: Val Loss 251.85555
Epoch 79: Val Loss 243.10104
Epoch 80: Val Loss 234.39812
Epoch 81: Val Loss 226.22301
Epoch 82: Val Loss 218.39940
Epoch 83: Val Loss 210.98538
Epoch 84: Val Loss 204.02438
Epoch 85: Val Loss 197.64586
Epoch 86: Val Loss 191.69434
Epoch 87: Val Loss 186.20638
Epoch 88: Val Loss 181.05507
Epoch 89: Val Loss 176.28242
Epoch 90: Val Loss 171.92401
Epoch 91: Val Loss 167.95811
Epoch 92: Val Loss 164.23837
Epoch 93: Val Loss 160.71649
Epoch 94: Val Loss 157.49364
Epoch 95: Val Loss 154.52344
Epoch 96: Val Loss 151.78336
Epoch 97: Val Loss 149.20392
Epoch 98: Val Loss 146.72490
Epoch 99: Val Loss 144.47565
{'MSE - mean': 263.9021424451318, 'MSE - std': 195.6622732347143, 'R2 - mean': -2.3590181560404537, 'R2 - std': 2.737534081842983} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 558.97937
Epoch 1: Val Loss 558.63495
Epoch 2: Val Loss 558.29785
Epoch 3: Val Loss 557.96851
Epoch 4: Val Loss 557.65137
Epoch 5: Val Loss 557.34479
Epoch 6: Val Loss 557.04816
Epoch 7: Val Loss 556.75964
Epoch 8: Val Loss 556.47723
Epoch 9: Val Loss 556.20160
Epoch 10: Val Loss 555.93152
Epoch 11: Val Loss 555.66577
Epoch 12: Val Loss 555.40399
Epoch 13: Val Loss 555.14923
Epoch 14: Val Loss 554.90271
Epoch 15: Val Loss 554.66473
Epoch 16: Val Loss 554.43256
Epoch 17: Val Loss 554.20782
Epoch 18: Val Loss 553.98865
Epoch 19: Val Loss 553.77228
Epoch 20: Val Loss 553.55872
Epoch 21: Val Loss 553.34753
Epoch 22: Val Loss 553.13708
Epoch 23: Val Loss 552.92792
Epoch 24: Val Loss 552.72247
Epoch 25: Val Loss 552.51776
Epoch 26: Val Loss 552.31476
Epoch 27: Val Loss 552.11554
Epoch 28: Val Loss 551.92096
Epoch 29: Val Loss 551.73010
Epoch 30: Val Loss 551.54199
Epoch 31: Val Loss 551.35504
Epoch 32: Val Loss 551.16766
Epoch 33: Val Loss 550.97949
Epoch 34: Val Loss 550.79077
Epoch 35: Val Loss 550.60388
Epoch 36: Val Loss 550.41650
Epoch 37: Val Loss 550.22778
Epoch 38: Val Loss 550.03876
Epoch 39: Val Loss 549.85199
Epoch 40: Val Loss 549.66644
Epoch 41: Val Loss 549.48199
Epoch 42: Val Loss 549.29810
Epoch 43: Val Loss 549.11456
Epoch 44: Val Loss 548.93109
Epoch 45: Val Loss 548.74963
Epoch 46: Val Loss 548.56708
Epoch 47: Val Loss 548.38574
Epoch 48: Val Loss 548.20490
Epoch 49: Val Loss 548.02301
Epoch 50: Val Loss 547.84326
Epoch 51: Val Loss 547.66498
Epoch 52: Val Loss 547.48651
Epoch 53: Val Loss 547.30811
Epoch 54: Val Loss 547.13110
Epoch 55: Val Loss 546.95447
Epoch 56: Val Loss 546.77972
Epoch 57: Val Loss 546.60388
Epoch 58: Val Loss 546.42853
Epoch 59: Val Loss 546.25348
Epoch 60: Val Loss 546.07855
Epoch 61: Val Loss 545.90430
Epoch 62: Val Loss 545.73071
Epoch 63: Val Loss 545.55786
Epoch 64: Val Loss 545.38495
Epoch 65: Val Loss 545.21075
Epoch 66: Val Loss 545.03577
Epoch 67: Val Loss 544.86188
Epoch 68: Val Loss 544.68860
Epoch 69: Val Loss 544.51398
Epoch 70: Val Loss 544.33862
Epoch 71: Val Loss 544.16351
Epoch 72: Val Loss 543.98792
Epoch 73: Val Loss 543.81317
Epoch 74: Val Loss 543.63800
Epoch 75: Val Loss 543.46277
Epoch 76: Val Loss 543.28833
Epoch 77: Val Loss 543.11578
Epoch 78: Val Loss 542.94409
Epoch 79: Val Loss 542.76996
Epoch 80: Val Loss 542.59729
Epoch 81: Val Loss 542.42572
Epoch 82: Val Loss 542.25464
Epoch 83: Val Loss 542.08527
Epoch 84: Val Loss 541.91553
Epoch 85: Val Loss 541.74561
Epoch 86: Val Loss 541.57581
Epoch 87: Val Loss 541.40637
Epoch 88: Val Loss 541.23639
Epoch 89: Val Loss 541.06744
Epoch 90: Val Loss 540.89862
Epoch 91: Val Loss 540.72827
Epoch 92: Val Loss 540.55707
Epoch 93: Val Loss 540.38611
Epoch 94: Val Loss 540.21552
Epoch 95: Val Loss 540.04480
Epoch 96: Val Loss 539.87494
Epoch 97: Val Loss 539.70587
Epoch 98: Val Loss 539.53796
Epoch 99: Val Loss 539.36920
{'MSE - mean': 332.76892579223323, 'MSE - std': 207.22136815797214, 'R2 - mean': -3.3063064377198508, 'R2 - std': 2.883164043678445} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 575.69336
Epoch 1: Val Loss 574.76801
Epoch 2: Val Loss 573.90271
Epoch 3: Val Loss 573.11859
Epoch 4: Val Loss 572.37500
Epoch 5: Val Loss 571.67548
Epoch 6: Val Loss 571.01227
Epoch 7: Val Loss 570.40448
Epoch 8: Val Loss 569.83722
Epoch 9: Val Loss 569.30359
Epoch 10: Val Loss 568.80298
Epoch 11: Val Loss 568.32477
Epoch 12: Val Loss 567.87738
Epoch 13: Val Loss 567.46844
Epoch 14: Val Loss 567.11426
Epoch 15: Val Loss 566.78802
Epoch 16: Val Loss 566.47711
Epoch 17: Val Loss 566.18256
Epoch 18: Val Loss 565.90381
Epoch 19: Val Loss 565.63574
Epoch 20: Val Loss 565.37262
Epoch 21: Val Loss 565.11572
Epoch 22: Val Loss 564.86383
Epoch 23: Val Loss 564.62164
Epoch 24: Val Loss 564.38458
Epoch 25: Val Loss 564.15509
Epoch 26: Val Loss 563.92969
Epoch 27: Val Loss 563.70624
Epoch 28: Val Loss 563.48596
Epoch 29: Val Loss 563.26788
Epoch 30: Val Loss 563.05530
Epoch 31: Val Loss 562.84631
Epoch 32: Val Loss 562.63916
Epoch 33: Val Loss 562.43347
Epoch 34: Val Loss 562.23022
Epoch 35: Val Loss 562.02991
Epoch 36: Val Loss 561.82727
Epoch 37: Val Loss 561.62671
Epoch 38: Val Loss 561.43213
Epoch 39: Val Loss 561.24036
Epoch 40: Val Loss 561.05164
Epoch 41: Val Loss 560.86365
Epoch 42: Val Loss 560.67694
Epoch 43: Val Loss 560.48981
Epoch 44: Val Loss 560.30267
Epoch 45: Val Loss 560.11713
Epoch 46: Val Loss 559.93097
Epoch 47: Val Loss 559.74487
Epoch 48: Val Loss 559.55878
Epoch 49: Val Loss 559.37408
Epoch 50: Val Loss 559.19073
Epoch 51: Val Loss 559.00818
Epoch 52: Val Loss 558.82385
Epoch 53: Val Loss 558.63983
Epoch 54: Val Loss 558.45862
Epoch 55: Val Loss 558.27838
Epoch 56: Val Loss 558.10187
Epoch 57: Val Loss 557.92542
Epoch 58: Val Loss 557.74823
Epoch 59: Val Loss 557.57086
Epoch 60: Val Loss 557.39490
Epoch 61: Val Loss 557.22058
Epoch 62: Val Loss 557.04602
Epoch 63: Val Loss 556.87006
Epoch 64: Val Loss 556.69513
Epoch 65: Val Loss 556.52002
Epoch 66: Val Loss 556.34595
Epoch 67: Val Loss 556.17151
Epoch 68: Val Loss 555.99689
Epoch 69: Val Loss 555.82214
Epoch 70: Val Loss 555.64752
Epoch 71: Val Loss 555.47357
Epoch 72: Val Loss 555.29999
Epoch 73: Val Loss 555.12683
Epoch 74: Val Loss 554.95319
Epoch 75: Val Loss 554.77863
Epoch 76: Val Loss 554.60364
Epoch 77: Val Loss 554.42834
Epoch 78: Val Loss 554.25220
Epoch 79: Val Loss 554.07629
Epoch 80: Val Loss 553.90186
Epoch 81: Val Loss 553.72656
Epoch 82: Val Loss 553.55249
Epoch 83: Val Loss 553.38104
Epoch 84: Val Loss 553.20917
Epoch 85: Val Loss 553.03668
Epoch 86: Val Loss 552.86353
Epoch 87: Val Loss 552.68878
Epoch 88: Val Loss 552.51190
Epoch 89: Val Loss 552.33771
Epoch 90: Val Loss 552.16382
Epoch 91: Val Loss 551.99042
Epoch 92: Val Loss 551.81696
Epoch 93: Val Loss 551.64453
Epoch 94: Val Loss 551.47437
Epoch 95: Val Loss 551.30304
Epoch 96: Val Loss 551.13220
Epoch 97: Val Loss 550.96088
Epoch 98: Val Loss 550.79541
Epoch 99: Val Loss 550.63434
{'MSE - mean': 376.3420039050619, 'MSE - std': 204.80968942390436, 'R2 - mean': -3.6515651674875427, 'R2 - std': 2.669629615533098} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 376.3420039050619 and parameters: {'dim': 128, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 521.96362
Epoch 1: Val Loss 521.78906
Epoch 2: Val Loss 521.61237
Epoch 3: Val Loss 521.43170
Epoch 4: Val Loss 521.24969
Epoch 5: Val Loss 521.06641
Epoch 6: Val Loss 520.87976
Epoch 7: Val Loss 520.68024
Epoch 8: Val Loss 520.46802
Epoch 9: Val Loss 520.24072
Epoch 10: Val Loss 519.99207
Epoch 11: Val Loss 519.71948
Epoch 12: Val Loss 519.42230
Epoch 13: Val Loss 519.08936
Epoch 14: Val Loss 518.71796
Epoch 15: Val Loss 518.31207
Epoch 16: Val Loss 517.88403
Epoch 17: Val Loss 517.41821
Epoch 18: Val Loss 516.88867
Epoch 19: Val Loss 516.31012
Epoch 20: Val Loss 515.65674
Epoch 21: Val Loss 514.91034
Epoch 22: Val Loss 514.06177
Epoch 23: Val Loss 513.10303
Epoch 24: Val Loss 512.07916
Epoch 25: Val Loss 510.98010
Epoch 26: Val Loss 509.77527
Epoch 27: Val Loss 508.45361
Epoch 28: Val Loss 507.03391
Epoch 29: Val Loss 505.53137
Epoch 30: Val Loss 503.91830
Epoch 31: Val Loss 502.19946
Epoch 32: Val Loss 500.37534
Epoch 33: Val Loss 498.43393
Epoch 34: Val Loss 496.36707
Epoch 35: Val Loss 494.16681
Epoch 36: Val Loss 491.85199
Epoch 37: Val Loss 489.41156
Epoch 38: Val Loss 486.86920
Epoch 39: Val Loss 484.20001
Epoch 40: Val Loss 481.39917
Epoch 41: Val Loss 478.45706
Epoch 42: Val Loss 475.37283
Epoch 43: Val Loss 472.17093
Epoch 44: Val Loss 468.83557
Epoch 45: Val Loss 465.30975
Epoch 46: Val Loss 461.56876
Epoch 47: Val Loss 457.63947
Epoch 48: Val Loss 453.56668
Epoch 49: Val Loss 449.34100
Epoch 50: Val Loss 444.90564
Epoch 51: Val Loss 440.33527
Epoch 52: Val Loss 435.59064
Epoch 53: Val Loss 430.65024
Epoch 54: Val Loss 425.50833
Epoch 55: Val Loss 420.15741
Epoch 56: Val Loss 414.54279
Epoch 57: Val Loss 408.80969
Epoch 58: Val Loss 402.96234
Epoch 59: Val Loss 396.98587
Epoch 60: Val Loss 390.90817
Epoch 61: Val Loss 384.70950
Epoch 62: Val Loss 378.18814
Epoch 63: Val Loss 371.46213
Epoch 64: Val Loss 364.54160
Epoch 65: Val Loss 357.43655
Epoch 66: Val Loss 350.07483
Epoch 67: Val Loss 342.69363
Epoch 68: Val Loss 335.08331
Epoch 69: Val Loss 327.39578
Epoch 70: Val Loss 319.67807
Epoch 71: Val Loss 311.73965
Epoch 72: Val Loss 303.76425
Epoch 73: Val Loss 295.75180
Epoch 74: Val Loss 287.64087
Epoch 75: Val Loss 279.61127
Epoch 76: Val Loss 271.60687
Epoch 77: Val Loss 263.72226
Epoch 78: Val Loss 255.89313
Epoch 79: Val Loss 248.12779
Epoch 80: Val Loss 240.29562
Epoch 81: Val Loss 232.48210
Epoch 82: Val Loss 224.62169
Epoch 83: Val Loss 216.85858
Epoch 84: Val Loss 209.19815
Epoch 85: Val Loss 201.87686
Epoch 86: Val Loss 194.85782
Epoch 87: Val Loss 187.97113
Epoch 88: Val Loss 181.26352
Epoch 89: Val Loss 174.71840
Epoch 90: Val Loss 168.37642
Epoch 91: Val Loss 162.36084
Epoch 92: Val Loss 156.66725
Epoch 93: Val Loss 151.12727
Epoch 94: Val Loss 145.82466
Epoch 95: Val Loss 140.69594
Epoch 96: Val Loss 135.86784
Epoch 97: Val Loss 131.34747
Epoch 98: Val Loss 127.04404
Epoch 99: Val Loss 122.98218
{'MSE - mean': 122.98217136820634, 'MSE - std': 0.0, 'R2 - mean': -0.6470469267153043, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 592.64404
Epoch 1: Val Loss 590.95325
Epoch 2: Val Loss 589.19183
Epoch 3: Val Loss 587.35602
Epoch 4: Val Loss 585.39960
Epoch 5: Val Loss 583.33246
Epoch 6: Val Loss 581.18933
Epoch 7: Val Loss 578.88263
Epoch 8: Val Loss 576.39392
Epoch 9: Val Loss 573.79535
Epoch 10: Val Loss 571.04321
Epoch 11: Val Loss 568.14001
Epoch 12: Val Loss 565.07074
Epoch 13: Val Loss 561.75677
Epoch 14: Val Loss 558.16241
Epoch 15: Val Loss 554.32349
Epoch 16: Val Loss 550.21002
Epoch 17: Val Loss 545.82361
Epoch 18: Val Loss 541.17542
Epoch 19: Val Loss 536.26978
Epoch 20: Val Loss 531.10931
Epoch 21: Val Loss 525.70184
Epoch 22: Val Loss 519.98389
Epoch 23: Val Loss 513.87769
Epoch 24: Val Loss 507.55399
Epoch 25: Val Loss 500.80859
Epoch 26: Val Loss 493.72672
Epoch 27: Val Loss 486.25946
Epoch 28: Val Loss 478.44577
Epoch 29: Val Loss 470.13461
Epoch 30: Val Loss 461.56219
Epoch 31: Val Loss 452.73907
Epoch 32: Val Loss 443.68723
Epoch 33: Val Loss 434.15198
Epoch 34: Val Loss 424.51221
Epoch 35: Val Loss 414.72577
Epoch 36: Val Loss 404.75769
Epoch 37: Val Loss 394.55066
Epoch 38: Val Loss 384.14090
Epoch 39: Val Loss 373.45499
Epoch 40: Val Loss 362.68524
Epoch 41: Val Loss 351.60736
Epoch 42: Val Loss 340.29843
Epoch 43: Val Loss 328.76425
Epoch 44: Val Loss 317.12265
Epoch 45: Val Loss 305.67548
Epoch 46: Val Loss 294.30942
Epoch 47: Val Loss 282.91278
Epoch 48: Val Loss 271.74457
Epoch 49: Val Loss 260.96225
Epoch 50: Val Loss 250.36511
Epoch 51: Val Loss 239.97301
Epoch 52: Val Loss 229.78040
Epoch 53: Val Loss 220.03186
Epoch 54: Val Loss 210.91127
Epoch 55: Val Loss 202.42531
Epoch 56: Val Loss 194.57045
Epoch 57: Val Loss 187.26216
Epoch 58: Val Loss 180.38483
Epoch 59: Val Loss 174.14778
Epoch 60: Val Loss 168.29283
Epoch 61: Val Loss 162.92630
Epoch 62: Val Loss 157.90652
Epoch 63: Val Loss 153.39272
Epoch 64: Val Loss 149.30646
Epoch 65: Val Loss 145.80432
Epoch 66: Val Loss 142.53047
Epoch 67: Val Loss 139.55315
Epoch 68: Val Loss 136.89459
Epoch 69: Val Loss 134.58118
Epoch 70: Val Loss 132.58255
Epoch 71: Val Loss 130.75401
Epoch 72: Val Loss 128.96857
Epoch 73: Val Loss 127.35158
Epoch 74: Val Loss 125.86958
Epoch 75: Val Loss 124.44357
Epoch 76: Val Loss 122.99262
Epoch 77: Val Loss 121.70255
Epoch 78: Val Loss 120.47604
Epoch 79: Val Loss 119.27924
Epoch 80: Val Loss 118.14208
Epoch 81: Val Loss 117.04674
Epoch 82: Val Loss 115.90705
Epoch 83: Val Loss 114.87420
Epoch 84: Val Loss 113.81535
Epoch 85: Val Loss 112.81600
Epoch 86: Val Loss 111.84840
Epoch 87: Val Loss 110.85033
Epoch 88: Val Loss 109.90507
Epoch 89: Val Loss 108.99696
Epoch 90: Val Loss 108.16801
Epoch 91: Val Loss 107.34459
Epoch 92: Val Loss 106.56638
Epoch 93: Val Loss 105.84241
Epoch 94: Val Loss 105.10497
Epoch 95: Val Loss 104.38726
Epoch 96: Val Loss 103.69788
Epoch 97: Val Loss 102.94437
Epoch 98: Val Loss 102.11836
Epoch 99: Val Loss 101.33616
{'MSE - mean': 112.15916529428881, 'MSE - std': 10.823006073917519, 'R2 - mean': -0.4526270533554537, 'R2 - std': 0.19441987335985056} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 656.63104
Epoch 1: Val Loss 656.33752
Epoch 2: Val Loss 656.02069
Epoch 3: Val Loss 655.67694
Epoch 4: Val Loss 655.30566
Epoch 5: Val Loss 654.91644
Epoch 6: Val Loss 654.50171
Epoch 7: Val Loss 654.05695
Epoch 8: Val Loss 653.59869
Epoch 9: Val Loss 653.11572
Epoch 10: Val Loss 652.60010
Epoch 11: Val Loss 652.05042
Epoch 12: Val Loss 651.47260
Epoch 13: Val Loss 650.83850
Epoch 14: Val Loss 650.14941
Epoch 15: Val Loss 649.39728
Epoch 16: Val Loss 648.59155
Epoch 17: Val Loss 647.71753
Epoch 18: Val Loss 646.76965
Epoch 19: Val Loss 645.73547
Epoch 20: Val Loss 644.59216
Epoch 21: Val Loss 643.32983
Epoch 22: Val Loss 641.92859
Epoch 23: Val Loss 640.38538
Epoch 24: Val Loss 638.69366
Epoch 25: Val Loss 636.82507
Epoch 26: Val Loss 634.78357
Epoch 27: Val Loss 632.56927
Epoch 28: Val Loss 630.15771
Epoch 29: Val Loss 627.51672
Epoch 30: Val Loss 624.60071
Epoch 31: Val Loss 621.42432
Epoch 32: Val Loss 617.96881
Epoch 33: Val Loss 614.24530
Epoch 34: Val Loss 610.27301
Epoch 35: Val Loss 606.07623
Epoch 36: Val Loss 601.59796
Epoch 37: Val Loss 596.92847
Epoch 38: Val Loss 591.96985
Epoch 39: Val Loss 586.68396
Epoch 40: Val Loss 581.10925
Epoch 41: Val Loss 575.27582
Epoch 42: Val Loss 569.15784
Epoch 43: Val Loss 562.66406
Epoch 44: Val Loss 555.85150
Epoch 45: Val Loss 548.69672
Epoch 46: Val Loss 541.28015
Epoch 47: Val Loss 533.41663
Epoch 48: Val Loss 525.18811
Epoch 49: Val Loss 516.64386
Epoch 50: Val Loss 507.84836
Epoch 51: Val Loss 498.74512
Epoch 52: Val Loss 489.22186
Epoch 53: Val Loss 479.28329
Epoch 54: Val Loss 468.94894
Epoch 55: Val Loss 458.26590
Epoch 56: Val Loss 447.26880
Epoch 57: Val Loss 435.77414
Epoch 58: Val Loss 423.93402
Epoch 59: Val Loss 411.70157
Epoch 60: Val Loss 399.08881
Epoch 61: Val Loss 386.32947
Epoch 62: Val Loss 373.28546
Epoch 63: Val Loss 360.09799
Epoch 64: Val Loss 346.98538
Epoch 65: Val Loss 333.56921
Epoch 66: Val Loss 320.09579
Epoch 67: Val Loss 306.47366
Epoch 68: Val Loss 292.77060
Epoch 69: Val Loss 279.18195
Epoch 70: Val Loss 265.60892
Epoch 71: Val Loss 252.33604
Epoch 72: Val Loss 239.38637
Epoch 73: Val Loss 226.76276
Epoch 74: Val Loss 214.71449
Epoch 75: Val Loss 203.28319
Epoch 76: Val Loss 192.07271
Epoch 77: Val Loss 181.59077
Epoch 78: Val Loss 171.29997
Epoch 79: Val Loss 161.78732
Epoch 80: Val Loss 152.85609
Epoch 81: Val Loss 144.76347
Epoch 82: Val Loss 137.52083
Epoch 83: Val Loss 131.17332
Epoch 84: Val Loss 125.54826
Epoch 85: Val Loss 120.47256
Epoch 86: Val Loss 115.82578
Epoch 87: Val Loss 111.72627
Epoch 88: Val Loss 107.83129
Epoch 89: Val Loss 104.38813
Epoch 90: Val Loss 101.27090
Epoch 91: Val Loss 98.41540
Epoch 92: Val Loss 96.02315
Epoch 93: Val Loss 93.97332
Epoch 94: Val Loss 92.07716
Epoch 95: Val Loss 90.29738
Epoch 96: Val Loss 88.55972
Epoch 97: Val Loss 86.98542
Epoch 98: Val Loss 85.50025
Epoch 99: Val Loss 84.17390
{'MSE - mean': 102.8307425362989, 'MSE - std': 15.87862029966516, 'R2 - mean': -0.26242268048385814, 'R2 - std': 0.3123376351916744} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 556.98694
Epoch 1: Val Loss 556.81049
Epoch 2: Val Loss 556.63440
Epoch 3: Val Loss 556.45856
Epoch 4: Val Loss 556.28235
Epoch 5: Val Loss 556.10645
Epoch 6: Val Loss 555.93085
Epoch 7: Val Loss 555.75702
Epoch 8: Val Loss 555.58331
Epoch 9: Val Loss 555.40924
Epoch 10: Val Loss 555.23566
Epoch 11: Val Loss 555.06104
Epoch 12: Val Loss 554.88647
Epoch 13: Val Loss 554.71240
Epoch 14: Val Loss 554.53961
Epoch 15: Val Loss 554.36584
Epoch 16: Val Loss 554.19110
Epoch 17: Val Loss 554.01660
Epoch 18: Val Loss 553.84241
Epoch 19: Val Loss 553.66650
Epoch 20: Val Loss 553.49188
Epoch 21: Val Loss 553.31696
Epoch 22: Val Loss 553.14233
Epoch 23: Val Loss 552.96875
Epoch 24: Val Loss 552.79498
Epoch 25: Val Loss 552.62036
Epoch 26: Val Loss 552.44611
Epoch 27: Val Loss 552.27039
Epoch 28: Val Loss 552.09491
Epoch 29: Val Loss 551.91925
Epoch 30: Val Loss 551.74445
Epoch 31: Val Loss 551.56897
Epoch 32: Val Loss 551.39301
Epoch 33: Val Loss 551.21759
Epoch 34: Val Loss 551.04279
Epoch 35: Val Loss 550.86774
Epoch 36: Val Loss 550.69147
Epoch 37: Val Loss 550.51569
Epoch 38: Val Loss 550.34039
Epoch 39: Val Loss 550.16553
Epoch 40: Val Loss 549.98987
Epoch 41: Val Loss 549.81476
Epoch 42: Val Loss 549.63959
Epoch 43: Val Loss 549.46564
Epoch 44: Val Loss 549.29340
Epoch 45: Val Loss 549.12231
Epoch 46: Val Loss 548.95026
Epoch 47: Val Loss 548.77692
Epoch 48: Val Loss 548.60413
Epoch 49: Val Loss 548.43188
Epoch 50: Val Loss 548.25775
Epoch 51: Val Loss 548.08215
Epoch 52: Val Loss 547.90668
Epoch 53: Val Loss 547.73187
Epoch 54: Val Loss 547.55725
Epoch 55: Val Loss 547.38361
Epoch 56: Val Loss 547.21014
Epoch 57: Val Loss 547.03802
Epoch 58: Val Loss 546.86688
Epoch 59: Val Loss 546.69623
Epoch 60: Val Loss 546.52582
Epoch 61: Val Loss 546.35559
Epoch 62: Val Loss 546.18518
Epoch 63: Val Loss 546.01282
Epoch 64: Val Loss 545.84064
Epoch 65: Val Loss 545.66876
Epoch 66: Val Loss 545.49646
Epoch 67: Val Loss 545.32458
Epoch 68: Val Loss 545.15216
Epoch 69: Val Loss 544.98004
Epoch 70: Val Loss 544.80927
Epoch 71: Val Loss 544.63855
Epoch 72: Val Loss 544.46686
Epoch 73: Val Loss 544.29492
Epoch 74: Val Loss 544.12238
Epoch 75: Val Loss 543.94843
Epoch 76: Val Loss 543.77484
Epoch 77: Val Loss 543.60120
Epoch 78: Val Loss 543.42737
Epoch 79: Val Loss 543.25403
Epoch 80: Val Loss 543.08185
Epoch 81: Val Loss 542.91040
Epoch 82: Val Loss 542.73938
Epoch 83: Val Loss 542.56982
Epoch 84: Val Loss 542.39917
Epoch 85: Val Loss 542.23029
Epoch 86: Val Loss 542.06219
Epoch 87: Val Loss 541.89355
Epoch 88: Val Loss 541.72455
Epoch 89: Val Loss 541.55554
Epoch 90: Val Loss 541.38666
Epoch 91: Val Loss 541.21686
Epoch 92: Val Loss 541.04614
Epoch 93: Val Loss 540.87421
Epoch 94: Val Loss 540.70276
Epoch 95: Val Loss 540.52954
Epoch 96: Val Loss 540.35516
Epoch 97: Val Loss 540.18195
Epoch 98: Val Loss 540.00903
Epoch 99: Val Loss 539.83655
{'MSE - mean': 212.0821922485697, 'MSE - std': 189.72805730746367, 'R2 - mean': -1.7354079792083945, 'R2 - std': 2.5655843719401785} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 569.29004
Epoch 1: Val Loss 569.10718
Epoch 2: Val Loss 568.92578
Epoch 3: Val Loss 568.74536
Epoch 4: Val Loss 568.56879
Epoch 5: Val Loss 568.39349
Epoch 6: Val Loss 568.21887
Epoch 7: Val Loss 568.04407
Epoch 8: Val Loss 567.86963
Epoch 9: Val Loss 567.69592
Epoch 10: Val Loss 567.52148
Epoch 11: Val Loss 567.34747
Epoch 12: Val Loss 567.17426
Epoch 13: Val Loss 567.00220
Epoch 14: Val Loss 566.83014
Epoch 15: Val Loss 566.65857
Epoch 16: Val Loss 566.48676
Epoch 17: Val Loss 566.31342
Epoch 18: Val Loss 566.13928
Epoch 19: Val Loss 565.96741
Epoch 20: Val Loss 565.79425
Epoch 21: Val Loss 565.62018
Epoch 22: Val Loss 565.44586
Epoch 23: Val Loss 565.27161
Epoch 24: Val Loss 565.09827
Epoch 25: Val Loss 564.92480
Epoch 26: Val Loss 564.75067
Epoch 27: Val Loss 564.57654
Epoch 28: Val Loss 564.40265
Epoch 29: Val Loss 564.22943
Epoch 30: Val Loss 564.05652
Epoch 31: Val Loss 563.88513
Epoch 32: Val Loss 563.71381
Epoch 33: Val Loss 563.54236
Epoch 34: Val Loss 563.37091
Epoch 35: Val Loss 563.19806
Epoch 36: Val Loss 563.02527
Epoch 37: Val Loss 562.85223
Epoch 38: Val Loss 562.68054
Epoch 39: Val Loss 562.50897
Epoch 40: Val Loss 562.33765
Epoch 41: Val Loss 562.16644
Epoch 42: Val Loss 561.99396
Epoch 43: Val Loss 561.82068
Epoch 44: Val Loss 561.64856
Epoch 45: Val Loss 561.47662
Epoch 46: Val Loss 561.30389
Epoch 47: Val Loss 561.13086
Epoch 48: Val Loss 560.95703
Epoch 49: Val Loss 560.78381
Epoch 50: Val Loss 560.61163
Epoch 51: Val Loss 560.43988
Epoch 52: Val Loss 560.26923
Epoch 53: Val Loss 560.10022
Epoch 54: Val Loss 559.93091
Epoch 55: Val Loss 559.76086
Epoch 56: Val Loss 559.59058
Epoch 57: Val Loss 559.41986
Epoch 58: Val Loss 559.24982
Epoch 59: Val Loss 559.08044
Epoch 60: Val Loss 558.91132
Epoch 61: Val Loss 558.74158
Epoch 62: Val Loss 558.57043
Epoch 63: Val Loss 558.39838
Epoch 64: Val Loss 558.22681
Epoch 65: Val Loss 558.05609
Epoch 66: Val Loss 557.88477
Epoch 67: Val Loss 557.71454
Epoch 68: Val Loss 557.54333
Epoch 69: Val Loss 557.37238
Epoch 70: Val Loss 557.20258
Epoch 71: Val Loss 557.03284
Epoch 72: Val Loss 556.86230
Epoch 73: Val Loss 556.69128
Epoch 74: Val Loss 556.51996
Epoch 75: Val Loss 556.35034
Epoch 76: Val Loss 556.18024
Epoch 77: Val Loss 556.01093
Epoch 78: Val Loss 555.84296
Epoch 79: Val Loss 555.67450
Epoch 80: Val Loss 555.50269
Epoch 81: Val Loss 555.33057
Epoch 82: Val Loss 555.15851
Epoch 83: Val Loss 554.98761
Epoch 84: Val Loss 554.81696
Epoch 85: Val Loss 554.64740
Epoch 86: Val Loss 554.47870
Epoch 87: Val Loss 554.31116
Epoch 88: Val Loss 554.14185
Epoch 89: Val Loss 553.97247
Epoch 90: Val Loss 553.80121
Epoch 91: Val Loss 553.63086
Epoch 92: Val Loss 553.46063
Epoch 93: Val Loss 553.29089
Epoch 94: Val Loss 553.12048
Epoch 95: Val Loss 552.95001
Epoch 96: Val Loss 552.78033
Epoch 97: Val Loss 552.61108
Epoch 98: Val Loss 552.44183
Epoch 99: Val Loss 552.27271
{'MSE - mean': 280.12029789881234, 'MSE - std': 217.51810008738516, 'R2 - mean': -2.398436382880262, 'R2 - std': 2.6503217147994698} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 280.12029789881234 and parameters: {'dim': 128, 'depth': 1, 'heads': 4, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 527.60236
Epoch 1: Val Loss 527.58624
Epoch 2: Val Loss 527.57007
Epoch 3: Val Loss 527.55389
Epoch 4: Val Loss 527.53772
Epoch 5: Val Loss 527.52161
Epoch 6: Val Loss 527.50555
Epoch 7: Val Loss 527.48956
Epoch 8: Val Loss 527.47357
Epoch 9: Val Loss 527.45758
Epoch 10: Val Loss 527.44153
Epoch 11: Val Loss 527.42548
Epoch 12: Val Loss 527.40942
Epoch 13: Val Loss 527.39337
Epoch 14: Val Loss 527.37714
Epoch 15: Val Loss 527.36102
Epoch 16: Val Loss 527.34473
Epoch 17: Val Loss 527.32861
Epoch 18: Val Loss 527.31262
Epoch 19: Val Loss 527.29657
Epoch 20: Val Loss 527.28058
Epoch 21: Val Loss 527.26440
Epoch 22: Val Loss 527.24817
Epoch 23: Val Loss 527.23212
Epoch 24: Val Loss 527.21600
Epoch 25: Val Loss 527.19995
Epoch 26: Val Loss 527.18372
Epoch 27: Val Loss 527.16766
Epoch 28: Val Loss 527.15161
Epoch 29: Val Loss 527.13544
Epoch 30: Val Loss 527.11945
Epoch 31: Val Loss 527.10327
Epoch 32: Val Loss 527.08698
Epoch 33: Val Loss 527.07056
Epoch 34: Val Loss 527.05432
Epoch 35: Val Loss 527.03827
Epoch 36: Val Loss 527.02191
Epoch 37: Val Loss 527.00574
Epoch 38: Val Loss 526.98962
Epoch 39: Val Loss 526.97345
Epoch 40: Val Loss 526.95746
Epoch 41: Val Loss 526.94141
Epoch 42: Val Loss 526.92535
Epoch 43: Val Loss 526.90924
Epoch 44: Val Loss 526.89331
Epoch 45: Val Loss 526.87738
Epoch 46: Val Loss 526.86145
Epoch 47: Val Loss 526.84558
Epoch 48: Val Loss 526.82965
Epoch 49: Val Loss 526.81366
Epoch 50: Val Loss 526.79749
Epoch 51: Val Loss 526.78119
Epoch 52: Val Loss 526.76501
Epoch 53: Val Loss 526.74878
Epoch 54: Val Loss 526.73279
Epoch 55: Val Loss 526.71686
Epoch 56: Val Loss 526.70074
Epoch 57: Val Loss 526.68457
Epoch 58: Val Loss 526.66858
Epoch 59: Val Loss 526.65277
Epoch 60: Val Loss 526.63684
Epoch 61: Val Loss 526.62085
Epoch 62: Val Loss 526.60468
Epoch 63: Val Loss 526.58838
Epoch 64: Val Loss 526.57227
Epoch 65: Val Loss 526.55621
Epoch 66: Val Loss 526.54016
Epoch 67: Val Loss 526.52423
Epoch 68: Val Loss 526.50830
Epoch 69: Val Loss 526.49237
Epoch 70: Val Loss 526.47644
Epoch 71: Val Loss 526.46051
Epoch 72: Val Loss 526.44452
Epoch 73: Val Loss 526.42834
Epoch 74: Val Loss 526.41211
Epoch 75: Val Loss 526.39587
Epoch 76: Val Loss 526.37994
Epoch 77: Val Loss 526.36395
Epoch 78: Val Loss 526.34821
Epoch 79: Val Loss 526.33221
Epoch 80: Val Loss 526.31610
Epoch 81: Val Loss 526.30005
Epoch 82: Val Loss 526.28400
Epoch 83: Val Loss 526.26801
Epoch 84: Val Loss 526.25208
Epoch 85: Val Loss 526.23627
Epoch 86: Val Loss 526.22040
Epoch 87: Val Loss 526.20435
Epoch 88: Val Loss 526.18829
Epoch 89: Val Loss 526.17236
Epoch 90: Val Loss 526.15643
Epoch 91: Val Loss 526.14044
Epoch 92: Val Loss 526.12451
Epoch 93: Val Loss 526.10846
Epoch 94: Val Loss 526.09253
Epoch 95: Val Loss 526.07654
Epoch 96: Val Loss 526.06042
Epoch 97: Val Loss 526.04443
Epoch 98: Val Loss 526.02814
Epoch 99: Val Loss 526.01184
{'MSE - mean': 526.0117686307873, 'MSE - std': 0.0, 'R2 - mean': -6.044647669665353, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 618.29181
Epoch 1: Val Loss 618.19830
Epoch 2: Val Loss 618.10516
Epoch 3: Val Loss 618.01135
Epoch 4: Val Loss 617.91724
Epoch 5: Val Loss 617.82318
Epoch 6: Val Loss 617.72839
Epoch 7: Val Loss 617.63348
Epoch 8: Val Loss 617.53870
Epoch 9: Val Loss 617.44440
Epoch 10: Val Loss 617.35077
Epoch 11: Val Loss 617.25665
Epoch 12: Val Loss 617.16168
Epoch 13: Val Loss 617.06604
Epoch 14: Val Loss 616.97144
Epoch 15: Val Loss 616.87646
Epoch 16: Val Loss 616.78223
Epoch 17: Val Loss 616.68701
Epoch 18: Val Loss 616.59113
Epoch 19: Val Loss 616.49463
Epoch 20: Val Loss 616.39819
Epoch 21: Val Loss 616.30127
Epoch 22: Val Loss 616.20209
Epoch 23: Val Loss 616.10370
Epoch 24: Val Loss 616.00580
Epoch 25: Val Loss 615.90765
Epoch 26: Val Loss 615.80914
Epoch 27: Val Loss 615.71002
Epoch 28: Val Loss 615.60925
Epoch 29: Val Loss 615.50696
Epoch 30: Val Loss 615.40491
Epoch 31: Val Loss 615.30231
Epoch 32: Val Loss 615.20135
Epoch 33: Val Loss 615.10138
Epoch 34: Val Loss 615.00098
Epoch 35: Val Loss 614.89911
Epoch 36: Val Loss 614.79614
Epoch 37: Val Loss 614.69269
Epoch 38: Val Loss 614.58936
Epoch 39: Val Loss 614.48456
Epoch 40: Val Loss 614.38074
Epoch 41: Val Loss 614.27594
Epoch 42: Val Loss 614.17126
Epoch 43: Val Loss 614.06635
Epoch 44: Val Loss 613.96271
Epoch 45: Val Loss 613.85889
Epoch 46: Val Loss 613.75269
Epoch 47: Val Loss 613.64630
Epoch 48: Val Loss 613.54065
Epoch 49: Val Loss 613.43457
Epoch 50: Val Loss 613.32855
Epoch 51: Val Loss 613.22192
Epoch 52: Val Loss 613.11548
Epoch 53: Val Loss 613.00781
Epoch 54: Val Loss 612.89783
Epoch 55: Val Loss 612.78687
Epoch 56: Val Loss 612.67529
Epoch 57: Val Loss 612.56158
Epoch 58: Val Loss 612.44800
Epoch 59: Val Loss 612.33496
Epoch 60: Val Loss 612.22083
Epoch 61: Val Loss 612.10590
Epoch 62: Val Loss 611.98859
Epoch 63: Val Loss 611.87158
Epoch 64: Val Loss 611.75378
Epoch 65: Val Loss 611.63507
Epoch 66: Val Loss 611.51617
Epoch 67: Val Loss 611.39581
Epoch 68: Val Loss 611.27570
Epoch 69: Val Loss 611.15613
Epoch 70: Val Loss 611.03485
Epoch 71: Val Loss 610.91052
Epoch 72: Val Loss 610.78571
Epoch 73: Val Loss 610.66150
Epoch 74: Val Loss 610.53833
Epoch 75: Val Loss 610.41327
Epoch 76: Val Loss 610.28851
Epoch 77: Val Loss 610.16394
Epoch 78: Val Loss 610.03784
Epoch 79: Val Loss 609.91101
Epoch 80: Val Loss 609.78168
Epoch 81: Val Loss 609.65283
Epoch 82: Val Loss 609.52344
Epoch 83: Val Loss 609.39075
Epoch 84: Val Loss 609.25812
Epoch 85: Val Loss 609.12616
Epoch 86: Val Loss 608.99170
Epoch 87: Val Loss 608.85510
Epoch 88: Val Loss 608.71692
Epoch 89: Val Loss 608.57666
Epoch 90: Val Loss 608.43390
Epoch 91: Val Loss 608.28918
Epoch 92: Val Loss 608.14301
Epoch 93: Val Loss 607.99506
Epoch 94: Val Loss 607.84662
Epoch 95: Val Loss 607.69855
Epoch 96: Val Loss 607.54688
Epoch 97: Val Loss 607.39435
Epoch 98: Val Loss 607.24170
Epoch 99: Val Loss 607.08618
{'MSE - mean': 566.5489726749493, 'MSE - std': 40.53720404416191, 'R2 - mean': -6.291167021216795, 'R2 - std': 0.24651935155144278} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 679.33740
Epoch 1: Val Loss 679.31805
Epoch 2: Val Loss 679.29889
Epoch 3: Val Loss 679.27954
Epoch 4: Val Loss 679.26038
Epoch 5: Val Loss 679.24103
Epoch 6: Val Loss 679.22162
Epoch 7: Val Loss 679.20209
Epoch 8: Val Loss 679.18280
Epoch 9: Val Loss 679.16333
Epoch 10: Val Loss 679.14410
Epoch 11: Val Loss 679.12469
Epoch 12: Val Loss 679.10535
Epoch 13: Val Loss 679.08588
Epoch 14: Val Loss 679.06635
Epoch 15: Val Loss 679.04688
Epoch 16: Val Loss 679.02747
Epoch 17: Val Loss 679.00793
Epoch 18: Val Loss 678.98853
Epoch 19: Val Loss 678.96906
Epoch 20: Val Loss 678.94958
Epoch 21: Val Loss 678.93024
Epoch 22: Val Loss 678.91071
Epoch 23: Val Loss 678.89130
Epoch 24: Val Loss 678.87195
Epoch 25: Val Loss 678.85254
Epoch 26: Val Loss 678.83319
Epoch 27: Val Loss 678.81390
Epoch 28: Val Loss 678.79468
Epoch 29: Val Loss 678.77557
Epoch 30: Val Loss 678.75635
Epoch 31: Val Loss 678.73712
Epoch 32: Val Loss 678.71783
Epoch 33: Val Loss 678.69824
Epoch 34: Val Loss 678.67877
Epoch 35: Val Loss 678.65924
Epoch 36: Val Loss 678.63989
Epoch 37: Val Loss 678.62067
Epoch 38: Val Loss 678.60150
Epoch 39: Val Loss 678.58215
Epoch 40: Val Loss 678.56281
Epoch 41: Val Loss 678.54346
Epoch 42: Val Loss 678.52429
Epoch 43: Val Loss 678.50494
Epoch 44: Val Loss 678.48547
Epoch 45: Val Loss 678.46619
Epoch 46: Val Loss 678.44690
Epoch 47: Val Loss 678.42767
Epoch 48: Val Loss 678.40857
Epoch 49: Val Loss 678.38922
Epoch 50: Val Loss 678.36981
Epoch 51: Val Loss 678.35040
Epoch 52: Val Loss 678.33099
Epoch 53: Val Loss 678.31171
Epoch 54: Val Loss 678.29254
Epoch 55: Val Loss 678.27338
Epoch 56: Val Loss 678.25415
Epoch 57: Val Loss 678.23523
Epoch 58: Val Loss 678.21594
Epoch 59: Val Loss 678.19659
Epoch 60: Val Loss 678.17731
Epoch 61: Val Loss 678.15808
Epoch 62: Val Loss 678.13892
Epoch 63: Val Loss 678.11975
Epoch 64: Val Loss 678.10071
Epoch 65: Val Loss 678.08136
Epoch 66: Val Loss 678.06219
Epoch 67: Val Loss 678.04297
Epoch 68: Val Loss 678.02344
Epoch 69: Val Loss 678.00378
Epoch 70: Val Loss 677.98407
Epoch 71: Val Loss 677.96478
Epoch 72: Val Loss 677.94525
Epoch 73: Val Loss 677.92590
Epoch 74: Val Loss 677.90656
Epoch 75: Val Loss 677.88745
Epoch 76: Val Loss 677.86835
Epoch 77: Val Loss 677.84937
Epoch 78: Val Loss 677.83008
Epoch 79: Val Loss 677.81104
Epoch 80: Val Loss 677.79205
Epoch 81: Val Loss 677.77301
Epoch 82: Val Loss 677.75391
Epoch 83: Val Loss 677.73480
Epoch 84: Val Loss 677.71582
Epoch 85: Val Loss 677.69690
Epoch 86: Val Loss 677.67792
Epoch 87: Val Loss 677.65857
Epoch 88: Val Loss 677.63928
Epoch 89: Val Loss 677.62012
Epoch 90: Val Loss 677.60101
Epoch 91: Val Loss 677.58221
Epoch 92: Val Loss 677.56342
Epoch 93: Val Loss 677.54437
Epoch 94: Val Loss 677.52545
Epoch 95: Val Loss 677.50647
Epoch 96: Val Loss 677.48755
Epoch 97: Val Loss 677.46826
Epoch 98: Val Loss 677.44891
Epoch 99: Val Loss 677.42950
{'MSE - mean': 603.5091574059599, 'MSE - std': 61.8677658546533, 'R2 - mean': -6.226920423503348, 'R2 - std': 0.22083880492734143} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 539.26965
Epoch 1: Val Loss 539.25244
Epoch 2: Val Loss 539.23529
Epoch 3: Val Loss 539.21814
Epoch 4: Val Loss 539.20093
Epoch 5: Val Loss 539.18378
Epoch 6: Val Loss 539.16644
Epoch 7: Val Loss 539.14923
Epoch 8: Val Loss 539.13208
Epoch 9: Val Loss 539.11505
Epoch 10: Val Loss 539.09808
Epoch 11: Val Loss 539.08112
Epoch 12: Val Loss 539.06409
Epoch 13: Val Loss 539.04694
Epoch 14: Val Loss 539.02966
Epoch 15: Val Loss 539.01239
Epoch 16: Val Loss 538.99506
Epoch 17: Val Loss 538.97772
Epoch 18: Val Loss 538.96045
Epoch 19: Val Loss 538.94318
Epoch 20: Val Loss 538.92590
Epoch 21: Val Loss 538.90869
Epoch 22: Val Loss 538.89160
Epoch 23: Val Loss 538.87457
Epoch 24: Val Loss 538.85748
Epoch 25: Val Loss 538.84058
Epoch 26: Val Loss 538.82355
Epoch 27: Val Loss 538.80646
Epoch 28: Val Loss 538.78918
Epoch 29: Val Loss 538.77216
Epoch 30: Val Loss 538.75500
Epoch 31: Val Loss 538.73779
Epoch 32: Val Loss 538.72052
Epoch 33: Val Loss 538.70337
Epoch 34: Val Loss 538.68634
Epoch 35: Val Loss 538.66925
Epoch 36: Val Loss 538.65216
Epoch 37: Val Loss 538.63477
Epoch 38: Val Loss 538.61743
Epoch 39: Val Loss 538.60010
Epoch 40: Val Loss 538.58301
Epoch 41: Val Loss 538.56610
Epoch 42: Val Loss 538.54919
Epoch 43: Val Loss 538.53217
Epoch 44: Val Loss 538.51508
Epoch 45: Val Loss 538.49792
Epoch 46: Val Loss 538.48096
Epoch 47: Val Loss 538.46387
Epoch 48: Val Loss 538.44678
Epoch 49: Val Loss 538.42963
Epoch 50: Val Loss 538.41241
Epoch 51: Val Loss 538.39520
Epoch 52: Val Loss 538.37811
Epoch 53: Val Loss 538.36090
Epoch 54: Val Loss 538.34381
Epoch 55: Val Loss 538.32672
Epoch 56: Val Loss 538.30969
Epoch 57: Val Loss 538.29254
Epoch 58: Val Loss 538.27539
Epoch 59: Val Loss 538.25812
Epoch 60: Val Loss 538.24078
Epoch 61: Val Loss 538.22345
Epoch 62: Val Loss 538.20624
Epoch 63: Val Loss 538.18896
Epoch 64: Val Loss 538.17181
Epoch 65: Val Loss 538.15454
Epoch 66: Val Loss 538.13739
Epoch 67: Val Loss 538.12018
Epoch 68: Val Loss 538.10297
Epoch 69: Val Loss 538.08557
Epoch 70: Val Loss 538.06842
Epoch 71: Val Loss 538.05133
Epoch 72: Val Loss 538.03418
Epoch 73: Val Loss 538.01691
Epoch 74: Val Loss 537.99982
Epoch 75: Val Loss 537.98291
Epoch 76: Val Loss 537.96588
Epoch 77: Val Loss 537.94873
Epoch 78: Val Loss 537.93170
Epoch 79: Val Loss 537.91461
Epoch 80: Val Loss 537.89746
Epoch 81: Val Loss 537.88025
Epoch 82: Val Loss 537.86328
Epoch 83: Val Loss 537.84637
Epoch 84: Val Loss 537.82928
Epoch 85: Val Loss 537.81219
Epoch 86: Val Loss 537.79529
Epoch 87: Val Loss 537.77838
Epoch 88: Val Loss 537.76129
Epoch 89: Val Loss 537.74420
Epoch 90: Val Loss 537.72723
Epoch 91: Val Loss 537.71021
Epoch 92: Val Loss 537.69324
Epoch 93: Val Loss 537.67615
Epoch 94: Val Loss 537.65894
Epoch 95: Val Loss 537.64160
Epoch 96: Val Loss 537.62427
Epoch 97: Val Loss 537.60718
Epoch 98: Val Loss 537.59009
Epoch 99: Val Loss 537.57312
{'MSE - mean': 587.0251608354422, 'MSE - std': 60.71146323137968, 'R2 - mean': -6.201282269088099, 'R2 - std': 0.19633970104582163} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 568.27026
Epoch 1: Val Loss 568.23322
Epoch 2: Val Loss 568.19629
Epoch 3: Val Loss 568.15961
Epoch 4: Val Loss 568.12335
Epoch 5: Val Loss 568.08710
Epoch 6: Val Loss 568.05078
Epoch 7: Val Loss 568.01459
Epoch 8: Val Loss 567.97845
Epoch 9: Val Loss 567.94226
Epoch 10: Val Loss 567.90637
Epoch 11: Val Loss 567.87054
Epoch 12: Val Loss 567.83459
Epoch 13: Val Loss 567.79883
Epoch 14: Val Loss 567.76312
Epoch 15: Val Loss 567.72754
Epoch 16: Val Loss 567.69196
Epoch 17: Val Loss 567.65637
Epoch 18: Val Loss 567.62079
Epoch 19: Val Loss 567.58557
Epoch 20: Val Loss 567.55011
Epoch 21: Val Loss 567.51508
Epoch 22: Val Loss 567.48047
Epoch 23: Val Loss 567.44598
Epoch 24: Val Loss 567.41168
Epoch 25: Val Loss 567.37714
Epoch 26: Val Loss 567.34253
Epoch 27: Val Loss 567.30798
Epoch 28: Val Loss 567.27368
Epoch 29: Val Loss 567.23938
Epoch 30: Val Loss 567.20508
Epoch 31: Val Loss 567.17065
Epoch 32: Val Loss 567.13666
Epoch 33: Val Loss 567.10242
Epoch 34: Val Loss 567.06836
Epoch 35: Val Loss 567.03455
Epoch 36: Val Loss 567.00067
Epoch 37: Val Loss 566.96686
Epoch 38: Val Loss 566.93311
Epoch 39: Val Loss 566.89935
Epoch 40: Val Loss 566.86572
Epoch 41: Val Loss 566.83221
Epoch 42: Val Loss 566.79858
Epoch 43: Val Loss 566.76495
Epoch 44: Val Loss 566.73151
Epoch 45: Val Loss 566.69830
Epoch 46: Val Loss 566.66492
Epoch 47: Val Loss 566.63147
Epoch 48: Val Loss 566.59802
Epoch 49: Val Loss 566.56421
Epoch 50: Val Loss 566.53052
Epoch 51: Val Loss 566.49707
Epoch 52: Val Loss 566.46368
Epoch 53: Val Loss 566.43036
Epoch 54: Val Loss 566.39752
Epoch 55: Val Loss 566.36450
Epoch 56: Val Loss 566.33154
Epoch 57: Val Loss 566.29840
Epoch 58: Val Loss 566.26556
Epoch 59: Val Loss 566.23254
Epoch 60: Val Loss 566.19958
Epoch 61: Val Loss 566.16669
Epoch 62: Val Loss 566.13409
Epoch 63: Val Loss 566.10156
Epoch 64: Val Loss 566.06909
Epoch 65: Val Loss 566.03668
Epoch 66: Val Loss 566.00427
Epoch 67: Val Loss 565.97198
Epoch 68: Val Loss 565.93994
Epoch 69: Val Loss 565.90802
Epoch 70: Val Loss 565.87616
Epoch 71: Val Loss 565.84430
Epoch 72: Val Loss 565.81244
Epoch 73: Val Loss 565.78052
Epoch 74: Val Loss 565.74866
Epoch 75: Val Loss 565.71680
Epoch 76: Val Loss 565.68488
Epoch 77: Val Loss 565.65320
Epoch 78: Val Loss 565.62152
Epoch 79: Val Loss 565.58984
Epoch 80: Val Loss 565.55817
Epoch 81: Val Loss 565.52673
Epoch 82: Val Loss 565.49512
Epoch 83: Val Loss 565.46332
Epoch 84: Val Loss 565.43152
Epoch 85: Val Loss 565.39954
Epoch 86: Val Loss 565.36725
Epoch 87: Val Loss 565.33514
Epoch 88: Val Loss 565.30298
Epoch 89: Val Loss 565.27100
Epoch 90: Val Loss 565.23938
Epoch 91: Val Loss 565.20758
Epoch 92: Val Loss 565.17566
Epoch 93: Val Loss 565.14380
Epoch 94: Val Loss 565.11206
Epoch 95: Val Loss 565.08008
Epoch 96: Val Loss 565.04816
Epoch 97: Val Loss 565.01642
Epoch 98: Val Loss 564.98456
Epoch 99: Val Loss 564.95270
{'MSE - mean': 582.6106659608547, 'MSE - std': 55.01505679490998, 'R2 - mean': -5.99891946774283, 'R2 - std': 0.4411827694262663} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 582.6106659608547 and parameters: {'dim': 128, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 24 with value: 127.08393124253458.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 575.92798
Epoch 1: Val Loss 575.73883
Epoch 2: Val Loss 575.55005
Epoch 3: Val Loss 575.36127
Epoch 4: Val Loss 575.17157
Epoch 5: Val Loss 574.98169
Epoch 6: Val Loss 574.79205
Epoch 7: Val Loss 574.60217
Epoch 8: Val Loss 574.41266
Epoch 9: Val Loss 574.22412
Epoch 10: Val Loss 574.03638
Epoch 11: Val Loss 573.84753
Epoch 12: Val Loss 573.65796
Epoch 13: Val Loss 573.46906
Epoch 14: Val Loss 573.28168
Epoch 15: Val Loss 573.09583
Epoch 16: Val Loss 572.91058
Epoch 17: Val Loss 572.72388
Epoch 18: Val Loss 572.53619
Epoch 19: Val Loss 572.34857
Epoch 20: Val Loss 572.16150
Epoch 21: Val Loss 571.97357
Epoch 22: Val Loss 571.78534
Epoch 23: Val Loss 571.59778
Epoch 24: Val Loss 571.41046
Epoch 25: Val Loss 571.22278
Epoch 26: Val Loss 571.03479
Epoch 27: Val Loss 570.84729
Epoch 28: Val Loss 570.66052
Epoch 29: Val Loss 570.47296
Epoch 30: Val Loss 570.28833
Epoch 31: Val Loss 570.10437
Epoch 32: Val Loss 569.91931
Epoch 33: Val Loss 569.73407
Epoch 34: Val Loss 569.54980
Epoch 35: Val Loss 569.36560
Epoch 36: Val Loss 569.18170
Epoch 37: Val Loss 568.99768
Epoch 38: Val Loss 568.81396
Epoch 39: Val Loss 568.62976
Epoch 40: Val Loss 568.44635
Epoch 41: Val Loss 568.26318
Epoch 42: Val Loss 568.08112
Epoch 43: Val Loss 567.89868
Epoch 44: Val Loss 567.71564
Epoch 45: Val Loss 567.53290
Epoch 46: Val Loss 567.35034
Epoch 47: Val Loss 567.16644
Epoch 48: Val Loss 566.98157
Epoch 49: Val Loss 566.79541
Epoch 50: Val Loss 566.61041
Epoch 51: Val Loss 566.42633
Epoch 52: Val Loss 566.24304
Epoch 53: Val Loss 566.05945
Epoch 54: Val Loss 565.87524
Epoch 55: Val Loss 565.69214
Epoch 56: Val Loss 565.50873
Epoch 57: Val Loss 565.32556
Epoch 58: Val Loss 565.14294
Epoch 59: Val Loss 564.96027
Epoch 60: Val Loss 564.77686
Epoch 61: Val Loss 564.59540
Epoch 62: Val Loss 564.41510
Epoch 63: Val Loss 564.23395
Epoch 64: Val Loss 564.05151
Epoch 65: Val Loss 563.86914
Epoch 66: Val Loss 563.68652
Epoch 67: Val Loss 563.50476
Epoch 68: Val Loss 563.32117
Epoch 69: Val Loss 563.13898
Epoch 70: Val Loss 562.95782
Epoch 71: Val Loss 562.77661
Epoch 72: Val Loss 562.59589
Epoch 73: Val Loss 562.41522
Epoch 74: Val Loss 562.23572
Epoch 75: Val Loss 562.05670
Epoch 76: Val Loss 561.87787
Epoch 77: Val Loss 561.69806
Epoch 78: Val Loss 561.51746
Epoch 79: Val Loss 561.33789
Epoch 80: Val Loss 561.15729
Epoch 81: Val Loss 560.97717
Epoch 82: Val Loss 560.79645
Epoch 83: Val Loss 560.61511
Epoch 84: Val Loss 560.43353
Epoch 85: Val Loss 560.25330
Epoch 86: Val Loss 560.07538
Epoch 87: Val Loss 559.89783
Epoch 88: Val Loss 559.72076
Epoch 89: Val Loss 559.54382
Epoch 90: Val Loss 559.36597
Epoch 91: Val Loss 559.18732
Epoch 92: Val Loss 559.00757
Epoch 93: Val Loss 558.82751
Epoch 94: Val Loss 558.64813
Epoch 95: Val Loss 558.46991
Epoch 96: Val Loss 558.29144
Epoch 97: Val Loss 558.11359
Epoch 98: Val Loss 557.93475
Epoch 99: Val Loss 557.75635
{'MSE - mean': 557.7563424568142, 'MSE - std': 0.0, 'R2 - mean': -6.469788990381712, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 591.22894
Epoch 1: Val Loss 590.73993
Epoch 2: Val Loss 590.30963
Epoch 3: Val Loss 589.91388
Epoch 4: Val Loss 589.54224
Epoch 5: Val Loss 589.19525
Epoch 6: Val Loss 588.85675
Epoch 7: Val Loss 588.52374
Epoch 8: Val Loss 588.19647
Epoch 9: Val Loss 587.87567
Epoch 10: Val Loss 587.56201
Epoch 11: Val Loss 587.26660
Epoch 12: Val Loss 586.97424
Epoch 13: Val Loss 586.69141
Epoch 14: Val Loss 586.41589
Epoch 15: Val Loss 586.15784
Epoch 16: Val Loss 585.90320
Epoch 17: Val Loss 585.65967
Epoch 18: Val Loss 585.43866
Epoch 19: Val Loss 585.22156
Epoch 20: Val Loss 585.00977
Epoch 21: Val Loss 584.80902
Epoch 22: Val Loss 584.61798
Epoch 23: Val Loss 584.43048
Epoch 24: Val Loss 584.24768
Epoch 25: Val Loss 584.07013
Epoch 26: Val Loss 583.89142
Epoch 27: Val Loss 583.71320
Epoch 28: Val Loss 583.53589
Epoch 29: Val Loss 583.36078
Epoch 30: Val Loss 583.18542
Epoch 31: Val Loss 583.00940
Epoch 32: Val Loss 582.83545
Epoch 33: Val Loss 582.66241
Epoch 34: Val Loss 582.49030
Epoch 35: Val Loss 582.32031
Epoch 36: Val Loss 582.15314
Epoch 37: Val Loss 581.98761
Epoch 38: Val Loss 581.82385
Epoch 39: Val Loss 581.66010
Epoch 40: Val Loss 581.49701
Epoch 41: Val Loss 581.33185
Epoch 42: Val Loss 581.16797
Epoch 43: Val Loss 581.00433
Epoch 44: Val Loss 580.84033
Epoch 45: Val Loss 580.67560
Epoch 46: Val Loss 580.51074
Epoch 47: Val Loss 580.34540
Epoch 48: Val Loss 580.18066
Epoch 49: Val Loss 580.01752
Epoch 50: Val Loss 579.85443
Epoch 51: Val Loss 579.69098
Epoch 52: Val Loss 579.52789
Epoch 53: Val Loss 579.36395
Epoch 54: Val Loss 579.19824
Epoch 55: Val Loss 579.03265
Epoch 56: Val Loss 578.86859
Epoch 57: Val Loss 578.70612
Epoch 58: Val Loss 578.54486
Epoch 59: Val Loss 578.38483
Epoch 60: Val Loss 578.22400
Epoch 61: Val Loss 578.06140
Epoch 62: Val Loss 577.89734
Epoch 63: Val Loss 577.73376
Epoch 64: Val Loss 577.57043
Epoch 65: Val Loss 577.40576
Epoch 66: Val Loss 577.24103
Epoch 67: Val Loss 577.07935
Epoch 68: Val Loss 576.91809
Epoch 69: Val Loss 576.75751
Epoch 70: Val Loss 576.59729
Epoch 71: Val Loss 576.43713
Epoch 72: Val Loss 576.27765
Epoch 73: Val Loss 576.11853
Epoch 74: Val Loss 575.95770
Epoch 75: Val Loss 575.79718
Epoch 76: Val Loss 575.63641
Epoch 77: Val Loss 575.47791
Epoch 78: Val Loss 575.31891
Epoch 79: Val Loss 575.15753
Epoch 80: Val Loss 574.99670
Epoch 81: Val Loss 574.83575
Epoch 82: Val Loss 574.67731
Epoch 83: Val Loss 574.51923
Epoch 84: Val Loss 574.36053
Epoch 85: Val Loss 574.20111
Epoch 86: Val Loss 574.04224
Epoch 87: Val Loss 573.88220
Epoch 88: Val Loss 573.72260
Epoch 89: Val Loss 573.56366
Epoch 90: Val Loss 573.40344
Epoch 91: Val Loss 573.24261
Epoch 92: Val Loss 573.08276
Epoch 93: Val Loss 572.92163
Epoch 94: Val Loss 572.76025
Epoch 95: Val Loss 572.59961
Epoch 96: Val Loss 572.44049
Epoch 97: Val Loss 572.28180
Epoch 98: Val Loss 572.12347
Epoch 99: Val Loss 571.96539
{'MSE - mean': 564.8608977659201, 'MSE - std': 7.104555309105933, 'R2 - mean': -6.285705209278818, 'R2 - std': 0.18408378110289414} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 711.82965
Epoch 1: Val Loss 711.61847
Epoch 2: Val Loss 711.40637
Epoch 3: Val Loss 711.19421
Epoch 4: Val Loss 710.98315
Epoch 5: Val Loss 710.77222
Epoch 6: Val Loss 710.56158
Epoch 7: Val Loss 710.35181
Epoch 8: Val Loss 710.14105
Epoch 9: Val Loss 709.93146
Epoch 10: Val Loss 709.72162
Epoch 11: Val Loss 709.51074
Epoch 12: Val Loss 709.30133
Epoch 13: Val Loss 709.09155
Epoch 14: Val Loss 708.88190
Epoch 15: Val Loss 708.67334
Epoch 16: Val Loss 708.46405
Epoch 17: Val Loss 708.25464
Epoch 18: Val Loss 708.04456
Epoch 19: Val Loss 707.83539
Epoch 20: Val Loss 707.62592
Epoch 21: Val Loss 707.41644
Epoch 22: Val Loss 707.20740
Epoch 23: Val Loss 706.99890
Epoch 24: Val Loss 706.78961
Epoch 25: Val Loss 706.58026
Epoch 26: Val Loss 706.37030
Epoch 27: Val Loss 706.16052
Epoch 28: Val Loss 705.95117
Epoch 29: Val Loss 705.74072
Epoch 30: Val Loss 705.53070
Epoch 31: Val Loss 705.32098
Epoch 32: Val Loss 705.11255
Epoch 33: Val Loss 704.90607
Epoch 34: Val Loss 704.70154
Epoch 35: Val Loss 704.49768
Epoch 36: Val Loss 704.29291
Epoch 37: Val Loss 704.08868
Epoch 38: Val Loss 703.88379
Epoch 39: Val Loss 703.67883
Epoch 40: Val Loss 703.47278
Epoch 41: Val Loss 703.26746
Epoch 42: Val Loss 703.06250
Epoch 43: Val Loss 702.85712
Epoch 44: Val Loss 702.65222
Epoch 45: Val Loss 702.44690
Epoch 46: Val Loss 702.24219
Epoch 47: Val Loss 702.03821
Epoch 48: Val Loss 701.83386
Epoch 49: Val Loss 701.62988
Epoch 50: Val Loss 701.42651
Epoch 51: Val Loss 701.22321
Epoch 52: Val Loss 701.01990
Epoch 53: Val Loss 700.81757
Epoch 54: Val Loss 700.61481
Epoch 55: Val Loss 700.40857
Epoch 56: Val Loss 700.20367
Epoch 57: Val Loss 700.00024
Epoch 58: Val Loss 699.79944
Epoch 59: Val Loss 699.59723
Epoch 60: Val Loss 699.39423
Epoch 61: Val Loss 699.19220
Epoch 62: Val Loss 698.98956
Epoch 63: Val Loss 698.78528
Epoch 64: Val Loss 698.58215
Epoch 65: Val Loss 698.37860
Epoch 66: Val Loss 698.17450
Epoch 67: Val Loss 697.97028
Epoch 68: Val Loss 697.76636
Epoch 69: Val Loss 697.56158
Epoch 70: Val Loss 697.35712
Epoch 71: Val Loss 697.15405
Epoch 72: Val Loss 696.95227
Epoch 73: Val Loss 696.75012
Epoch 74: Val Loss 696.54919
Epoch 75: Val Loss 696.34851
Epoch 76: Val Loss 696.14856
Epoch 77: Val Loss 695.94818
Epoch 78: Val Loss 695.74799
Epoch 79: Val Loss 695.54657
Epoch 80: Val Loss 695.34637
Epoch 81: Val Loss 695.14850
Epoch 82: Val Loss 694.94977
Epoch 83: Val Loss 694.74829
Epoch 84: Val Loss 694.54761
Epoch 85: Val Loss 694.34821
Epoch 86: Val Loss 694.14972
Epoch 87: Val Loss 693.95062
Epoch 88: Val Loss 693.75244
Epoch 89: Val Loss 693.55505
Epoch 90: Val Loss 693.35657
Epoch 91: Val Loss 693.15625
Epoch 92: Val Loss 692.95636
Epoch 93: Val Loss 692.75739
Epoch 94: Val Loss 692.55896
Epoch 95: Val Loss 692.36078
Epoch 96: Val Loss 692.16321
Epoch 97: Val Loss 691.96301
Epoch 98: Val Loss 691.76208
Epoch 99: Val Loss 691.56073
{'MSE - mean': 607.0941714575196, 'MSE - std': 60.00790462524512, 'R2 - mean': -6.272636988265461, 'R2 - std': 0.15143573701188712} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 559.54779
Epoch 1: Val Loss 559.09576
Epoch 2: Val Loss 558.65356
Epoch 3: Val Loss 558.23480
Epoch 4: Val Loss 557.83398
Epoch 5: Val Loss 557.46252
Epoch 6: Val Loss 557.12305
Epoch 7: Val Loss 556.80432
Epoch 8: Val Loss 556.50024
Epoch 9: Val Loss 556.21417
Epoch 10: Val Loss 555.95282
Epoch 11: Val Loss 555.70508
Epoch 12: Val Loss 555.46118
Epoch 13: Val Loss 555.22314
Epoch 14: Val Loss 554.99274
Epoch 15: Val Loss 554.76855
Epoch 16: Val Loss 554.54712
Epoch 17: Val Loss 554.32721
Epoch 18: Val Loss 554.10974
Epoch 19: Val Loss 553.89941
Epoch 20: Val Loss 553.69360
Epoch 21: Val Loss 553.48914
Epoch 22: Val Loss 553.28735
Epoch 23: Val Loss 553.08850
Epoch 24: Val Loss 552.89081
Epoch 25: Val Loss 552.69513
Epoch 26: Val Loss 552.50208
Epoch 27: Val Loss 552.30933
Epoch 28: Val Loss 552.11682
Epoch 29: Val Loss 551.92487
Epoch 30: Val Loss 551.73260
Epoch 31: Val Loss 551.54077
Epoch 32: Val Loss 551.35107
Epoch 33: Val Loss 551.16187
Epoch 34: Val Loss 550.97449
Epoch 35: Val Loss 550.78827
Epoch 36: Val Loss 550.60394
Epoch 37: Val Loss 550.41962
Epoch 38: Val Loss 550.23517
Epoch 39: Val Loss 550.05231
Epoch 40: Val Loss 549.86902
Epoch 41: Val Loss 549.68744
Epoch 42: Val Loss 549.50635
Epoch 43: Val Loss 549.32526
Epoch 44: Val Loss 549.14490
Epoch 45: Val Loss 548.96381
Epoch 46: Val Loss 548.78369
Epoch 47: Val Loss 548.60364
Epoch 48: Val Loss 548.42334
Epoch 49: Val Loss 548.24432
Epoch 50: Val Loss 548.06689
Epoch 51: Val Loss 547.89062
Epoch 52: Val Loss 547.71558
Epoch 53: Val Loss 547.54034
Epoch 54: Val Loss 547.36310
Epoch 55: Val Loss 547.18445
Epoch 56: Val Loss 547.00720
Epoch 57: Val Loss 546.83038
Epoch 58: Val Loss 546.65234
Epoch 59: Val Loss 546.47552
Epoch 60: Val Loss 546.29742
Epoch 61: Val Loss 546.12048
Epoch 62: Val Loss 545.94342
Epoch 63: Val Loss 545.76514
Epoch 64: Val Loss 545.58813
Epoch 65: Val Loss 545.40955
Epoch 66: Val Loss 545.23145
Epoch 67: Val Loss 545.05353
Epoch 68: Val Loss 544.87738
Epoch 69: Val Loss 544.70221
Epoch 70: Val Loss 544.52661
Epoch 71: Val Loss 544.35315
Epoch 72: Val Loss 544.17950
Epoch 73: Val Loss 544.00562
Epoch 74: Val Loss 543.83368
Epoch 75: Val Loss 543.66211
Epoch 76: Val Loss 543.49054
Epoch 77: Val Loss 543.31866
Epoch 78: Val Loss 543.14349
Epoch 79: Val Loss 542.96771
Epoch 80: Val Loss 542.79333
Epoch 81: Val Loss 542.61926
Epoch 82: Val Loss 542.44690
Epoch 83: Val Loss 542.27460
Epoch 84: Val Loss 542.10162
Epoch 85: Val Loss 541.92853
Epoch 86: Val Loss 541.75580
Epoch 87: Val Loss 541.58276
Epoch 88: Val Loss 541.40900
Epoch 89: Val Loss 541.23615
Epoch 90: Val Loss 541.06415
Epoch 91: Val Loss 540.89301
Epoch 92: Val Loss 540.72229
Epoch 93: Val Loss 540.55273
Epoch 94: Val Loss 540.38177
Epoch 95: Val Loss 540.20972
Epoch 96: Val Loss 540.03711
Epoch 97: Val Loss 539.86499
Epoch 98: Val Loss 539.69354
Epoch 99: Val Loss 539.52313
{'MSE - mean': 590.201405083535, 'MSE - std': 59.63898163477018, 'R2 - mean': -6.242030233162525, 'R2 - std': 0.1414563791670188} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 608.08807
Epoch 1: Val Loss 607.47742
Epoch 2: Val Loss 606.87463
Epoch 3: Val Loss 606.27698
Epoch 4: Val Loss 605.68811
Epoch 5: Val Loss 605.10828
Epoch 6: Val Loss 604.54364
Epoch 7: Val Loss 603.98639
Epoch 8: Val Loss 603.43585
Epoch 9: Val Loss 602.88995
Epoch 10: Val Loss 602.34705
Epoch 11: Val Loss 601.80975
Epoch 12: Val Loss 601.28265
Epoch 13: Val Loss 600.76392
Epoch 14: Val Loss 600.24298
Epoch 15: Val Loss 599.72076
Epoch 16: Val Loss 599.20471
Epoch 17: Val Loss 598.69116
Epoch 18: Val Loss 598.17828
Epoch 19: Val Loss 597.67151
Epoch 20: Val Loss 597.16248
Epoch 21: Val Loss 596.64783
Epoch 22: Val Loss 596.13257
Epoch 23: Val Loss 595.61340
Epoch 24: Val Loss 595.09167
Epoch 25: Val Loss 594.56549
Epoch 26: Val Loss 594.02832
Epoch 27: Val Loss 593.48450
Epoch 28: Val Loss 592.93805
Epoch 29: Val Loss 592.38708
Epoch 30: Val Loss 591.82733
Epoch 31: Val Loss 591.25726
Epoch 32: Val Loss 590.67413
Epoch 33: Val Loss 590.08490
Epoch 34: Val Loss 589.49457
Epoch 35: Val Loss 588.90344
Epoch 36: Val Loss 588.30042
Epoch 37: Val Loss 587.68048
Epoch 38: Val Loss 587.03729
Epoch 39: Val Loss 586.36859
Epoch 40: Val Loss 585.66083
Epoch 41: Val Loss 584.88092
Epoch 42: Val Loss 584.03284
Epoch 43: Val Loss 583.09473
Epoch 44: Val Loss 582.06738
Epoch 45: Val Loss 580.93512
Epoch 46: Val Loss 579.69415
Epoch 47: Val Loss 578.33978
Epoch 48: Val Loss 576.84814
Epoch 49: Val Loss 575.24768
Epoch 50: Val Loss 573.55194
Epoch 51: Val Loss 571.74390
Epoch 52: Val Loss 569.83173
Epoch 53: Val Loss 567.81488
Epoch 54: Val Loss 565.70502
Epoch 55: Val Loss 563.46466
Epoch 56: Val Loss 561.08722
Epoch 57: Val Loss 558.59229
Epoch 58: Val Loss 555.94995
Epoch 59: Val Loss 553.14447
Epoch 60: Val Loss 550.20044
Epoch 61: Val Loss 547.14612
Epoch 62: Val Loss 543.91534
Epoch 63: Val Loss 540.46704
Epoch 64: Val Loss 536.85742
Epoch 65: Val Loss 533.06891
Epoch 66: Val Loss 529.22302
Epoch 67: Val Loss 525.21973
Epoch 68: Val Loss 521.00098
Epoch 69: Val Loss 516.60315
Epoch 70: Val Loss 511.98206
Epoch 71: Val Loss 507.13269
Epoch 72: Val Loss 502.01273
Epoch 73: Val Loss 496.64240
Epoch 74: Val Loss 491.03442
Epoch 75: Val Loss 485.15747
Epoch 76: Val Loss 479.05328
Epoch 77: Val Loss 472.66196
Epoch 78: Val Loss 466.08051
Epoch 79: Val Loss 459.18539
Epoch 80: Val Loss 451.98843
Epoch 81: Val Loss 444.44089
Epoch 82: Val Loss 436.68597
Epoch 83: Val Loss 428.57883
Epoch 84: Val Loss 420.34061
Epoch 85: Val Loss 411.89731
Epoch 86: Val Loss 403.11026
Epoch 87: Val Loss 393.92490
Epoch 88: Val Loss 384.36090
Epoch 89: Val Loss 374.60385
Epoch 90: Val Loss 364.71271
Epoch 91: Val Loss 354.76636
Epoch 92: Val Loss 344.65005
Epoch 93: Val Loss 334.37659
Epoch 94: Val Loss 324.04343
Epoch 95: Val Loss 313.59793
Epoch 96: Val Loss 303.04648
Epoch 97: Val Loss 292.47598
Epoch 98: Val Loss 281.95609
Epoch 99: Val Loss 271.36041
{'MSE - mean': 526.4332093856733, 'MSE - std': 138.2424595939652, 'R2 - mean': -5.388214419769802, 'R2 - std': 1.7123123834638174} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 526.4332093856733 and parameters: {'dim': 128, 'depth': 1, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 127.08393124253458.
Best parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 542.67084
Epoch 1: Val Loss 541.74805
Epoch 2: Val Loss 540.78577
Epoch 3: Val Loss 539.79572
Epoch 4: Val Loss 538.76685
Epoch 5: Val Loss 537.69476
Epoch 6: Val Loss 536.58490
Epoch 7: Val Loss 535.43970
Epoch 8: Val Loss 534.26740
Epoch 9: Val Loss 533.05017
Epoch 10: Val Loss 531.78247
Epoch 11: Val Loss 530.46924
Epoch 12: Val Loss 529.10468
Epoch 13: Val Loss 527.68561
Epoch 14: Val Loss 526.20667
Epoch 15: Val Loss 524.69348
Epoch 16: Val Loss 523.11279
Epoch 17: Val Loss 521.48212
Epoch 18: Val Loss 519.78638
Epoch 19: Val Loss 518.01703
Epoch 20: Val Loss 516.17749
Epoch 21: Val Loss 514.27802
Epoch 22: Val Loss 512.31232
Epoch 23: Val Loss 510.26468
Epoch 24: Val Loss 508.16071
Epoch 25: Val Loss 505.96609
Epoch 26: Val Loss 503.68292
Epoch 27: Val Loss 501.31644
Epoch 28: Val Loss 498.83737
Epoch 29: Val Loss 496.26288
Epoch 30: Val Loss 493.59418
Epoch 31: Val Loss 490.81110
Epoch 32: Val Loss 487.91867
Epoch 33: Val Loss 484.89105
Epoch 34: Val Loss 481.76196
Epoch 35: Val Loss 478.49167
Epoch 36: Val Loss 475.10480
Epoch 37: Val Loss 471.55441
Epoch 38: Val Loss 467.85178
Epoch 39: Val Loss 464.00357
Epoch 40: Val Loss 460.00214
Epoch 41: Val Loss 455.87265
Epoch 42: Val Loss 451.59222
Epoch 43: Val Loss 447.16840
Epoch 44: Val Loss 442.55432
Epoch 45: Val Loss 437.72845
Epoch 46: Val Loss 432.75681
Epoch 47: Val Loss 427.59598
Epoch 48: Val Loss 422.21857
Epoch 49: Val Loss 416.53815
Epoch 50: Val Loss 410.70215
Epoch 51: Val Loss 404.62784
Epoch 52: Val Loss 398.26312
Epoch 53: Val Loss 391.72000
Epoch 54: Val Loss 384.95993
Epoch 55: Val Loss 378.06229
Epoch 56: Val Loss 370.91632
Epoch 57: Val Loss 363.57797
Epoch 58: Val Loss 355.96063
Epoch 59: Val Loss 348.19843
Epoch 60: Val Loss 340.28821
Epoch 61: Val Loss 332.18909
Epoch 62: Val Loss 324.00745
Epoch 63: Val Loss 315.64044
Epoch 64: Val Loss 307.11703
Epoch 65: Val Loss 298.51904
Epoch 66: Val Loss 289.79941
Epoch 67: Val Loss 281.10321
Epoch 68: Val Loss 272.37418
Epoch 69: Val Loss 263.64981
Epoch 70: Val Loss 254.86371
Epoch 71: Val Loss 245.92868
Epoch 72: Val Loss 236.91733
Epoch 73: Val Loss 227.97354
Epoch 74: Val Loss 219.01025
Epoch 75: Val Loss 210.10724
Epoch 76: Val Loss 201.15752
Epoch 77: Val Loss 192.33830
Epoch 78: Val Loss 183.65648
Epoch 79: Val Loss 175.10414
Epoch 80: Val Loss 166.76007
Epoch 81: Val Loss 158.70352
Epoch 82: Val Loss 150.84732
Epoch 83: Val Loss 143.21346
Epoch 84: Val Loss 135.94148
Epoch 85: Val Loss 128.96263
Epoch 86: Val Loss 122.24998
Epoch 87: Val Loss 115.74907
Epoch 88: Val Loss 109.52418
Epoch 89: Val Loss 103.79150
Epoch 90: Val Loss 98.30774
Epoch 91: Val Loss 93.23508
Epoch 92: Val Loss 88.48426
Epoch 93: Val Loss 84.31684
Epoch 94: Val Loss 80.55337
Epoch 95: Val Loss 77.04353
Epoch 96: Val Loss 73.86982
Epoch 97: Val Loss 71.18389
Epoch 98: Val Loss 68.82396
Epoch 99: Val Loss 66.62872
Saved Losses
{'MSE - mean': 66.62871314228532, 'MSE - std': 0.0, 'R2 - mean': 0.10767051848974696, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 645.28937
Epoch 1: Val Loss 644.94128
Epoch 2: Val Loss 644.58374
Epoch 3: Val Loss 644.21771
Epoch 4: Val Loss 643.83508
Epoch 5: Val Loss 643.43433
Epoch 6: Val Loss 643.01996
Epoch 7: Val Loss 642.58636
Epoch 8: Val Loss 642.12708
Epoch 9: Val Loss 641.63550
Epoch 10: Val Loss 641.11212
Epoch 11: Val Loss 640.55463
Epoch 12: Val Loss 639.96387
Epoch 13: Val Loss 639.33447
Epoch 14: Val Loss 638.66461
Epoch 15: Val Loss 637.95221
Epoch 16: Val Loss 637.18829
Epoch 17: Val Loss 636.38202
Epoch 18: Val Loss 635.51477
Epoch 19: Val Loss 634.58228
Epoch 20: Val Loss 633.57617
Epoch 21: Val Loss 632.50067
Epoch 22: Val Loss 631.35541
Epoch 23: Val Loss 630.10852
Epoch 24: Val Loss 628.77850
Epoch 25: Val Loss 627.35675
Epoch 26: Val Loss 625.81696
Epoch 27: Val Loss 624.18433
Epoch 28: Val Loss 622.41736
Epoch 29: Val Loss 620.50751
Epoch 30: Val Loss 618.48132
Epoch 31: Val Loss 616.33679
Epoch 32: Val Loss 614.06085
Epoch 33: Val Loss 611.61139
Epoch 34: Val Loss 608.98944
Epoch 35: Val Loss 606.20428
Epoch 36: Val Loss 603.17731
Epoch 37: Val Loss 599.94171
Epoch 38: Val Loss 596.44702
Epoch 39: Val Loss 592.71216
Epoch 40: Val Loss 588.74146
Epoch 41: Val Loss 584.57068
Epoch 42: Val Loss 580.12567
Epoch 43: Val Loss 575.41418
Epoch 44: Val Loss 570.43250
Epoch 45: Val Loss 565.29767
Epoch 46: Val Loss 559.89594
Epoch 47: Val Loss 554.13684
Epoch 48: Val Loss 548.01044
Epoch 49: Val Loss 541.50879
Epoch 50: Val Loss 534.63263
Epoch 51: Val Loss 527.43579
Epoch 52: Val Loss 519.92468
Epoch 53: Val Loss 512.18292
Epoch 54: Val Loss 504.07889
Epoch 55: Val Loss 495.58597
Epoch 56: Val Loss 486.81314
Epoch 57: Val Loss 477.69101
Epoch 58: Val Loss 468.32617
Epoch 59: Val Loss 458.76450
Epoch 60: Val Loss 449.04022
Epoch 61: Val Loss 439.00851
Epoch 62: Val Loss 428.69635
Epoch 63: Val Loss 418.26373
Epoch 64: Val Loss 407.54974
Epoch 65: Val Loss 396.61475
Epoch 66: Val Loss 385.43304
Epoch 67: Val Loss 374.13181
Epoch 68: Val Loss 362.76965
Epoch 69: Val Loss 351.20209
Epoch 70: Val Loss 339.49774
Epoch 71: Val Loss 327.53940
Epoch 72: Val Loss 315.37671
Epoch 73: Val Loss 303.09848
Epoch 74: Val Loss 290.92316
Epoch 75: Val Loss 278.58902
Epoch 76: Val Loss 266.41367
Epoch 77: Val Loss 254.41913
Epoch 78: Val Loss 242.57649
Epoch 79: Val Loss 231.11853
Epoch 80: Val Loss 220.06331
Epoch 81: Val Loss 209.21971
Epoch 82: Val Loss 198.58914
Epoch 83: Val Loss 188.47432
Epoch 84: Val Loss 178.51093
Epoch 85: Val Loss 168.87166
Epoch 86: Val Loss 159.57053
Epoch 87: Val Loss 150.87720
Epoch 88: Val Loss 142.46254
Epoch 89: Val Loss 134.64368
Epoch 90: Val Loss 127.22285
Epoch 91: Val Loss 120.17017
Epoch 92: Val Loss 113.60531
Epoch 93: Val Loss 107.62888
Epoch 94: Val Loss 102.10243
Epoch 95: Val Loss 96.97050
Epoch 96: Val Loss 92.27116
Epoch 97: Val Loss 88.03270
Epoch 98: Val Loss 84.20341
Epoch 99: Val Loss 80.70358
Saved Losses
{'MSE - mean': 73.66614605285181, 'MSE - std': 7.037432910566487, 'R2 - mean': 0.052820501608872805, 'R2 - std': 0.05485001688087415} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 672.62238
Epoch 1: Val Loss 671.47083
Epoch 2: Val Loss 670.29407
Epoch 3: Val Loss 669.10309
Epoch 4: Val Loss 667.90900
Epoch 5: Val Loss 666.68182
Epoch 6: Val Loss 665.41949
Epoch 7: Val Loss 664.12823
Epoch 8: Val Loss 662.79065
Epoch 9: Val Loss 661.40375
Epoch 10: Val Loss 659.95715
Epoch 11: Val Loss 658.44568
Epoch 12: Val Loss 656.86139
Epoch 13: Val Loss 655.20441
Epoch 14: Val Loss 653.49451
Epoch 15: Val Loss 651.68677
Epoch 16: Val Loss 649.76678
Epoch 17: Val Loss 647.78815
Epoch 18: Val Loss 645.71082
Epoch 19: Val Loss 643.56024
Epoch 20: Val Loss 641.28394
Epoch 21: Val Loss 638.87292
Epoch 22: Val Loss 636.32446
Epoch 23: Val Loss 633.63556
Epoch 24: Val Loss 630.81494
Epoch 25: Val Loss 627.89490
Epoch 26: Val Loss 624.84760
Epoch 27: Val Loss 621.68964
Epoch 28: Val Loss 618.37189
Epoch 29: Val Loss 614.87738
Epoch 30: Val Loss 611.35583
Epoch 31: Val Loss 607.64087
Epoch 32: Val Loss 603.73065
Epoch 33: Val Loss 599.73358
Epoch 34: Val Loss 595.64972
Epoch 35: Val Loss 591.38049
Epoch 36: Val Loss 586.86285
Epoch 37: Val Loss 582.15460
Epoch 38: Val Loss 577.29913
Epoch 39: Val Loss 572.25775
Epoch 40: Val Loss 567.07312
Epoch 41: Val Loss 561.71478
Epoch 42: Val Loss 556.11322
Epoch 43: Val Loss 550.32635
Epoch 44: Val Loss 544.38129
Epoch 45: Val Loss 538.25964
Epoch 46: Val Loss 531.97717
Epoch 47: Val Loss 525.55316
Epoch 48: Val Loss 518.93176
Epoch 49: Val Loss 512.07434
Epoch 50: Val Loss 505.20096
Epoch 51: Val Loss 498.13229
Epoch 52: Val Loss 490.81735
Epoch 53: Val Loss 483.31879
Epoch 54: Val Loss 475.57416
Epoch 55: Val Loss 467.68951
Epoch 56: Val Loss 459.63187
Epoch 57: Val Loss 451.44992
Epoch 58: Val Loss 443.07397
Epoch 59: Val Loss 434.63950
Epoch 60: Val Loss 425.98792
Epoch 61: Val Loss 417.23776
Epoch 62: Val Loss 408.55786
Epoch 63: Val Loss 399.62585
Epoch 64: Val Loss 390.67511
Epoch 65: Val Loss 381.78580
Epoch 66: Val Loss 372.78891
Epoch 67: Val Loss 363.66684
Epoch 68: Val Loss 354.70029
Epoch 69: Val Loss 345.73242
Epoch 70: Val Loss 336.83731
Epoch 71: Val Loss 327.91696
Epoch 72: Val Loss 319.16818
Epoch 73: Val Loss 310.49899
Epoch 74: Val Loss 301.93018
Epoch 75: Val Loss 293.54858
Epoch 76: Val Loss 285.26547
Epoch 77: Val Loss 277.15295
Epoch 78: Val Loss 269.13055
Epoch 79: Val Loss 261.28928
Epoch 80: Val Loss 253.47989
Epoch 81: Val Loss 246.04416
Epoch 82: Val Loss 238.99077
Epoch 83: Val Loss 232.23839
Epoch 84: Val Loss 225.64742
Epoch 85: Val Loss 219.34007
Epoch 86: Val Loss 213.22946
Epoch 87: Val Loss 207.44708
Epoch 88: Val Loss 201.89665
Epoch 89: Val Loss 196.63528
Epoch 90: Val Loss 191.71550
Epoch 91: Val Loss 187.10146
Epoch 92: Val Loss 182.73917
Epoch 93: Val Loss 178.67326
Epoch 94: Val Loss 174.78458
Epoch 95: Val Loss 171.28168
Epoch 96: Val Loss 167.95103
Epoch 97: Val Loss 164.83125
Epoch 98: Val Loss 161.91899
Epoch 99: Val Loss 159.18399
Saved Losses
{'MSE - mean': 102.17209510169239, 'MSE - std': 40.720943468899875, 'R2 - mean': -0.18745474119208042, 'R2 - std': 0.3427390662112148} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 515.96655
Epoch 1: Val Loss 515.37646
Epoch 2: Val Loss 514.85071
Epoch 3: Val Loss 514.35602
Epoch 4: Val Loss 513.88263
Epoch 5: Val Loss 513.43542
Epoch 6: Val Loss 513.01471
Epoch 7: Val Loss 512.62109
Epoch 8: Val Loss 512.25732
Epoch 9: Val Loss 511.90628
Epoch 10: Val Loss 511.56931
Epoch 11: Val Loss 511.24179
Epoch 12: Val Loss 510.92078
Epoch 13: Val Loss 510.61124
Epoch 14: Val Loss 510.30673
Epoch 15: Val Loss 510.00403
Epoch 16: Val Loss 509.70404
Epoch 17: Val Loss 509.41281
Epoch 18: Val Loss 509.12833
Epoch 19: Val Loss 508.84769
Epoch 20: Val Loss 508.57611
Epoch 21: Val Loss 508.31808
Epoch 22: Val Loss 508.06586
Epoch 23: Val Loss 507.82269
Epoch 24: Val Loss 507.58710
Epoch 25: Val Loss 507.35953
Epoch 26: Val Loss 507.14294
Epoch 27: Val Loss 506.93329
Epoch 28: Val Loss 506.72809
Epoch 29: Val Loss 506.52655
Epoch 30: Val Loss 506.32874
Epoch 31: Val Loss 506.13251
Epoch 32: Val Loss 505.93854
Epoch 33: Val Loss 505.74603
Epoch 34: Val Loss 505.55688
Epoch 35: Val Loss 505.36847
Epoch 36: Val Loss 505.17960
Epoch 37: Val Loss 504.99155
Epoch 38: Val Loss 504.80042
Epoch 39: Val Loss 504.60822
Epoch 40: Val Loss 504.41165
Epoch 41: Val Loss 504.21194
Epoch 42: Val Loss 504.00873
Epoch 43: Val Loss 503.80490
Epoch 44: Val Loss 503.59894
Epoch 45: Val Loss 503.38922
Epoch 46: Val Loss 503.17273
Epoch 47: Val Loss 502.94638
Epoch 48: Val Loss 502.71179
Epoch 49: Val Loss 502.46649
Epoch 50: Val Loss 502.21164
Epoch 51: Val Loss 501.94199
Epoch 52: Val Loss 501.65744
Epoch 53: Val Loss 501.35812
Epoch 54: Val Loss 501.03403
Epoch 55: Val Loss 500.67975
Epoch 56: Val Loss 500.29404
Epoch 57: Val Loss 499.87402
Epoch 58: Val Loss 499.40424
Epoch 59: Val Loss 498.88297
Epoch 60: Val Loss 498.30286
Epoch 61: Val Loss 497.65213
Epoch 62: Val Loss 496.91754
Epoch 63: Val Loss 496.09894
Epoch 64: Val Loss 495.16901
Epoch 65: Val Loss 494.11591
Epoch 66: Val Loss 492.91812
Epoch 67: Val Loss 491.56204
Epoch 68: Val Loss 490.03458
Epoch 69: Val Loss 488.33368
Epoch 70: Val Loss 486.40729
Epoch 71: Val Loss 484.23987
Epoch 72: Val Loss 481.78079
Epoch 73: Val Loss 478.99289
Epoch 74: Val Loss 475.89139
Epoch 75: Val Loss 472.42673
Epoch 76: Val Loss 468.56357
Epoch 77: Val Loss 464.32306
Epoch 78: Val Loss 459.65515
Epoch 79: Val Loss 454.55756
Epoch 80: Val Loss 449.08221
Epoch 81: Val Loss 443.11499
Epoch 82: Val Loss 436.56436
Epoch 83: Val Loss 429.40546
Epoch 84: Val Loss 421.69537
Epoch 85: Val Loss 413.33783
Epoch 86: Val Loss 404.42810
Epoch 87: Val Loss 394.99225
Epoch 88: Val Loss 384.97485
Epoch 89: Val Loss 374.46298
Epoch 90: Val Loss 363.49573
Epoch 91: Val Loss 351.97610
Epoch 92: Val Loss 339.98904
Epoch 93: Val Loss 327.66623
Epoch 94: Val Loss 315.15610
Epoch 95: Val Loss 302.54984
Epoch 96: Val Loss 289.74390
Epoch 97: Val Loss 276.72354
Epoch 98: Val Loss 263.42834
Epoch 99: Val Loss 250.21623
Saved Losses
{'MSE - mean': 139.1831281508474, 'MSE - std': 73.16485583833908, 'R2 - mean': -0.719609564451942, 'R2 - std': 0.9683330100676654} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 598.82666
Epoch 1: Val Loss 598.57220
Epoch 2: Val Loss 598.32971
Epoch 3: Val Loss 598.09583
Epoch 4: Val Loss 597.87073
Epoch 5: Val Loss 597.65131
Epoch 6: Val Loss 597.43719
Epoch 7: Val Loss 597.22919
Epoch 8: Val Loss 597.02448
Epoch 9: Val Loss 596.82245
Epoch 10: Val Loss 596.62134
Epoch 11: Val Loss 596.42194
Epoch 12: Val Loss 596.22333
Epoch 13: Val Loss 596.02887
Epoch 14: Val Loss 595.83636
Epoch 15: Val Loss 595.64410
Epoch 16: Val Loss 595.45276
Epoch 17: Val Loss 595.26227
Epoch 18: Val Loss 595.07294
Epoch 19: Val Loss 594.88440
Epoch 20: Val Loss 594.69458
Epoch 21: Val Loss 594.50500
Epoch 22: Val Loss 594.31610
Epoch 23: Val Loss 594.12891
Epoch 24: Val Loss 593.94257
Epoch 25: Val Loss 593.75598
Epoch 26: Val Loss 593.56921
Epoch 27: Val Loss 593.38177
Epoch 28: Val Loss 593.19598
Epoch 29: Val Loss 593.01196
Epoch 30: Val Loss 592.82764
Epoch 31: Val Loss 592.64362
Epoch 32: Val Loss 592.45978
Epoch 33: Val Loss 592.27515
Epoch 34: Val Loss 592.09125
Epoch 35: Val Loss 591.90521
Epoch 36: Val Loss 591.72021
Epoch 37: Val Loss 591.53455
Epoch 38: Val Loss 591.34937
Epoch 39: Val Loss 591.16461
Epoch 40: Val Loss 590.97803
Epoch 41: Val Loss 590.79144
Epoch 42: Val Loss 590.60669
Epoch 43: Val Loss 590.42212
Epoch 44: Val Loss 590.23663
Epoch 45: Val Loss 590.05121
Epoch 46: Val Loss 589.86627
Epoch 47: Val Loss 589.68243
Epoch 48: Val Loss 589.50037
Epoch 49: Val Loss 589.31812
Epoch 50: Val Loss 589.13611
Epoch 51: Val Loss 588.95465
Epoch 52: Val Loss 588.77252
Epoch 53: Val Loss 588.59009
Epoch 54: Val Loss 588.40741
Epoch 55: Val Loss 588.22351
Epoch 56: Val Loss 588.03882
Epoch 57: Val Loss 587.85431
Epoch 58: Val Loss 587.67108
Epoch 59: Val Loss 587.48706
Epoch 60: Val Loss 587.30420
Epoch 61: Val Loss 587.12238
Epoch 62: Val Loss 586.94043
Epoch 63: Val Loss 586.75909
Epoch 64: Val Loss 586.57697
Epoch 65: Val Loss 586.39459
Epoch 66: Val Loss 586.21387
Epoch 67: Val Loss 586.03192
Epoch 68: Val Loss 585.85156
Epoch 69: Val Loss 585.67188
Epoch 70: Val Loss 585.49109
Epoch 71: Val Loss 585.31195
Epoch 72: Val Loss 585.13153
Epoch 73: Val Loss 584.95001
Epoch 74: Val Loss 584.76819
Epoch 75: Val Loss 584.58679
Epoch 76: Val Loss 584.40546
Epoch 77: Val Loss 584.22668
Epoch 78: Val Loss 584.04889
Epoch 79: Val Loss 583.87030
Epoch 80: Val Loss 583.68945
Epoch 81: Val Loss 583.50970
Epoch 82: Val Loss 583.33038
Epoch 83: Val Loss 583.15002
Epoch 84: Val Loss 582.97028
Epoch 85: Val Loss 582.79059
Epoch 86: Val Loss 582.61053
Epoch 87: Val Loss 582.43176
Epoch 88: Val Loss 582.25342
Epoch 89: Val Loss 582.07520
Epoch 90: Val Loss 581.89734
Epoch 91: Val Loss 581.71851
Epoch 92: Val Loss 581.53876
Epoch 93: Val Loss 581.36005
Epoch 94: Val Loss 581.18225
Epoch 95: Val Loss 581.00525
Epoch 96: Val Loss 580.82776
Epoch 97: Val Loss 580.64923
Epoch 98: Val Loss 580.47101
Epoch 99: Val Loss 580.29285
Saved Losses
{'MSE - mean': 227.4050802127641, 'MSE - std': 188.18854430154997, 'R2 - mean': -1.6471939217555416, 'R2 - std': 2.0473851651232344} 
 

Saving model.....
Results After CV: {'MSE - mean': 227.4050802127641, 'MSE - std': 188.18854430154997, 'R2 - mean': -1.6471939217555416, 'R2 - std': 2.0473851651232344}
Train time: 8.225437221200036
Inference time: 0.045785734599985514
Finished cross validation


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/socmob.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/socmob.yml', data_parallel=False, dataset='Socmob', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 2, 3], num_classes=1, num_features=5, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Socmob...
Dataset loaded! 

X b4 encoding : ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 22.9] 

(1156, 5)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 2, 3]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 2, 3]
Cat Idx Part II: [0, 1, 2, 3] 
ENDE 
 

X after Nominal Encoding: ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 22.9] 
 

Scaling the data...
X after Scaling: ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 0.13440189107338416] 
 

One Hot Encoding...
X after One Hot Encoding: [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0
 0.0 1.0 0.13440189107338416] 
 

args.num_features: 39
args.cat_idx: None
Cat Dims: []
New Shape: (1156, 39)
False 
 

Using an existing study with name 'TabTransformer_Socmob' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1287.32007
Epoch 1: Val Loss 1287.31787
Epoch 2: Val Loss 1287.31604
Epoch 3: Val Loss 1287.31384
Epoch 4: Val Loss 1287.31177
Epoch 5: Val Loss 1287.30981
Epoch 6: Val Loss 1287.30786
Epoch 7: Val Loss 1287.30579
Epoch 8: Val Loss 1287.30383
Epoch 9: Val Loss 1287.30188
Epoch 10: Val Loss 1287.29980
Epoch 11: Val Loss 1287.29773
Epoch 12: Val Loss 1287.29578
Epoch 13: Val Loss 1287.29358
Epoch 14: Val Loss 1287.29175
Epoch 15: Val Loss 1287.28955
Epoch 16: Val Loss 1287.28760
Epoch 17: Val Loss 1287.28552
Epoch 18: Val Loss 1287.28345
Epoch 19: Val Loss 1287.28137
Epoch 20: Val Loss 1287.27942
Epoch 21: Val Loss 1287.27722
Epoch 22: Val Loss 1287.27527
Epoch 23: Val Loss 1287.27332
Epoch 24: Val Loss 1287.27100
Epoch 25: Val Loss 1287.26904
Epoch 26: Val Loss 1287.26672
Epoch 27: Val Loss 1287.26465
Epoch 28: Val Loss 1287.26257
Epoch 29: Val Loss 1287.26050
Epoch 30: Val Loss 1287.25818
Epoch 31: Val Loss 1287.25623
Epoch 32: Val Loss 1287.25403
Epoch 33: Val Loss 1287.25183
Epoch 34: Val Loss 1287.24988
Epoch 35: Val Loss 1287.24780
Epoch 36: Val Loss 1287.24573
Epoch 37: Val Loss 1287.24353
Epoch 38: Val Loss 1287.24133
Epoch 39: Val Loss 1287.23926
Epoch 40: Val Loss 1287.23706
Epoch 41: Val Loss 1287.23486
Epoch 42: Val Loss 1287.23279
Epoch 43: Val Loss 1287.23059
Epoch 44: Val Loss 1287.22839
Epoch 45: Val Loss 1287.22632
Epoch 46: Val Loss 1287.22424
Epoch 47: Val Loss 1287.22217
Epoch 48: Val Loss 1287.22009
Epoch 49: Val Loss 1287.21777
Epoch 50: Val Loss 1287.21582
Epoch 51: Val Loss 1287.21350
Epoch 52: Val Loss 1287.21118
Epoch 53: Val Loss 1287.20911
Epoch 54: Val Loss 1287.20691
Epoch 55: Val Loss 1287.20471
Epoch 56: Val Loss 1287.20264
Epoch 57: Val Loss 1287.20044
Epoch 58: Val Loss 1287.19824
Epoch 59: Val Loss 1287.19617
Epoch 60: Val Loss 1287.19397
Epoch 61: Val Loss 1287.19177
Epoch 62: Val Loss 1287.18970
Epoch 63: Val Loss 1287.18750
Epoch 64: Val Loss 1287.18530
Epoch 65: Val Loss 1287.18311
Epoch 66: Val Loss 1287.18079
Epoch 67: Val Loss 1287.17871
Epoch 68: Val Loss 1287.17664
Epoch 69: Val Loss 1287.17432
Epoch 70: Val Loss 1287.17224
Epoch 71: Val Loss 1287.17004
Epoch 72: Val Loss 1287.16785
Epoch 73: Val Loss 1287.16565
Epoch 74: Val Loss 1287.16333
Epoch 75: Val Loss 1287.16113
Epoch 76: Val Loss 1287.15906
Epoch 77: Val Loss 1287.15674
Epoch 78: Val Loss 1287.15479
Epoch 79: Val Loss 1287.15247
Epoch 80: Val Loss 1287.15027
Epoch 81: Val Loss 1287.14819
Epoch 82: Val Loss 1287.14600
Epoch 83: Val Loss 1287.14380
Epoch 84: Val Loss 1287.14172
Epoch 85: Val Loss 1287.13953
Epoch 86: Val Loss 1287.13708
Epoch 87: Val Loss 1287.13501
Epoch 88: Val Loss 1287.13306
Epoch 89: Val Loss 1287.13062
Epoch 90: Val Loss 1287.12854
Epoch 91: Val Loss 1287.12634
Epoch 92: Val Loss 1287.12427
Epoch 93: Val Loss 1287.12219
Epoch 94: Val Loss 1287.12000
Epoch 95: Val Loss 1287.11780
Epoch 96: Val Loss 1287.11584
Epoch 97: Val Loss 1287.11365
Epoch 98: Val Loss 1287.11157
Epoch 99: Val Loss 1287.10950
{'MSE - mean': 1287.1095261608148, 'MSE - std': 0.0, 'R2 - mean': -0.25726394963468824, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1308.89502
Epoch 1: Val Loss 1308.89233
Epoch 2: Val Loss 1308.88977
Epoch 3: Val Loss 1308.88708
Epoch 4: Val Loss 1308.88452
Epoch 5: Val Loss 1308.88196
Epoch 6: Val Loss 1308.87915
Epoch 7: Val Loss 1308.87646
Epoch 8: Val Loss 1308.87390
Epoch 9: Val Loss 1308.87122
Epoch 10: Val Loss 1308.86853
Epoch 11: Val Loss 1308.86609
Epoch 12: Val Loss 1308.86340
Epoch 13: Val Loss 1308.86047
Epoch 14: Val Loss 1308.85791
Epoch 15: Val Loss 1308.85522
Epoch 16: Val Loss 1308.85229
Epoch 17: Val Loss 1308.84985
Epoch 18: Val Loss 1308.84729
Epoch 19: Val Loss 1308.84473
Epoch 20: Val Loss 1308.84216
Epoch 21: Val Loss 1308.83960
Epoch 22: Val Loss 1308.83704
Epoch 23: Val Loss 1308.83423
Epoch 24: Val Loss 1308.83167
Epoch 25: Val Loss 1308.82898
Epoch 26: Val Loss 1308.82666
Epoch 27: Val Loss 1308.82410
Epoch 28: Val Loss 1308.82153
Epoch 29: Val Loss 1308.81885
Epoch 30: Val Loss 1308.81628
Epoch 31: Val Loss 1308.81372
Epoch 32: Val Loss 1308.81116
Epoch 33: Val Loss 1308.80847
Epoch 34: Val Loss 1308.80579
Epoch 35: Val Loss 1308.80298
Epoch 36: Val Loss 1308.80054
Epoch 37: Val Loss 1308.79785
Epoch 38: Val Loss 1308.79517
Epoch 39: Val Loss 1308.79260
Epoch 40: Val Loss 1308.78992
Epoch 41: Val Loss 1308.78735
Epoch 42: Val Loss 1308.78467
Epoch 43: Val Loss 1308.78210
Epoch 44: Val Loss 1308.77954
Epoch 45: Val Loss 1308.77698
Epoch 46: Val Loss 1308.77429
Epoch 47: Val Loss 1308.77161
Epoch 48: Val Loss 1308.76904
Epoch 49: Val Loss 1308.76624
Epoch 50: Val Loss 1308.76379
Epoch 51: Val Loss 1308.76099
Epoch 52: Val Loss 1308.75842
Epoch 53: Val Loss 1308.75574
Epoch 54: Val Loss 1308.75293
Epoch 55: Val Loss 1308.75024
Epoch 56: Val Loss 1308.74756
Epoch 57: Val Loss 1308.74512
Epoch 58: Val Loss 1308.74243
Epoch 59: Val Loss 1308.73975
Epoch 60: Val Loss 1308.73730
Epoch 61: Val Loss 1308.73462
Epoch 62: Val Loss 1308.73206
Epoch 63: Val Loss 1308.72949
Epoch 64: Val Loss 1308.72668
Epoch 65: Val Loss 1308.72412
Epoch 66: Val Loss 1308.72156
Epoch 67: Val Loss 1308.71887
Epoch 68: Val Loss 1308.71606
Epoch 69: Val Loss 1308.71338
Epoch 70: Val Loss 1308.71082
Epoch 71: Val Loss 1308.70801
Epoch 72: Val Loss 1308.70532
Epoch 73: Val Loss 1308.70276
Epoch 74: Val Loss 1308.70020
Epoch 75: Val Loss 1308.69751
Epoch 76: Val Loss 1308.69482
Epoch 77: Val Loss 1308.69226
Epoch 78: Val Loss 1308.68970
Epoch 79: Val Loss 1308.68713
Epoch 80: Val Loss 1308.68457
Epoch 81: Val Loss 1308.68176
Epoch 82: Val Loss 1308.67908
Epoch 83: Val Loss 1308.67664
Epoch 84: Val Loss 1308.67371
Epoch 85: Val Loss 1308.67114
Epoch 86: Val Loss 1308.66846
Epoch 87: Val Loss 1308.66589
Epoch 88: Val Loss 1308.66296
Epoch 89: Val Loss 1308.66040
Epoch 90: Val Loss 1308.65796
Epoch 91: Val Loss 1308.65552
Epoch 92: Val Loss 1308.65283
Epoch 93: Val Loss 1308.65015
Epoch 94: Val Loss 1308.64758
Epoch 95: Val Loss 1308.64502
Epoch 96: Val Loss 1308.64246
Epoch 97: Val Loss 1308.63965
Epoch 98: Val Loss 1308.63721
Epoch 99: Val Loss 1308.63428
{'MSE - mean': 1297.871934779619, 'MSE - std': 10.76240861880433, 'R2 - mean': -0.25174557144691834, 'R2 - std': 0.005518378187769901} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2815.52930
Epoch 1: Val Loss 2815.52710
Epoch 2: Val Loss 2815.52441
Epoch 3: Val Loss 2815.52222
Epoch 4: Val Loss 2815.52002
Epoch 5: Val Loss 2815.51758
Epoch 6: Val Loss 2815.51514
Epoch 7: Val Loss 2815.51270
Epoch 8: Val Loss 2815.51025
Epoch 9: Val Loss 2815.50781
Epoch 10: Val Loss 2815.50562
Epoch 11: Val Loss 2815.50342
Epoch 12: Val Loss 2815.50098
Epoch 13: Val Loss 2815.49854
Epoch 14: Val Loss 2815.49609
Epoch 15: Val Loss 2815.49390
Epoch 16: Val Loss 2815.49170
Epoch 17: Val Loss 2815.48926
Epoch 18: Val Loss 2815.48682
Epoch 19: Val Loss 2815.48438
Epoch 20: Val Loss 2815.48218
Epoch 21: Val Loss 2815.47998
Epoch 22: Val Loss 2815.47778
Epoch 23: Val Loss 2815.47534
Epoch 24: Val Loss 2815.47290
Epoch 25: Val Loss 2815.47070
Epoch 26: Val Loss 2815.46826
Epoch 27: Val Loss 2815.46582
Epoch 28: Val Loss 2815.46362
Epoch 29: Val Loss 2815.46094
Epoch 30: Val Loss 2815.45850
Epoch 31: Val Loss 2815.45605
Epoch 32: Val Loss 2815.45361
Epoch 33: Val Loss 2815.45166
Epoch 34: Val Loss 2815.44922
Epoch 35: Val Loss 2815.44702
Epoch 36: Val Loss 2815.44482
Epoch 37: Val Loss 2815.44287
Epoch 38: Val Loss 2815.43994
Epoch 39: Val Loss 2815.43774
Epoch 40: Val Loss 2815.43530
Epoch 41: Val Loss 2815.43311
Epoch 42: Val Loss 2815.43066
Epoch 43: Val Loss 2815.42847
Epoch 44: Val Loss 2815.42578
Epoch 45: Val Loss 2815.42358
Epoch 46: Val Loss 2815.42163
Epoch 47: Val Loss 2815.41919
Epoch 48: Val Loss 2815.41675
Epoch 49: Val Loss 2815.41455
Epoch 50: Val Loss 2815.41211
Epoch 51: Val Loss 2815.40967
Epoch 52: Val Loss 2815.40747
Epoch 53: Val Loss 2815.40527
Epoch 54: Val Loss 2815.40259
Epoch 55: Val Loss 2815.40039
Epoch 56: Val Loss 2815.39819
Epoch 57: Val Loss 2815.39600
Epoch 58: Val Loss 2815.39404
Epoch 59: Val Loss 2815.39111
Epoch 60: Val Loss 2815.38916
Epoch 61: Val Loss 2815.38696
Epoch 62: Val Loss 2815.38452
Epoch 63: Val Loss 2815.38232
Epoch 64: Val Loss 2815.37988
Epoch 65: Val Loss 2815.37769
Epoch 66: Val Loss 2815.37524
Epoch 67: Val Loss 2815.37305
Epoch 68: Val Loss 2815.37061
Epoch 69: Val Loss 2815.36841
Epoch 70: Val Loss 2815.36646
Epoch 71: Val Loss 2815.36426
Epoch 72: Val Loss 2815.36206
Epoch 73: Val Loss 2815.35986
Epoch 74: Val Loss 2815.35718
Epoch 75: Val Loss 2815.35498
Epoch 76: Val Loss 2815.35278
Epoch 77: Val Loss 2815.35010
Epoch 78: Val Loss 2815.34790
Epoch 79: Val Loss 2815.34644
Epoch 80: Val Loss 2815.34351
Epoch 81: Val Loss 2815.34155
Epoch 82: Val Loss 2815.33911
Epoch 83: Val Loss 2815.33716
Epoch 84: Val Loss 2815.33447
Epoch 85: Val Loss 2815.33228
Epoch 86: Val Loss 2815.33008
Epoch 87: Val Loss 2815.32788
Epoch 88: Val Loss 2815.32593
Epoch 89: Val Loss 2815.32349
Epoch 90: Val Loss 2815.32153
Epoch 91: Val Loss 2815.31934
Epoch 92: Val Loss 2815.31714
Epoch 93: Val Loss 2815.31494
Epoch 94: Val Loss 2815.31274
Epoch 95: Val Loss 2815.31079
Epoch 96: Val Loss 2815.30859
Epoch 97: Val Loss 2815.30640
Epoch 98: Val Loss 2815.30420
Epoch 99: Val Loss 2815.30200
{'MSE - mean': 1803.6819279160402, 'MSE - std': 715.3773255698832, 'R2 - mean': -0.2175400101475824, 'R2 - std': 0.04858335633548551} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2125.43677
Epoch 1: Val Loss 2125.43311
Epoch 2: Val Loss 2125.42993
Epoch 3: Val Loss 2125.42676
Epoch 4: Val Loss 2125.42334
Epoch 5: Val Loss 2125.42017
Epoch 6: Val Loss 2125.41699
Epoch 7: Val Loss 2125.41333
Epoch 8: Val Loss 2125.40991
Epoch 9: Val Loss 2125.40674
Epoch 10: Val Loss 2125.40332
Epoch 11: Val Loss 2125.40039
Epoch 12: Val Loss 2125.39697
Epoch 13: Val Loss 2125.39331
Epoch 14: Val Loss 2125.39014
Epoch 15: Val Loss 2125.38696
Epoch 16: Val Loss 2125.38354
Epoch 17: Val Loss 2125.38037
Epoch 18: Val Loss 2125.37695
Epoch 19: Val Loss 2125.37378
Epoch 20: Val Loss 2125.37061
Epoch 21: Val Loss 2125.36719
Epoch 22: Val Loss 2125.36377
Epoch 23: Val Loss 2125.36060
Epoch 24: Val Loss 2125.35767
Epoch 25: Val Loss 2125.35400
Epoch 26: Val Loss 2125.35107
Epoch 27: Val Loss 2125.34766
Epoch 28: Val Loss 2125.34424
Epoch 29: Val Loss 2125.34106
Epoch 30: Val Loss 2125.33789
Epoch 31: Val Loss 2125.33472
Epoch 32: Val Loss 2125.33154
Epoch 33: Val Loss 2125.32788
Epoch 34: Val Loss 2125.32471
Epoch 35: Val Loss 2125.32153
Epoch 36: Val Loss 2125.31812
Epoch 37: Val Loss 2125.31470
Epoch 38: Val Loss 2125.31104
Epoch 39: Val Loss 2125.30762
Epoch 40: Val Loss 2125.30444
Epoch 41: Val Loss 2125.30078
Epoch 42: Val Loss 2125.29785
Epoch 43: Val Loss 2125.29443
Epoch 44: Val Loss 2125.29077
Epoch 45: Val Loss 2125.28760
Epoch 46: Val Loss 2125.28418
Epoch 47: Val Loss 2125.28101
Epoch 48: Val Loss 2125.27783
Epoch 49: Val Loss 2125.27466
Epoch 50: Val Loss 2125.27173
Epoch 51: Val Loss 2125.26831
Epoch 52: Val Loss 2125.26538
Epoch 53: Val Loss 2125.26196
Epoch 54: Val Loss 2125.25879
Epoch 55: Val Loss 2125.25537
Epoch 56: Val Loss 2125.25220
Epoch 57: Val Loss 2125.24854
Epoch 58: Val Loss 2125.24536
Epoch 59: Val Loss 2125.24219
Epoch 60: Val Loss 2125.23853
Epoch 61: Val Loss 2125.23511
Epoch 62: Val Loss 2125.23193
Epoch 63: Val Loss 2125.22852
Epoch 64: Val Loss 2125.22510
Epoch 65: Val Loss 2125.22217
Epoch 66: Val Loss 2125.21851
Epoch 67: Val Loss 2125.21509
Epoch 68: Val Loss 2125.21191
Epoch 69: Val Loss 2125.20874
Epoch 70: Val Loss 2125.20532
Epoch 71: Val Loss 2125.20190
Epoch 72: Val Loss 2125.19849
Epoch 73: Val Loss 2125.19531
Epoch 74: Val Loss 2125.19189
Epoch 75: Val Loss 2125.18872
Epoch 76: Val Loss 2125.18555
Epoch 77: Val Loss 2125.18237
Epoch 78: Val Loss 2125.17896
Epoch 79: Val Loss 2125.17554
Epoch 80: Val Loss 2125.17212
Epoch 81: Val Loss 2125.16895
Epoch 82: Val Loss 2125.16553
Epoch 83: Val Loss 2125.16211
Epoch 84: Val Loss 2125.15894
Epoch 85: Val Loss 2125.15552
Epoch 86: Val Loss 2125.15234
Epoch 87: Val Loss 2125.14917
Epoch 88: Val Loss 2125.14575
Epoch 89: Val Loss 2125.14233
Epoch 90: Val Loss 2125.13892
Epoch 91: Val Loss 2125.13525
Epoch 92: Val Loss 2125.13208
Epoch 93: Val Loss 2125.12891
Epoch 94: Val Loss 2125.12573
Epoch 95: Val Loss 2125.12231
Epoch 96: Val Loss 2125.11890
Epoch 97: Val Loss 2125.11572
Epoch 98: Val Loss 2125.11230
Epoch 99: Val Loss 2125.10913
{'MSE - mean': 1884.0386924623922, 'MSE - std': 634.9765092345641, 'R2 - mean': -0.2214283244835395, 'R2 - std': 0.04261002053204385} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2531.80249
Epoch 1: Val Loss 2531.80029
Epoch 2: Val Loss 2531.79810
Epoch 3: Val Loss 2531.79614
Epoch 4: Val Loss 2531.79443
Epoch 5: Val Loss 2531.79199
Epoch 6: Val Loss 2531.79004
Epoch 7: Val Loss 2531.78784
Epoch 8: Val Loss 2531.78564
Epoch 9: Val Loss 2531.78345
Epoch 10: Val Loss 2531.78149
Epoch 11: Val Loss 2531.77930
Epoch 12: Val Loss 2531.77710
Epoch 13: Val Loss 2531.77515
Epoch 14: Val Loss 2531.77295
Epoch 15: Val Loss 2531.77075
Epoch 16: Val Loss 2531.76904
Epoch 17: Val Loss 2531.76660
Epoch 18: Val Loss 2531.76465
Epoch 19: Val Loss 2531.76245
Epoch 20: Val Loss 2531.76025
Epoch 21: Val Loss 2531.75806
Epoch 22: Val Loss 2531.75610
Epoch 23: Val Loss 2531.75391
Epoch 24: Val Loss 2531.75171
Epoch 25: Val Loss 2531.74976
Epoch 26: Val Loss 2531.74756
Epoch 27: Val Loss 2531.74561
Epoch 28: Val Loss 2531.74341
Epoch 29: Val Loss 2531.74146
Epoch 30: Val Loss 2531.73926
Epoch 31: Val Loss 2531.73706
Epoch 32: Val Loss 2531.73486
Epoch 33: Val Loss 2531.73291
Epoch 34: Val Loss 2531.73047
Epoch 35: Val Loss 2531.72852
Epoch 36: Val Loss 2531.72656
Epoch 37: Val Loss 2531.72437
Epoch 38: Val Loss 2531.72241
Epoch 39: Val Loss 2531.71997
Epoch 40: Val Loss 2531.71753
Epoch 41: Val Loss 2531.71533
Epoch 42: Val Loss 2531.71313
Epoch 43: Val Loss 2531.71143
Epoch 44: Val Loss 2531.70947
Epoch 45: Val Loss 2531.70728
Epoch 46: Val Loss 2531.70483
Epoch 47: Val Loss 2531.70264
Epoch 48: Val Loss 2531.70068
Epoch 49: Val Loss 2531.69849
Epoch 50: Val Loss 2531.69653
Epoch 51: Val Loss 2531.69458
Epoch 52: Val Loss 2531.69263
Epoch 53: Val Loss 2531.69043
Epoch 54: Val Loss 2531.68823
Epoch 55: Val Loss 2531.68677
Epoch 56: Val Loss 2531.68433
Epoch 57: Val Loss 2531.68237
Epoch 58: Val Loss 2531.68018
Epoch 59: Val Loss 2531.67822
Epoch 60: Val Loss 2531.67603
Epoch 61: Val Loss 2531.67407
Epoch 62: Val Loss 2531.67212
Epoch 63: Val Loss 2531.66992
Epoch 64: Val Loss 2531.66772
Epoch 65: Val Loss 2531.66553
Epoch 66: Val Loss 2531.66357
Epoch 67: Val Loss 2531.66187
Epoch 68: Val Loss 2531.65967
Epoch 69: Val Loss 2531.65747
Epoch 70: Val Loss 2531.65527
Epoch 71: Val Loss 2531.65308
Epoch 72: Val Loss 2531.65088
Epoch 73: Val Loss 2531.64893
Epoch 74: Val Loss 2531.64673
Epoch 75: Val Loss 2531.64453
Epoch 76: Val Loss 2531.64233
Epoch 77: Val Loss 2531.64014
Epoch 78: Val Loss 2531.63794
Epoch 79: Val Loss 2531.63574
Epoch 80: Val Loss 2531.63428
Epoch 81: Val Loss 2531.63208
Epoch 82: Val Loss 2531.62988
Epoch 83: Val Loss 2531.62769
Epoch 84: Val Loss 2531.62573
Epoch 85: Val Loss 2531.62329
Epoch 86: Val Loss 2531.62158
Epoch 87: Val Loss 2531.61938
Epoch 88: Val Loss 2531.61694
Epoch 89: Val Loss 2531.61475
Epoch 90: Val Loss 2531.61255
Epoch 91: Val Loss 2531.61035
Epoch 92: Val Loss 2531.60840
Epoch 93: Val Loss 2531.60669
Epoch 94: Val Loss 2531.60425
Epoch 95: Val Loss 2531.60229
Epoch 96: Val Loss 2531.60010
Epoch 97: Val Loss 2531.59790
Epoch 98: Val Loss 2531.59570
Epoch 99: Val Loss 2531.59351
{'MSE - mean': 2013.5497187285746, 'MSE - std': 624.2183572451622, 'R2 - mean': -0.21384208684414605, 'R2 - std': 0.04102066657038082} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 2013.5497187285746 and parameters: {'dim': 256, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -6, 'dropout': 0.2}. Best is trial 2 with value: 2013.5497187285746.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1278.08508
Epoch 1: Val Loss 1278.06433
Epoch 2: Val Loss 1278.04333
Epoch 3: Val Loss 1278.02246
Epoch 4: Val Loss 1278.00183
Epoch 5: Val Loss 1277.98096
Epoch 6: Val Loss 1277.96082
Epoch 7: Val Loss 1277.94055
Epoch 8: Val Loss 1277.92029
Epoch 9: Val Loss 1277.89978
Epoch 10: Val Loss 1277.87988
Epoch 11: Val Loss 1277.85950
Epoch 12: Val Loss 1277.83862
Epoch 13: Val Loss 1277.81812
Epoch 14: Val Loss 1277.79907
Epoch 15: Val Loss 1277.77917
Epoch 16: Val Loss 1277.75964
Epoch 17: Val Loss 1277.73999
Epoch 18: Val Loss 1277.72046
Epoch 19: Val Loss 1277.70129
Epoch 20: Val Loss 1277.68237
Epoch 21: Val Loss 1277.66418
Epoch 22: Val Loss 1277.64490
Epoch 23: Val Loss 1277.62610
Epoch 24: Val Loss 1277.60718
Epoch 25: Val Loss 1277.58875
Epoch 26: Val Loss 1277.57007
Epoch 27: Val Loss 1277.55029
Epoch 28: Val Loss 1277.53088
Epoch 29: Val Loss 1277.51208
Epoch 30: Val Loss 1277.49377
Epoch 31: Val Loss 1277.47620
Epoch 32: Val Loss 1277.45825
Epoch 33: Val Loss 1277.43970
Epoch 34: Val Loss 1277.42102
Epoch 35: Val Loss 1277.40332
Epoch 36: Val Loss 1277.38586
Epoch 37: Val Loss 1277.36829
Epoch 38: Val Loss 1277.35046
Epoch 39: Val Loss 1277.33264
Epoch 40: Val Loss 1277.31494
Epoch 41: Val Loss 1277.29712
Epoch 42: Val Loss 1277.27966
Epoch 43: Val Loss 1277.26184
Epoch 44: Val Loss 1277.24353
Epoch 45: Val Loss 1277.22571
Epoch 46: Val Loss 1277.20850
Epoch 47: Val Loss 1277.19104
Epoch 48: Val Loss 1277.17297
Epoch 49: Val Loss 1277.15540
Epoch 50: Val Loss 1277.13818
Epoch 51: Val Loss 1277.12012
Epoch 52: Val Loss 1277.10303
Epoch 53: Val Loss 1277.08582
Epoch 54: Val Loss 1277.06799
Epoch 55: Val Loss 1277.05066
Epoch 56: Val Loss 1277.03345
Epoch 57: Val Loss 1277.01648
Epoch 58: Val Loss 1276.99976
Epoch 59: Val Loss 1276.98254
Epoch 60: Val Loss 1276.96521
Epoch 61: Val Loss 1276.94788
Epoch 62: Val Loss 1276.93054
Epoch 63: Val Loss 1276.91357
Epoch 64: Val Loss 1276.89636
Epoch 65: Val Loss 1276.87952
Epoch 66: Val Loss 1276.86316
Epoch 67: Val Loss 1276.84644
Epoch 68: Val Loss 1276.82947
Epoch 69: Val Loss 1276.81226
Epoch 70: Val Loss 1276.79529
Epoch 71: Val Loss 1276.77795
Epoch 72: Val Loss 1276.76135
Epoch 73: Val Loss 1276.74463
Epoch 74: Val Loss 1276.72791
Epoch 75: Val Loss 1276.71094
Epoch 76: Val Loss 1276.69434
Epoch 77: Val Loss 1276.67737
Epoch 78: Val Loss 1276.66113
Epoch 79: Val Loss 1276.64417
Epoch 80: Val Loss 1276.62769
Epoch 81: Val Loss 1276.61096
Epoch 82: Val Loss 1276.59399
Epoch 83: Val Loss 1276.57788
Epoch 84: Val Loss 1276.56140
Epoch 85: Val Loss 1276.54456
Epoch 86: Val Loss 1276.52771
Epoch 87: Val Loss 1276.51160
Epoch 88: Val Loss 1276.49573
Epoch 89: Val Loss 1276.48010
Epoch 90: Val Loss 1276.46375
Epoch 91: Val Loss 1276.44775
Epoch 92: Val Loss 1276.43127
Epoch 93: Val Loss 1276.41455
Epoch 94: Val Loss 1276.39758
Epoch 95: Val Loss 1276.38171
Epoch 96: Val Loss 1276.36597
Epoch 97: Val Loss 1276.34998
Epoch 98: Val Loss 1276.33325
Epoch 99: Val Loss 1276.31689
{'MSE - mean': 1276.3169537749704, 'MSE - std': 0.0, 'R2 - mean': -0.24672163609512587, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1309.96826
Epoch 1: Val Loss 1309.95532
Epoch 2: Val Loss 1309.94263
Epoch 3: Val Loss 1309.93042
Epoch 4: Val Loss 1309.91785
Epoch 5: Val Loss 1309.90540
Epoch 6: Val Loss 1309.89355
Epoch 7: Val Loss 1309.88171
Epoch 8: Val Loss 1309.87000
Epoch 9: Val Loss 1309.85803
Epoch 10: Val Loss 1309.84656
Epoch 11: Val Loss 1309.83472
Epoch 12: Val Loss 1309.82300
Epoch 13: Val Loss 1309.81140
Epoch 14: Val Loss 1309.79980
Epoch 15: Val Loss 1309.78821
Epoch 16: Val Loss 1309.77673
Epoch 17: Val Loss 1309.76538
Epoch 18: Val Loss 1309.75427
Epoch 19: Val Loss 1309.74304
Epoch 20: Val Loss 1309.73157
Epoch 21: Val Loss 1309.72046
Epoch 22: Val Loss 1309.70923
Epoch 23: Val Loss 1309.69836
Epoch 24: Val Loss 1309.68713
Epoch 25: Val Loss 1309.67590
Epoch 26: Val Loss 1309.66431
Epoch 27: Val Loss 1309.65320
Epoch 28: Val Loss 1309.64233
Epoch 29: Val Loss 1309.63147
Epoch 30: Val Loss 1309.62085
Epoch 31: Val Loss 1309.61023
Epoch 32: Val Loss 1309.59924
Epoch 33: Val Loss 1309.58789
Epoch 34: Val Loss 1309.57715
Epoch 35: Val Loss 1309.56628
Epoch 36: Val Loss 1309.55603
Epoch 37: Val Loss 1309.54541
Epoch 38: Val Loss 1309.53467
Epoch 39: Val Loss 1309.52393
Epoch 40: Val Loss 1309.51331
Epoch 41: Val Loss 1309.50281
Epoch 42: Val Loss 1309.49231
Epoch 43: Val Loss 1309.48181
Epoch 44: Val Loss 1309.47156
Epoch 45: Val Loss 1309.46143
Epoch 46: Val Loss 1309.45142
Epoch 47: Val Loss 1309.44141
Epoch 48: Val Loss 1309.43164
Epoch 49: Val Loss 1309.42200
Epoch 50: Val Loss 1309.41235
Epoch 51: Val Loss 1309.40247
Epoch 52: Val Loss 1309.39270
Epoch 53: Val Loss 1309.38306
Epoch 54: Val Loss 1309.37305
Epoch 55: Val Loss 1309.36365
Epoch 56: Val Loss 1309.35376
Epoch 57: Val Loss 1309.34363
Epoch 58: Val Loss 1309.33374
Epoch 59: Val Loss 1309.32385
Epoch 60: Val Loss 1309.31458
Epoch 61: Val Loss 1309.30518
Epoch 62: Val Loss 1309.29529
Epoch 63: Val Loss 1309.28613
Epoch 64: Val Loss 1309.27722
Epoch 65: Val Loss 1309.26831
Epoch 66: Val Loss 1309.25879
Epoch 67: Val Loss 1309.24976
Epoch 68: Val Loss 1309.24036
Epoch 69: Val Loss 1309.23157
Epoch 70: Val Loss 1309.22241
Epoch 71: Val Loss 1309.21350
Epoch 72: Val Loss 1309.20398
Epoch 73: Val Loss 1309.19482
Epoch 74: Val Loss 1309.18530
Epoch 75: Val Loss 1309.17615
Epoch 76: Val Loss 1309.16663
Epoch 77: Val Loss 1309.15735
Epoch 78: Val Loss 1309.14807
Epoch 79: Val Loss 1309.13904
Epoch 80: Val Loss 1309.13013
Epoch 81: Val Loss 1309.12134
Epoch 82: Val Loss 1309.11255
Epoch 83: Val Loss 1309.10376
Epoch 84: Val Loss 1309.09473
Epoch 85: Val Loss 1309.08618
Epoch 86: Val Loss 1309.07727
Epoch 87: Val Loss 1309.06873
Epoch 88: Val Loss 1309.05981
Epoch 89: Val Loss 1309.05127
Epoch 90: Val Loss 1309.04248
Epoch 91: Val Loss 1309.03418
Epoch 92: Val Loss 1309.02600
Epoch 93: Val Loss 1309.01733
Epoch 94: Val Loss 1309.00879
Epoch 95: Val Loss 1309.00037
Epoch 96: Val Loss 1308.99158
Epoch 97: Val Loss 1308.98291
Epoch 98: Val Loss 1308.97412
Epoch 99: Val Loss 1308.96533
{'MSE - mean': 1292.6411578214138, 'MSE - std': 16.32420404644347, 'R2 - mean': -0.24663203098155428, 'R2 - std': 8.960511357158918e-05} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2828.23169
Epoch 1: Val Loss 2828.20288
Epoch 2: Val Loss 2828.17432
Epoch 3: Val Loss 2828.14697
Epoch 4: Val Loss 2828.11914
Epoch 5: Val Loss 2828.09155
Epoch 6: Val Loss 2828.06396
Epoch 7: Val Loss 2828.03613
Epoch 8: Val Loss 2828.00732
Epoch 9: Val Loss 2827.97900
Epoch 10: Val Loss 2827.95020
Epoch 11: Val Loss 2827.92188
Epoch 12: Val Loss 2827.89453
Epoch 13: Val Loss 2827.86719
Epoch 14: Val Loss 2827.83936
Epoch 15: Val Loss 2827.81030
Epoch 16: Val Loss 2827.78101
Epoch 17: Val Loss 2827.75391
Epoch 18: Val Loss 2827.72510
Epoch 19: Val Loss 2827.69751
Epoch 20: Val Loss 2827.66919
Epoch 21: Val Loss 2827.64062
Epoch 22: Val Loss 2827.61353
Epoch 23: Val Loss 2827.58594
Epoch 24: Val Loss 2827.55591
Epoch 25: Val Loss 2827.52661
Epoch 26: Val Loss 2827.49609
Epoch 27: Val Loss 2827.46777
Epoch 28: Val Loss 2827.43726
Epoch 29: Val Loss 2827.40918
Epoch 30: Val Loss 2827.38086
Epoch 31: Val Loss 2827.35352
Epoch 32: Val Loss 2827.32300
Epoch 33: Val Loss 2827.29443
Epoch 34: Val Loss 2827.26562
Epoch 35: Val Loss 2827.23706
Epoch 36: Val Loss 2827.20898
Epoch 37: Val Loss 2827.17993
Epoch 38: Val Loss 2827.15063
Epoch 39: Val Loss 2827.12158
Epoch 40: Val Loss 2827.09082
Epoch 41: Val Loss 2827.06152
Epoch 42: Val Loss 2827.03247
Epoch 43: Val Loss 2827.00391
Epoch 44: Val Loss 2826.97412
Epoch 45: Val Loss 2826.94507
Epoch 46: Val Loss 2826.91577
Epoch 47: Val Loss 2826.88745
Epoch 48: Val Loss 2826.85840
Epoch 49: Val Loss 2826.83008
Epoch 50: Val Loss 2826.79980
Epoch 51: Val Loss 2826.77002
Epoch 52: Val Loss 2826.74097
Epoch 53: Val Loss 2826.70898
Epoch 54: Val Loss 2826.67798
Epoch 55: Val Loss 2826.64893
Epoch 56: Val Loss 2826.61963
Epoch 57: Val Loss 2826.59155
Epoch 58: Val Loss 2826.56030
Epoch 59: Val Loss 2826.52783
Epoch 60: Val Loss 2826.49609
Epoch 61: Val Loss 2826.46533
Epoch 62: Val Loss 2826.43408
Epoch 63: Val Loss 2826.40356
Epoch 64: Val Loss 2826.37354
Epoch 65: Val Loss 2826.34302
Epoch 66: Val Loss 2826.31445
Epoch 67: Val Loss 2826.28345
Epoch 68: Val Loss 2826.25342
Epoch 69: Val Loss 2826.22217
Epoch 70: Val Loss 2826.18945
Epoch 71: Val Loss 2826.15894
Epoch 72: Val Loss 2826.12671
Epoch 73: Val Loss 2826.09497
Epoch 74: Val Loss 2826.06274
Epoch 75: Val Loss 2826.03149
Epoch 76: Val Loss 2825.99854
Epoch 77: Val Loss 2825.96802
Epoch 78: Val Loss 2825.93579
Epoch 79: Val Loss 2825.90405
Epoch 80: Val Loss 2825.87280
Epoch 81: Val Loss 2825.84155
Epoch 82: Val Loss 2825.81055
Epoch 83: Val Loss 2825.78003
Epoch 84: Val Loss 2825.74951
Epoch 85: Val Loss 2825.71899
Epoch 86: Val Loss 2825.68604
Epoch 87: Val Loss 2825.65405
Epoch 88: Val Loss 2825.62231
Epoch 89: Val Loss 2825.59033
Epoch 90: Val Loss 2825.55835
Epoch 91: Val Loss 2825.52490
Epoch 92: Val Loss 2825.49219
Epoch 93: Val Loss 2825.46216
Epoch 94: Val Loss 2825.42798
Epoch 95: Val Loss 2825.39429
Epoch 96: Val Loss 2825.36182
Epoch 97: Val Loss 2825.32861
Epoch 98: Val Loss 2825.29590
Epoch 99: Val Loss 2825.26514
{'MSE - mean': 1803.5157632344096, 'MSE - std': 722.6087309212377, 'R2 - mean': -0.21548653226815354, 'R2 - std': 0.0440464474496012} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2132.28345
Epoch 1: Val Loss 2132.26611
Epoch 2: Val Loss 2132.24951
Epoch 3: Val Loss 2132.23218
Epoch 4: Val Loss 2132.21509
Epoch 5: Val Loss 2132.19751
Epoch 6: Val Loss 2132.18066
Epoch 7: Val Loss 2132.16431
Epoch 8: Val Loss 2132.14722
Epoch 9: Val Loss 2132.12988
Epoch 10: Val Loss 2132.11255
Epoch 11: Val Loss 2132.09570
Epoch 12: Val Loss 2132.07837
Epoch 13: Val Loss 2132.06128
Epoch 14: Val Loss 2132.04346
Epoch 15: Val Loss 2132.02588
Epoch 16: Val Loss 2132.00952
Epoch 17: Val Loss 2131.99219
Epoch 18: Val Loss 2131.97559
Epoch 19: Val Loss 2131.95825
Epoch 20: Val Loss 2131.94092
Epoch 21: Val Loss 2131.92480
Epoch 22: Val Loss 2131.90771
Epoch 23: Val Loss 2131.89062
Epoch 24: Val Loss 2131.87354
Epoch 25: Val Loss 2131.85571
Epoch 26: Val Loss 2131.83813
Epoch 27: Val Loss 2131.82056
Epoch 28: Val Loss 2131.80298
Epoch 29: Val Loss 2131.78540
Epoch 30: Val Loss 2131.76807
Epoch 31: Val Loss 2131.75073
Epoch 32: Val Loss 2131.73291
Epoch 33: Val Loss 2131.71484
Epoch 34: Val Loss 2131.69678
Epoch 35: Val Loss 2131.67822
Epoch 36: Val Loss 2131.65942
Epoch 37: Val Loss 2131.64038
Epoch 38: Val Loss 2131.62158
Epoch 39: Val Loss 2131.60278
Epoch 40: Val Loss 2131.58447
Epoch 41: Val Loss 2131.56665
Epoch 42: Val Loss 2131.54834
Epoch 43: Val Loss 2131.53003
Epoch 44: Val Loss 2131.51074
Epoch 45: Val Loss 2131.49219
Epoch 46: Val Loss 2131.47266
Epoch 47: Val Loss 2131.45337
Epoch 48: Val Loss 2131.43433
Epoch 49: Val Loss 2131.41504
Epoch 50: Val Loss 2131.39600
Epoch 51: Val Loss 2131.37671
Epoch 52: Val Loss 2131.35791
Epoch 53: Val Loss 2131.33862
Epoch 54: Val Loss 2131.31934
Epoch 55: Val Loss 2131.29980
Epoch 56: Val Loss 2131.28052
Epoch 57: Val Loss 2131.26025
Epoch 58: Val Loss 2131.24097
Epoch 59: Val Loss 2131.22168
Epoch 60: Val Loss 2131.20215
Epoch 61: Val Loss 2131.18237
Epoch 62: Val Loss 2131.16235
Epoch 63: Val Loss 2131.14233
Epoch 64: Val Loss 2131.12085
Epoch 65: Val Loss 2131.10107
Epoch 66: Val Loss 2131.07983
Epoch 67: Val Loss 2131.05835
Epoch 68: Val Loss 2131.03687
Epoch 69: Val Loss 2131.01489
Epoch 70: Val Loss 2130.99390
Epoch 71: Val Loss 2130.97266
Epoch 72: Val Loss 2130.95020
Epoch 73: Val Loss 2130.92920
Epoch 74: Val Loss 2130.90796
Epoch 75: Val Loss 2130.88647
Epoch 76: Val Loss 2130.86450
Epoch 77: Val Loss 2130.84204
Epoch 78: Val Loss 2130.82080
Epoch 79: Val Loss 2130.79932
Epoch 80: Val Loss 2130.77832
Epoch 81: Val Loss 2130.75659
Epoch 82: Val Loss 2130.73462
Epoch 83: Val Loss 2130.71265
Epoch 84: Val Loss 2130.69067
Epoch 85: Val Loss 2130.66846
Epoch 86: Val Loss 2130.64600
Epoch 87: Val Loss 2130.62305
Epoch 88: Val Loss 2130.60034
Epoch 89: Val Loss 2130.57739
Epoch 90: Val Loss 2130.55444
Epoch 91: Val Loss 2130.53198
Epoch 92: Val Loss 2130.50830
Epoch 93: Val Loss 2130.48511
Epoch 94: Val Loss 2130.46069
Epoch 95: Val Loss 2130.43701
Epoch 96: Val Loss 2130.41138
Epoch 97: Val Loss 2130.38623
Epoch 98: Val Loss 2130.36182
Epoch 99: Val Loss 2130.33765
{'MSE - mean': 1885.2211941740984, 'MSE - std': 641.599459037975, 'R2 - mean': -0.22064667473312544, 'R2 - std': 0.03917841702358938} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2531.33447
Epoch 1: Val Loss 2531.29834
Epoch 2: Val Loss 2531.26294
Epoch 3: Val Loss 2531.22827
Epoch 4: Val Loss 2531.19263
Epoch 5: Val Loss 2531.15747
Epoch 6: Val Loss 2531.12231
Epoch 7: Val Loss 2531.08740
Epoch 8: Val Loss 2531.05151
Epoch 9: Val Loss 2531.01562
Epoch 10: Val Loss 2530.97998
Epoch 11: Val Loss 2530.94580
Epoch 12: Val Loss 2530.91113
Epoch 13: Val Loss 2530.87427
Epoch 14: Val Loss 2530.83813
Epoch 15: Val Loss 2530.80151
Epoch 16: Val Loss 2530.76514
Epoch 17: Val Loss 2530.72998
Epoch 18: Val Loss 2530.69556
Epoch 19: Val Loss 2530.66113
Epoch 20: Val Loss 2530.62646
Epoch 21: Val Loss 2530.59082
Epoch 22: Val Loss 2530.55542
Epoch 23: Val Loss 2530.51904
Epoch 24: Val Loss 2530.48340
Epoch 25: Val Loss 2530.44702
Epoch 26: Val Loss 2530.40918
Epoch 27: Val Loss 2530.37256
Epoch 28: Val Loss 2530.33496
Epoch 29: Val Loss 2530.29712
Epoch 30: Val Loss 2530.26025
Epoch 31: Val Loss 2530.22412
Epoch 32: Val Loss 2530.18726
Epoch 33: Val Loss 2530.15063
Epoch 34: Val Loss 2530.11279
Epoch 35: Val Loss 2530.07520
Epoch 36: Val Loss 2530.03735
Epoch 37: Val Loss 2530.00073
Epoch 38: Val Loss 2529.96021
Epoch 39: Val Loss 2529.92163
Epoch 40: Val Loss 2529.88184
Epoch 41: Val Loss 2529.84326
Epoch 42: Val Loss 2529.80493
Epoch 43: Val Loss 2529.76758
Epoch 44: Val Loss 2529.72900
Epoch 45: Val Loss 2529.68896
Epoch 46: Val Loss 2529.64990
Epoch 47: Val Loss 2529.61182
Epoch 48: Val Loss 2529.57300
Epoch 49: Val Loss 2529.53101
Epoch 50: Val Loss 2529.49194
Epoch 51: Val Loss 2529.45190
Epoch 52: Val Loss 2529.41309
Epoch 53: Val Loss 2529.37231
Epoch 54: Val Loss 2529.33325
Epoch 55: Val Loss 2529.29468
Epoch 56: Val Loss 2529.25513
Epoch 57: Val Loss 2529.21558
Epoch 58: Val Loss 2529.17700
Epoch 59: Val Loss 2529.13794
Epoch 60: Val Loss 2529.09985
Epoch 61: Val Loss 2529.06055
Epoch 62: Val Loss 2529.02002
Epoch 63: Val Loss 2528.97827
Epoch 64: Val Loss 2528.93799
Epoch 65: Val Loss 2528.89722
Epoch 66: Val Loss 2528.85596
Epoch 67: Val Loss 2528.81665
Epoch 68: Val Loss 2528.77759
Epoch 69: Val Loss 2528.73584
Epoch 70: Val Loss 2528.69434
Epoch 71: Val Loss 2528.65356
Epoch 72: Val Loss 2528.61084
Epoch 73: Val Loss 2528.56812
Epoch 74: Val Loss 2528.52563
Epoch 75: Val Loss 2528.48413
Epoch 76: Val Loss 2528.44360
Epoch 77: Val Loss 2528.40161
Epoch 78: Val Loss 2528.36084
Epoch 79: Val Loss 2528.31836
Epoch 80: Val Loss 2528.27588
Epoch 81: Val Loss 2528.23364
Epoch 82: Val Loss 2528.19092
Epoch 83: Val Loss 2528.14673
Epoch 84: Val Loss 2528.10400
Epoch 85: Val Loss 2528.06079
Epoch 86: Val Loss 2528.01611
Epoch 87: Val Loss 2527.97290
Epoch 88: Val Loss 2527.92847
Epoch 89: Val Loss 2527.88574
Epoch 90: Val Loss 2527.84058
Epoch 91: Val Loss 2527.79517
Epoch 92: Val Loss 2527.75000
Epoch 93: Val Loss 2527.70532
Epoch 94: Val Loss 2527.65967
Epoch 95: Val Loss 2527.61353
Epoch 96: Val Loss 2527.56860
Epoch 97: Val Loss 2527.52417
Epoch 98: Val Loss 2527.47925
Epoch 99: Val Loss 2527.43286
{'MSE - mean': 2013.6635688130918, 'MSE - std': 628.7365641362621, 'R2 - mean': -0.21282774387061928, 'R2 - std': 0.03837318605106992} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 2013.6635688130918 and parameters: {'dim': 32, 'depth': 3, 'heads': 4, 'weight_decay': -5, 'learning_rate': -5, 'dropout': 0}. Best is trial 2 with value: 2013.5497187285746.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1287.42615
Epoch 1: Val Loss 1287.39502
Epoch 2: Val Loss 1287.36340
Epoch 3: Val Loss 1287.33264
Epoch 4: Val Loss 1287.30237
Epoch 5: Val Loss 1287.27173
Epoch 6: Val Loss 1287.23950
Epoch 7: Val Loss 1287.20886
Epoch 8: Val Loss 1287.17834
Epoch 9: Val Loss 1287.14819
Epoch 10: Val Loss 1287.11890
Epoch 11: Val Loss 1287.08960
Epoch 12: Val Loss 1287.05981
Epoch 13: Val Loss 1287.02991
Epoch 14: Val Loss 1287.00024
Epoch 15: Val Loss 1286.96960
Epoch 16: Val Loss 1286.93921
Epoch 17: Val Loss 1286.90894
Epoch 18: Val Loss 1286.88013
Epoch 19: Val Loss 1286.85083
Epoch 20: Val Loss 1286.82141
Epoch 21: Val Loss 1286.79150
Epoch 22: Val Loss 1286.76172
Epoch 23: Val Loss 1286.73328
Epoch 24: Val Loss 1286.70435
Epoch 25: Val Loss 1286.67371
Epoch 26: Val Loss 1286.64429
Epoch 27: Val Loss 1286.61475
Epoch 28: Val Loss 1286.58594
Epoch 29: Val Loss 1286.55688
Epoch 30: Val Loss 1286.52771
Epoch 31: Val Loss 1286.50000
Epoch 32: Val Loss 1286.47168
Epoch 33: Val Loss 1286.44446
Epoch 34: Val Loss 1286.41638
Epoch 35: Val Loss 1286.38721
Epoch 36: Val Loss 1286.35889
Epoch 37: Val Loss 1286.33105
Epoch 38: Val Loss 1286.30212
Epoch 39: Val Loss 1286.27380
Epoch 40: Val Loss 1286.24658
Epoch 41: Val Loss 1286.21875
Epoch 42: Val Loss 1286.19116
Epoch 43: Val Loss 1286.16357
Epoch 44: Val Loss 1286.13562
Epoch 45: Val Loss 1286.10889
Epoch 46: Val Loss 1286.08215
Epoch 47: Val Loss 1286.05457
Epoch 48: Val Loss 1286.02795
Epoch 49: Val Loss 1286.00220
Epoch 50: Val Loss 1285.97522
Epoch 51: Val Loss 1285.94873
Epoch 52: Val Loss 1285.92053
Epoch 53: Val Loss 1285.89417
Epoch 54: Val Loss 1285.86694
Epoch 55: Val Loss 1285.84009
Epoch 56: Val Loss 1285.81226
Epoch 57: Val Loss 1285.78467
Epoch 58: Val Loss 1285.75781
Epoch 59: Val Loss 1285.73096
Epoch 60: Val Loss 1285.70447
Epoch 61: Val Loss 1285.67627
Epoch 62: Val Loss 1285.64856
Epoch 63: Val Loss 1285.62231
Epoch 64: Val Loss 1285.59595
Epoch 65: Val Loss 1285.56787
Epoch 66: Val Loss 1285.54126
Epoch 67: Val Loss 1285.51282
Epoch 68: Val Loss 1285.48608
Epoch 69: Val Loss 1285.45972
Epoch 70: Val Loss 1285.43262
Epoch 71: Val Loss 1285.40601
Epoch 72: Val Loss 1285.37915
Epoch 73: Val Loss 1285.35400
Epoch 74: Val Loss 1285.32812
Epoch 75: Val Loss 1285.30176
Epoch 76: Val Loss 1285.27600
Epoch 77: Val Loss 1285.25012
Epoch 78: Val Loss 1285.22412
Epoch 79: Val Loss 1285.19849
Epoch 80: Val Loss 1285.17249
Epoch 81: Val Loss 1285.14758
Epoch 82: Val Loss 1285.12207
Epoch 83: Val Loss 1285.09644
Epoch 84: Val Loss 1285.07117
Epoch 85: Val Loss 1285.04578
Epoch 86: Val Loss 1285.02124
Epoch 87: Val Loss 1284.99512
Epoch 88: Val Loss 1284.97058
Epoch 89: Val Loss 1284.94666
Epoch 90: Val Loss 1284.92163
Epoch 91: Val Loss 1284.89624
Epoch 92: Val Loss 1284.87207
Epoch 93: Val Loss 1284.84814
Epoch 94: Val Loss 1284.82434
Epoch 95: Val Loss 1284.79993
Epoch 96: Val Loss 1284.77588
Epoch 97: Val Loss 1284.74988
Epoch 98: Val Loss 1284.72485
Epoch 99: Val Loss 1284.70020
{'MSE - mean': 1284.7002082312033, 'MSE - std': 0.0, 'R2 - mean': -0.25491049912053976, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1310.50134
Epoch 1: Val Loss 1310.47668
Epoch 2: Val Loss 1310.45239
Epoch 3: Val Loss 1310.42773
Epoch 4: Val Loss 1310.40393
Epoch 5: Val Loss 1310.38025
Epoch 6: Val Loss 1310.35571
Epoch 7: Val Loss 1310.33118
Epoch 8: Val Loss 1310.30737
Epoch 9: Val Loss 1310.28357
Epoch 10: Val Loss 1310.25891
Epoch 11: Val Loss 1310.23486
Epoch 12: Val Loss 1310.20984
Epoch 13: Val Loss 1310.18616
Epoch 14: Val Loss 1310.16296
Epoch 15: Val Loss 1310.13855
Epoch 16: Val Loss 1310.11414
Epoch 17: Val Loss 1310.09021
Epoch 18: Val Loss 1310.06555
Epoch 19: Val Loss 1310.04126
Epoch 20: Val Loss 1310.01709
Epoch 21: Val Loss 1309.99329
Epoch 22: Val Loss 1309.97021
Epoch 23: Val Loss 1309.94763
Epoch 24: Val Loss 1309.92432
Epoch 25: Val Loss 1309.90076
Epoch 26: Val Loss 1309.87720
Epoch 27: Val Loss 1309.85327
Epoch 28: Val Loss 1309.82983
Epoch 29: Val Loss 1309.80603
Epoch 30: Val Loss 1309.78247
Epoch 31: Val Loss 1309.75745
Epoch 32: Val Loss 1309.73242
Epoch 33: Val Loss 1309.70862
Epoch 34: Val Loss 1309.68372
Epoch 35: Val Loss 1309.65906
Epoch 36: Val Loss 1309.63477
Epoch 37: Val Loss 1309.61035
Epoch 38: Val Loss 1309.58618
Epoch 39: Val Loss 1309.56128
Epoch 40: Val Loss 1309.53674
Epoch 41: Val Loss 1309.51135
Epoch 42: Val Loss 1309.48633
Epoch 43: Val Loss 1309.46240
Epoch 44: Val Loss 1309.43774
Epoch 45: Val Loss 1309.41357
Epoch 46: Val Loss 1309.38928
Epoch 47: Val Loss 1309.36548
Epoch 48: Val Loss 1309.34131
Epoch 49: Val Loss 1309.31653
Epoch 50: Val Loss 1309.29199
Epoch 51: Val Loss 1309.26843
Epoch 52: Val Loss 1309.24402
Epoch 53: Val Loss 1309.21924
Epoch 54: Val Loss 1309.19470
Epoch 55: Val Loss 1309.17017
Epoch 56: Val Loss 1309.14526
Epoch 57: Val Loss 1309.12085
Epoch 58: Val Loss 1309.09521
Epoch 59: Val Loss 1309.06921
Epoch 60: Val Loss 1309.04407
Epoch 61: Val Loss 1309.01978
Epoch 62: Val Loss 1308.99500
Epoch 63: Val Loss 1308.97046
Epoch 64: Val Loss 1308.94531
Epoch 65: Val Loss 1308.92126
Epoch 66: Val Loss 1308.89673
Epoch 67: Val Loss 1308.87158
Epoch 68: Val Loss 1308.84680
Epoch 69: Val Loss 1308.82300
Epoch 70: Val Loss 1308.79871
Epoch 71: Val Loss 1308.77344
Epoch 72: Val Loss 1308.74768
Epoch 73: Val Loss 1308.72253
Epoch 74: Val Loss 1308.69775
Epoch 75: Val Loss 1308.67261
Epoch 76: Val Loss 1308.64771
Epoch 77: Val Loss 1308.62146
Epoch 78: Val Loss 1308.59656
Epoch 79: Val Loss 1308.57153
Epoch 80: Val Loss 1308.54651
Epoch 81: Val Loss 1308.52136
Epoch 82: Val Loss 1308.49670
Epoch 83: Val Loss 1308.47119
Epoch 84: Val Loss 1308.44604
Epoch 85: Val Loss 1308.41968
Epoch 86: Val Loss 1308.39453
Epoch 87: Val Loss 1308.36877
Epoch 88: Val Loss 1308.34363
Epoch 89: Val Loss 1308.31787
Epoch 90: Val Loss 1308.29358
Epoch 91: Val Loss 1308.26855
Epoch 92: Val Loss 1308.24304
Epoch 93: Val Loss 1308.21802
Epoch 94: Val Loss 1308.19360
Epoch 95: Val Loss 1308.16846
Epoch 96: Val Loss 1308.14294
Epoch 97: Val Loss 1308.11633
Epoch 98: Val Loss 1308.09009
Epoch 99: Val Loss 1308.06445
{'MSE - mean': 1296.3823641907532, 'MSE - std': 11.682155959549846, 'R2 - mean': -0.2502975216480646, 'R2 - std': 0.004612977472475133} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2824.75903
Epoch 1: Val Loss 2824.73364
Epoch 2: Val Loss 2824.70776
Epoch 3: Val Loss 2824.68286
Epoch 4: Val Loss 2824.65747
Epoch 5: Val Loss 2824.63208
Epoch 6: Val Loss 2824.60669
Epoch 7: Val Loss 2824.58228
Epoch 8: Val Loss 2824.55737
Epoch 9: Val Loss 2824.53198
Epoch 10: Val Loss 2824.50781
Epoch 11: Val Loss 2824.48267
Epoch 12: Val Loss 2824.45898
Epoch 13: Val Loss 2824.43408
Epoch 14: Val Loss 2824.40918
Epoch 15: Val Loss 2824.38306
Epoch 16: Val Loss 2824.35864
Epoch 17: Val Loss 2824.33398
Epoch 18: Val Loss 2824.30957
Epoch 19: Val Loss 2824.28467
Epoch 20: Val Loss 2824.26025
Epoch 21: Val Loss 2824.23584
Epoch 22: Val Loss 2824.21289
Epoch 23: Val Loss 2824.18945
Epoch 24: Val Loss 2824.16504
Epoch 25: Val Loss 2824.14111
Epoch 26: Val Loss 2824.11743
Epoch 27: Val Loss 2824.09253
Epoch 28: Val Loss 2824.06714
Epoch 29: Val Loss 2824.04272
Epoch 30: Val Loss 2824.01855
Epoch 31: Val Loss 2823.99463
Epoch 32: Val Loss 2823.97095
Epoch 33: Val Loss 2823.94751
Epoch 34: Val Loss 2823.92480
Epoch 35: Val Loss 2823.90161
Epoch 36: Val Loss 2823.87939
Epoch 37: Val Loss 2823.85693
Epoch 38: Val Loss 2823.83447
Epoch 39: Val Loss 2823.81152
Epoch 40: Val Loss 2823.78809
Epoch 41: Val Loss 2823.76489
Epoch 42: Val Loss 2823.74072
Epoch 43: Val Loss 2823.71704
Epoch 44: Val Loss 2823.69434
Epoch 45: Val Loss 2823.67090
Epoch 46: Val Loss 2823.64746
Epoch 47: Val Loss 2823.62402
Epoch 48: Val Loss 2823.60107
Epoch 49: Val Loss 2823.57837
Epoch 50: Val Loss 2823.55518
Epoch 51: Val Loss 2823.53198
Epoch 52: Val Loss 2823.50830
Epoch 53: Val Loss 2823.48486
Epoch 54: Val Loss 2823.46045
Epoch 55: Val Loss 2823.43555
Epoch 56: Val Loss 2823.41187
Epoch 57: Val Loss 2823.38647
Epoch 58: Val Loss 2823.36182
Epoch 59: Val Loss 2823.33569
Epoch 60: Val Loss 2823.31104
Epoch 61: Val Loss 2823.28662
Epoch 62: Val Loss 2823.26196
Epoch 63: Val Loss 2823.23779
Epoch 64: Val Loss 2823.21362
Epoch 65: Val Loss 2823.19043
Epoch 66: Val Loss 2823.16406
Epoch 67: Val Loss 2823.13965
Epoch 68: Val Loss 2823.11304
Epoch 69: Val Loss 2823.08813
Epoch 70: Val Loss 2823.06323
Epoch 71: Val Loss 2823.03906
Epoch 72: Val Loss 2823.01514
Epoch 73: Val Loss 2822.98901
Epoch 74: Val Loss 2822.96362
Epoch 75: Val Loss 2822.93921
Epoch 76: Val Loss 2822.91406
Epoch 77: Val Loss 2822.88745
Epoch 78: Val Loss 2822.86182
Epoch 79: Val Loss 2822.83545
Epoch 80: Val Loss 2822.80908
Epoch 81: Val Loss 2822.78271
Epoch 82: Val Loss 2822.75708
Epoch 83: Val Loss 2822.73218
Epoch 84: Val Loss 2822.70801
Epoch 85: Val Loss 2822.68188
Epoch 86: Val Loss 2822.65503
Epoch 87: Val Loss 2822.62939
Epoch 88: Val Loss 2822.60278
Epoch 89: Val Loss 2822.57739
Epoch 90: Val Loss 2822.55078
Epoch 91: Val Loss 2822.52563
Epoch 92: Val Loss 2822.49805
Epoch 93: Val Loss 2822.47241
Epoch 94: Val Loss 2822.44653
Epoch 95: Val Loss 2822.42090
Epoch 96: Val Loss 2822.39575
Epoch 97: Val Loss 2822.36963
Epoch 98: Val Loss 2822.34473
Epoch 99: Val Loss 2822.32031
{'MSE - mean': 1805.028333122237, 'MSE - std': 719.3972652615089, 'R2 - mean': -0.2175295437504873, 'R2 - std': 0.04649373210552084} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2125.03418
Epoch 1: Val Loss 2125.00073
Epoch 2: Val Loss 2124.96777
Epoch 3: Val Loss 2124.93457
Epoch 4: Val Loss 2124.90234
Epoch 5: Val Loss 2124.86890
Epoch 6: Val Loss 2124.83618
Epoch 7: Val Loss 2124.80396
Epoch 8: Val Loss 2124.77051
Epoch 9: Val Loss 2124.73755
Epoch 10: Val Loss 2124.70361
Epoch 11: Val Loss 2124.67017
Epoch 12: Val Loss 2124.63647
Epoch 13: Val Loss 2124.60400
Epoch 14: Val Loss 2124.57251
Epoch 15: Val Loss 2124.53931
Epoch 16: Val Loss 2124.50732
Epoch 17: Val Loss 2124.47266
Epoch 18: Val Loss 2124.43994
Epoch 19: Val Loss 2124.40820
Epoch 20: Val Loss 2124.37573
Epoch 21: Val Loss 2124.34448
Epoch 22: Val Loss 2124.31348
Epoch 23: Val Loss 2124.28174
Epoch 24: Val Loss 2124.24878
Epoch 25: Val Loss 2124.21606
Epoch 26: Val Loss 2124.18408
Epoch 27: Val Loss 2124.15283
Epoch 28: Val Loss 2124.12061
Epoch 29: Val Loss 2124.08887
Epoch 30: Val Loss 2124.05542
Epoch 31: Val Loss 2124.02222
Epoch 32: Val Loss 2123.98828
Epoch 33: Val Loss 2123.95337
Epoch 34: Val Loss 2123.92139
Epoch 35: Val Loss 2123.88794
Epoch 36: Val Loss 2123.85474
Epoch 37: Val Loss 2123.82056
Epoch 38: Val Loss 2123.78809
Epoch 39: Val Loss 2123.75659
Epoch 40: Val Loss 2123.72363
Epoch 41: Val Loss 2123.68994
Epoch 42: Val Loss 2123.65723
Epoch 43: Val Loss 2123.62402
Epoch 44: Val Loss 2123.59204
Epoch 45: Val Loss 2123.56079
Epoch 46: Val Loss 2123.52734
Epoch 47: Val Loss 2123.49487
Epoch 48: Val Loss 2123.46362
Epoch 49: Val Loss 2123.43164
Epoch 50: Val Loss 2123.39795
Epoch 51: Val Loss 2123.36572
Epoch 52: Val Loss 2123.33423
Epoch 53: Val Loss 2123.30151
Epoch 54: Val Loss 2123.26904
Epoch 55: Val Loss 2123.23608
Epoch 56: Val Loss 2123.20337
Epoch 57: Val Loss 2123.17041
Epoch 58: Val Loss 2123.13599
Epoch 59: Val Loss 2123.10278
Epoch 60: Val Loss 2123.06909
Epoch 61: Val Loss 2123.03467
Epoch 62: Val Loss 2122.99976
Epoch 63: Val Loss 2122.96460
Epoch 64: Val Loss 2122.92969
Epoch 65: Val Loss 2122.89380
Epoch 66: Val Loss 2122.85913
Epoch 67: Val Loss 2122.82568
Epoch 68: Val Loss 2122.79126
Epoch 69: Val Loss 2122.75562
Epoch 70: Val Loss 2122.72144
Epoch 71: Val Loss 2122.68530
Epoch 72: Val Loss 2122.65063
Epoch 73: Val Loss 2122.61572
Epoch 74: Val Loss 2122.58105
Epoch 75: Val Loss 2122.54468
Epoch 76: Val Loss 2122.50830
Epoch 77: Val Loss 2122.47339
Epoch 78: Val Loss 2122.43848
Epoch 79: Val Loss 2122.40430
Epoch 80: Val Loss 2122.37109
Epoch 81: Val Loss 2122.33594
Epoch 82: Val Loss 2122.29834
Epoch 83: Val Loss 2122.26147
Epoch 84: Val Loss 2122.22607
Epoch 85: Val Loss 2122.19189
Epoch 86: Val Loss 2122.15430
Epoch 87: Val Loss 2122.11914
Epoch 88: Val Loss 2122.08350
Epoch 89: Val Loss 2122.04761
Epoch 90: Val Loss 2122.01172
Epoch 91: Val Loss 2121.97607
Epoch 92: Val Loss 2121.94141
Epoch 93: Val Loss 2121.90747
Epoch 94: Val Loss 2121.87402
Epoch 95: Val Loss 2121.83911
Epoch 96: Val Loss 2121.80151
Epoch 97: Val Loss 2121.76587
Epoch 98: Val Loss 2121.72778
Epoch 99: Val Loss 2121.69067
{'MSE - mean': 1884.1939168260299, 'MSE - std': 637.927023944581, 'R2 - mean': -0.22092460539833247, 'R2 - std': 0.040691887085793424} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2516.16064
Epoch 1: Val Loss 2516.13159
Epoch 2: Val Loss 2516.10278
Epoch 3: Val Loss 2516.07349
Epoch 4: Val Loss 2516.04346
Epoch 5: Val Loss 2516.01416
Epoch 6: Val Loss 2515.98364
Epoch 7: Val Loss 2515.95410
Epoch 8: Val Loss 2515.92358
Epoch 9: Val Loss 2515.89331
Epoch 10: Val Loss 2515.86328
Epoch 11: Val Loss 2515.83203
Epoch 12: Val Loss 2515.80225
Epoch 13: Val Loss 2515.77197
Epoch 14: Val Loss 2515.74170
Epoch 15: Val Loss 2515.71094
Epoch 16: Val Loss 2515.67944
Epoch 17: Val Loss 2515.64795
Epoch 18: Val Loss 2515.61694
Epoch 19: Val Loss 2515.58252
Epoch 20: Val Loss 2515.55054
Epoch 21: Val Loss 2515.51733
Epoch 22: Val Loss 2515.48706
Epoch 23: Val Loss 2515.45654
Epoch 24: Val Loss 2515.42505
Epoch 25: Val Loss 2515.39355
Epoch 26: Val Loss 2515.36255
Epoch 27: Val Loss 2515.33057
Epoch 28: Val Loss 2515.29761
Epoch 29: Val Loss 2515.26562
Epoch 30: Val Loss 2515.23364
Epoch 31: Val Loss 2515.20068
Epoch 32: Val Loss 2515.16724
Epoch 33: Val Loss 2515.13501
Epoch 34: Val Loss 2515.10327
Epoch 35: Val Loss 2515.07080
Epoch 36: Val Loss 2515.03833
Epoch 37: Val Loss 2515.00708
Epoch 38: Val Loss 2514.97412
Epoch 39: Val Loss 2514.94141
Epoch 40: Val Loss 2514.90747
Epoch 41: Val Loss 2514.87402
Epoch 42: Val Loss 2514.84009
Epoch 43: Val Loss 2514.80664
Epoch 44: Val Loss 2514.77441
Epoch 45: Val Loss 2514.74194
Epoch 46: Val Loss 2514.70947
Epoch 47: Val Loss 2514.67505
Epoch 48: Val Loss 2514.64185
Epoch 49: Val Loss 2514.60596
Epoch 50: Val Loss 2514.57202
Epoch 51: Val Loss 2514.53809
Epoch 52: Val Loss 2514.50342
Epoch 53: Val Loss 2514.46753
Epoch 54: Val Loss 2514.43237
Epoch 55: Val Loss 2514.39478
Epoch 56: Val Loss 2514.35645
Epoch 57: Val Loss 2514.31934
Epoch 58: Val Loss 2514.28223
Epoch 59: Val Loss 2514.24609
Epoch 60: Val Loss 2514.20776
Epoch 61: Val Loss 2514.17188
Epoch 62: Val Loss 2514.13428
Epoch 63: Val Loss 2514.09448
Epoch 64: Val Loss 2514.05420
Epoch 65: Val Loss 2514.01489
Epoch 66: Val Loss 2513.97559
Epoch 67: Val Loss 2513.93555
Epoch 68: Val Loss 2513.89697
Epoch 69: Val Loss 2513.85767
Epoch 70: Val Loss 2513.81934
Epoch 71: Val Loss 2513.78027
Epoch 72: Val Loss 2513.74072
Epoch 73: Val Loss 2513.70068
Epoch 74: Val Loss 2513.66113
Epoch 75: Val Loss 2513.62158
Epoch 76: Val Loss 2513.57959
Epoch 77: Val Loss 2513.53931
Epoch 78: Val Loss 2513.49609
Epoch 79: Val Loss 2513.45410
Epoch 80: Val Loss 2513.41455
Epoch 81: Val Loss 2513.37476
Epoch 82: Val Loss 2513.33398
Epoch 83: Val Loss 2513.29248
Epoch 84: Val Loss 2513.25024
Epoch 85: Val Loss 2513.20728
Epoch 86: Val Loss 2513.16602
Epoch 87: Val Loss 2513.12476
Epoch 88: Val Loss 2513.08154
Epoch 89: Val Loss 2513.03833
Epoch 90: Val Loss 2512.99561
Epoch 91: Val Loss 2512.95508
Epoch 92: Val Loss 2512.91553
Epoch 93: Val Loss 2512.87500
Epoch 94: Val Loss 2512.83447
Epoch 95: Val Loss 2512.79224
Epoch 96: Val Loss 2512.75000
Epoch 97: Val Loss 2512.70679
Epoch 98: Val Loss 2512.66406
Epoch 99: Val Loss 2512.62256
{'MSE - mean': 2009.879632560957, 'MSE - std': 623.4968378927318, 'R2 - mean': -0.21166532681676115, 'R2 - std': 0.040836266958268246} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 2009.879632560957 and parameters: {'dim': 32, 'depth': 2, 'heads': 4, 'weight_decay': -3, 'learning_rate': -5, 'dropout': 0}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1275.52612
Epoch 1: Val Loss 1275.52344
Epoch 2: Val Loss 1275.52100
Epoch 3: Val Loss 1275.51843
Epoch 4: Val Loss 1275.51611
Epoch 5: Val Loss 1275.51379
Epoch 6: Val Loss 1275.51135
Epoch 7: Val Loss 1275.50879
Epoch 8: Val Loss 1275.50635
Epoch 9: Val Loss 1275.50391
Epoch 10: Val Loss 1275.50134
Epoch 11: Val Loss 1275.49890
Epoch 12: Val Loss 1275.49646
Epoch 13: Val Loss 1275.49390
Epoch 14: Val Loss 1275.49133
Epoch 15: Val Loss 1275.48889
Epoch 16: Val Loss 1275.48657
Epoch 17: Val Loss 1275.48401
Epoch 18: Val Loss 1275.48145
Epoch 19: Val Loss 1275.47900
Epoch 20: Val Loss 1275.47644
Epoch 21: Val Loss 1275.47412
Epoch 22: Val Loss 1275.47144
Epoch 23: Val Loss 1275.46899
Epoch 24: Val Loss 1275.46643
Epoch 25: Val Loss 1275.46411
Epoch 26: Val Loss 1275.46179
Epoch 27: Val Loss 1275.45923
Epoch 28: Val Loss 1275.45679
Epoch 29: Val Loss 1275.45422
Epoch 30: Val Loss 1275.45178
Epoch 31: Val Loss 1275.44922
Epoch 32: Val Loss 1275.44653
Epoch 33: Val Loss 1275.44409
Epoch 34: Val Loss 1275.44165
Epoch 35: Val Loss 1275.43921
Epoch 36: Val Loss 1275.43677
Epoch 37: Val Loss 1275.43445
Epoch 38: Val Loss 1275.43201
Epoch 39: Val Loss 1275.42944
Epoch 40: Val Loss 1275.42725
Epoch 41: Val Loss 1275.42480
Epoch 42: Val Loss 1275.42236
Epoch 43: Val Loss 1275.41980
Epoch 44: Val Loss 1275.41724
Epoch 45: Val Loss 1275.41492
Epoch 46: Val Loss 1275.41248
Epoch 47: Val Loss 1275.41003
Epoch 48: Val Loss 1275.40759
Epoch 49: Val Loss 1275.40527
Epoch 50: Val Loss 1275.40271
Epoch 51: Val Loss 1275.40051
Epoch 52: Val Loss 1275.39807
Epoch 53: Val Loss 1275.39575
Epoch 54: Val Loss 1275.39331
Epoch 55: Val Loss 1275.39087
Epoch 56: Val Loss 1275.38843
Epoch 57: Val Loss 1275.38586
Epoch 58: Val Loss 1275.38367
Epoch 59: Val Loss 1275.38123
Epoch 60: Val Loss 1275.37878
Epoch 61: Val Loss 1275.37622
Epoch 62: Val Loss 1275.37390
Epoch 63: Val Loss 1275.37134
Epoch 64: Val Loss 1275.36877
Epoch 65: Val Loss 1275.36633
Epoch 66: Val Loss 1275.36389
Epoch 67: Val Loss 1275.36157
Epoch 68: Val Loss 1275.35901
Epoch 69: Val Loss 1275.35657
Epoch 70: Val Loss 1275.35400
Epoch 71: Val Loss 1275.35156
Epoch 72: Val Loss 1275.34888
Epoch 73: Val Loss 1275.34644
Epoch 74: Val Loss 1275.34399
Epoch 75: Val Loss 1275.34167
Epoch 76: Val Loss 1275.33911
Epoch 77: Val Loss 1275.33679
Epoch 78: Val Loss 1275.33423
Epoch 79: Val Loss 1275.33167
Epoch 80: Val Loss 1275.32922
Epoch 81: Val Loss 1275.32690
Epoch 82: Val Loss 1275.32446
Epoch 83: Val Loss 1275.32190
Epoch 84: Val Loss 1275.31934
Epoch 85: Val Loss 1275.31677
Epoch 86: Val Loss 1275.31409
Epoch 87: Val Loss 1275.31177
Epoch 88: Val Loss 1275.30920
Epoch 89: Val Loss 1275.30713
Epoch 90: Val Loss 1275.30444
Epoch 91: Val Loss 1275.30212
Epoch 92: Val Loss 1275.29968
Epoch 93: Val Loss 1275.29724
Epoch 94: Val Loss 1275.29504
Epoch 95: Val Loss 1275.29260
Epoch 96: Val Loss 1275.29016
Epoch 97: Val Loss 1275.28772
Epoch 98: Val Loss 1275.28528
Epoch 99: Val Loss 1275.28284
{'MSE - mean': 1275.2828915473776, 'MSE - std': 0.0, 'R2 - mean': -0.24571155176740822, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1301.50500
Epoch 1: Val Loss 1301.50415
Epoch 2: Val Loss 1301.50330
Epoch 3: Val Loss 1301.50232
Epoch 4: Val Loss 1301.50159
Epoch 5: Val Loss 1301.50085
Epoch 6: Val Loss 1301.49988
Epoch 7: Val Loss 1301.49890
Epoch 8: Val Loss 1301.49805
Epoch 9: Val Loss 1301.49731
Epoch 10: Val Loss 1301.49646
Epoch 11: Val Loss 1301.49573
Epoch 12: Val Loss 1301.49475
Epoch 13: Val Loss 1301.49390
Epoch 14: Val Loss 1301.49304
Epoch 15: Val Loss 1301.49219
Epoch 16: Val Loss 1301.49133
Epoch 17: Val Loss 1301.49048
Epoch 18: Val Loss 1301.48975
Epoch 19: Val Loss 1301.48889
Epoch 20: Val Loss 1301.48804
Epoch 21: Val Loss 1301.48706
Epoch 22: Val Loss 1301.48633
Epoch 23: Val Loss 1301.48535
Epoch 24: Val Loss 1301.48462
Epoch 25: Val Loss 1301.48364
Epoch 26: Val Loss 1301.48267
Epoch 27: Val Loss 1301.48206
Epoch 28: Val Loss 1301.48108
Epoch 29: Val Loss 1301.48022
Epoch 30: Val Loss 1301.47925
Epoch 31: Val Loss 1301.47852
Epoch 32: Val Loss 1301.47754
Epoch 33: Val Loss 1301.47656
Epoch 34: Val Loss 1301.47571
Epoch 35: Val Loss 1301.47485
Epoch 36: Val Loss 1301.47400
Epoch 37: Val Loss 1301.47327
Epoch 38: Val Loss 1301.47217
Epoch 39: Val Loss 1301.47131
Epoch 40: Val Loss 1301.47046
Epoch 41: Val Loss 1301.46948
Epoch 42: Val Loss 1301.46875
Epoch 43: Val Loss 1301.46777
Epoch 44: Val Loss 1301.46680
Epoch 45: Val Loss 1301.46619
Epoch 46: Val Loss 1301.46509
Epoch 47: Val Loss 1301.46423
Epoch 48: Val Loss 1301.46350
Epoch 49: Val Loss 1301.46240
Epoch 50: Val Loss 1301.46155
Epoch 51: Val Loss 1301.46082
Epoch 52: Val Loss 1301.45984
Epoch 53: Val Loss 1301.45898
Epoch 54: Val Loss 1301.45801
Epoch 55: Val Loss 1301.45728
Epoch 56: Val Loss 1301.45630
Epoch 57: Val Loss 1301.45532
Epoch 58: Val Loss 1301.45459
Epoch 59: Val Loss 1301.45374
Epoch 60: Val Loss 1301.45288
Epoch 61: Val Loss 1301.45203
Epoch 62: Val Loss 1301.45117
Epoch 63: Val Loss 1301.45020
Epoch 64: Val Loss 1301.44922
Epoch 65: Val Loss 1301.44836
Epoch 66: Val Loss 1301.44751
Epoch 67: Val Loss 1301.44666
Epoch 68: Val Loss 1301.44592
Epoch 69: Val Loss 1301.44482
Epoch 70: Val Loss 1301.44397
Epoch 71: Val Loss 1301.44299
Epoch 72: Val Loss 1301.44214
Epoch 73: Val Loss 1301.44128
Epoch 74: Val Loss 1301.44031
Epoch 75: Val Loss 1301.43945
Epoch 76: Val Loss 1301.43860
Epoch 77: Val Loss 1301.43750
Epoch 78: Val Loss 1301.43677
Epoch 79: Val Loss 1301.43591
Epoch 80: Val Loss 1301.43506
Epoch 81: Val Loss 1301.43408
Epoch 82: Val Loss 1301.43323
Epoch 83: Val Loss 1301.43213
Epoch 84: Val Loss 1301.43127
Epoch 85: Val Loss 1301.43018
Epoch 86: Val Loss 1301.42932
Epoch 87: Val Loss 1301.42847
Epoch 88: Val Loss 1301.42749
Epoch 89: Val Loss 1301.42651
Epoch 90: Val Loss 1301.42554
Epoch 91: Val Loss 1301.42480
Epoch 92: Val Loss 1301.42395
Epoch 93: Val Loss 1301.42297
Epoch 94: Val Loss 1301.42212
Epoch 95: Val Loss 1301.42126
Epoch 96: Val Loss 1301.42029
Epoch 97: Val Loss 1301.41931
Epoch 98: Val Loss 1301.41858
Epoch 99: Val Loss 1301.41760
{'MSE - mean': 1288.3502306114697, 'MSE - std': 13.067339064092153, 'R2 - mean': -0.24253306514965212, 'R2 - std': 0.0031784866177561044} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2835.53247
Epoch 1: Val Loss 2835.52930
Epoch 2: Val Loss 2835.52661
Epoch 3: Val Loss 2835.52319
Epoch 4: Val Loss 2835.52026
Epoch 5: Val Loss 2835.51758
Epoch 6: Val Loss 2835.51465
Epoch 7: Val Loss 2835.51147
Epoch 8: Val Loss 2835.50830
Epoch 9: Val Loss 2835.50537
Epoch 10: Val Loss 2835.50269
Epoch 11: Val Loss 2835.49951
Epoch 12: Val Loss 2835.49683
Epoch 13: Val Loss 2835.49341
Epoch 14: Val Loss 2835.49048
Epoch 15: Val Loss 2835.48755
Epoch 16: Val Loss 2835.48486
Epoch 17: Val Loss 2835.48169
Epoch 18: Val Loss 2835.47900
Epoch 19: Val Loss 2835.47559
Epoch 20: Val Loss 2835.47290
Epoch 21: Val Loss 2835.46973
Epoch 22: Val Loss 2835.46704
Epoch 23: Val Loss 2835.46362
Epoch 24: Val Loss 2835.46094
Epoch 25: Val Loss 2835.45776
Epoch 26: Val Loss 2835.45459
Epoch 27: Val Loss 2835.45190
Epoch 28: Val Loss 2835.44849
Epoch 29: Val Loss 2835.44580
Epoch 30: Val Loss 2835.44312
Epoch 31: Val Loss 2835.43994
Epoch 32: Val Loss 2835.43604
Epoch 33: Val Loss 2835.43335
Epoch 34: Val Loss 2835.43042
Epoch 35: Val Loss 2835.42749
Epoch 36: Val Loss 2835.42456
Epoch 37: Val Loss 2835.42090
Epoch 38: Val Loss 2835.41772
Epoch 39: Val Loss 2835.41504
Epoch 40: Val Loss 2835.41211
Epoch 41: Val Loss 2835.40918
Epoch 42: Val Loss 2835.40576
Epoch 43: Val Loss 2835.40308
Epoch 44: Val Loss 2835.39990
Epoch 45: Val Loss 2835.39697
Epoch 46: Val Loss 2835.39404
Epoch 47: Val Loss 2835.39111
Epoch 48: Val Loss 2835.38843
Epoch 49: Val Loss 2835.38525
Epoch 50: Val Loss 2835.38208
Epoch 51: Val Loss 2835.37939
Epoch 52: Val Loss 2835.37646
Epoch 53: Val Loss 2835.37329
Epoch 54: Val Loss 2835.37012
Epoch 55: Val Loss 2835.36719
Epoch 56: Val Loss 2835.36426
Epoch 57: Val Loss 2835.36084
Epoch 58: Val Loss 2835.35791
Epoch 59: Val Loss 2835.35498
Epoch 60: Val Loss 2835.35229
Epoch 61: Val Loss 2835.34912
Epoch 62: Val Loss 2835.34644
Epoch 63: Val Loss 2835.34326
Epoch 64: Val Loss 2835.34033
Epoch 65: Val Loss 2835.33740
Epoch 66: Val Loss 2835.33447
Epoch 67: Val Loss 2835.33179
Epoch 68: Val Loss 2835.32837
Epoch 69: Val Loss 2835.32544
Epoch 70: Val Loss 2835.32251
Epoch 71: Val Loss 2835.31982
Epoch 72: Val Loss 2835.31665
Epoch 73: Val Loss 2835.31396
Epoch 74: Val Loss 2835.31055
Epoch 75: Val Loss 2835.30737
Epoch 76: Val Loss 2835.30469
Epoch 77: Val Loss 2835.30200
Epoch 78: Val Loss 2835.29858
Epoch 79: Val Loss 2835.29590
Epoch 80: Val Loss 2835.29272
Epoch 81: Val Loss 2835.29004
Epoch 82: Val Loss 2835.28687
Epoch 83: Val Loss 2835.28394
Epoch 84: Val Loss 2835.28076
Epoch 85: Val Loss 2835.27783
Epoch 86: Val Loss 2835.27441
Epoch 87: Val Loss 2835.27173
Epoch 88: Val Loss 2835.26855
Epoch 89: Val Loss 2835.26562
Epoch 90: Val Loss 2835.26294
Epoch 91: Val Loss 2835.26001
Epoch 92: Val Loss 2835.25708
Epoch 93: Val Loss 2835.25391
Epoch 94: Val Loss 2835.25146
Epoch 95: Val Loss 2835.24829
Epoch 96: Val Loss 2835.24561
Epoch 97: Val Loss 2835.24292
Epoch 98: Val Loss 2835.23999
Epoch 99: Val Loss 2835.23730
{'MSE - mean': 1803.9792043359103, 'MSE - std': 729.2875386213476, 'R2 - mean': -0.21411067803003858, 'R2 - std': 0.04027901890489038} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2141.86719
Epoch 1: Val Loss 2141.86328
Epoch 2: Val Loss 2141.85938
Epoch 3: Val Loss 2141.85498
Epoch 4: Val Loss 2141.85083
Epoch 5: Val Loss 2141.84717
Epoch 6: Val Loss 2141.84302
Epoch 7: Val Loss 2141.83887
Epoch 8: Val Loss 2141.83472
Epoch 9: Val Loss 2141.83081
Epoch 10: Val Loss 2141.82690
Epoch 11: Val Loss 2141.82251
Epoch 12: Val Loss 2141.81836
Epoch 13: Val Loss 2141.81421
Epoch 14: Val Loss 2141.81030
Epoch 15: Val Loss 2141.80591
Epoch 16: Val Loss 2141.80176
Epoch 17: Val Loss 2141.79761
Epoch 18: Val Loss 2141.79346
Epoch 19: Val Loss 2141.79004
Epoch 20: Val Loss 2141.78613
Epoch 21: Val Loss 2141.78223
Epoch 22: Val Loss 2141.77832
Epoch 23: Val Loss 2141.77441
Epoch 24: Val Loss 2141.77002
Epoch 25: Val Loss 2141.76587
Epoch 26: Val Loss 2141.76196
Epoch 27: Val Loss 2141.75781
Epoch 28: Val Loss 2141.75391
Epoch 29: Val Loss 2141.74976
Epoch 30: Val Loss 2141.74561
Epoch 31: Val Loss 2141.74194
Epoch 32: Val Loss 2141.73779
Epoch 33: Val Loss 2141.73389
Epoch 34: Val Loss 2141.72998
Epoch 35: Val Loss 2141.72607
Epoch 36: Val Loss 2141.72217
Epoch 37: Val Loss 2141.71826
Epoch 38: Val Loss 2141.71460
Epoch 39: Val Loss 2141.71069
Epoch 40: Val Loss 2141.70679
Epoch 41: Val Loss 2141.70239
Epoch 42: Val Loss 2141.69824
Epoch 43: Val Loss 2141.69409
Epoch 44: Val Loss 2141.69019
Epoch 45: Val Loss 2141.68604
Epoch 46: Val Loss 2141.68188
Epoch 47: Val Loss 2141.67798
Epoch 48: Val Loss 2141.67407
Epoch 49: Val Loss 2141.66968
Epoch 50: Val Loss 2141.66553
Epoch 51: Val Loss 2141.66162
Epoch 52: Val Loss 2141.65723
Epoch 53: Val Loss 2141.65308
Epoch 54: Val Loss 2141.64917
Epoch 55: Val Loss 2141.64502
Epoch 56: Val Loss 2141.64087
Epoch 57: Val Loss 2141.63721
Epoch 58: Val Loss 2141.63281
Epoch 59: Val Loss 2141.62915
Epoch 60: Val Loss 2141.62476
Epoch 61: Val Loss 2141.62061
Epoch 62: Val Loss 2141.61670
Epoch 63: Val Loss 2141.61279
Epoch 64: Val Loss 2141.60840
Epoch 65: Val Loss 2141.60400
Epoch 66: Val Loss 2141.60010
Epoch 67: Val Loss 2141.59595
Epoch 68: Val Loss 2141.59204
Epoch 69: Val Loss 2141.58813
Epoch 70: Val Loss 2141.58398
Epoch 71: Val Loss 2141.57959
Epoch 72: Val Loss 2141.57568
Epoch 73: Val Loss 2141.57178
Epoch 74: Val Loss 2141.56787
Epoch 75: Val Loss 2141.56396
Epoch 76: Val Loss 2141.55957
Epoch 77: Val Loss 2141.55566
Epoch 78: Val Loss 2141.55151
Epoch 79: Val Loss 2141.54761
Epoch 80: Val Loss 2141.54346
Epoch 81: Val Loss 2141.53931
Epoch 82: Val Loss 2141.53516
Epoch 83: Val Loss 2141.53149
Epoch 84: Val Loss 2141.52734
Epoch 85: Val Loss 2141.52319
Epoch 86: Val Loss 2141.51929
Epoch 87: Val Loss 2141.51514
Epoch 88: Val Loss 2141.51099
Epoch 89: Val Loss 2141.50708
Epoch 90: Val Loss 2141.50293
Epoch 91: Val Loss 2141.49902
Epoch 92: Val Loss 2141.49512
Epoch 93: Val Loss 2141.49097
Epoch 94: Val Loss 2141.48706
Epoch 95: Val Loss 2141.48291
Epoch 96: Val Loss 2141.47900
Epoch 97: Val Loss 2141.47510
Epoch 98: Val Loss 2141.47095
Epoch 99: Val Loss 2141.46704
{'MSE - mean': 1888.351144062608, 'MSE - std': 648.2677750212672, 'R2 - mean': -0.22122925210344452, 'R2 - std': 0.03699759199557573} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2525.40845
Epoch 1: Val Loss 2525.40479
Epoch 2: Val Loss 2525.40039
Epoch 3: Val Loss 2525.39648
Epoch 4: Val Loss 2525.39209
Epoch 5: Val Loss 2525.38794
Epoch 6: Val Loss 2525.38428
Epoch 7: Val Loss 2525.37988
Epoch 8: Val Loss 2525.37549
Epoch 9: Val Loss 2525.37183
Epoch 10: Val Loss 2525.36743
Epoch 11: Val Loss 2525.36353
Epoch 12: Val Loss 2525.35938
Epoch 13: Val Loss 2525.35498
Epoch 14: Val Loss 2525.35059
Epoch 15: Val Loss 2525.34668
Epoch 16: Val Loss 2525.34204
Epoch 17: Val Loss 2525.33789
Epoch 18: Val Loss 2525.33350
Epoch 19: Val Loss 2525.32935
Epoch 20: Val Loss 2525.32495
Epoch 21: Val Loss 2525.32080
Epoch 22: Val Loss 2525.31714
Epoch 23: Val Loss 2525.31274
Epoch 24: Val Loss 2525.30859
Epoch 25: Val Loss 2525.30469
Epoch 26: Val Loss 2525.30029
Epoch 27: Val Loss 2525.29663
Epoch 28: Val Loss 2525.29272
Epoch 29: Val Loss 2525.28833
Epoch 30: Val Loss 2525.28418
Epoch 31: Val Loss 2525.27979
Epoch 32: Val Loss 2525.27563
Epoch 33: Val Loss 2525.27173
Epoch 34: Val Loss 2525.26733
Epoch 35: Val Loss 2525.26294
Epoch 36: Val Loss 2525.25928
Epoch 37: Val Loss 2525.25488
Epoch 38: Val Loss 2525.25049
Epoch 39: Val Loss 2525.24609
Epoch 40: Val Loss 2525.24194
Epoch 41: Val Loss 2525.23755
Epoch 42: Val Loss 2525.23315
Epoch 43: Val Loss 2525.22949
Epoch 44: Val Loss 2525.22510
Epoch 45: Val Loss 2525.22144
Epoch 46: Val Loss 2525.21704
Epoch 47: Val Loss 2525.21313
Epoch 48: Val Loss 2525.20923
Epoch 49: Val Loss 2525.20483
Epoch 50: Val Loss 2525.20068
Epoch 51: Val Loss 2525.19604
Epoch 52: Val Loss 2525.19214
Epoch 53: Val Loss 2525.18799
Epoch 54: Val Loss 2525.18359
Epoch 55: Val Loss 2525.17969
Epoch 56: Val Loss 2525.17554
Epoch 57: Val Loss 2525.17163
Epoch 58: Val Loss 2525.16724
Epoch 59: Val Loss 2525.16284
Epoch 60: Val Loss 2525.15894
Epoch 61: Val Loss 2525.15479
Epoch 62: Val Loss 2525.15063
Epoch 63: Val Loss 2525.14648
Epoch 64: Val Loss 2525.14233
Epoch 65: Val Loss 2525.13818
Epoch 66: Val Loss 2525.13428
Epoch 67: Val Loss 2525.13013
Epoch 68: Val Loss 2525.12646
Epoch 69: Val Loss 2525.12207
Epoch 70: Val Loss 2525.11792
Epoch 71: Val Loss 2525.11401
Epoch 72: Val Loss 2525.10986
Epoch 73: Val Loss 2525.10522
Epoch 74: Val Loss 2525.10083
Epoch 75: Val Loss 2525.09717
Epoch 76: Val Loss 2525.09277
Epoch 77: Val Loss 2525.08862
Epoch 78: Val Loss 2525.08496
Epoch 79: Val Loss 2525.08105
Epoch 80: Val Loss 2525.07715
Epoch 81: Val Loss 2525.07324
Epoch 82: Val Loss 2525.06958
Epoch 83: Val Loss 2525.06494
Epoch 84: Val Loss 2525.06104
Epoch 85: Val Loss 2525.05688
Epoch 86: Val Loss 2525.05273
Epoch 87: Val Loss 2525.04858
Epoch 88: Val Loss 2525.04443
Epoch 89: Val Loss 2525.04004
Epoch 90: Val Loss 2525.03564
Epoch 91: Val Loss 2525.03198
Epoch 92: Val Loss 2525.02759
Epoch 93: Val Loss 2525.02344
Epoch 94: Val Loss 2525.02002
Epoch 95: Val Loss 2525.01562
Epoch 96: Val Loss 2525.01172
Epoch 97: Val Loss 2525.00781
Epoch 98: Val Loss 2525.00391
Epoch 99: Val Loss 2525.00000
{'MSE - mean': 2015.6809182524848, 'MSE - std': 633.2869587207678, 'R2 - mean': -0.21306631978788615, 'R2 - std': 0.036899746702794854} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 2015.6809182524848 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1286.54529
Epoch 1: Val Loss 1286.51489
Epoch 2: Val Loss 1286.48450
Epoch 3: Val Loss 1286.45398
Epoch 4: Val Loss 1286.42334
Epoch 5: Val Loss 1286.39221
Epoch 6: Val Loss 1286.36218
Epoch 7: Val Loss 1286.33191
Epoch 8: Val Loss 1286.30176
Epoch 9: Val Loss 1286.27075
Epoch 10: Val Loss 1286.24036
Epoch 11: Val Loss 1286.21069
Epoch 12: Val Loss 1286.18140
Epoch 13: Val Loss 1286.15332
Epoch 14: Val Loss 1286.12512
Epoch 15: Val Loss 1286.09619
Epoch 16: Val Loss 1286.06775
Epoch 17: Val Loss 1286.03918
Epoch 18: Val Loss 1286.01001
Epoch 19: Val Loss 1285.97913
Epoch 20: Val Loss 1285.95056
Epoch 21: Val Loss 1285.92249
Epoch 22: Val Loss 1285.89380
Epoch 23: Val Loss 1285.86475
Epoch 24: Val Loss 1285.83569
Epoch 25: Val Loss 1285.80688
Epoch 26: Val Loss 1285.77759
Epoch 27: Val Loss 1285.74780
Epoch 28: Val Loss 1285.71826
Epoch 29: Val Loss 1285.68921
Epoch 30: Val Loss 1285.66003
Epoch 31: Val Loss 1285.63062
Epoch 32: Val Loss 1285.60059
Epoch 33: Val Loss 1285.57141
Epoch 34: Val Loss 1285.54321
Epoch 35: Val Loss 1285.51318
Epoch 36: Val Loss 1285.48218
Epoch 37: Val Loss 1285.45203
Epoch 38: Val Loss 1285.42102
Epoch 39: Val Loss 1285.39075
Epoch 40: Val Loss 1285.36121
Epoch 41: Val Loss 1285.33069
Epoch 42: Val Loss 1285.29980
Epoch 43: Val Loss 1285.26782
Epoch 44: Val Loss 1285.23657
Epoch 45: Val Loss 1285.20349
Epoch 46: Val Loss 1285.17041
Epoch 47: Val Loss 1285.13843
Epoch 48: Val Loss 1285.10718
Epoch 49: Val Loss 1285.07629
Epoch 50: Val Loss 1285.04443
Epoch 51: Val Loss 1285.01343
Epoch 52: Val Loss 1284.98389
Epoch 53: Val Loss 1284.95300
Epoch 54: Val Loss 1284.92053
Epoch 55: Val Loss 1284.88904
Epoch 56: Val Loss 1284.85803
Epoch 57: Val Loss 1284.82654
Epoch 58: Val Loss 1284.79553
Epoch 59: Val Loss 1284.76489
Epoch 60: Val Loss 1284.73438
Epoch 61: Val Loss 1284.70276
Epoch 62: Val Loss 1284.67078
Epoch 63: Val Loss 1284.63892
Epoch 64: Val Loss 1284.60828
Epoch 65: Val Loss 1284.57544
Epoch 66: Val Loss 1284.54333
Epoch 67: Val Loss 1284.51294
Epoch 68: Val Loss 1284.48010
Epoch 69: Val Loss 1284.44568
Epoch 70: Val Loss 1284.41431
Epoch 71: Val Loss 1284.38098
Epoch 72: Val Loss 1284.34888
Epoch 73: Val Loss 1284.31409
Epoch 74: Val Loss 1284.27954
Epoch 75: Val Loss 1284.24609
Epoch 76: Val Loss 1284.21399
Epoch 77: Val Loss 1284.18225
Epoch 78: Val Loss 1284.14954
Epoch 79: Val Loss 1284.11804
Epoch 80: Val Loss 1284.08484
Epoch 81: Val Loss 1284.05176
Epoch 82: Val Loss 1284.01941
Epoch 83: Val Loss 1283.98743
Epoch 84: Val Loss 1283.95471
Epoch 85: Val Loss 1283.92285
Epoch 86: Val Loss 1283.88977
Epoch 87: Val Loss 1283.85803
Epoch 88: Val Loss 1283.82581
Epoch 89: Val Loss 1283.79285
Epoch 90: Val Loss 1283.76013
Epoch 91: Val Loss 1283.72742
Epoch 92: Val Loss 1283.69348
Epoch 93: Val Loss 1283.65869
Epoch 94: Val Loss 1283.62451
Epoch 95: Val Loss 1283.58948
Epoch 96: Val Loss 1283.55530
Epoch 97: Val Loss 1283.52100
Epoch 98: Val Loss 1283.48755
Epoch 99: Val Loss 1283.45312
{'MSE - mean': 1283.4531728986638, 'MSE - std': 0.0, 'R2 - mean': -0.2536923801216078, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1313.61780
Epoch 1: Val Loss 1313.58789
Epoch 2: Val Loss 1313.55725
Epoch 3: Val Loss 1313.52722
Epoch 4: Val Loss 1313.49646
Epoch 5: Val Loss 1313.46680
Epoch 6: Val Loss 1313.43701
Epoch 7: Val Loss 1313.40649
Epoch 8: Val Loss 1313.37671
Epoch 9: Val Loss 1313.34705
Epoch 10: Val Loss 1313.31653
Epoch 11: Val Loss 1313.28699
Epoch 12: Val Loss 1313.25806
Epoch 13: Val Loss 1313.22925
Epoch 14: Val Loss 1313.19885
Epoch 15: Val Loss 1313.16931
Epoch 16: Val Loss 1313.13916
Epoch 17: Val Loss 1313.10913
Epoch 18: Val Loss 1313.08093
Epoch 19: Val Loss 1313.05164
Epoch 20: Val Loss 1313.02222
Epoch 21: Val Loss 1312.99353
Epoch 22: Val Loss 1312.96338
Epoch 23: Val Loss 1312.93469
Epoch 24: Val Loss 1312.90613
Epoch 25: Val Loss 1312.87695
Epoch 26: Val Loss 1312.84656
Epoch 27: Val Loss 1312.81714
Epoch 28: Val Loss 1312.78845
Epoch 29: Val Loss 1312.75903
Epoch 30: Val Loss 1312.73083
Epoch 31: Val Loss 1312.70349
Epoch 32: Val Loss 1312.67554
Epoch 33: Val Loss 1312.64661
Epoch 34: Val Loss 1312.61902
Epoch 35: Val Loss 1312.59106
Epoch 36: Val Loss 1312.56201
Epoch 37: Val Loss 1312.53271
Epoch 38: Val Loss 1312.50427
Epoch 39: Val Loss 1312.47534
Epoch 40: Val Loss 1312.44592
Epoch 41: Val Loss 1312.41785
Epoch 42: Val Loss 1312.39075
Epoch 43: Val Loss 1312.36353
Epoch 44: Val Loss 1312.33496
Epoch 45: Val Loss 1312.30713
Epoch 46: Val Loss 1312.27917
Epoch 47: Val Loss 1312.25085
Epoch 48: Val Loss 1312.22327
Epoch 49: Val Loss 1312.19556
Epoch 50: Val Loss 1312.16858
Epoch 51: Val Loss 1312.14172
Epoch 52: Val Loss 1312.11340
Epoch 53: Val Loss 1312.08545
Epoch 54: Val Loss 1312.05872
Epoch 55: Val Loss 1312.03235
Epoch 56: Val Loss 1312.00586
Epoch 57: Val Loss 1311.97803
Epoch 58: Val Loss 1311.94995
Epoch 59: Val Loss 1311.92322
Epoch 60: Val Loss 1311.89661
Epoch 61: Val Loss 1311.87036
Epoch 62: Val Loss 1311.84387
Epoch 63: Val Loss 1311.81787
Epoch 64: Val Loss 1311.79138
Epoch 65: Val Loss 1311.76514
Epoch 66: Val Loss 1311.73901
Epoch 67: Val Loss 1311.71289
Epoch 68: Val Loss 1311.68750
Epoch 69: Val Loss 1311.66235
Epoch 70: Val Loss 1311.63733
Epoch 71: Val Loss 1311.61121
Epoch 72: Val Loss 1311.58411
Epoch 73: Val Loss 1311.55896
Epoch 74: Val Loss 1311.53333
Epoch 75: Val Loss 1311.50671
Epoch 76: Val Loss 1311.48181
Epoch 77: Val Loss 1311.45593
Epoch 78: Val Loss 1311.43042
Epoch 79: Val Loss 1311.40479
Epoch 80: Val Loss 1311.38074
Epoch 81: Val Loss 1311.35632
Epoch 82: Val Loss 1311.33276
Epoch 83: Val Loss 1311.30908
Epoch 84: Val Loss 1311.28540
Epoch 85: Val Loss 1311.26086
Epoch 86: Val Loss 1311.23621
Epoch 87: Val Loss 1311.21094
Epoch 88: Val Loss 1311.18640
Epoch 89: Val Loss 1311.16272
Epoch 90: Val Loss 1311.13965
Epoch 91: Val Loss 1311.11646
Epoch 92: Val Loss 1311.09326
Epoch 93: Val Loss 1311.06885
Epoch 94: Val Loss 1311.04407
Epoch 95: Val Loss 1311.01990
Epoch 96: Val Loss 1310.99597
Epoch 97: Val Loss 1310.97290
Epoch 98: Val Loss 1310.94910
Epoch 99: Val Loss 1310.92639
{'MSE - mean': 1297.1897787898931, 'MSE - std': 13.736605891229374, 'R2 - mean': -0.2510511550275234, 'R2 - std': 0.0026412250940843762} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2831.71533
Epoch 1: Val Loss 2831.67920
Epoch 2: Val Loss 2831.64331
Epoch 3: Val Loss 2831.60815
Epoch 4: Val Loss 2831.57202
Epoch 5: Val Loss 2831.53735
Epoch 6: Val Loss 2831.50098
Epoch 7: Val Loss 2831.46484
Epoch 8: Val Loss 2831.42920
Epoch 9: Val Loss 2831.39404
Epoch 10: Val Loss 2831.35938
Epoch 11: Val Loss 2831.32397
Epoch 12: Val Loss 2831.28833
Epoch 13: Val Loss 2831.25269
Epoch 14: Val Loss 2831.21533
Epoch 15: Val Loss 2831.17896
Epoch 16: Val Loss 2831.14453
Epoch 17: Val Loss 2831.11035
Epoch 18: Val Loss 2831.07788
Epoch 19: Val Loss 2831.04492
Epoch 20: Val Loss 2831.01196
Epoch 21: Val Loss 2830.97803
Epoch 22: Val Loss 2830.94360
Epoch 23: Val Loss 2830.90918
Epoch 24: Val Loss 2830.87549
Epoch 25: Val Loss 2830.83936
Epoch 26: Val Loss 2830.80518
Epoch 27: Val Loss 2830.77075
Epoch 28: Val Loss 2830.73755
Epoch 29: Val Loss 2830.70337
Epoch 30: Val Loss 2830.66992
Epoch 31: Val Loss 2830.63647
Epoch 32: Val Loss 2830.60327
Epoch 33: Val Loss 2830.56836
Epoch 34: Val Loss 2830.53516
Epoch 35: Val Loss 2830.50098
Epoch 36: Val Loss 2830.46753
Epoch 37: Val Loss 2830.43457
Epoch 38: Val Loss 2830.40015
Epoch 39: Val Loss 2830.36670
Epoch 40: Val Loss 2830.33447
Epoch 41: Val Loss 2830.30029
Epoch 42: Val Loss 2830.26831
Epoch 43: Val Loss 2830.23535
Epoch 44: Val Loss 2830.20142
Epoch 45: Val Loss 2830.16724
Epoch 46: Val Loss 2830.13306
Epoch 47: Val Loss 2830.09839
Epoch 48: Val Loss 2830.06323
Epoch 49: Val Loss 2830.03149
Epoch 50: Val Loss 2829.99731
Epoch 51: Val Loss 2829.96484
Epoch 52: Val Loss 2829.93237
Epoch 53: Val Loss 2829.89990
Epoch 54: Val Loss 2829.86694
Epoch 55: Val Loss 2829.83447
Epoch 56: Val Loss 2829.80200
Epoch 57: Val Loss 2829.76855
Epoch 58: Val Loss 2829.73535
Epoch 59: Val Loss 2829.70239
Epoch 60: Val Loss 2829.66992
Epoch 61: Val Loss 2829.63721
Epoch 62: Val Loss 2829.60449
Epoch 63: Val Loss 2829.57153
Epoch 64: Val Loss 2829.53906
Epoch 65: Val Loss 2829.50732
Epoch 66: Val Loss 2829.47510
Epoch 67: Val Loss 2829.44214
Epoch 68: Val Loss 2829.40894
Epoch 69: Val Loss 2829.37671
Epoch 70: Val Loss 2829.34424
Epoch 71: Val Loss 2829.31152
Epoch 72: Val Loss 2829.27832
Epoch 73: Val Loss 2829.24585
Epoch 74: Val Loss 2829.21362
Epoch 75: Val Loss 2829.18018
Epoch 76: Val Loss 2829.14844
Epoch 77: Val Loss 2829.11646
Epoch 78: Val Loss 2829.08447
Epoch 79: Val Loss 2829.05225
Epoch 80: Val Loss 2829.01953
Epoch 81: Val Loss 2828.98486
Epoch 82: Val Loss 2828.95190
Epoch 83: Val Loss 2828.91943
Epoch 84: Val Loss 2828.88745
Epoch 85: Val Loss 2828.85474
Epoch 86: Val Loss 2828.82275
Epoch 87: Val Loss 2828.79004
Epoch 88: Val Loss 2828.75806
Epoch 89: Val Loss 2828.72729
Epoch 90: Val Loss 2828.69531
Epoch 91: Val Loss 2828.66162
Epoch 92: Val Loss 2828.62915
Epoch 93: Val Loss 2828.59351
Epoch 94: Val Loss 2828.55981
Epoch 95: Val Loss 2828.52686
Epoch 96: Val Loss 2828.49512
Epoch 97: Val Loss 2828.46265
Epoch 98: Val Loss 2828.43140
Epoch 99: Val Loss 2828.39844
{'MSE - mean': 1807.5927426792562, 'MSE - std': 721.9059269196367, 'R2 - mean': -0.2188589778879316, 'R2 - std': 0.0455776617577675} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2124.16431
Epoch 1: Val Loss 2124.13062
Epoch 2: Val Loss 2124.09790
Epoch 3: Val Loss 2124.06519
Epoch 4: Val Loss 2124.03174
Epoch 5: Val Loss 2123.99780
Epoch 6: Val Loss 2123.96460
Epoch 7: Val Loss 2123.93237
Epoch 8: Val Loss 2123.90015
Epoch 9: Val Loss 2123.86987
Epoch 10: Val Loss 2123.83936
Epoch 11: Val Loss 2123.80786
Epoch 12: Val Loss 2123.77637
Epoch 13: Val Loss 2123.74487
Epoch 14: Val Loss 2123.71387
Epoch 15: Val Loss 2123.68091
Epoch 16: Val Loss 2123.64966
Epoch 17: Val Loss 2123.61938
Epoch 18: Val Loss 2123.58862
Epoch 19: Val Loss 2123.55811
Epoch 20: Val Loss 2123.52808
Epoch 21: Val Loss 2123.49658
Epoch 22: Val Loss 2123.46558
Epoch 23: Val Loss 2123.43530
Epoch 24: Val Loss 2123.40454
Epoch 25: Val Loss 2123.37329
Epoch 26: Val Loss 2123.34302
Epoch 27: Val Loss 2123.31274
Epoch 28: Val Loss 2123.28271
Epoch 29: Val Loss 2123.25195
Epoch 30: Val Loss 2123.22192
Epoch 31: Val Loss 2123.19165
Epoch 32: Val Loss 2123.16187
Epoch 33: Val Loss 2123.13037
Epoch 34: Val Loss 2123.09961
Epoch 35: Val Loss 2123.06860
Epoch 36: Val Loss 2123.03735
Epoch 37: Val Loss 2123.00708
Epoch 38: Val Loss 2122.97607
Epoch 39: Val Loss 2122.94458
Epoch 40: Val Loss 2122.91431
Epoch 41: Val Loss 2122.88403
Epoch 42: Val Loss 2122.85449
Epoch 43: Val Loss 2122.82520
Epoch 44: Val Loss 2122.79614
Epoch 45: Val Loss 2122.76660
Epoch 46: Val Loss 2122.73682
Epoch 47: Val Loss 2122.70654
Epoch 48: Val Loss 2122.67627
Epoch 49: Val Loss 2122.64502
Epoch 50: Val Loss 2122.61499
Epoch 51: Val Loss 2122.58496
Epoch 52: Val Loss 2122.55493
Epoch 53: Val Loss 2122.52515
Epoch 54: Val Loss 2122.49292
Epoch 55: Val Loss 2122.46265
Epoch 56: Val Loss 2122.43262
Epoch 57: Val Loss 2122.40430
Epoch 58: Val Loss 2122.37451
Epoch 59: Val Loss 2122.34473
Epoch 60: Val Loss 2122.31470
Epoch 61: Val Loss 2122.28516
Epoch 62: Val Loss 2122.25464
Epoch 63: Val Loss 2122.22363
Epoch 64: Val Loss 2122.19312
Epoch 65: Val Loss 2122.16479
Epoch 66: Val Loss 2122.13501
Epoch 67: Val Loss 2122.10669
Epoch 68: Val Loss 2122.07715
Epoch 69: Val Loss 2122.04810
Epoch 70: Val Loss 2122.01904
Epoch 71: Val Loss 2121.98975
Epoch 72: Val Loss 2121.96118
Epoch 73: Val Loss 2121.93164
Epoch 74: Val Loss 2121.90234
Epoch 75: Val Loss 2121.87354
Epoch 76: Val Loss 2121.84448
Epoch 77: Val Loss 2121.81567
Epoch 78: Val Loss 2121.78516
Epoch 79: Val Loss 2121.75488
Epoch 80: Val Loss 2121.72559
Epoch 81: Val Loss 2121.69580
Epoch 82: Val Loss 2121.66699
Epoch 83: Val Loss 2121.63843
Epoch 84: Val Loss 2121.60938
Epoch 85: Val Loss 2121.57959
Epoch 86: Val Loss 2121.54858
Epoch 87: Val Loss 2121.51782
Epoch 88: Val Loss 2121.48560
Epoch 89: Val Loss 2121.45654
Epoch 90: Val Loss 2121.42554
Epoch 91: Val Loss 2121.39575
Epoch 92: Val Loss 2121.36646
Epoch 93: Val Loss 2121.33691
Epoch 94: Val Loss 2121.30786
Epoch 95: Val Loss 2121.27759
Epoch 96: Val Loss 2121.24780
Epoch 97: Val Loss 2121.21924
Epoch 98: Val Loss 2121.18921
Epoch 99: Val Loss 2121.15942
{'MSE - mean': 1885.984423671066, 'MSE - std': 639.7631533915032, 'R2 - mean': -0.22184462369049746, 'R2 - std': 0.03980872619230907} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2531.00488
Epoch 1: Val Loss 2530.99438
Epoch 2: Val Loss 2530.98364
Epoch 3: Val Loss 2530.97363
Epoch 4: Val Loss 2530.96362
Epoch 5: Val Loss 2530.95435
Epoch 6: Val Loss 2530.94434
Epoch 7: Val Loss 2530.93506
Epoch 8: Val Loss 2530.92578
Epoch 9: Val Loss 2530.91650
Epoch 10: Val Loss 2530.90698
Epoch 11: Val Loss 2530.89722
Epoch 12: Val Loss 2530.88745
Epoch 13: Val Loss 2530.87793
Epoch 14: Val Loss 2530.86841
Epoch 15: Val Loss 2530.85913
Epoch 16: Val Loss 2530.84912
Epoch 17: Val Loss 2530.83984
Epoch 18: Val Loss 2530.83032
Epoch 19: Val Loss 2530.82153
Epoch 20: Val Loss 2530.81177
Epoch 21: Val Loss 2530.80225
Epoch 22: Val Loss 2530.79272
Epoch 23: Val Loss 2530.78345
Epoch 24: Val Loss 2530.77441
Epoch 25: Val Loss 2530.76562
Epoch 26: Val Loss 2530.75757
Epoch 27: Val Loss 2530.74951
Epoch 28: Val Loss 2530.74146
Epoch 29: Val Loss 2530.73291
Epoch 30: Val Loss 2530.72461
Epoch 31: Val Loss 2530.71655
Epoch 32: Val Loss 2530.70801
Epoch 33: Val Loss 2530.69995
Epoch 34: Val Loss 2530.69189
Epoch 35: Val Loss 2530.68311
Epoch 36: Val Loss 2530.67505
Epoch 37: Val Loss 2530.66748
Epoch 38: Val Loss 2530.65967
Epoch 39: Val Loss 2530.65210
Epoch 40: Val Loss 2530.64453
Epoch 41: Val Loss 2530.63721
Epoch 42: Val Loss 2530.62891
Epoch 43: Val Loss 2530.62109
Epoch 44: Val Loss 2530.61328
Epoch 45: Val Loss 2530.60596
Epoch 46: Val Loss 2530.59839
Epoch 47: Val Loss 2530.59106
Epoch 48: Val Loss 2530.58398
Epoch 49: Val Loss 2530.57642
Epoch 50: Val Loss 2530.56934
Epoch 51: Val Loss 2530.56226
Epoch 52: Val Loss 2530.55518
Epoch 53: Val Loss 2530.54858
Epoch 54: Val Loss 2530.54199
Epoch 55: Val Loss 2530.53516
Epoch 56: Val Loss 2530.52808
Epoch 57: Val Loss 2530.52075
Epoch 58: Val Loss 2530.51392
Epoch 59: Val Loss 2530.50708
Epoch 60: Val Loss 2530.49976
Epoch 61: Val Loss 2530.49292
Epoch 62: Val Loss 2530.48584
Epoch 63: Val Loss 2530.47949
Epoch 64: Val Loss 2530.47290
Epoch 65: Val Loss 2530.46655
Epoch 66: Val Loss 2530.45996
Epoch 67: Val Loss 2530.45264
Epoch 68: Val Loss 2530.44580
Epoch 69: Val Loss 2530.43896
Epoch 70: Val Loss 2530.43262
Epoch 71: Val Loss 2530.42554
Epoch 72: Val Loss 2530.41895
Epoch 73: Val Loss 2530.41235
Epoch 74: Val Loss 2530.40527
Epoch 75: Val Loss 2530.39844
Epoch 76: Val Loss 2530.39160
Epoch 77: Val Loss 2530.38501
Epoch 78: Val Loss 2530.37793
Epoch 79: Val Loss 2530.37158
Epoch 80: Val Loss 2530.36450
Epoch 81: Val Loss 2530.35767
Epoch 82: Val Loss 2530.35059
Epoch 83: Val Loss 2530.34326
Epoch 84: Val Loss 2530.33594
Epoch 85: Val Loss 2530.32959
Epoch 86: Val Loss 2530.32275
Epoch 87: Val Loss 2530.31543
Epoch 88: Val Loss 2530.30811
Epoch 89: Val Loss 2530.30054
Epoch 90: Val Loss 2530.29321
Epoch 91: Val Loss 2530.28613
Epoch 92: Val Loss 2530.27856
Epoch 93: Val Loss 2530.27100
Epoch 94: Val Loss 2530.26440
Epoch 95: Val Loss 2530.25586
Epoch 96: Val Loss 2530.24805
Epoch 97: Val Loss 2530.23999
Epoch 98: Val Loss 2530.23193
Epoch 99: Val Loss 2530.22412
{'MSE - mean': 2014.8323668378725, 'MSE - std': 627.5704612099094, 'R2 - mean': -0.21404706321497186, 'R2 - std': 0.03887152611258406} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 2014.8323668378725 and parameters: {'dim': 128, 'depth': 3, 'heads': 2, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1292.55383
Epoch 1: Val Loss 1292.55115
Epoch 2: Val Loss 1292.54871
Epoch 3: Val Loss 1292.54639
Epoch 4: Val Loss 1292.54407
Epoch 5: Val Loss 1292.54150
Epoch 6: Val Loss 1292.53906
Epoch 7: Val Loss 1292.53650
Epoch 8: Val Loss 1292.53418
Epoch 9: Val Loss 1292.53162
Epoch 10: Val Loss 1292.52917
Epoch 11: Val Loss 1292.52661
Epoch 12: Val Loss 1292.52441
Epoch 13: Val Loss 1292.52185
Epoch 14: Val Loss 1292.51941
Epoch 15: Val Loss 1292.51697
Epoch 16: Val Loss 1292.51453
Epoch 17: Val Loss 1292.51208
Epoch 18: Val Loss 1292.50964
Epoch 19: Val Loss 1292.50696
Epoch 20: Val Loss 1292.50439
Epoch 21: Val Loss 1292.50183
Epoch 22: Val Loss 1292.49951
Epoch 23: Val Loss 1292.49707
Epoch 24: Val Loss 1292.49451
Epoch 25: Val Loss 1292.49219
Epoch 26: Val Loss 1292.48962
Epoch 27: Val Loss 1292.48743
Epoch 28: Val Loss 1292.48499
Epoch 29: Val Loss 1292.48267
Epoch 30: Val Loss 1292.48010
Epoch 31: Val Loss 1292.47791
Epoch 32: Val Loss 1292.47546
Epoch 33: Val Loss 1292.47302
Epoch 34: Val Loss 1292.47046
Epoch 35: Val Loss 1292.46826
Epoch 36: Val Loss 1292.46570
Epoch 37: Val Loss 1292.46313
Epoch 38: Val Loss 1292.46069
Epoch 39: Val Loss 1292.45837
Epoch 40: Val Loss 1292.45581
Epoch 41: Val Loss 1292.45337
Epoch 42: Val Loss 1292.45081
Epoch 43: Val Loss 1292.44824
Epoch 44: Val Loss 1292.44580
Epoch 45: Val Loss 1292.44324
Epoch 46: Val Loss 1292.44092
Epoch 47: Val Loss 1292.43835
Epoch 48: Val Loss 1292.43604
Epoch 49: Val Loss 1292.43372
Epoch 50: Val Loss 1292.43115
Epoch 51: Val Loss 1292.42871
Epoch 52: Val Loss 1292.42615
Epoch 53: Val Loss 1292.42358
Epoch 54: Val Loss 1292.42114
Epoch 55: Val Loss 1292.41858
Epoch 56: Val Loss 1292.41626
Epoch 57: Val Loss 1292.41382
Epoch 58: Val Loss 1292.41138
Epoch 59: Val Loss 1292.40881
Epoch 60: Val Loss 1292.40637
Epoch 61: Val Loss 1292.40393
Epoch 62: Val Loss 1292.40137
Epoch 63: Val Loss 1292.39893
Epoch 64: Val Loss 1292.39661
Epoch 65: Val Loss 1292.39417
Epoch 66: Val Loss 1292.39172
Epoch 67: Val Loss 1292.38904
Epoch 68: Val Loss 1292.38684
Epoch 69: Val Loss 1292.38428
Epoch 70: Val Loss 1292.38184
Epoch 71: Val Loss 1292.37939
Epoch 72: Val Loss 1292.37720
Epoch 73: Val Loss 1292.37488
Epoch 74: Val Loss 1292.37231
Epoch 75: Val Loss 1292.36987
Epoch 76: Val Loss 1292.36743
Epoch 77: Val Loss 1292.36499
Epoch 78: Val Loss 1292.36243
Epoch 79: Val Loss 1292.35999
Epoch 80: Val Loss 1292.35779
Epoch 81: Val Loss 1292.35535
Epoch 82: Val Loss 1292.35291
Epoch 83: Val Loss 1292.35046
Epoch 84: Val Loss 1292.34790
Epoch 85: Val Loss 1292.34546
Epoch 86: Val Loss 1292.34290
Epoch 87: Val Loss 1292.34045
Epoch 88: Val Loss 1292.33777
Epoch 89: Val Loss 1292.33569
Epoch 90: Val Loss 1292.33325
Epoch 91: Val Loss 1292.33081
Epoch 92: Val Loss 1292.32837
Epoch 93: Val Loss 1292.32581
Epoch 94: Val Loss 1292.32324
Epoch 95: Val Loss 1292.32068
Epoch 96: Val Loss 1292.31812
Epoch 97: Val Loss 1292.31567
Epoch 98: Val Loss 1292.31311
Epoch 99: Val Loss 1292.31067
{'MSE - mean': 1292.3106894313526, 'MSE - std': 0.0, 'R2 - mean': -0.2623445080046636, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1313.05884
Epoch 1: Val Loss 1313.05652
Epoch 2: Val Loss 1313.05432
Epoch 3: Val Loss 1313.05225
Epoch 4: Val Loss 1313.05005
Epoch 5: Val Loss 1313.04785
Epoch 6: Val Loss 1313.04590
Epoch 7: Val Loss 1313.04370
Epoch 8: Val Loss 1313.04150
Epoch 9: Val Loss 1313.03955
Epoch 10: Val Loss 1313.03748
Epoch 11: Val Loss 1313.03540
Epoch 12: Val Loss 1313.03333
Epoch 13: Val Loss 1313.03113
Epoch 14: Val Loss 1313.02893
Epoch 15: Val Loss 1313.02698
Epoch 16: Val Loss 1313.02490
Epoch 17: Val Loss 1313.02271
Epoch 18: Val Loss 1313.02051
Epoch 19: Val Loss 1313.01843
Epoch 20: Val Loss 1313.01636
Epoch 21: Val Loss 1313.01428
Epoch 22: Val Loss 1313.01221
Epoch 23: Val Loss 1313.01001
Epoch 24: Val Loss 1313.00793
Epoch 25: Val Loss 1313.00598
Epoch 26: Val Loss 1313.00378
Epoch 27: Val Loss 1313.00171
Epoch 28: Val Loss 1312.99963
Epoch 29: Val Loss 1312.99756
Epoch 30: Val Loss 1312.99536
Epoch 31: Val Loss 1312.99341
Epoch 32: Val Loss 1312.99133
Epoch 33: Val Loss 1312.98926
Epoch 34: Val Loss 1312.98718
Epoch 35: Val Loss 1312.98499
Epoch 36: Val Loss 1312.98291
Epoch 37: Val Loss 1312.98096
Epoch 38: Val Loss 1312.97864
Epoch 39: Val Loss 1312.97644
Epoch 40: Val Loss 1312.97449
Epoch 41: Val Loss 1312.97241
Epoch 42: Val Loss 1312.96997
Epoch 43: Val Loss 1312.96777
Epoch 44: Val Loss 1312.96582
Epoch 45: Val Loss 1312.96375
Epoch 46: Val Loss 1312.96155
Epoch 47: Val Loss 1312.95947
Epoch 48: Val Loss 1312.95728
Epoch 49: Val Loss 1312.95520
Epoch 50: Val Loss 1312.95325
Epoch 51: Val Loss 1312.95117
Epoch 52: Val Loss 1312.94910
Epoch 53: Val Loss 1312.94714
Epoch 54: Val Loss 1312.94495
Epoch 55: Val Loss 1312.94275
Epoch 56: Val Loss 1312.94080
Epoch 57: Val Loss 1312.93872
Epoch 58: Val Loss 1312.93665
Epoch 59: Val Loss 1312.93445
Epoch 60: Val Loss 1312.93237
Epoch 61: Val Loss 1312.93042
Epoch 62: Val Loss 1312.92834
Epoch 63: Val Loss 1312.92639
Epoch 64: Val Loss 1312.92419
Epoch 65: Val Loss 1312.92236
Epoch 66: Val Loss 1312.92017
Epoch 67: Val Loss 1312.91821
Epoch 68: Val Loss 1312.91614
Epoch 69: Val Loss 1312.91394
Epoch 70: Val Loss 1312.91199
Epoch 71: Val Loss 1312.90991
Epoch 72: Val Loss 1312.90771
Epoch 73: Val Loss 1312.90588
Epoch 74: Val Loss 1312.90381
Epoch 75: Val Loss 1312.90173
Epoch 76: Val Loss 1312.89978
Epoch 77: Val Loss 1312.89758
Epoch 78: Val Loss 1312.89575
Epoch 79: Val Loss 1312.89368
Epoch 80: Val Loss 1312.89148
Epoch 81: Val Loss 1312.88965
Epoch 82: Val Loss 1312.88757
Epoch 83: Val Loss 1312.88550
Epoch 84: Val Loss 1312.88342
Epoch 85: Val Loss 1312.88147
Epoch 86: Val Loss 1312.87952
Epoch 87: Val Loss 1312.87744
Epoch 88: Val Loss 1312.87524
Epoch 89: Val Loss 1312.87341
Epoch 90: Val Loss 1312.87134
Epoch 91: Val Loss 1312.86914
Epoch 92: Val Loss 1312.86707
Epoch 93: Val Loss 1312.86499
Epoch 94: Val Loss 1312.86292
Epoch 95: Val Loss 1312.86096
Epoch 96: Val Loss 1312.85901
Epoch 97: Val Loss 1312.85706
Epoch 98: Val Loss 1312.85486
Epoch 99: Val Loss 1312.85278
{'MSE - mean': 1302.5817567853105, 'MSE - std': 10.271067353958074, 'R2 - mean': -0.25629450394289455, 'R2 - std': 0.006050004061769032} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2831.92432
Epoch 1: Val Loss 2831.92261
Epoch 2: Val Loss 2831.92065
Epoch 3: Val Loss 2831.91895
Epoch 4: Val Loss 2831.91699
Epoch 5: Val Loss 2831.91528
Epoch 6: Val Loss 2831.91333
Epoch 7: Val Loss 2831.91187
Epoch 8: Val Loss 2831.91016
Epoch 9: Val Loss 2831.90820
Epoch 10: Val Loss 2831.90601
Epoch 11: Val Loss 2831.90454
Epoch 12: Val Loss 2831.90259
Epoch 13: Val Loss 2831.90088
Epoch 14: Val Loss 2831.89941
Epoch 15: Val Loss 2831.89746
Epoch 16: Val Loss 2831.89551
Epoch 17: Val Loss 2831.89404
Epoch 18: Val Loss 2831.89233
Epoch 19: Val Loss 2831.89014
Epoch 20: Val Loss 2831.88818
Epoch 21: Val Loss 2831.88647
Epoch 22: Val Loss 2831.88477
Epoch 23: Val Loss 2831.88306
Epoch 24: Val Loss 2831.88110
Epoch 25: Val Loss 2831.87939
Epoch 26: Val Loss 2831.87744
Epoch 27: Val Loss 2831.87573
Epoch 28: Val Loss 2831.87402
Epoch 29: Val Loss 2831.87231
Epoch 30: Val Loss 2831.87036
Epoch 31: Val Loss 2831.86841
Epoch 32: Val Loss 2831.86670
Epoch 33: Val Loss 2831.86499
Epoch 34: Val Loss 2831.86304
Epoch 35: Val Loss 2831.86157
Epoch 36: Val Loss 2831.85938
Epoch 37: Val Loss 2831.85767
Epoch 38: Val Loss 2831.85571
Epoch 39: Val Loss 2831.85400
Epoch 40: Val Loss 2831.85205
Epoch 41: Val Loss 2831.85010
Epoch 42: Val Loss 2831.84790
Epoch 43: Val Loss 2831.84644
Epoch 44: Val Loss 2831.84448
Epoch 45: Val Loss 2831.84253
Epoch 46: Val Loss 2831.84082
Epoch 47: Val Loss 2831.83911
Epoch 48: Val Loss 2831.83716
Epoch 49: Val Loss 2831.83521
Epoch 50: Val Loss 2831.83301
Epoch 51: Val Loss 2831.83105
Epoch 52: Val Loss 2831.82910
Epoch 53: Val Loss 2831.82715
Epoch 54: Val Loss 2831.82520
Epoch 55: Val Loss 2831.82349
Epoch 56: Val Loss 2831.82153
Epoch 57: Val Loss 2831.81934
Epoch 58: Val Loss 2831.81738
Epoch 59: Val Loss 2831.81519
Epoch 60: Val Loss 2831.81323
Epoch 61: Val Loss 2831.81177
Epoch 62: Val Loss 2831.80957
Epoch 63: Val Loss 2831.80762
Epoch 64: Val Loss 2831.80542
Epoch 65: Val Loss 2831.80322
Epoch 66: Val Loss 2831.80151
Epoch 67: Val Loss 2831.79956
Epoch 68: Val Loss 2831.79736
Epoch 69: Val Loss 2831.79541
Epoch 70: Val Loss 2831.79321
Epoch 71: Val Loss 2831.79102
Epoch 72: Val Loss 2831.78906
Epoch 73: Val Loss 2831.78711
Epoch 74: Val Loss 2831.78491
Epoch 75: Val Loss 2831.78296
Epoch 76: Val Loss 2831.78076
Epoch 77: Val Loss 2831.77905
Epoch 78: Val Loss 2831.77661
Epoch 79: Val Loss 2831.77441
Epoch 80: Val Loss 2831.77246
Epoch 81: Val Loss 2831.77051
Epoch 82: Val Loss 2831.76831
Epoch 83: Val Loss 2831.76660
Epoch 84: Val Loss 2831.76440
Epoch 85: Val Loss 2831.76221
Epoch 86: Val Loss 2831.76025
Epoch 87: Val Loss 2831.75806
Epoch 88: Val Loss 2831.75586
Epoch 89: Val Loss 2831.75415
Epoch 90: Val Loss 2831.75195
Epoch 91: Val Loss 2831.74976
Epoch 92: Val Loss 2831.74780
Epoch 93: Val Loss 2831.74536
Epoch 94: Val Loss 2831.74341
Epoch 95: Val Loss 2831.74146
Epoch 96: Val Loss 2831.73926
Epoch 97: Val Loss 2831.73706
Epoch 98: Val Loss 2831.73486
Epoch 99: Val Loss 2831.73315
{'MSE - mean': 1812.2988550530215, 'MSE - std': 720.8976143780546, 'R2 - mean': -0.22280821141861332, 'R2 - std': 0.04761370888173992} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2144.57153
Epoch 1: Val Loss 2144.56787
Epoch 2: Val Loss 2144.56396
Epoch 3: Val Loss 2144.56055
Epoch 4: Val Loss 2144.55688
Epoch 5: Val Loss 2144.55347
Epoch 6: Val Loss 2144.54956
Epoch 7: Val Loss 2144.54614
Epoch 8: Val Loss 2144.54248
Epoch 9: Val Loss 2144.53857
Epoch 10: Val Loss 2144.53516
Epoch 11: Val Loss 2144.53149
Epoch 12: Val Loss 2144.52783
Epoch 13: Val Loss 2144.52441
Epoch 14: Val Loss 2144.52075
Epoch 15: Val Loss 2144.51709
Epoch 16: Val Loss 2144.51343
Epoch 17: Val Loss 2144.50977
Epoch 18: Val Loss 2144.50610
Epoch 19: Val Loss 2144.50293
Epoch 20: Val Loss 2144.49927
Epoch 21: Val Loss 2144.49585
Epoch 22: Val Loss 2144.49219
Epoch 23: Val Loss 2144.48828
Epoch 24: Val Loss 2144.48462
Epoch 25: Val Loss 2144.48096
Epoch 26: Val Loss 2144.47754
Epoch 27: Val Loss 2144.47388
Epoch 28: Val Loss 2144.47046
Epoch 29: Val Loss 2144.46680
Epoch 30: Val Loss 2144.46338
Epoch 31: Val Loss 2144.45972
Epoch 32: Val Loss 2144.45605
Epoch 33: Val Loss 2144.45264
Epoch 34: Val Loss 2144.44922
Epoch 35: Val Loss 2144.44531
Epoch 36: Val Loss 2144.44189
Epoch 37: Val Loss 2144.43823
Epoch 38: Val Loss 2144.43433
Epoch 39: Val Loss 2144.43091
Epoch 40: Val Loss 2144.42725
Epoch 41: Val Loss 2144.42358
Epoch 42: Val Loss 2144.41992
Epoch 43: Val Loss 2144.41650
Epoch 44: Val Loss 2144.41260
Epoch 45: Val Loss 2144.40894
Epoch 46: Val Loss 2144.40552
Epoch 47: Val Loss 2144.40186
Epoch 48: Val Loss 2144.39844
Epoch 49: Val Loss 2144.39502
Epoch 50: Val Loss 2144.39160
Epoch 51: Val Loss 2144.38770
Epoch 52: Val Loss 2144.38428
Epoch 53: Val Loss 2144.38062
Epoch 54: Val Loss 2144.37695
Epoch 55: Val Loss 2144.37354
Epoch 56: Val Loss 2144.37012
Epoch 57: Val Loss 2144.36670
Epoch 58: Val Loss 2144.36304
Epoch 59: Val Loss 2144.35938
Epoch 60: Val Loss 2144.35571
Epoch 61: Val Loss 2144.35205
Epoch 62: Val Loss 2144.34863
Epoch 63: Val Loss 2144.34497
Epoch 64: Val Loss 2144.34180
Epoch 65: Val Loss 2144.33813
Epoch 66: Val Loss 2144.33472
Epoch 67: Val Loss 2144.33105
Epoch 68: Val Loss 2144.32739
Epoch 69: Val Loss 2144.32373
Epoch 70: Val Loss 2144.32031
Epoch 71: Val Loss 2144.31641
Epoch 72: Val Loss 2144.31274
Epoch 73: Val Loss 2144.30933
Epoch 74: Val Loss 2144.30566
Epoch 75: Val Loss 2144.30225
Epoch 76: Val Loss 2144.29858
Epoch 77: Val Loss 2144.29517
Epoch 78: Val Loss 2144.29175
Epoch 79: Val Loss 2144.28809
Epoch 80: Val Loss 2144.28467
Epoch 81: Val Loss 2144.28101
Epoch 82: Val Loss 2144.27759
Epoch 83: Val Loss 2144.27417
Epoch 84: Val Loss 2144.27075
Epoch 85: Val Loss 2144.26733
Epoch 86: Val Loss 2144.26392
Epoch 87: Val Loss 2144.26050
Epoch 88: Val Loss 2144.25684
Epoch 89: Val Loss 2144.25342
Epoch 90: Val Loss 2144.24976
Epoch 91: Val Loss 2144.24634
Epoch 92: Val Loss 2144.24292
Epoch 93: Val Loss 2144.23950
Epoch 94: Val Loss 2144.23584
Epoch 95: Val Loss 2144.23267
Epoch 96: Val Loss 2144.22925
Epoch 97: Val Loss 2144.22559
Epoch 98: Val Loss 2144.22241
Epoch 99: Val Loss 2144.21899
{'MSE - mean': 1895.2788677981607, 'MSE - std': 640.6458267697748, 'R2 - mean': -0.2281516054504643, 'R2 - std': 0.04226055530213927} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2517.48926
Epoch 1: Val Loss 2517.48755
Epoch 2: Val Loss 2517.48560
Epoch 3: Val Loss 2517.48340
Epoch 4: Val Loss 2517.48193
Epoch 5: Val Loss 2517.47998
Epoch 6: Val Loss 2517.47827
Epoch 7: Val Loss 2517.47656
Epoch 8: Val Loss 2517.47485
Epoch 9: Val Loss 2517.47314
Epoch 10: Val Loss 2517.47095
Epoch 11: Val Loss 2517.46948
Epoch 12: Val Loss 2517.46777
Epoch 13: Val Loss 2517.46582
Epoch 14: Val Loss 2517.46436
Epoch 15: Val Loss 2517.46240
Epoch 16: Val Loss 2517.46045
Epoch 17: Val Loss 2517.45898
Epoch 18: Val Loss 2517.45679
Epoch 19: Val Loss 2517.45508
Epoch 20: Val Loss 2517.45337
Epoch 21: Val Loss 2517.45190
Epoch 22: Val Loss 2517.44995
Epoch 23: Val Loss 2517.44824
Epoch 24: Val Loss 2517.44653
Epoch 25: Val Loss 2517.44482
Epoch 26: Val Loss 2517.44312
Epoch 27: Val Loss 2517.44141
Epoch 28: Val Loss 2517.43994
Epoch 29: Val Loss 2517.43774
Epoch 30: Val Loss 2517.43604
Epoch 31: Val Loss 2517.43457
Epoch 32: Val Loss 2517.43237
Epoch 33: Val Loss 2517.43066
Epoch 34: Val Loss 2517.42920
Epoch 35: Val Loss 2517.42725
Epoch 36: Val Loss 2517.42529
Epoch 37: Val Loss 2517.42358
Epoch 38: Val Loss 2517.42212
Epoch 39: Val Loss 2517.42041
Epoch 40: Val Loss 2517.41895
Epoch 41: Val Loss 2517.41724
Epoch 42: Val Loss 2517.41528
Epoch 43: Val Loss 2517.41333
Epoch 44: Val Loss 2517.41162
Epoch 45: Val Loss 2517.40967
Epoch 46: Val Loss 2517.40771
Epoch 47: Val Loss 2517.40576
Epoch 48: Val Loss 2517.40430
Epoch 49: Val Loss 2517.40259
Epoch 50: Val Loss 2517.40063
Epoch 51: Val Loss 2517.39917
Epoch 52: Val Loss 2517.39746
Epoch 53: Val Loss 2517.39526
Epoch 54: Val Loss 2517.39355
Epoch 55: Val Loss 2517.39209
Epoch 56: Val Loss 2517.39038
Epoch 57: Val Loss 2517.38916
Epoch 58: Val Loss 2517.38696
Epoch 59: Val Loss 2517.38550
Epoch 60: Val Loss 2517.38403
Epoch 61: Val Loss 2517.38208
Epoch 62: Val Loss 2517.38037
Epoch 63: Val Loss 2517.37842
Epoch 64: Val Loss 2517.37671
Epoch 65: Val Loss 2517.37500
Epoch 66: Val Loss 2517.37305
Epoch 67: Val Loss 2517.37158
Epoch 68: Val Loss 2517.36987
Epoch 69: Val Loss 2517.36792
Epoch 70: Val Loss 2517.36646
Epoch 71: Val Loss 2517.36475
Epoch 72: Val Loss 2517.36304
Epoch 73: Val Loss 2517.36157
Epoch 74: Val Loss 2517.35986
Epoch 75: Val Loss 2517.35815
Epoch 76: Val Loss 2517.35669
Epoch 77: Val Loss 2517.35498
Epoch 78: Val Loss 2517.35327
Epoch 79: Val Loss 2517.35156
Epoch 80: Val Loss 2517.34985
Epoch 81: Val Loss 2517.34814
Epoch 82: Val Loss 2517.34668
Epoch 83: Val Loss 2517.34473
Epoch 84: Val Loss 2517.34302
Epoch 85: Val Loss 2517.34155
Epoch 86: Val Loss 2517.33984
Epoch 87: Val Loss 2517.33813
Epoch 88: Val Loss 2517.33667
Epoch 89: Val Loss 2517.33447
Epoch 90: Val Loss 2517.33301
Epoch 91: Val Loss 2517.33105
Epoch 92: Val Loss 2517.32959
Epoch 93: Val Loss 2517.32764
Epoch 94: Val Loss 2517.32593
Epoch 95: Val Loss 2517.32446
Epoch 96: Val Loss 2517.32275
Epoch 97: Val Loss 2517.32104
Epoch 98: Val Loss 2517.31958
Epoch 99: Val Loss 2517.31787
{'MSE - mean': 2019.6866636919542, 'MSE - std': 624.7006154972393, 'R2 - mean': -0.21788593372578, 'R2 - std': 0.04301511003580102} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 2019.6866636919542 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1276.34082
Epoch 1: Val Loss 1276.33813
Epoch 2: Val Loss 1276.33508
Epoch 3: Val Loss 1276.33228
Epoch 4: Val Loss 1276.32947
Epoch 5: Val Loss 1276.32666
Epoch 6: Val Loss 1276.32397
Epoch 7: Val Loss 1276.32117
Epoch 8: Val Loss 1276.31812
Epoch 9: Val Loss 1276.31531
Epoch 10: Val Loss 1276.31250
Epoch 11: Val Loss 1276.30981
Epoch 12: Val Loss 1276.30688
Epoch 13: Val Loss 1276.30396
Epoch 14: Val Loss 1276.30115
Epoch 15: Val Loss 1276.29822
Epoch 16: Val Loss 1276.29565
Epoch 17: Val Loss 1276.29297
Epoch 18: Val Loss 1276.29016
Epoch 19: Val Loss 1276.28748
Epoch 20: Val Loss 1276.28467
Epoch 21: Val Loss 1276.28174
Epoch 22: Val Loss 1276.27905
Epoch 23: Val Loss 1276.27637
Epoch 24: Val Loss 1276.27356
Epoch 25: Val Loss 1276.27087
Epoch 26: Val Loss 1276.26807
Epoch 27: Val Loss 1276.26538
Epoch 28: Val Loss 1276.26233
Epoch 29: Val Loss 1276.25964
Epoch 30: Val Loss 1276.25696
Epoch 31: Val Loss 1276.25403
Epoch 32: Val Loss 1276.25122
Epoch 33: Val Loss 1276.24841
Epoch 34: Val Loss 1276.24573
Epoch 35: Val Loss 1276.24304
Epoch 36: Val Loss 1276.23999
Epoch 37: Val Loss 1276.23706
Epoch 38: Val Loss 1276.23438
Epoch 39: Val Loss 1276.23145
Epoch 40: Val Loss 1276.22839
Epoch 41: Val Loss 1276.22559
Epoch 42: Val Loss 1276.22290
Epoch 43: Val Loss 1276.22009
Epoch 44: Val Loss 1276.21741
Epoch 45: Val Loss 1276.21472
Epoch 46: Val Loss 1276.21191
Epoch 47: Val Loss 1276.20923
Epoch 48: Val Loss 1276.20630
Epoch 49: Val Loss 1276.20361
Epoch 50: Val Loss 1276.20081
Epoch 51: Val Loss 1276.19800
Epoch 52: Val Loss 1276.19507
Epoch 53: Val Loss 1276.19238
Epoch 54: Val Loss 1276.18933
Epoch 55: Val Loss 1276.18665
Epoch 56: Val Loss 1276.18396
Epoch 57: Val Loss 1276.18127
Epoch 58: Val Loss 1276.17859
Epoch 59: Val Loss 1276.17590
Epoch 60: Val Loss 1276.17297
Epoch 61: Val Loss 1276.17029
Epoch 62: Val Loss 1276.16724
Epoch 63: Val Loss 1276.16455
Epoch 64: Val Loss 1276.16162
Epoch 65: Val Loss 1276.15881
Epoch 66: Val Loss 1276.15588
Epoch 67: Val Loss 1276.15295
Epoch 68: Val Loss 1276.15015
Epoch 69: Val Loss 1276.14734
Epoch 70: Val Loss 1276.14465
Epoch 71: Val Loss 1276.14172
Epoch 72: Val Loss 1276.13892
Epoch 73: Val Loss 1276.13599
Epoch 74: Val Loss 1276.13330
Epoch 75: Val Loss 1276.13037
Epoch 76: Val Loss 1276.12769
Epoch 77: Val Loss 1276.12500
Epoch 78: Val Loss 1276.12219
Epoch 79: Val Loss 1276.11938
Epoch 80: Val Loss 1276.11670
Epoch 81: Val Loss 1276.11365
Epoch 82: Val Loss 1276.11096
Epoch 83: Val Loss 1276.10815
Epoch 84: Val Loss 1276.10535
Epoch 85: Val Loss 1276.10266
Epoch 86: Val Loss 1276.09998
Epoch 87: Val Loss 1276.09692
Epoch 88: Val Loss 1276.09399
Epoch 89: Val Loss 1276.09106
Epoch 90: Val Loss 1276.08826
Epoch 91: Val Loss 1276.08545
Epoch 92: Val Loss 1276.08240
Epoch 93: Val Loss 1276.07959
Epoch 94: Val Loss 1276.07678
Epoch 95: Val Loss 1276.07385
Epoch 96: Val Loss 1276.07056
Epoch 97: Val Loss 1276.06763
Epoch 98: Val Loss 1276.06494
Epoch 99: Val Loss 1276.06201
{'MSE - mean': 1276.0620370852769, 'MSE - std': 0.0, 'R2 - mean': -0.2464726304296423, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1311.82898
Epoch 1: Val Loss 1311.82654
Epoch 2: Val Loss 1311.82397
Epoch 3: Val Loss 1311.82153
Epoch 4: Val Loss 1311.81897
Epoch 5: Val Loss 1311.81641
Epoch 6: Val Loss 1311.81409
Epoch 7: Val Loss 1311.81165
Epoch 8: Val Loss 1311.80908
Epoch 9: Val Loss 1311.80664
Epoch 10: Val Loss 1311.80408
Epoch 11: Val Loss 1311.80151
Epoch 12: Val Loss 1311.79895
Epoch 13: Val Loss 1311.79651
Epoch 14: Val Loss 1311.79407
Epoch 15: Val Loss 1311.79138
Epoch 16: Val Loss 1311.78894
Epoch 17: Val Loss 1311.78638
Epoch 18: Val Loss 1311.78369
Epoch 19: Val Loss 1311.78125
Epoch 20: Val Loss 1311.77881
Epoch 21: Val Loss 1311.77625
Epoch 22: Val Loss 1311.77380
Epoch 23: Val Loss 1311.77112
Epoch 24: Val Loss 1311.76868
Epoch 25: Val Loss 1311.76599
Epoch 26: Val Loss 1311.76367
Epoch 27: Val Loss 1311.76135
Epoch 28: Val Loss 1311.75891
Epoch 29: Val Loss 1311.75635
Epoch 30: Val Loss 1311.75391
Epoch 31: Val Loss 1311.75134
Epoch 32: Val Loss 1311.74878
Epoch 33: Val Loss 1311.74622
Epoch 34: Val Loss 1311.74365
Epoch 35: Val Loss 1311.74109
Epoch 36: Val Loss 1311.73865
Epoch 37: Val Loss 1311.73621
Epoch 38: Val Loss 1311.73364
Epoch 39: Val Loss 1311.73120
Epoch 40: Val Loss 1311.72864
Epoch 41: Val Loss 1311.72595
Epoch 42: Val Loss 1311.72351
Epoch 43: Val Loss 1311.72083
Epoch 44: Val Loss 1311.71802
Epoch 45: Val Loss 1311.71582
Epoch 46: Val Loss 1311.71326
Epoch 47: Val Loss 1311.71082
Epoch 48: Val Loss 1311.70801
Epoch 49: Val Loss 1311.70557
Epoch 50: Val Loss 1311.70288
Epoch 51: Val Loss 1311.70020
Epoch 52: Val Loss 1311.69763
Epoch 53: Val Loss 1311.69531
Epoch 54: Val Loss 1311.69287
Epoch 55: Val Loss 1311.69043
Epoch 56: Val Loss 1311.68787
Epoch 57: Val Loss 1311.68555
Epoch 58: Val Loss 1311.68298
Epoch 59: Val Loss 1311.68042
Epoch 60: Val Loss 1311.67822
Epoch 61: Val Loss 1311.67554
Epoch 62: Val Loss 1311.67297
Epoch 63: Val Loss 1311.67053
Epoch 64: Val Loss 1311.66797
Epoch 65: Val Loss 1311.66541
Epoch 66: Val Loss 1311.66284
Epoch 67: Val Loss 1311.66016
Epoch 68: Val Loss 1311.65771
Epoch 69: Val Loss 1311.65527
Epoch 70: Val Loss 1311.65259
Epoch 71: Val Loss 1311.65002
Epoch 72: Val Loss 1311.64746
Epoch 73: Val Loss 1311.64478
Epoch 74: Val Loss 1311.64221
Epoch 75: Val Loss 1311.63965
Epoch 76: Val Loss 1311.63708
Epoch 77: Val Loss 1311.63452
Epoch 78: Val Loss 1311.63208
Epoch 79: Val Loss 1311.62927
Epoch 80: Val Loss 1311.62659
Epoch 81: Val Loss 1311.62402
Epoch 82: Val Loss 1311.62146
Epoch 83: Val Loss 1311.61877
Epoch 84: Val Loss 1311.61609
Epoch 85: Val Loss 1311.61328
Epoch 86: Val Loss 1311.61072
Epoch 87: Val Loss 1311.60803
Epoch 88: Val Loss 1311.60571
Epoch 89: Val Loss 1311.60303
Epoch 90: Val Loss 1311.60071
Epoch 91: Val Loss 1311.59790
Epoch 92: Val Loss 1311.59546
Epoch 93: Val Loss 1311.59302
Epoch 94: Val Loss 1311.59070
Epoch 95: Val Loss 1311.58826
Epoch 96: Val Loss 1311.58582
Epoch 97: Val Loss 1311.58301
Epoch 98: Val Loss 1311.58044
Epoch 99: Val Loss 1311.57800
{'MSE - mean': 1293.8200164105865, 'MSE - std': 17.757979325309634, 'R2 - mean': -0.24775154844707403, 'R2 - std': 0.0012789180174317405} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2838.13477
Epoch 1: Val Loss 2838.13086
Epoch 2: Val Loss 2838.12744
Epoch 3: Val Loss 2838.12427
Epoch 4: Val Loss 2838.12036
Epoch 5: Val Loss 2838.11694
Epoch 6: Val Loss 2838.11304
Epoch 7: Val Loss 2838.10986
Epoch 8: Val Loss 2838.10645
Epoch 9: Val Loss 2838.10278
Epoch 10: Val Loss 2838.09961
Epoch 11: Val Loss 2838.09570
Epoch 12: Val Loss 2838.09253
Epoch 13: Val Loss 2838.08936
Epoch 14: Val Loss 2838.08545
Epoch 15: Val Loss 2838.08179
Epoch 16: Val Loss 2838.07837
Epoch 17: Val Loss 2838.07495
Epoch 18: Val Loss 2838.07178
Epoch 19: Val Loss 2838.06836
Epoch 20: Val Loss 2838.06470
Epoch 21: Val Loss 2838.06104
Epoch 22: Val Loss 2838.05786
Epoch 23: Val Loss 2838.05420
Epoch 24: Val Loss 2838.05078
Epoch 25: Val Loss 2838.04736
Epoch 26: Val Loss 2838.04346
Epoch 27: Val Loss 2838.04004
Epoch 28: Val Loss 2838.03687
Epoch 29: Val Loss 2838.03296
Epoch 30: Val Loss 2838.02930
Epoch 31: Val Loss 2838.02539
Epoch 32: Val Loss 2838.02246
Epoch 33: Val Loss 2838.01904
Epoch 34: Val Loss 2838.01562
Epoch 35: Val Loss 2838.01221
Epoch 36: Val Loss 2838.00854
Epoch 37: Val Loss 2838.00513
Epoch 38: Val Loss 2838.00146
Epoch 39: Val Loss 2837.99829
Epoch 40: Val Loss 2837.99487
Epoch 41: Val Loss 2837.99146
Epoch 42: Val Loss 2837.98804
Epoch 43: Val Loss 2837.98486
Epoch 44: Val Loss 2837.98145
Epoch 45: Val Loss 2837.97778
Epoch 46: Val Loss 2837.97412
Epoch 47: Val Loss 2837.97070
Epoch 48: Val Loss 2837.96704
Epoch 49: Val Loss 2837.96362
Epoch 50: Val Loss 2837.96021
Epoch 51: Val Loss 2837.95679
Epoch 52: Val Loss 2837.95337
Epoch 53: Val Loss 2837.94995
Epoch 54: Val Loss 2837.94604
Epoch 55: Val Loss 2837.94263
Epoch 56: Val Loss 2837.93945
Epoch 57: Val Loss 2837.93604
Epoch 58: Val Loss 2837.93237
Epoch 59: Val Loss 2837.92920
Epoch 60: Val Loss 2837.92529
Epoch 61: Val Loss 2837.92212
Epoch 62: Val Loss 2837.91821
Epoch 63: Val Loss 2837.91504
Epoch 64: Val Loss 2837.91187
Epoch 65: Val Loss 2837.90820
Epoch 66: Val Loss 2837.90503
Epoch 67: Val Loss 2837.90161
Epoch 68: Val Loss 2837.89795
Epoch 69: Val Loss 2837.89478
Epoch 70: Val Loss 2837.89111
Epoch 71: Val Loss 2837.88794
Epoch 72: Val Loss 2837.88428
Epoch 73: Val Loss 2837.88062
Epoch 74: Val Loss 2837.87720
Epoch 75: Val Loss 2837.87402
Epoch 76: Val Loss 2837.87012
Epoch 77: Val Loss 2837.86694
Epoch 78: Val Loss 2837.86353
Epoch 79: Val Loss 2837.86035
Epoch 80: Val Loss 2837.85693
Epoch 81: Val Loss 2837.85327
Epoch 82: Val Loss 2837.85010
Epoch 83: Val Loss 2837.84644
Epoch 84: Val Loss 2837.84302
Epoch 85: Val Loss 2837.83960
Epoch 86: Val Loss 2837.83594
Epoch 87: Val Loss 2837.83276
Epoch 88: Val Loss 2837.82959
Epoch 89: Val Loss 2837.82568
Epoch 90: Val Loss 2837.82251
Epoch 91: Val Loss 2837.81909
Epoch 92: Val Loss 2837.81543
Epoch 93: Val Loss 2837.81226
Epoch 94: Val Loss 2837.80835
Epoch 95: Val Loss 2837.80469
Epoch 96: Val Loss 2837.80103
Epoch 97: Val Loss 2837.79736
Epoch 98: Val Loss 2837.79395
Epoch 99: Val Loss 2837.79053
{'MSE - mean': 1808.4767924683122, 'MSE - std': 727.9790002722672, 'R2 - mean': -0.21793704794448057, 'R2 - std': 0.042176999671643685} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2123.00269
Epoch 1: Val Loss 2123.00146
Epoch 2: Val Loss 2123.00024
Epoch 3: Val Loss 2122.99927
Epoch 4: Val Loss 2122.99805
Epoch 5: Val Loss 2122.99683
Epoch 6: Val Loss 2122.99561
Epoch 7: Val Loss 2122.99463
Epoch 8: Val Loss 2122.99365
Epoch 9: Val Loss 2122.99243
Epoch 10: Val Loss 2122.99146
Epoch 11: Val Loss 2122.99023
Epoch 12: Val Loss 2122.98901
Epoch 13: Val Loss 2122.98804
Epoch 14: Val Loss 2122.98706
Epoch 15: Val Loss 2122.98608
Epoch 16: Val Loss 2122.98511
Epoch 17: Val Loss 2122.98389
Epoch 18: Val Loss 2122.98291
Epoch 19: Val Loss 2122.98169
Epoch 20: Val Loss 2122.98071
Epoch 21: Val Loss 2122.97949
Epoch 22: Val Loss 2122.97876
Epoch 23: Val Loss 2122.97754
Epoch 24: Val Loss 2122.97656
Epoch 25: Val Loss 2122.97534
Epoch 26: Val Loss 2122.97412
Epoch 27: Val Loss 2122.97314
Epoch 28: Val Loss 2122.97192
Epoch 29: Val Loss 2122.97070
Epoch 30: Val Loss 2122.96973
Epoch 31: Val Loss 2122.96851
Epoch 32: Val Loss 2122.96753
Epoch 33: Val Loss 2122.96655
Epoch 34: Val Loss 2122.96533
Epoch 35: Val Loss 2122.96436
Epoch 36: Val Loss 2122.96338
Epoch 37: Val Loss 2122.96240
Epoch 38: Val Loss 2122.96143
Epoch 39: Val Loss 2122.96045
Epoch 40: Val Loss 2122.95923
Epoch 41: Val Loss 2122.95801
Epoch 42: Val Loss 2122.95703
Epoch 43: Val Loss 2122.95581
Epoch 44: Val Loss 2122.95483
Epoch 45: Val Loss 2122.95410
Epoch 46: Val Loss 2122.95288
Epoch 47: Val Loss 2122.95166
Epoch 48: Val Loss 2122.95044
Epoch 49: Val Loss 2122.94946
Epoch 50: Val Loss 2122.94824
Epoch 51: Val Loss 2122.94727
Epoch 52: Val Loss 2122.94604
Epoch 53: Val Loss 2122.94507
Epoch 54: Val Loss 2122.94385
Epoch 55: Val Loss 2122.94263
Epoch 56: Val Loss 2122.94189
Epoch 57: Val Loss 2122.94067
Epoch 58: Val Loss 2122.93970
Epoch 59: Val Loss 2122.93823
Epoch 60: Val Loss 2122.93726
Epoch 61: Val Loss 2122.93628
Epoch 62: Val Loss 2122.93506
Epoch 63: Val Loss 2122.93408
Epoch 64: Val Loss 2122.93286
Epoch 65: Val Loss 2122.93164
Epoch 66: Val Loss 2122.93042
Epoch 67: Val Loss 2122.92944
Epoch 68: Val Loss 2122.92822
Epoch 69: Val Loss 2122.92725
Epoch 70: Val Loss 2122.92603
Epoch 71: Val Loss 2122.92480
Epoch 72: Val Loss 2122.92358
Epoch 73: Val Loss 2122.92261
Epoch 74: Val Loss 2122.92163
Epoch 75: Val Loss 2122.92041
Epoch 76: Val Loss 2122.91919
Epoch 77: Val Loss 2122.91821
Epoch 78: Val Loss 2122.91724
Epoch 79: Val Loss 2122.91577
Epoch 80: Val Loss 2122.91504
Epoch 81: Val Loss 2122.91406
Epoch 82: Val Loss 2122.91284
Epoch 83: Val Loss 2122.91162
Epoch 84: Val Loss 2122.91064
Epoch 85: Val Loss 2122.90942
Epoch 86: Val Loss 2122.90845
Epoch 87: Val Loss 2122.90723
Epoch 88: Val Loss 2122.90601
Epoch 89: Val Loss 2122.90479
Epoch 90: Val Loss 2122.90405
Epoch 91: Val Loss 2122.90259
Epoch 92: Val Loss 2122.90161
Epoch 93: Val Loss 2122.90015
Epoch 94: Val Loss 2122.89893
Epoch 95: Val Loss 2122.89795
Epoch 96: Val Loss 2122.89673
Epoch 97: Val Loss 2122.89551
Epoch 98: Val Loss 2122.89453
Epoch 99: Val Loss 2122.89307
{'MSE - mean': 1887.0808613794313, 'MSE - std': 644.9812924234384, 'R2 - mean': -0.2214046565464579, 'R2 - std': 0.03701685297571763} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2525.24609
Epoch 1: Val Loss 2525.24414
Epoch 2: Val Loss 2525.24194
Epoch 3: Val Loss 2525.23926
Epoch 4: Val Loss 2525.23706
Epoch 5: Val Loss 2525.23511
Epoch 6: Val Loss 2525.23267
Epoch 7: Val Loss 2525.23022
Epoch 8: Val Loss 2525.22827
Epoch 9: Val Loss 2525.22583
Epoch 10: Val Loss 2525.22363
Epoch 11: Val Loss 2525.22168
Epoch 12: Val Loss 2525.21924
Epoch 13: Val Loss 2525.21704
Epoch 14: Val Loss 2525.21484
Epoch 15: Val Loss 2525.21265
Epoch 16: Val Loss 2525.21021
Epoch 17: Val Loss 2525.20776
Epoch 18: Val Loss 2525.20557
Epoch 19: Val Loss 2525.20361
Epoch 20: Val Loss 2525.20142
Epoch 21: Val Loss 2525.19922
Epoch 22: Val Loss 2525.19678
Epoch 23: Val Loss 2525.19458
Epoch 24: Val Loss 2525.19214
Epoch 25: Val Loss 2525.18994
Epoch 26: Val Loss 2525.18774
Epoch 27: Val Loss 2525.18530
Epoch 28: Val Loss 2525.18286
Epoch 29: Val Loss 2525.18066
Epoch 30: Val Loss 2525.17847
Epoch 31: Val Loss 2525.17603
Epoch 32: Val Loss 2525.17432
Epoch 33: Val Loss 2525.17212
Epoch 34: Val Loss 2525.16943
Epoch 35: Val Loss 2525.16724
Epoch 36: Val Loss 2525.16504
Epoch 37: Val Loss 2525.16284
Epoch 38: Val Loss 2525.16064
Epoch 39: Val Loss 2525.15820
Epoch 40: Val Loss 2525.15576
Epoch 41: Val Loss 2525.15332
Epoch 42: Val Loss 2525.15161
Epoch 43: Val Loss 2525.14893
Epoch 44: Val Loss 2525.14648
Epoch 45: Val Loss 2525.14429
Epoch 46: Val Loss 2525.14209
Epoch 47: Val Loss 2525.13965
Epoch 48: Val Loss 2525.13745
Epoch 49: Val Loss 2525.13501
Epoch 50: Val Loss 2525.13257
Epoch 51: Val Loss 2525.13013
Epoch 52: Val Loss 2525.12793
Epoch 53: Val Loss 2525.12549
Epoch 54: Val Loss 2525.12329
Epoch 55: Val Loss 2525.12085
Epoch 56: Val Loss 2525.11841
Epoch 57: Val Loss 2525.11597
Epoch 58: Val Loss 2525.11401
Epoch 59: Val Loss 2525.11157
Epoch 60: Val Loss 2525.10938
Epoch 61: Val Loss 2525.10718
Epoch 62: Val Loss 2525.10498
Epoch 63: Val Loss 2525.10254
Epoch 64: Val Loss 2525.10034
Epoch 65: Val Loss 2525.09790
Epoch 66: Val Loss 2525.09570
Epoch 67: Val Loss 2525.09302
Epoch 68: Val Loss 2525.09082
Epoch 69: Val Loss 2525.08911
Epoch 70: Val Loss 2525.08667
Epoch 71: Val Loss 2525.08447
Epoch 72: Val Loss 2525.08228
Epoch 73: Val Loss 2525.07983
Epoch 74: Val Loss 2525.07764
Epoch 75: Val Loss 2525.07520
Epoch 76: Val Loss 2525.07275
Epoch 77: Val Loss 2525.07056
Epoch 78: Val Loss 2525.06836
Epoch 79: Val Loss 2525.06592
Epoch 80: Val Loss 2525.06396
Epoch 81: Val Loss 2525.06177
Epoch 82: Val Loss 2525.05957
Epoch 83: Val Loss 2525.05737
Epoch 84: Val Loss 2525.05493
Epoch 85: Val Loss 2525.05249
Epoch 86: Val Loss 2525.05005
Epoch 87: Val Loss 2525.04761
Epoch 88: Val Loss 2525.04541
Epoch 89: Val Loss 2525.04321
Epoch 90: Val Loss 2525.04102
Epoch 91: Val Loss 2525.03833
Epoch 92: Val Loss 2525.03662
Epoch 93: Val Loss 2525.03442
Epoch 94: Val Loss 2525.03174
Epoch 95: Val Loss 2525.02954
Epoch 96: Val Loss 2525.02759
Epoch 97: Val Loss 2525.02490
Epoch 98: Val Loss 2525.02271
Epoch 99: Val Loss 2525.02051
{'MSE - mean': 2014.668776514567, 'MSE - std': 630.8053569961892, 'R2 - mean': -0.21320855276625905, 'R2 - std': 0.03694458542308841} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 2014.668776514567 and parameters: {'dim': 32, 'depth': 12, 'heads': 8, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.1}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1291.87341
Epoch 1: Val Loss 1291.87036
Epoch 2: Val Loss 1291.86743
Epoch 3: Val Loss 1291.86438
Epoch 4: Val Loss 1291.86157
Epoch 5: Val Loss 1291.85889
Epoch 6: Val Loss 1291.85571
Epoch 7: Val Loss 1291.85291
Epoch 8: Val Loss 1291.84985
Epoch 9: Val Loss 1291.84692
Epoch 10: Val Loss 1291.84399
Epoch 11: Val Loss 1291.84131
Epoch 12: Val Loss 1291.83838
Epoch 13: Val Loss 1291.83569
Epoch 14: Val Loss 1291.83301
Epoch 15: Val Loss 1291.82983
Epoch 16: Val Loss 1291.82703
Epoch 17: Val Loss 1291.82410
Epoch 18: Val Loss 1291.82117
Epoch 19: Val Loss 1291.81812
Epoch 20: Val Loss 1291.81531
Epoch 21: Val Loss 1291.81238
Epoch 22: Val Loss 1291.80945
Epoch 23: Val Loss 1291.80627
Epoch 24: Val Loss 1291.80334
Epoch 25: Val Loss 1291.80005
Epoch 26: Val Loss 1291.79712
Epoch 27: Val Loss 1291.79419
Epoch 28: Val Loss 1291.79114
Epoch 29: Val Loss 1291.78821
Epoch 30: Val Loss 1291.78528
Epoch 31: Val Loss 1291.78235
Epoch 32: Val Loss 1291.77942
Epoch 33: Val Loss 1291.77625
Epoch 34: Val Loss 1291.77344
Epoch 35: Val Loss 1291.77051
Epoch 36: Val Loss 1291.76746
Epoch 37: Val Loss 1291.76453
Epoch 38: Val Loss 1291.76184
Epoch 39: Val Loss 1291.75891
Epoch 40: Val Loss 1291.75598
Epoch 41: Val Loss 1291.75305
Epoch 42: Val Loss 1291.75012
Epoch 43: Val Loss 1291.74719
Epoch 44: Val Loss 1291.74426
Epoch 45: Val Loss 1291.74133
Epoch 46: Val Loss 1291.73853
Epoch 47: Val Loss 1291.73547
Epoch 48: Val Loss 1291.73267
Epoch 49: Val Loss 1291.72974
Epoch 50: Val Loss 1291.72668
Epoch 51: Val Loss 1291.72375
Epoch 52: Val Loss 1291.72083
Epoch 53: Val Loss 1291.71790
Epoch 54: Val Loss 1291.71521
Epoch 55: Val Loss 1291.71252
Epoch 56: Val Loss 1291.70972
Epoch 57: Val Loss 1291.70703
Epoch 58: Val Loss 1291.70410
Epoch 59: Val Loss 1291.70093
Epoch 60: Val Loss 1291.69800
Epoch 61: Val Loss 1291.69531
Epoch 62: Val Loss 1291.69238
Epoch 63: Val Loss 1291.68909
Epoch 64: Val Loss 1291.68616
Epoch 65: Val Loss 1291.68311
Epoch 66: Val Loss 1291.68018
Epoch 67: Val Loss 1291.67725
Epoch 68: Val Loss 1291.67432
Epoch 69: Val Loss 1291.67163
Epoch 70: Val Loss 1291.66882
Epoch 71: Val Loss 1291.66577
Epoch 72: Val Loss 1291.66296
Epoch 73: Val Loss 1291.66028
Epoch 74: Val Loss 1291.65735
Epoch 75: Val Loss 1291.65466
Epoch 76: Val Loss 1291.65161
Epoch 77: Val Loss 1291.64880
Epoch 78: Val Loss 1291.64575
Epoch 79: Val Loss 1291.64307
Epoch 80: Val Loss 1291.64026
Epoch 81: Val Loss 1291.63733
Epoch 82: Val Loss 1291.63452
Epoch 83: Val Loss 1291.63171
Epoch 84: Val Loss 1291.62891
Epoch 85: Val Loss 1291.62610
Epoch 86: Val Loss 1291.62354
Epoch 87: Val Loss 1291.62036
Epoch 88: Val Loss 1291.61755
Epoch 89: Val Loss 1291.61475
Epoch 90: Val Loss 1291.61182
Epoch 91: Val Loss 1291.60913
Epoch 92: Val Loss 1291.60608
Epoch 93: Val Loss 1291.60339
Epoch 94: Val Loss 1291.60071
Epoch 95: Val Loss 1291.59790
Epoch 96: Val Loss 1291.59509
Epoch 97: Val Loss 1291.59229
Epoch 98: Val Loss 1291.58948
Epoch 99: Val Loss 1291.58679
{'MSE - mean': 1291.5866687036184, 'MSE - std': 0.0, 'R2 - mean': -0.2616372759150345, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1314.29541
Epoch 1: Val Loss 1314.29382
Epoch 2: Val Loss 1314.29211
Epoch 3: Val Loss 1314.29028
Epoch 4: Val Loss 1314.28870
Epoch 5: Val Loss 1314.28711
Epoch 6: Val Loss 1314.28528
Epoch 7: Val Loss 1314.28369
Epoch 8: Val Loss 1314.28198
Epoch 9: Val Loss 1314.28052
Epoch 10: Val Loss 1314.27869
Epoch 11: Val Loss 1314.27710
Epoch 12: Val Loss 1314.27539
Epoch 13: Val Loss 1314.27368
Epoch 14: Val Loss 1314.27222
Epoch 15: Val Loss 1314.27026
Epoch 16: Val Loss 1314.26880
Epoch 17: Val Loss 1314.26709
Epoch 18: Val Loss 1314.26538
Epoch 19: Val Loss 1314.26379
Epoch 20: Val Loss 1314.26233
Epoch 21: Val Loss 1314.26074
Epoch 22: Val Loss 1314.25891
Epoch 23: Val Loss 1314.25720
Epoch 24: Val Loss 1314.25574
Epoch 25: Val Loss 1314.25378
Epoch 26: Val Loss 1314.25220
Epoch 27: Val Loss 1314.25049
Epoch 28: Val Loss 1314.24890
Epoch 29: Val Loss 1314.24731
Epoch 30: Val Loss 1314.24573
Epoch 31: Val Loss 1314.24402
Epoch 32: Val Loss 1314.24243
Epoch 33: Val Loss 1314.24084
Epoch 34: Val Loss 1314.23914
Epoch 35: Val Loss 1314.23755
Epoch 36: Val Loss 1314.23584
Epoch 37: Val Loss 1314.23413
Epoch 38: Val Loss 1314.23267
Epoch 39: Val Loss 1314.23083
Epoch 40: Val Loss 1314.22913
Epoch 41: Val Loss 1314.22742
Epoch 42: Val Loss 1314.22595
Epoch 43: Val Loss 1314.22424
Epoch 44: Val Loss 1314.22266
Epoch 45: Val Loss 1314.22107
Epoch 46: Val Loss 1314.21948
Epoch 47: Val Loss 1314.21777
Epoch 48: Val Loss 1314.21619
Epoch 49: Val Loss 1314.21460
Epoch 50: Val Loss 1314.21289
Epoch 51: Val Loss 1314.21130
Epoch 52: Val Loss 1314.20947
Epoch 53: Val Loss 1314.20801
Epoch 54: Val Loss 1314.20642
Epoch 55: Val Loss 1314.20471
Epoch 56: Val Loss 1314.20300
Epoch 57: Val Loss 1314.20154
Epoch 58: Val Loss 1314.19995
Epoch 59: Val Loss 1314.19836
Epoch 60: Val Loss 1314.19666
Epoch 61: Val Loss 1314.19507
Epoch 62: Val Loss 1314.19348
Epoch 63: Val Loss 1314.19177
Epoch 64: Val Loss 1314.19019
Epoch 65: Val Loss 1314.18848
Epoch 66: Val Loss 1314.18665
Epoch 67: Val Loss 1314.18506
Epoch 68: Val Loss 1314.18359
Epoch 69: Val Loss 1314.18176
Epoch 70: Val Loss 1314.18018
Epoch 71: Val Loss 1314.17859
Epoch 72: Val Loss 1314.17712
Epoch 73: Val Loss 1314.17529
Epoch 74: Val Loss 1314.17371
Epoch 75: Val Loss 1314.17200
Epoch 76: Val Loss 1314.17041
Epoch 77: Val Loss 1314.16858
Epoch 78: Val Loss 1314.16699
Epoch 79: Val Loss 1314.16528
Epoch 80: Val Loss 1314.16370
Epoch 81: Val Loss 1314.16211
Epoch 82: Val Loss 1314.16040
Epoch 83: Val Loss 1314.15881
Epoch 84: Val Loss 1314.15723
Epoch 85: Val Loss 1314.15588
Epoch 86: Val Loss 1314.15405
Epoch 87: Val Loss 1314.15234
Epoch 88: Val Loss 1314.15088
Epoch 89: Val Loss 1314.14929
Epoch 90: Val Loss 1314.14746
Epoch 91: Val Loss 1314.14600
Epoch 92: Val Loss 1314.14453
Epoch 93: Val Loss 1314.14270
Epoch 94: Val Loss 1314.14124
Epoch 95: Val Loss 1314.13965
Epoch 96: Val Loss 1314.13794
Epoch 97: Val Loss 1314.13635
Epoch 98: Val Loss 1314.13489
Epoch 99: Val Loss 1314.13330
{'MSE - mean': 1302.8599513897022, 'MSE - std': 11.273282686083803, 'R2 - mean': -0.2565505622847629, 'R2 - std': 0.005086713630271578} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2830.46313
Epoch 1: Val Loss 2830.45996
Epoch 2: Val Loss 2830.45679
Epoch 3: Val Loss 2830.45288
Epoch 4: Val Loss 2830.44971
Epoch 5: Val Loss 2830.44653
Epoch 6: Val Loss 2830.44287
Epoch 7: Val Loss 2830.43970
Epoch 8: Val Loss 2830.43604
Epoch 9: Val Loss 2830.43286
Epoch 10: Val Loss 2830.42969
Epoch 11: Val Loss 2830.42651
Epoch 12: Val Loss 2830.42310
Epoch 13: Val Loss 2830.41968
Epoch 14: Val Loss 2830.41675
Epoch 15: Val Loss 2830.41284
Epoch 16: Val Loss 2830.40967
Epoch 17: Val Loss 2830.40601
Epoch 18: Val Loss 2830.40308
Epoch 19: Val Loss 2830.39941
Epoch 20: Val Loss 2830.39600
Epoch 21: Val Loss 2830.39282
Epoch 22: Val Loss 2830.38965
Epoch 23: Val Loss 2830.38599
Epoch 24: Val Loss 2830.38281
Epoch 25: Val Loss 2830.37939
Epoch 26: Val Loss 2830.37598
Epoch 27: Val Loss 2830.37280
Epoch 28: Val Loss 2830.36914
Epoch 29: Val Loss 2830.36572
Epoch 30: Val Loss 2830.36255
Epoch 31: Val Loss 2830.35913
Epoch 32: Val Loss 2830.35547
Epoch 33: Val Loss 2830.35229
Epoch 34: Val Loss 2830.34912
Epoch 35: Val Loss 2830.34570
Epoch 36: Val Loss 2830.34229
Epoch 37: Val Loss 2830.33911
Epoch 38: Val Loss 2830.33545
Epoch 39: Val Loss 2830.33203
Epoch 40: Val Loss 2830.32861
Epoch 41: Val Loss 2830.32520
Epoch 42: Val Loss 2830.32202
Epoch 43: Val Loss 2830.31812
Epoch 44: Val Loss 2830.31494
Epoch 45: Val Loss 2830.31177
Epoch 46: Val Loss 2830.30835
Epoch 47: Val Loss 2830.30493
Epoch 48: Val Loss 2830.30151
Epoch 49: Val Loss 2830.29810
Epoch 50: Val Loss 2830.29492
Epoch 51: Val Loss 2830.29150
Epoch 52: Val Loss 2830.28809
Epoch 53: Val Loss 2830.28516
Epoch 54: Val Loss 2830.28149
Epoch 55: Val Loss 2830.27808
Epoch 56: Val Loss 2830.27490
Epoch 57: Val Loss 2830.27148
Epoch 58: Val Loss 2830.26782
Epoch 59: Val Loss 2830.26440
Epoch 60: Val Loss 2830.26074
Epoch 61: Val Loss 2830.25757
Epoch 62: Val Loss 2830.25415
Epoch 63: Val Loss 2830.25073
Epoch 64: Val Loss 2830.24756
Epoch 65: Val Loss 2830.24463
Epoch 66: Val Loss 2830.24097
Epoch 67: Val Loss 2830.23804
Epoch 68: Val Loss 2830.23462
Epoch 69: Val Loss 2830.23169
Epoch 70: Val Loss 2830.22778
Epoch 71: Val Loss 2830.22461
Epoch 72: Val Loss 2830.22168
Epoch 73: Val Loss 2830.21777
Epoch 74: Val Loss 2830.21460
Epoch 75: Val Loss 2830.21143
Epoch 76: Val Loss 2830.20801
Epoch 77: Val Loss 2830.20508
Epoch 78: Val Loss 2830.20190
Epoch 79: Val Loss 2830.19849
Epoch 80: Val Loss 2830.19531
Epoch 81: Val Loss 2830.19214
Epoch 82: Val Loss 2830.18896
Epoch 83: Val Loss 2830.18579
Epoch 84: Val Loss 2830.18286
Epoch 85: Val Loss 2830.17944
Epoch 86: Val Loss 2830.17578
Epoch 87: Val Loss 2830.17261
Epoch 88: Val Loss 2830.16943
Epoch 89: Val Loss 2830.16650
Epoch 90: Val Loss 2830.16284
Epoch 91: Val Loss 2830.15967
Epoch 92: Val Loss 2830.15649
Epoch 93: Val Loss 2830.15308
Epoch 94: Val Loss 2830.14966
Epoch 95: Val Loss 2830.14600
Epoch 96: Val Loss 2830.14233
Epoch 97: Val Loss 2830.13892
Epoch 98: Val Loss 2830.13574
Epoch 99: Val Loss 2830.13208
{'MSE - mean': 1811.9506971805804, 'MSE - std': 720.0218743215414, 'R2 - mean': -0.22276110757320033, 'R2 - std': 0.04796565719116548} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2122.23608
Epoch 1: Val Loss 2122.23413
Epoch 2: Val Loss 2122.23193
Epoch 3: Val Loss 2122.23022
Epoch 4: Val Loss 2122.22803
Epoch 5: Val Loss 2122.22607
Epoch 6: Val Loss 2122.22437
Epoch 7: Val Loss 2122.22217
Epoch 8: Val Loss 2122.22021
Epoch 9: Val Loss 2122.21802
Epoch 10: Val Loss 2122.21606
Epoch 11: Val Loss 2122.21411
Epoch 12: Val Loss 2122.21191
Epoch 13: Val Loss 2122.21021
Epoch 14: Val Loss 2122.20801
Epoch 15: Val Loss 2122.20605
Epoch 16: Val Loss 2122.20410
Epoch 17: Val Loss 2122.20215
Epoch 18: Val Loss 2122.19971
Epoch 19: Val Loss 2122.19800
Epoch 20: Val Loss 2122.19580
Epoch 21: Val Loss 2122.19385
Epoch 22: Val Loss 2122.19189
Epoch 23: Val Loss 2122.18994
Epoch 24: Val Loss 2122.18799
Epoch 25: Val Loss 2122.18579
Epoch 26: Val Loss 2122.18359
Epoch 27: Val Loss 2122.18164
Epoch 28: Val Loss 2122.17969
Epoch 29: Val Loss 2122.17749
Epoch 30: Val Loss 2122.17554
Epoch 31: Val Loss 2122.17334
Epoch 32: Val Loss 2122.17163
Epoch 33: Val Loss 2122.16943
Epoch 34: Val Loss 2122.16748
Epoch 35: Val Loss 2122.16528
Epoch 36: Val Loss 2122.16333
Epoch 37: Val Loss 2122.16138
Epoch 38: Val Loss 2122.15942
Epoch 39: Val Loss 2122.15698
Epoch 40: Val Loss 2122.15527
Epoch 41: Val Loss 2122.15283
Epoch 42: Val Loss 2122.15112
Epoch 43: Val Loss 2122.14917
Epoch 44: Val Loss 2122.14697
Epoch 45: Val Loss 2122.14478
Epoch 46: Val Loss 2122.14282
Epoch 47: Val Loss 2122.14087
Epoch 48: Val Loss 2122.13892
Epoch 49: Val Loss 2122.13672
Epoch 50: Val Loss 2122.13452
Epoch 51: Val Loss 2122.13281
Epoch 52: Val Loss 2122.13062
Epoch 53: Val Loss 2122.12866
Epoch 54: Val Loss 2122.12646
Epoch 55: Val Loss 2122.12451
Epoch 56: Val Loss 2122.12256
Epoch 57: Val Loss 2122.12036
Epoch 58: Val Loss 2122.11841
Epoch 59: Val Loss 2122.11646
Epoch 60: Val Loss 2122.11426
Epoch 61: Val Loss 2122.11230
Epoch 62: Val Loss 2122.11011
Epoch 63: Val Loss 2122.10791
Epoch 64: Val Loss 2122.10596
Epoch 65: Val Loss 2122.10400
Epoch 66: Val Loss 2122.10205
Epoch 67: Val Loss 2122.09985
Epoch 68: Val Loss 2122.09814
Epoch 69: Val Loss 2122.09595
Epoch 70: Val Loss 2122.09399
Epoch 71: Val Loss 2122.09204
Epoch 72: Val Loss 2122.08984
Epoch 73: Val Loss 2122.08789
Epoch 74: Val Loss 2122.08569
Epoch 75: Val Loss 2122.08398
Epoch 76: Val Loss 2122.08179
Epoch 77: Val Loss 2122.07983
Epoch 78: Val Loss 2122.07788
Epoch 79: Val Loss 2122.07568
Epoch 80: Val Loss 2122.07397
Epoch 81: Val Loss 2122.07178
Epoch 82: Val Loss 2122.06982
Epoch 83: Val Loss 2122.06787
Epoch 84: Val Loss 2122.06567
Epoch 85: Val Loss 2122.06372
Epoch 86: Val Loss 2122.06177
Epoch 87: Val Loss 2122.05981
Epoch 88: Val Loss 2122.05786
Epoch 89: Val Loss 2122.05566
Epoch 90: Val Loss 2122.05371
Epoch 91: Val Loss 2122.05151
Epoch 92: Val Loss 2122.04980
Epoch 93: Val Loss 2122.04785
Epoch 94: Val Loss 2122.04541
Epoch 95: Val Loss 2122.04346
Epoch 96: Val Loss 2122.04175
Epoch 97: Val Loss 2122.03931
Epoch 98: Val Loss 2122.03735
Epoch 99: Val Loss 2122.03516
{'MSE - mean': 1889.4718178572457, 'MSE - std': 637.8496665194922, 'R2 - mean': -0.2248982539805761, 'R2 - std': 0.041704081174808075} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2511.21753
Epoch 1: Val Loss 2511.21191
Epoch 2: Val Loss 2511.20605
Epoch 3: Val Loss 2511.20020
Epoch 4: Val Loss 2511.19458
Epoch 5: Val Loss 2511.18896
Epoch 6: Val Loss 2511.18311
Epoch 7: Val Loss 2511.17749
Epoch 8: Val Loss 2511.17163
Epoch 9: Val Loss 2511.16602
Epoch 10: Val Loss 2511.16040
Epoch 11: Val Loss 2511.15479
Epoch 12: Val Loss 2511.14893
Epoch 13: Val Loss 2511.14331
Epoch 14: Val Loss 2511.13770
Epoch 15: Val Loss 2511.13208
Epoch 16: Val Loss 2511.12671
Epoch 17: Val Loss 2511.12085
Epoch 18: Val Loss 2511.11523
Epoch 19: Val Loss 2511.10913
Epoch 20: Val Loss 2511.10303
Epoch 21: Val Loss 2511.09741
Epoch 22: Val Loss 2511.09204
Epoch 23: Val Loss 2511.08667
Epoch 24: Val Loss 2511.08081
Epoch 25: Val Loss 2511.07544
Epoch 26: Val Loss 2511.06958
Epoch 27: Val Loss 2511.06348
Epoch 28: Val Loss 2511.05786
Epoch 29: Val Loss 2511.05225
Epoch 30: Val Loss 2511.04663
Epoch 31: Val Loss 2511.04053
Epoch 32: Val Loss 2511.03467
Epoch 33: Val Loss 2511.02905
Epoch 34: Val Loss 2511.02319
Epoch 35: Val Loss 2511.01807
Epoch 36: Val Loss 2511.01245
Epoch 37: Val Loss 2511.00684
Epoch 38: Val Loss 2511.00098
Epoch 39: Val Loss 2510.99536
Epoch 40: Val Loss 2510.98975
Epoch 41: Val Loss 2510.98438
Epoch 42: Val Loss 2510.97852
Epoch 43: Val Loss 2510.97314
Epoch 44: Val Loss 2510.96753
Epoch 45: Val Loss 2510.96216
Epoch 46: Val Loss 2510.95605
Epoch 47: Val Loss 2510.95020
Epoch 48: Val Loss 2510.94434
Epoch 49: Val Loss 2510.93896
Epoch 50: Val Loss 2510.93286
Epoch 51: Val Loss 2510.92749
Epoch 52: Val Loss 2510.92188
Epoch 53: Val Loss 2510.91602
Epoch 54: Val Loss 2510.91089
Epoch 55: Val Loss 2510.90503
Epoch 56: Val Loss 2510.89966
Epoch 57: Val Loss 2510.89404
Epoch 58: Val Loss 2510.88794
Epoch 59: Val Loss 2510.88208
Epoch 60: Val Loss 2510.87671
Epoch 61: Val Loss 2510.87085
Epoch 62: Val Loss 2510.86523
Epoch 63: Val Loss 2510.85938
Epoch 64: Val Loss 2510.85352
Epoch 65: Val Loss 2510.84814
Epoch 66: Val Loss 2510.84253
Epoch 67: Val Loss 2510.83667
Epoch 68: Val Loss 2510.83105
Epoch 69: Val Loss 2510.82544
Epoch 70: Val Loss 2510.82007
Epoch 71: Val Loss 2510.81396
Epoch 72: Val Loss 2510.80811
Epoch 73: Val Loss 2510.80249
Epoch 74: Val Loss 2510.79663
Epoch 75: Val Loss 2510.79077
Epoch 76: Val Loss 2510.78516
Epoch 77: Val Loss 2510.77930
Epoch 78: Val Loss 2510.77344
Epoch 79: Val Loss 2510.76782
Epoch 80: Val Loss 2510.76245
Epoch 81: Val Loss 2510.75659
Epoch 82: Val Loss 2510.75098
Epoch 83: Val Loss 2510.74512
Epoch 84: Val Loss 2510.73926
Epoch 85: Val Loss 2510.73340
Epoch 86: Val Loss 2510.72778
Epoch 87: Val Loss 2510.72168
Epoch 88: Val Loss 2510.71655
Epoch 89: Val Loss 2510.71069
Epoch 90: Val Loss 2510.70532
Epoch 91: Val Loss 2510.69922
Epoch 92: Val Loss 2510.69336
Epoch 93: Val Loss 2510.68774
Epoch 94: Val Loss 2510.68188
Epoch 95: Val Loss 2510.67578
Epoch 96: Val Loss 2510.66992
Epoch 97: Val Loss 2510.66455
Epoch 98: Val Loss 2510.65918
Epoch 99: Val Loss 2510.65332
{'MSE - mean': 2013.7080977435064, 'MSE - std': 622.27033564451, 'R2 - mean': -0.21466012174429347, 'R2 - std': 0.042551870887875466} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 2013.7080977435064 and parameters: {'dim': 64, 'depth': 6, 'heads': 8, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1295.17786
Epoch 1: Val Loss 1295.17542
Epoch 2: Val Loss 1295.17297
Epoch 3: Val Loss 1295.17053
Epoch 4: Val Loss 1295.16797
Epoch 5: Val Loss 1295.16565
Epoch 6: Val Loss 1295.16321
Epoch 7: Val Loss 1295.16077
Epoch 8: Val Loss 1295.15845
Epoch 9: Val Loss 1295.15625
Epoch 10: Val Loss 1295.15381
Epoch 11: Val Loss 1295.15137
Epoch 12: Val Loss 1295.14893
Epoch 13: Val Loss 1295.14661
Epoch 14: Val Loss 1295.14429
Epoch 15: Val Loss 1295.14172
Epoch 16: Val Loss 1295.13928
Epoch 17: Val Loss 1295.13684
Epoch 18: Val Loss 1295.13428
Epoch 19: Val Loss 1295.13171
Epoch 20: Val Loss 1295.12927
Epoch 21: Val Loss 1295.12683
Epoch 22: Val Loss 1295.12463
Epoch 23: Val Loss 1295.12231
Epoch 24: Val Loss 1295.11987
Epoch 25: Val Loss 1295.11743
Epoch 26: Val Loss 1295.11487
Epoch 27: Val Loss 1295.11230
Epoch 28: Val Loss 1295.10999
Epoch 29: Val Loss 1295.10767
Epoch 30: Val Loss 1295.10535
Epoch 31: Val Loss 1295.10291
Epoch 32: Val Loss 1295.10046
Epoch 33: Val Loss 1295.09802
Epoch 34: Val Loss 1295.09558
Epoch 35: Val Loss 1295.09351
Epoch 36: Val Loss 1295.09094
Epoch 37: Val Loss 1295.08850
Epoch 38: Val Loss 1295.08618
Epoch 39: Val Loss 1295.08386
Epoch 40: Val Loss 1295.08167
Epoch 41: Val Loss 1295.07922
Epoch 42: Val Loss 1295.07690
Epoch 43: Val Loss 1295.07458
Epoch 44: Val Loss 1295.07214
Epoch 45: Val Loss 1295.07007
Epoch 46: Val Loss 1295.06763
Epoch 47: Val Loss 1295.06519
Epoch 48: Val Loss 1295.06274
Epoch 49: Val Loss 1295.06030
Epoch 50: Val Loss 1295.05823
Epoch 51: Val Loss 1295.05579
Epoch 52: Val Loss 1295.05334
Epoch 53: Val Loss 1295.05090
Epoch 54: Val Loss 1295.04858
Epoch 55: Val Loss 1295.04614
Epoch 56: Val Loss 1295.04407
Epoch 57: Val Loss 1295.04175
Epoch 58: Val Loss 1295.03931
Epoch 59: Val Loss 1295.03687
Epoch 60: Val Loss 1295.03442
Epoch 61: Val Loss 1295.03210
Epoch 62: Val Loss 1295.02966
Epoch 63: Val Loss 1295.02698
Epoch 64: Val Loss 1295.02466
Epoch 65: Val Loss 1295.02234
Epoch 66: Val Loss 1295.01990
Epoch 67: Val Loss 1295.01746
Epoch 68: Val Loss 1295.01538
Epoch 69: Val Loss 1295.01294
Epoch 70: Val Loss 1295.01050
Epoch 71: Val Loss 1295.00830
Epoch 72: Val Loss 1295.00598
Epoch 73: Val Loss 1295.00378
Epoch 74: Val Loss 1295.00134
Epoch 75: Val Loss 1294.99890
Epoch 76: Val Loss 1294.99658
Epoch 77: Val Loss 1294.99402
Epoch 78: Val Loss 1294.99182
Epoch 79: Val Loss 1294.98950
Epoch 80: Val Loss 1294.98694
Epoch 81: Val Loss 1294.98462
Epoch 82: Val Loss 1294.98218
Epoch 83: Val Loss 1294.97961
Epoch 84: Val Loss 1294.97717
Epoch 85: Val Loss 1294.97498
Epoch 86: Val Loss 1294.97241
Epoch 87: Val Loss 1294.96997
Epoch 88: Val Loss 1294.96753
Epoch 89: Val Loss 1294.96497
Epoch 90: Val Loss 1294.96252
Epoch 91: Val Loss 1294.96008
Epoch 92: Val Loss 1294.95764
Epoch 93: Val Loss 1294.95532
Epoch 94: Val Loss 1294.95288
Epoch 95: Val Loss 1294.95056
Epoch 96: Val Loss 1294.94800
Epoch 97: Val Loss 1294.94580
Epoch 98: Val Loss 1294.94348
Epoch 99: Val Loss 1294.94116
{'MSE - mean': 1294.9411346388572, 'MSE - std': 0.0, 'R2 - mean': -0.26491395828349873, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1310.41736
Epoch 1: Val Loss 1310.41492
Epoch 2: Val Loss 1310.41272
Epoch 3: Val Loss 1310.41040
Epoch 4: Val Loss 1310.40833
Epoch 5: Val Loss 1310.40588
Epoch 6: Val Loss 1310.40356
Epoch 7: Val Loss 1310.40125
Epoch 8: Val Loss 1310.39905
Epoch 9: Val Loss 1310.39673
Epoch 10: Val Loss 1310.39453
Epoch 11: Val Loss 1310.39233
Epoch 12: Val Loss 1310.38989
Epoch 13: Val Loss 1310.38770
Epoch 14: Val Loss 1310.38538
Epoch 15: Val Loss 1310.38306
Epoch 16: Val Loss 1310.38086
Epoch 17: Val Loss 1310.37854
Epoch 18: Val Loss 1310.37610
Epoch 19: Val Loss 1310.37390
Epoch 20: Val Loss 1310.37170
Epoch 21: Val Loss 1310.36963
Epoch 22: Val Loss 1310.36731
Epoch 23: Val Loss 1310.36499
Epoch 24: Val Loss 1310.36267
Epoch 25: Val Loss 1310.36023
Epoch 26: Val Loss 1310.35828
Epoch 27: Val Loss 1310.35583
Epoch 28: Val Loss 1310.35364
Epoch 29: Val Loss 1310.35144
Epoch 30: Val Loss 1310.34900
Epoch 31: Val Loss 1310.34668
Epoch 32: Val Loss 1310.34448
Epoch 33: Val Loss 1310.34229
Epoch 34: Val Loss 1310.33997
Epoch 35: Val Loss 1310.33752
Epoch 36: Val Loss 1310.33521
Epoch 37: Val Loss 1310.33301
Epoch 38: Val Loss 1310.33093
Epoch 39: Val Loss 1310.32849
Epoch 40: Val Loss 1310.32629
Epoch 41: Val Loss 1310.32410
Epoch 42: Val Loss 1310.32202
Epoch 43: Val Loss 1310.31958
Epoch 44: Val Loss 1310.31714
Epoch 45: Val Loss 1310.31482
Epoch 46: Val Loss 1310.31250
Epoch 47: Val Loss 1310.31030
Epoch 48: Val Loss 1310.30786
Epoch 49: Val Loss 1310.30579
Epoch 50: Val Loss 1310.30359
Epoch 51: Val Loss 1310.30127
Epoch 52: Val Loss 1310.29919
Epoch 53: Val Loss 1310.29712
Epoch 54: Val Loss 1310.29480
Epoch 55: Val Loss 1310.29260
Epoch 56: Val Loss 1310.29028
Epoch 57: Val Loss 1310.28821
Epoch 58: Val Loss 1310.28601
Epoch 59: Val Loss 1310.28381
Epoch 60: Val Loss 1310.28162
Epoch 61: Val Loss 1310.27954
Epoch 62: Val Loss 1310.27734
Epoch 63: Val Loss 1310.27490
Epoch 64: Val Loss 1310.27271
Epoch 65: Val Loss 1310.27039
Epoch 66: Val Loss 1310.26843
Epoch 67: Val Loss 1310.26599
Epoch 68: Val Loss 1310.26367
Epoch 69: Val Loss 1310.26135
Epoch 70: Val Loss 1310.25928
Epoch 71: Val Loss 1310.25708
Epoch 72: Val Loss 1310.25476
Epoch 73: Val Loss 1310.25244
Epoch 74: Val Loss 1310.25024
Epoch 75: Val Loss 1310.24780
Epoch 76: Val Loss 1310.24573
Epoch 77: Val Loss 1310.24341
Epoch 78: Val Loss 1310.24109
Epoch 79: Val Loss 1310.23901
Epoch 80: Val Loss 1310.23657
Epoch 81: Val Loss 1310.23450
Epoch 82: Val Loss 1310.23230
Epoch 83: Val Loss 1310.23010
Epoch 84: Val Loss 1310.22778
Epoch 85: Val Loss 1310.22571
Epoch 86: Val Loss 1310.22327
Epoch 87: Val Loss 1310.22083
Epoch 88: Val Loss 1310.21838
Epoch 89: Val Loss 1310.21594
Epoch 90: Val Loss 1310.21387
Epoch 91: Val Loss 1310.21155
Epoch 92: Val Loss 1310.20947
Epoch 93: Val Loss 1310.20728
Epoch 94: Val Loss 1310.20496
Epoch 95: Val Loss 1310.20264
Epoch 96: Val Loss 1310.20044
Epoch 97: Val Loss 1310.19800
Epoch 98: Val Loss 1310.19592
Epoch 99: Val Loss 1310.19348
{'MSE - mean': 1302.567267180612, 'MSE - std': 7.626132541754828, 'R2 - mean': -0.2563129292012646, 'R2 - std': 0.008601029082234146} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2836.62183
Epoch 1: Val Loss 2836.61987
Epoch 2: Val Loss 2836.61792
Epoch 3: Val Loss 2836.61597
Epoch 4: Val Loss 2836.61450
Epoch 5: Val Loss 2836.61255
Epoch 6: Val Loss 2836.61084
Epoch 7: Val Loss 2836.60864
Epoch 8: Val Loss 2836.60718
Epoch 9: Val Loss 2836.60547
Epoch 10: Val Loss 2836.60327
Epoch 11: Val Loss 2836.60156
Epoch 12: Val Loss 2836.59961
Epoch 13: Val Loss 2836.59790
Epoch 14: Val Loss 2836.59595
Epoch 15: Val Loss 2836.59424
Epoch 16: Val Loss 2836.59253
Epoch 17: Val Loss 2836.59033
Epoch 18: Val Loss 2836.58862
Epoch 19: Val Loss 2836.58716
Epoch 20: Val Loss 2836.58545
Epoch 21: Val Loss 2836.58350
Epoch 22: Val Loss 2836.58179
Epoch 23: Val Loss 2836.57959
Epoch 24: Val Loss 2836.57812
Epoch 25: Val Loss 2836.57642
Epoch 26: Val Loss 2836.57471
Epoch 27: Val Loss 2836.57275
Epoch 28: Val Loss 2836.57104
Epoch 29: Val Loss 2836.56934
Epoch 30: Val Loss 2836.56763
Epoch 31: Val Loss 2836.56592
Epoch 32: Val Loss 2836.56396
Epoch 33: Val Loss 2836.56226
Epoch 34: Val Loss 2836.56055
Epoch 35: Val Loss 2836.55908
Epoch 36: Val Loss 2836.55713
Epoch 37: Val Loss 2836.55542
Epoch 38: Val Loss 2836.55347
Epoch 39: Val Loss 2836.55200
Epoch 40: Val Loss 2836.55005
Epoch 41: Val Loss 2836.54834
Epoch 42: Val Loss 2836.54663
Epoch 43: Val Loss 2836.54468
Epoch 44: Val Loss 2836.54272
Epoch 45: Val Loss 2836.54102
Epoch 46: Val Loss 2836.53931
Epoch 47: Val Loss 2836.53735
Epoch 48: Val Loss 2836.53564
Epoch 49: Val Loss 2836.53418
Epoch 50: Val Loss 2836.53247
Epoch 51: Val Loss 2836.53027
Epoch 52: Val Loss 2836.52856
Epoch 53: Val Loss 2836.52686
Epoch 54: Val Loss 2836.52490
Epoch 55: Val Loss 2836.52319
Epoch 56: Val Loss 2836.52148
Epoch 57: Val Loss 2836.51953
Epoch 58: Val Loss 2836.51782
Epoch 59: Val Loss 2836.51587
Epoch 60: Val Loss 2836.51416
Epoch 61: Val Loss 2836.51245
Epoch 62: Val Loss 2836.51050
Epoch 63: Val Loss 2836.50928
Epoch 64: Val Loss 2836.50708
Epoch 65: Val Loss 2836.50537
Epoch 66: Val Loss 2836.50391
Epoch 67: Val Loss 2836.50195
Epoch 68: Val Loss 2836.50049
Epoch 69: Val Loss 2836.49829
Epoch 70: Val Loss 2836.49683
Epoch 71: Val Loss 2836.49487
Epoch 72: Val Loss 2836.49292
Epoch 73: Val Loss 2836.49146
Epoch 74: Val Loss 2836.48975
Epoch 75: Val Loss 2836.48779
Epoch 76: Val Loss 2836.48584
Epoch 77: Val Loss 2836.48438
Epoch 78: Val Loss 2836.48267
Epoch 79: Val Loss 2836.48071
Epoch 80: Val Loss 2836.47900
Epoch 81: Val Loss 2836.47729
Epoch 82: Val Loss 2836.47559
Epoch 83: Val Loss 2836.47412
Epoch 84: Val Loss 2836.47217
Epoch 85: Val Loss 2836.47021
Epoch 86: Val Loss 2836.46802
Epoch 87: Val Loss 2836.46655
Epoch 88: Val Loss 2836.46484
Epoch 89: Val Loss 2836.46313
Epoch 90: Val Loss 2836.46167
Epoch 91: Val Loss 2836.45947
Epoch 92: Val Loss 2836.45776
Epoch 93: Val Loss 2836.45605
Epoch 94: Val Loss 2836.45459
Epoch 95: Val Loss 2836.45288
Epoch 96: Val Loss 2836.45068
Epoch 97: Val Loss 2836.44922
Epoch 98: Val Loss 2836.44751
Epoch 99: Val Loss 2836.44531
{'MSE - mean': 1813.8599877215663, 'MSE - std': 723.1039096063835, 'R2 - mean': -0.22346164922449852, 'R2 - std': 0.04698650508113331} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2123.06055
Epoch 1: Val Loss 2123.05566
Epoch 2: Val Loss 2123.05078
Epoch 3: Val Loss 2123.04565
Epoch 4: Val Loss 2123.04077
Epoch 5: Val Loss 2123.03564
Epoch 6: Val Loss 2123.03027
Epoch 7: Val Loss 2123.02539
Epoch 8: Val Loss 2123.02051
Epoch 9: Val Loss 2123.01538
Epoch 10: Val Loss 2123.01025
Epoch 11: Val Loss 2123.00513
Epoch 12: Val Loss 2123.00024
Epoch 13: Val Loss 2122.99536
Epoch 14: Val Loss 2122.99072
Epoch 15: Val Loss 2122.98535
Epoch 16: Val Loss 2122.98071
Epoch 17: Val Loss 2122.97559
Epoch 18: Val Loss 2122.97046
Epoch 19: Val Loss 2122.96533
Epoch 20: Val Loss 2122.96045
Epoch 21: Val Loss 2122.95557
Epoch 22: Val Loss 2122.95068
Epoch 23: Val Loss 2122.94556
Epoch 24: Val Loss 2122.94092
Epoch 25: Val Loss 2122.93579
Epoch 26: Val Loss 2122.93066
Epoch 27: Val Loss 2122.92554
Epoch 28: Val Loss 2122.92041
Epoch 29: Val Loss 2122.91553
Epoch 30: Val Loss 2122.91064
Epoch 31: Val Loss 2122.90527
Epoch 32: Val Loss 2122.90015
Epoch 33: Val Loss 2122.89551
Epoch 34: Val Loss 2122.89014
Epoch 35: Val Loss 2122.88525
Epoch 36: Val Loss 2122.88013
Epoch 37: Val Loss 2122.87476
Epoch 38: Val Loss 2122.86987
Epoch 39: Val Loss 2122.86475
Epoch 40: Val Loss 2122.85986
Epoch 41: Val Loss 2122.85474
Epoch 42: Val Loss 2122.84985
Epoch 43: Val Loss 2122.84473
Epoch 44: Val Loss 2122.83984
Epoch 45: Val Loss 2122.83496
Epoch 46: Val Loss 2122.83032
Epoch 47: Val Loss 2122.82520
Epoch 48: Val Loss 2122.82031
Epoch 49: Val Loss 2122.81543
Epoch 50: Val Loss 2122.81055
Epoch 51: Val Loss 2122.80566
Epoch 52: Val Loss 2122.80078
Epoch 53: Val Loss 2122.79614
Epoch 54: Val Loss 2122.79077
Epoch 55: Val Loss 2122.78564
Epoch 56: Val Loss 2122.78076
Epoch 57: Val Loss 2122.77588
Epoch 58: Val Loss 2122.77124
Epoch 59: Val Loss 2122.76611
Epoch 60: Val Loss 2122.76172
Epoch 61: Val Loss 2122.75684
Epoch 62: Val Loss 2122.75146
Epoch 63: Val Loss 2122.74658
Epoch 64: Val Loss 2122.74170
Epoch 65: Val Loss 2122.73682
Epoch 66: Val Loss 2122.73193
Epoch 67: Val Loss 2122.72656
Epoch 68: Val Loss 2122.72144
Epoch 69: Val Loss 2122.71655
Epoch 70: Val Loss 2122.71094
Epoch 71: Val Loss 2122.70605
Epoch 72: Val Loss 2122.70117
Epoch 73: Val Loss 2122.69653
Epoch 74: Val Loss 2122.69165
Epoch 75: Val Loss 2122.68677
Epoch 76: Val Loss 2122.68188
Epoch 77: Val Loss 2122.67700
Epoch 78: Val Loss 2122.67212
Epoch 79: Val Loss 2122.66724
Epoch 80: Val Loss 2122.66211
Epoch 81: Val Loss 2122.65723
Epoch 82: Val Loss 2122.65210
Epoch 83: Val Loss 2122.64746
Epoch 84: Val Loss 2122.64282
Epoch 85: Val Loss 2122.63770
Epoch 86: Val Loss 2122.63306
Epoch 87: Val Loss 2122.62793
Epoch 88: Val Loss 2122.62329
Epoch 89: Val Loss 2122.61841
Epoch 90: Val Loss 2122.61353
Epoch 91: Val Loss 2122.60864
Epoch 92: Val Loss 2122.60376
Epoch 93: Val Loss 2122.59863
Epoch 94: Val Loss 2122.59375
Epoch 95: Val Loss 2122.58862
Epoch 96: Val Loss 2122.58398
Epoch 97: Val Loss 2122.57910
Epoch 98: Val Loss 2122.57422
Epoch 99: Val Loss 2122.56958
{'MSE - mean': 1891.037373497845, 'MSE - std': 640.3346739727665, 'R2 - mean': -0.22550117442540463, 'R2 - std': 0.04084455574418602} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2532.47266
Epoch 1: Val Loss 2532.47095
Epoch 2: Val Loss 2532.46973
Epoch 3: Val Loss 2532.46802
Epoch 4: Val Loss 2532.46704
Epoch 5: Val Loss 2532.46533
Epoch 6: Val Loss 2532.46411
Epoch 7: Val Loss 2532.46265
Epoch 8: Val Loss 2532.46143
Epoch 9: Val Loss 2532.45972
Epoch 10: Val Loss 2532.45825
Epoch 11: Val Loss 2532.45703
Epoch 12: Val Loss 2532.45557
Epoch 13: Val Loss 2532.45459
Epoch 14: Val Loss 2532.45288
Epoch 15: Val Loss 2532.45166
Epoch 16: Val Loss 2532.45020
Epoch 17: Val Loss 2532.44922
Epoch 18: Val Loss 2532.44751
Epoch 19: Val Loss 2532.44604
Epoch 20: Val Loss 2532.44482
Epoch 21: Val Loss 2532.44336
Epoch 22: Val Loss 2532.44214
Epoch 23: Val Loss 2532.44043
Epoch 24: Val Loss 2532.43921
Epoch 25: Val Loss 2532.43774
Epoch 26: Val Loss 2532.43604
Epoch 27: Val Loss 2532.43481
Epoch 28: Val Loss 2532.43335
Epoch 29: Val Loss 2532.43188
Epoch 30: Val Loss 2532.43018
Epoch 31: Val Loss 2532.42920
Epoch 32: Val Loss 2532.42749
Epoch 33: Val Loss 2532.42651
Epoch 34: Val Loss 2532.42480
Epoch 35: Val Loss 2532.42358
Epoch 36: Val Loss 2532.42212
Epoch 37: Val Loss 2532.42041
Epoch 38: Val Loss 2532.41943
Epoch 39: Val Loss 2532.41772
Epoch 40: Val Loss 2532.41650
Epoch 41: Val Loss 2532.41504
Epoch 42: Val Loss 2532.41406
Epoch 43: Val Loss 2532.41235
Epoch 44: Val Loss 2532.41064
Epoch 45: Val Loss 2532.40967
Epoch 46: Val Loss 2532.40796
Epoch 47: Val Loss 2532.40698
Epoch 48: Val Loss 2532.40527
Epoch 49: Val Loss 2532.40430
Epoch 50: Val Loss 2532.40259
Epoch 51: Val Loss 2532.40088
Epoch 52: Val Loss 2532.39966
Epoch 53: Val Loss 2532.39819
Epoch 54: Val Loss 2532.39722
Epoch 55: Val Loss 2532.39551
Epoch 56: Val Loss 2532.39429
Epoch 57: Val Loss 2532.39282
Epoch 58: Val Loss 2532.39160
Epoch 59: Val Loss 2532.39014
Epoch 60: Val Loss 2532.38843
Epoch 61: Val Loss 2532.38721
Epoch 62: Val Loss 2532.38574
Epoch 63: Val Loss 2532.38477
Epoch 64: Val Loss 2532.38306
Epoch 65: Val Loss 2532.38208
Epoch 66: Val Loss 2532.38037
Epoch 67: Val Loss 2532.37891
Epoch 68: Val Loss 2532.37769
Epoch 69: Val Loss 2532.37598
Epoch 70: Val Loss 2532.37500
Epoch 71: Val Loss 2532.37354
Epoch 72: Val Loss 2532.37207
Epoch 73: Val Loss 2532.37061
Epoch 74: Val Loss 2532.36963
Epoch 75: Val Loss 2532.36792
Epoch 76: Val Loss 2532.36670
Epoch 77: Val Loss 2532.36523
Epoch 78: Val Loss 2532.36401
Epoch 79: Val Loss 2532.36230
Epoch 80: Val Loss 2532.36084
Epoch 81: Val Loss 2532.35938
Epoch 82: Val Loss 2532.35815
Epoch 83: Val Loss 2532.35669
Epoch 84: Val Loss 2532.35522
Epoch 85: Val Loss 2532.35400
Epoch 86: Val Loss 2532.35254
Epoch 87: Val Loss 2532.35107
Epoch 88: Val Loss 2532.34961
Epoch 89: Val Loss 2532.34839
Epoch 90: Val Loss 2532.34692
Epoch 91: Val Loss 2532.34521
Epoch 92: Val Loss 2532.34399
Epoch 93: Val Loss 2532.34253
Epoch 94: Val Loss 2532.34106
Epoch 95: Val Loss 2532.33984
Epoch 96: Val Loss 2532.33862
Epoch 97: Val Loss 2532.33716
Epoch 98: Val Loss 2532.33569
Epoch 99: Val Loss 2532.33447
{'MSE - mean': 2019.296784330557, 'MSE - std': 627.5545407548773, 'R2 - mean': -0.21716961191498535, 'R2 - std': 0.04015323053767943} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 2019.296784330557 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -6, 'dropout': 0.1}. Best is trial 4 with value: 2009.879632560957.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1281.07153
Epoch 1: Val Loss 1281.05176
Epoch 2: Val Loss 1281.03149
Epoch 3: Val Loss 1281.01074
Epoch 4: Val Loss 1280.98987
Epoch 5: Val Loss 1280.96887
Epoch 6: Val Loss 1280.94812
Epoch 7: Val Loss 1280.92700
Epoch 8: Val Loss 1280.90625
Epoch 9: Val Loss 1280.88525
Epoch 10: Val Loss 1280.86414
Epoch 11: Val Loss 1280.84351
Epoch 12: Val Loss 1280.82324
Epoch 13: Val Loss 1280.80298
Epoch 14: Val Loss 1280.78210
Epoch 15: Val Loss 1280.76184
Epoch 16: Val Loss 1280.74109
Epoch 17: Val Loss 1280.71960
Epoch 18: Val Loss 1280.69897
Epoch 19: Val Loss 1280.67786
Epoch 20: Val Loss 1280.65735
Epoch 21: Val Loss 1280.63635
Epoch 22: Val Loss 1280.61536
Epoch 23: Val Loss 1280.59485
Epoch 24: Val Loss 1280.57336
Epoch 25: Val Loss 1280.55200
Epoch 26: Val Loss 1280.53052
Epoch 27: Val Loss 1280.50891
Epoch 28: Val Loss 1280.48706
Epoch 29: Val Loss 1280.46484
Epoch 30: Val Loss 1280.44348
Epoch 31: Val Loss 1280.42297
Epoch 32: Val Loss 1280.40137
Epoch 33: Val Loss 1280.38025
Epoch 34: Val Loss 1280.35864
Epoch 35: Val Loss 1280.33765
Epoch 36: Val Loss 1280.31653
Epoch 37: Val Loss 1280.29395
Epoch 38: Val Loss 1280.27209
Epoch 39: Val Loss 1280.25098
Epoch 40: Val Loss 1280.22925
Epoch 41: Val Loss 1280.20667
Epoch 42: Val Loss 1280.18506
Epoch 43: Val Loss 1280.16382
Epoch 44: Val Loss 1280.14172
Epoch 45: Val Loss 1280.12012
Epoch 46: Val Loss 1280.09863
Epoch 47: Val Loss 1280.07629
Epoch 48: Val Loss 1280.05383
Epoch 49: Val Loss 1280.03076
Epoch 50: Val Loss 1280.00806
Epoch 51: Val Loss 1279.98608
Epoch 52: Val Loss 1279.96448
Epoch 53: Val Loss 1279.94287
Epoch 54: Val Loss 1279.92139
Epoch 55: Val Loss 1279.90002
Epoch 56: Val Loss 1279.87817
Epoch 57: Val Loss 1279.85559
Epoch 58: Val Loss 1279.83362
Epoch 59: Val Loss 1279.81140
Epoch 60: Val Loss 1279.78857
Epoch 61: Val Loss 1279.76538
Epoch 62: Val Loss 1279.74329
Epoch 63: Val Loss 1279.72083
Epoch 64: Val Loss 1279.69885
Epoch 65: Val Loss 1279.67676
Epoch 66: Val Loss 1279.65454
Epoch 67: Val Loss 1279.63245
Epoch 68: Val Loss 1279.60962
Epoch 69: Val Loss 1279.58752
Epoch 70: Val Loss 1279.56543
Epoch 71: Val Loss 1279.54297
Epoch 72: Val Loss 1279.51990
Epoch 73: Val Loss 1279.49634
Epoch 74: Val Loss 1279.47314
Epoch 75: Val Loss 1279.44983
Epoch 76: Val Loss 1279.42639
Epoch 77: Val Loss 1279.40356
Epoch 78: Val Loss 1279.38013
Epoch 79: Val Loss 1279.35669
Epoch 80: Val Loss 1279.33374
Epoch 81: Val Loss 1279.31128
Epoch 82: Val Loss 1279.28857
Epoch 83: Val Loss 1279.26599
Epoch 84: Val Loss 1279.24329
Epoch 85: Val Loss 1279.22009
Epoch 86: Val Loss 1279.19666
Epoch 87: Val Loss 1279.17297
Epoch 88: Val Loss 1279.14880
Epoch 89: Val Loss 1279.12524
Epoch 90: Val Loss 1279.10156
Epoch 91: Val Loss 1279.07739
Epoch 92: Val Loss 1279.05359
Epoch 93: Val Loss 1279.02942
Epoch 94: Val Loss 1279.00427
Epoch 95: Val Loss 1278.98010
Epoch 96: Val Loss 1278.95532
Epoch 97: Val Loss 1278.93115
Epoch 98: Val Loss 1278.90735
Epoch 99: Val Loss 1278.88257
{'MSE - mean': 1278.8824935998346, 'MSE - std': 0.0, 'R2 - mean': -0.24922768602140932, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1305.23682
Epoch 1: Val Loss 1305.21338
Epoch 2: Val Loss 1305.18872
Epoch 3: Val Loss 1305.16418
Epoch 4: Val Loss 1305.14075
Epoch 5: Val Loss 1305.11682
Epoch 6: Val Loss 1305.09326
Epoch 7: Val Loss 1305.06897
Epoch 8: Val Loss 1305.04529
Epoch 9: Val Loss 1305.01965
Epoch 10: Val Loss 1304.99500
Epoch 11: Val Loss 1304.97034
Epoch 12: Val Loss 1304.94666
Epoch 13: Val Loss 1304.92249
Epoch 14: Val Loss 1304.89832
Epoch 15: Val Loss 1304.87415
Epoch 16: Val Loss 1304.85071
Epoch 17: Val Loss 1304.82788
Epoch 18: Val Loss 1304.80518
Epoch 19: Val Loss 1304.78223
Epoch 20: Val Loss 1304.75842
Epoch 21: Val Loss 1304.73376
Epoch 22: Val Loss 1304.70923
Epoch 23: Val Loss 1304.68542
Epoch 24: Val Loss 1304.66125
Epoch 25: Val Loss 1304.63721
Epoch 26: Val Loss 1304.61279
Epoch 27: Val Loss 1304.58850
Epoch 28: Val Loss 1304.56360
Epoch 29: Val Loss 1304.53992
Epoch 30: Val Loss 1304.51404
Epoch 31: Val Loss 1304.48926
Epoch 32: Val Loss 1304.46399
Epoch 33: Val Loss 1304.43884
Epoch 34: Val Loss 1304.41357
Epoch 35: Val Loss 1304.38867
Epoch 36: Val Loss 1304.36182
Epoch 37: Val Loss 1304.33545
Epoch 38: Val Loss 1304.30859
Epoch 39: Val Loss 1304.28198
Epoch 40: Val Loss 1304.25537
Epoch 41: Val Loss 1304.22839
Epoch 42: Val Loss 1304.20129
Epoch 43: Val Loss 1304.17407
Epoch 44: Val Loss 1304.14722
Epoch 45: Val Loss 1304.12158
Epoch 46: Val Loss 1304.09583
Epoch 47: Val Loss 1304.06946
Epoch 48: Val Loss 1304.04297
Epoch 49: Val Loss 1304.01526
Epoch 50: Val Loss 1303.98804
Epoch 51: Val Loss 1303.96082
Epoch 52: Val Loss 1303.93262
Epoch 53: Val Loss 1303.90417
Epoch 54: Val Loss 1303.87634
Epoch 55: Val Loss 1303.84790
Epoch 56: Val Loss 1303.82031
Epoch 57: Val Loss 1303.79211
Epoch 58: Val Loss 1303.76331
Epoch 59: Val Loss 1303.73254
Epoch 60: Val Loss 1303.70056
Epoch 61: Val Loss 1303.66980
Epoch 62: Val Loss 1303.63965
Epoch 63: Val Loss 1303.60962
Epoch 64: Val Loss 1303.58057
Epoch 65: Val Loss 1303.55212
Epoch 66: Val Loss 1303.52344
Epoch 67: Val Loss 1303.49377
Epoch 68: Val Loss 1303.46533
Epoch 69: Val Loss 1303.43713
Epoch 70: Val Loss 1303.40967
Epoch 71: Val Loss 1303.38135
Epoch 72: Val Loss 1303.35254
Epoch 73: Val Loss 1303.32397
Epoch 74: Val Loss 1303.29492
Epoch 75: Val Loss 1303.26379
Epoch 76: Val Loss 1303.23206
Epoch 77: Val Loss 1303.20117
Epoch 78: Val Loss 1303.17029
Epoch 79: Val Loss 1303.13928
Epoch 80: Val Loss 1303.11047
Epoch 81: Val Loss 1303.08093
Epoch 82: Val Loss 1303.05115
Epoch 83: Val Loss 1303.02271
Epoch 84: Val Loss 1302.99329
Epoch 85: Val Loss 1302.96326
Epoch 86: Val Loss 1302.93237
Epoch 87: Val Loss 1302.90198
Epoch 88: Val Loss 1302.87195
Epoch 89: Val Loss 1302.84131
Epoch 90: Val Loss 1302.80994
Epoch 91: Val Loss 1302.77979
Epoch 92: Val Loss 1302.74780
Epoch 93: Val Loss 1302.71729
Epoch 94: Val Loss 1302.68665
Epoch 95: Val Loss 1302.65613
Epoch 96: Val Loss 1302.62488
Epoch 97: Val Loss 1302.59399
Epoch 98: Val Loss 1302.56274
Epoch 99: Val Loss 1302.53247
{'MSE - mean': 1290.7074362653352, 'MSE - std': 11.824942665500544, 'R2 - mean': -0.24482195496834946, 'R2 - std': 0.004405731053059858} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2822.14429
Epoch 1: Val Loss 2822.12061
Epoch 2: Val Loss 2822.09741
Epoch 3: Val Loss 2822.07471
Epoch 4: Val Loss 2822.05078
Epoch 5: Val Loss 2822.02832
Epoch 6: Val Loss 2822.00537
Epoch 7: Val Loss 2821.98291
Epoch 8: Val Loss 2821.95972
Epoch 9: Val Loss 2821.93726
Epoch 10: Val Loss 2821.91553
Epoch 11: Val Loss 2821.89331
Epoch 12: Val Loss 2821.87231
Epoch 13: Val Loss 2821.85010
Epoch 14: Val Loss 2821.82788
Epoch 15: Val Loss 2821.80591
Epoch 16: Val Loss 2821.78345
Epoch 17: Val Loss 2821.76099
Epoch 18: Val Loss 2821.73755
Epoch 19: Val Loss 2821.71509
Epoch 20: Val Loss 2821.69287
Epoch 21: Val Loss 2821.67163
Epoch 22: Val Loss 2821.64844
Epoch 23: Val Loss 2821.62573
Epoch 24: Val Loss 2821.60400
Epoch 25: Val Loss 2821.58105
Epoch 26: Val Loss 2821.55737
Epoch 27: Val Loss 2821.53564
Epoch 28: Val Loss 2821.51294
Epoch 29: Val Loss 2821.49023
Epoch 30: Val Loss 2821.46851
Epoch 31: Val Loss 2821.44604
Epoch 32: Val Loss 2821.42432
Epoch 33: Val Loss 2821.40210
Epoch 34: Val Loss 2821.37988
Epoch 35: Val Loss 2821.35815
Epoch 36: Val Loss 2821.33569
Epoch 37: Val Loss 2821.31445
Epoch 38: Val Loss 2821.29224
Epoch 39: Val Loss 2821.27002
Epoch 40: Val Loss 2821.24731
Epoch 41: Val Loss 2821.22461
Epoch 42: Val Loss 2821.20215
Epoch 43: Val Loss 2821.17920
Epoch 44: Val Loss 2821.15698
Epoch 45: Val Loss 2821.13525
Epoch 46: Val Loss 2821.11401
Epoch 47: Val Loss 2821.09082
Epoch 48: Val Loss 2821.06860
Epoch 49: Val Loss 2821.04712
Epoch 50: Val Loss 2821.02563
Epoch 51: Val Loss 2821.00391
Epoch 52: Val Loss 2820.98218
Epoch 53: Val Loss 2820.96045
Epoch 54: Val Loss 2820.93823
Epoch 55: Val Loss 2820.91577
Epoch 56: Val Loss 2820.89404
Epoch 57: Val Loss 2820.87183
Epoch 58: Val Loss 2820.84937
Epoch 59: Val Loss 2820.82764
Epoch 60: Val Loss 2820.80688
Epoch 61: Val Loss 2820.78564
Epoch 62: Val Loss 2820.76343
Epoch 63: Val Loss 2820.74292
Epoch 64: Val Loss 2820.72070
Epoch 65: Val Loss 2820.70020
Epoch 66: Val Loss 2820.67847
Epoch 67: Val Loss 2820.65601
Epoch 68: Val Loss 2820.63428
Epoch 69: Val Loss 2820.61255
Epoch 70: Val Loss 2820.59155
Epoch 71: Val Loss 2820.57031
Epoch 72: Val Loss 2820.54810
Epoch 73: Val Loss 2820.52661
Epoch 74: Val Loss 2820.50586
Epoch 75: Val Loss 2820.48438
Epoch 76: Val Loss 2820.46289
Epoch 77: Val Loss 2820.44214
Epoch 78: Val Loss 2820.41943
Epoch 79: Val Loss 2820.39795
Epoch 80: Val Loss 2820.37646
Epoch 81: Val Loss 2820.35449
Epoch 82: Val Loss 2820.33228
Epoch 83: Val Loss 2820.31055
Epoch 84: Val Loss 2820.28955
Epoch 85: Val Loss 2820.26758
Epoch 86: Val Loss 2820.24609
Epoch 87: Val Loss 2820.22461
Epoch 88: Val Loss 2820.20190
Epoch 89: Val Loss 2820.18042
Epoch 90: Val Loss 2820.15918
Epoch 91: Val Loss 2820.13721
Epoch 92: Val Loss 2820.11548
Epoch 93: Val Loss 2820.09277
Epoch 94: Val Loss 2820.07202
Epoch 95: Val Loss 2820.05078
Epoch 96: Val Loss 2820.02979
Epoch 97: Val Loss 2820.00806
Epoch 98: Val Loss 2819.98584
Epoch 99: Val Loss 2819.96338
{'MSE - mean': 1800.4594595247527, 'MSE - std': 720.9628769547255, 'R2 - mean': -0.21355851039927654, 'R2 - std': 0.0443592858755442} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2113.47021
Epoch 1: Val Loss 2113.43945
Epoch 2: Val Loss 2113.40845
Epoch 3: Val Loss 2113.37695
Epoch 4: Val Loss 2113.34570
Epoch 5: Val Loss 2113.31494
Epoch 6: Val Loss 2113.28540
Epoch 7: Val Loss 2113.25415
Epoch 8: Val Loss 2113.22437
Epoch 9: Val Loss 2113.19238
Epoch 10: Val Loss 2113.16260
Epoch 11: Val Loss 2113.13306
Epoch 12: Val Loss 2113.10205
Epoch 13: Val Loss 2113.07007
Epoch 14: Val Loss 2113.03979
Epoch 15: Val Loss 2113.00708
Epoch 16: Val Loss 2112.97485
Epoch 17: Val Loss 2112.94385
Epoch 18: Val Loss 2112.91162
Epoch 19: Val Loss 2112.87866
Epoch 20: Val Loss 2112.84473
Epoch 21: Val Loss 2112.81104
Epoch 22: Val Loss 2112.77686
Epoch 23: Val Loss 2112.74390
Epoch 24: Val Loss 2112.71021
Epoch 25: Val Loss 2112.67651
Epoch 26: Val Loss 2112.64355
Epoch 27: Val Loss 2112.60889
Epoch 28: Val Loss 2112.57446
Epoch 29: Val Loss 2112.54077
Epoch 30: Val Loss 2112.50635
Epoch 31: Val Loss 2112.47046
Epoch 32: Val Loss 2112.43384
Epoch 33: Val Loss 2112.39746
Epoch 34: Val Loss 2112.36108
Epoch 35: Val Loss 2112.32520
Epoch 36: Val Loss 2112.28906
Epoch 37: Val Loss 2112.25073
Epoch 38: Val Loss 2112.21167
Epoch 39: Val Loss 2112.17407
Epoch 40: Val Loss 2112.13721
Epoch 41: Val Loss 2112.09863
Epoch 42: Val Loss 2112.05859
Epoch 43: Val Loss 2112.01953
Epoch 44: Val Loss 2111.98071
Epoch 45: Val Loss 2111.94263
Epoch 46: Val Loss 2111.90503
Epoch 47: Val Loss 2111.86841
Epoch 48: Val Loss 2111.82910
Epoch 49: Val Loss 2111.79053
Epoch 50: Val Loss 2111.75171
Epoch 51: Val Loss 2111.71313
Epoch 52: Val Loss 2111.67383
Epoch 53: Val Loss 2111.63501
Epoch 54: Val Loss 2111.59595
Epoch 55: Val Loss 2111.55688
Epoch 56: Val Loss 2111.51685
Epoch 57: Val Loss 2111.47559
Epoch 58: Val Loss 2111.43384
Epoch 59: Val Loss 2111.39429
Epoch 60: Val Loss 2111.35522
Epoch 61: Val Loss 2111.31641
Epoch 62: Val Loss 2111.27563
Epoch 63: Val Loss 2111.23511
Epoch 64: Val Loss 2111.19702
Epoch 65: Val Loss 2111.15674
Epoch 66: Val Loss 2111.11377
Epoch 67: Val Loss 2111.07227
Epoch 68: Val Loss 2111.03125
Epoch 69: Val Loss 2110.98828
Epoch 70: Val Loss 2110.94849
Epoch 71: Val Loss 2110.90210
Epoch 72: Val Loss 2110.85864
Epoch 73: Val Loss 2110.81494
Epoch 74: Val Loss 2110.77271
Epoch 75: Val Loss 2110.72754
Epoch 76: Val Loss 2110.68213
Epoch 77: Val Loss 2110.63989
Epoch 78: Val Loss 2110.59595
Epoch 79: Val Loss 2110.55200
Epoch 80: Val Loss 2110.50513
Epoch 81: Val Loss 2110.46240
Epoch 82: Val Loss 2110.41846
Epoch 83: Val Loss 2110.37183
Epoch 84: Val Loss 2110.32739
Epoch 85: Val Loss 2110.28418
Epoch 86: Val Loss 2110.23828
Epoch 87: Val Loss 2110.19312
Epoch 88: Val Loss 2110.14941
Epoch 89: Val Loss 2110.10620
Epoch 90: Val Loss 2110.05981
Epoch 91: Val Loss 2110.01538
Epoch 92: Val Loss 2109.96582
Epoch 93: Val Loss 2109.91846
Epoch 94: Val Loss 2109.87329
Epoch 95: Val Loss 2109.82666
Epoch 96: Val Loss 2109.77734
Epoch 97: Val Loss 2109.72778
Epoch 98: Val Loss 2109.68018
Epoch 99: Val Loss 2109.63208
{'MSE - mean': 1877.7526167144974, 'MSE - std': 638.5634650566863, 'R2 - mean': -0.21619708440698804, 'R2 - std': 0.03868715421071578} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2525.59717
Epoch 1: Val Loss 2525.58398
Epoch 2: Val Loss 2525.57031
Epoch 3: Val Loss 2525.55737
Epoch 4: Val Loss 2525.54468
Epoch 5: Val Loss 2525.53101
Epoch 6: Val Loss 2525.51782
Epoch 7: Val Loss 2525.50464
Epoch 8: Val Loss 2525.49170
Epoch 9: Val Loss 2525.47803
Epoch 10: Val Loss 2525.46484
Epoch 11: Val Loss 2525.45215
Epoch 12: Val Loss 2525.43823
Epoch 13: Val Loss 2525.42480
Epoch 14: Val Loss 2525.41064
Epoch 15: Val Loss 2525.39673
Epoch 16: Val Loss 2525.38330
Epoch 17: Val Loss 2525.37012
Epoch 18: Val Loss 2525.35645
Epoch 19: Val Loss 2525.34253
Epoch 20: Val Loss 2525.32910
Epoch 21: Val Loss 2525.31543
Epoch 22: Val Loss 2525.30176
Epoch 23: Val Loss 2525.28784
Epoch 24: Val Loss 2525.27393
Epoch 25: Val Loss 2525.25977
Epoch 26: Val Loss 2525.24585
Epoch 27: Val Loss 2525.23193
Epoch 28: Val Loss 2525.21802
Epoch 29: Val Loss 2525.20410
Epoch 30: Val Loss 2525.19019
Epoch 31: Val Loss 2525.17578
Epoch 32: Val Loss 2525.16162
Epoch 33: Val Loss 2525.14673
Epoch 34: Val Loss 2525.13232
Epoch 35: Val Loss 2525.11768
Epoch 36: Val Loss 2525.10327
Epoch 37: Val Loss 2525.08813
Epoch 38: Val Loss 2525.07397
Epoch 39: Val Loss 2525.05908
Epoch 40: Val Loss 2525.04419
Epoch 41: Val Loss 2525.02930
Epoch 42: Val Loss 2525.01416
Epoch 43: Val Loss 2524.99902
Epoch 44: Val Loss 2524.98364
Epoch 45: Val Loss 2524.96851
Epoch 46: Val Loss 2524.95337
Epoch 47: Val Loss 2524.93823
Epoch 48: Val Loss 2524.92310
Epoch 49: Val Loss 2524.90698
Epoch 50: Val Loss 2524.89185
Epoch 51: Val Loss 2524.87646
Epoch 52: Val Loss 2524.86060
Epoch 53: Val Loss 2524.84399
Epoch 54: Val Loss 2524.82837
Epoch 55: Val Loss 2524.81226
Epoch 56: Val Loss 2524.79590
Epoch 57: Val Loss 2524.78003
Epoch 58: Val Loss 2524.76440
Epoch 59: Val Loss 2524.74854
Epoch 60: Val Loss 2524.73242
Epoch 61: Val Loss 2524.71655
Epoch 62: Val Loss 2524.69971
Epoch 63: Val Loss 2524.68457
Epoch 64: Val Loss 2524.66821
Epoch 65: Val Loss 2524.65210
Epoch 66: Val Loss 2524.63574
Epoch 67: Val Loss 2524.61938
Epoch 68: Val Loss 2524.60303
Epoch 69: Val Loss 2524.58667
Epoch 70: Val Loss 2524.57007
Epoch 71: Val Loss 2524.55420
Epoch 72: Val Loss 2524.53809
Epoch 73: Val Loss 2524.52222
Epoch 74: Val Loss 2524.50562
Epoch 75: Val Loss 2524.48926
Epoch 76: Val Loss 2524.47290
Epoch 77: Val Loss 2524.45679
Epoch 78: Val Loss 2524.44165
Epoch 79: Val Loss 2524.42529
Epoch 80: Val Loss 2524.40845
Epoch 81: Val Loss 2524.39185
Epoch 82: Val Loss 2524.37427
Epoch 83: Val Loss 2524.35669
Epoch 84: Val Loss 2524.33862
Epoch 85: Val Loss 2524.32202
Epoch 86: Val Loss 2524.30469
Epoch 87: Val Loss 2524.28735
Epoch 88: Val Loss 2524.27051
Epoch 89: Val Loss 2524.25317
Epoch 90: Val Loss 2524.23706
Epoch 91: Val Loss 2524.21997
Epoch 92: Val Loss 2524.20215
Epoch 93: Val Loss 2524.18359
Epoch 94: Val Loss 2524.16553
Epoch 95: Val Loss 2524.14697
Epoch 96: Val Loss 2524.12891
Epoch 97: Val Loss 2524.11108
Epoch 98: Val Loss 2524.09302
Epoch 99: Val Loss 2524.07495
{'MSE - mean': 2007.0170688464727, 'MSE - std': 626.935270553572, 'R2 - mean': -0.20895408694088374, 'R2 - std': 0.037512674790195065} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 2007.0170688464727 and parameters: {'dim': 128, 'depth': 1, 'heads': 4, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0.1}. Best is trial 11 with value: 2007.0170688464727.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1288.95288
Epoch 1: Val Loss 1286.21094
Epoch 2: Val Loss 1282.94348
Epoch 3: Val Loss 1278.73645
Epoch 4: Val Loss 1272.73547
Epoch 5: Val Loss 1264.14746
Epoch 6: Val Loss 1252.54285
Epoch 7: Val Loss 1236.67749
Epoch 8: Val Loss 1215.99243
Epoch 9: Val Loss 1189.03186
Epoch 10: Val Loss 1155.70129
Epoch 11: Val Loss 1115.05554
Epoch 12: Val Loss 1066.52576
Epoch 13: Val Loss 1010.15387
Epoch 14: Val Loss 947.53235
Epoch 15: Val Loss 883.31897
Epoch 16: Val Loss 816.31018
Epoch 17: Val Loss 753.91553
Epoch 18: Val Loss 700.10223
Epoch 19: Val Loss 659.87518
Epoch 20: Val Loss 633.51398
Epoch 21: Val Loss 611.08722
Epoch 22: Val Loss 586.88336
Epoch 23: Val Loss 558.69226
Epoch 24: Val Loss 529.10266
Epoch 25: Val Loss 503.30856
Epoch 26: Val Loss 477.33142
Epoch 27: Val Loss 451.91531
Epoch 28: Val Loss 429.77930
Epoch 29: Val Loss 405.63208
Epoch 30: Val Loss 386.26770
Epoch 31: Val Loss 367.74426
Epoch 32: Val Loss 351.40823
Epoch 33: Val Loss 338.44135
Epoch 34: Val Loss 326.08395
Epoch 35: Val Loss 314.82187
Epoch 36: Val Loss 304.07962
Epoch 37: Val Loss 296.15646
Epoch 38: Val Loss 293.30450
Epoch 39: Val Loss 284.05563
Epoch 40: Val Loss 276.05521
Epoch 41: Val Loss 268.79910
Epoch 42: Val Loss 258.24515
Epoch 43: Val Loss 250.51822
Epoch 44: Val Loss 244.62047
Epoch 45: Val Loss 240.47548
Epoch 46: Val Loss 236.81978
Epoch 47: Val Loss 234.64372
Epoch 48: Val Loss 229.12828
Epoch 49: Val Loss 228.91100
Epoch 50: Val Loss 226.02286
Epoch 51: Val Loss 224.04933
Epoch 52: Val Loss 220.19908
Epoch 53: Val Loss 218.14667
Epoch 54: Val Loss 216.61775
Epoch 55: Val Loss 213.72986
Epoch 56: Val Loss 215.34605
Epoch 57: Val Loss 213.67474
Epoch 58: Val Loss 215.56964
Epoch 59: Val Loss 213.81526
Epoch 60: Val Loss 211.00470
Epoch 61: Val Loss 209.05280
Epoch 62: Val Loss 208.38020
Epoch 63: Val Loss 207.79829
Epoch 64: Val Loss 207.50108
Epoch 65: Val Loss 209.73714
Epoch 66: Val Loss 212.00294
Epoch 67: Val Loss 212.22632
Epoch 68: Val Loss 209.69711
Epoch 69: Val Loss 209.51016
Epoch 70: Val Loss 209.49483
Epoch 71: Val Loss 209.73596
Epoch 72: Val Loss 209.28786
Epoch 73: Val Loss 208.28954
Epoch 74: Val Loss 208.55226
Epoch 75: Val Loss 207.51126
Epoch 76: Val Loss 206.06284
Epoch 77: Val Loss 205.20100
Epoch 78: Val Loss 207.43657
Epoch 79: Val Loss 207.84337
Epoch 80: Val Loss 205.48633
Epoch 81: Val Loss 205.45927
Epoch 82: Val Loss 204.59639
Epoch 83: Val Loss 204.32649
Epoch 84: Val Loss 204.82509
Epoch 85: Val Loss 208.40137
Epoch 86: Val Loss 206.35271
Epoch 87: Val Loss 205.60338
Epoch 88: Val Loss 204.27199
Epoch 89: Val Loss 204.92111
Epoch 90: Val Loss 205.28920
Epoch 91: Val Loss 203.44640
Epoch 92: Val Loss 200.31035
Epoch 93: Val Loss 198.40033
Epoch 94: Val Loss 198.78079
Epoch 95: Val Loss 198.64719
Epoch 96: Val Loss 196.53726
Epoch 97: Val Loss 194.42511
Epoch 98: Val Loss 193.69261
Epoch 99: Val Loss 192.75157
{'MSE - mean': 192.75155921839948, 'MSE - std': 0.0, 'R2 - mean': 0.8117179760420086, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1311.62415
Epoch 1: Val Loss 1307.76550
Epoch 2: Val Loss 1303.42053
Epoch 3: Val Loss 1298.67590
Epoch 4: Val Loss 1293.14160
Epoch 5: Val Loss 1286.21130
Epoch 6: Val Loss 1277.22974
Epoch 7: Val Loss 1265.95459
Epoch 8: Val Loss 1251.34229
Epoch 9: Val Loss 1233.79712
Epoch 10: Val Loss 1211.50781
Epoch 11: Val Loss 1182.42371
Epoch 12: Val Loss 1147.21570
Epoch 13: Val Loss 1107.07263
Epoch 14: Val Loss 1060.51843
Epoch 15: Val Loss 1010.18433
Epoch 16: Val Loss 956.69678
Epoch 17: Val Loss 901.20874
Epoch 18: Val Loss 846.89856
Epoch 19: Val Loss 803.28491
Epoch 20: Val Loss 767.95020
Epoch 21: Val Loss 739.49323
Epoch 22: Val Loss 717.27600
Epoch 23: Val Loss 698.82812
Epoch 24: Val Loss 679.25732
Epoch 25: Val Loss 660.39264
Epoch 26: Val Loss 636.40387
Epoch 27: Val Loss 613.22821
Epoch 28: Val Loss 587.85028
Epoch 29: Val Loss 561.93243
Epoch 30: Val Loss 535.04620
Epoch 31: Val Loss 515.00616
Epoch 32: Val Loss 495.72308
Epoch 33: Val Loss 474.26157
Epoch 34: Val Loss 461.42587
Epoch 35: Val Loss 452.31882
Epoch 36: Val Loss 442.58096
Epoch 37: Val Loss 432.46225
Epoch 38: Val Loss 422.10794
Epoch 39: Val Loss 415.68619
Epoch 40: Val Loss 409.78091
Epoch 41: Val Loss 398.15509
Epoch 42: Val Loss 390.17020
Epoch 43: Val Loss 386.62607
Epoch 44: Val Loss 382.08817
Epoch 45: Val Loss 380.92184
Epoch 46: Val Loss 373.77936
Epoch 47: Val Loss 379.01056
Epoch 48: Val Loss 365.80539
Epoch 49: Val Loss 360.47336
Epoch 50: Val Loss 350.33075
Epoch 51: Val Loss 356.07669
Epoch 52: Val Loss 354.61917
Epoch 53: Val Loss 349.56940
Epoch 54: Val Loss 340.11368
Epoch 55: Val Loss 335.41031
Epoch 56: Val Loss 350.67572
Epoch 57: Val Loss 353.32523
Epoch 58: Val Loss 346.01410
Epoch 59: Val Loss 327.86426
Epoch 60: Val Loss 320.77243
Epoch 61: Val Loss 332.23364
Epoch 62: Val Loss 334.69107
Epoch 63: Val Loss 337.84665
Epoch 64: Val Loss 333.88257
Epoch 65: Val Loss 328.77283
Epoch 66: Val Loss 317.93134
Epoch 67: Val Loss 323.85132
Epoch 68: Val Loss 320.90201
Epoch 69: Val Loss 326.27008
Epoch 70: Val Loss 320.83301
Epoch 71: Val Loss 320.13519
Epoch 72: Val Loss 323.43622
Epoch 73: Val Loss 324.40350
Epoch 74: Val Loss 331.96936
Epoch 75: Val Loss 326.11728
Epoch 76: Val Loss 319.52664
Epoch 77: Val Loss 311.11331
Epoch 78: Val Loss 314.67416
Epoch 79: Val Loss 315.21948
Epoch 80: Val Loss 310.87186
Epoch 81: Val Loss 312.17212
Epoch 82: Val Loss 303.79492
Epoch 83: Val Loss 302.80161
Epoch 84: Val Loss 311.76169
Epoch 85: Val Loss 308.83286
Epoch 86: Val Loss 303.01334
Epoch 87: Val Loss 293.72507
Epoch 88: Val Loss 291.80392
Epoch 89: Val Loss 291.68088
Epoch 90: Val Loss 306.83899
Epoch 91: Val Loss 303.32568
Epoch 92: Val Loss 294.04605
Epoch 93: Val Loss 290.26779
Epoch 94: Val Loss 287.54773
Epoch 95: Val Loss 275.41403
Epoch 96: Val Loss 263.59567
Epoch 97: Val Loss 258.33383
Epoch 98: Val Loss 262.77917
Epoch 99: Val Loss 285.15652
{'MSE - mean': 225.54269817846557, 'MSE - std': 32.79113896006611, 'R2 - mean': 0.7828518797309384, 'R2 - std': 0.028866096311070177} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2805.82739
Epoch 1: Val Loss 2799.73804
Epoch 2: Val Loss 2792.05981
Epoch 3: Val Loss 2781.82178
Epoch 4: Val Loss 2768.19971
Epoch 5: Val Loss 2750.13232
Epoch 6: Val Loss 2726.07031
Epoch 7: Val Loss 2694.71753
Epoch 8: Val Loss 2656.81982
Epoch 9: Val Loss 2606.93408
Epoch 10: Val Loss 2544.07568
Epoch 11: Val Loss 2469.24487
Epoch 12: Val Loss 2378.07861
Epoch 13: Val Loss 2276.86548
Epoch 14: Val Loss 2169.25024
Epoch 15: Val Loss 2061.34229
Epoch 16: Val Loss 1957.10547
Epoch 17: Val Loss 1867.82715
Epoch 18: Val Loss 1788.31921
Epoch 19: Val Loss 1719.27539
Epoch 20: Val Loss 1661.79456
Epoch 21: Val Loss 1605.95630
Epoch 22: Val Loss 1554.26086
Epoch 23: Val Loss 1501.86682
Epoch 24: Val Loss 1443.31140
Epoch 25: Val Loss 1395.66528
Epoch 26: Val Loss 1347.21216
Epoch 27: Val Loss 1305.13416
Epoch 28: Val Loss 1269.50195
Epoch 29: Val Loss 1234.10327
Epoch 30: Val Loss 1191.39050
Epoch 31: Val Loss 1144.09875
Epoch 32: Val Loss 1109.32471
Epoch 33: Val Loss 1079.80481
Epoch 34: Val Loss 1054.25098
Epoch 35: Val Loss 1022.64539
Epoch 36: Val Loss 995.81067
Epoch 37: Val Loss 969.38934
Epoch 38: Val Loss 945.93427
Epoch 39: Val Loss 917.71320
Epoch 40: Val Loss 891.91949
Epoch 41: Val Loss 867.36499
Epoch 42: Val Loss 846.44763
Epoch 43: Val Loss 825.13599
Epoch 44: Val Loss 810.13556
Epoch 45: Val Loss 789.70117
Epoch 46: Val Loss 775.98132
Epoch 47: Val Loss 766.26910
Epoch 48: Val Loss 753.03979
Epoch 49: Val Loss 737.79633
Epoch 50: Val Loss 714.69855
Epoch 51: Val Loss 692.16418
Epoch 52: Val Loss 681.46387
Epoch 53: Val Loss 667.07849
Epoch 54: Val Loss 655.59308
Epoch 55: Val Loss 655.07391
Epoch 56: Val Loss 646.65558
Epoch 57: Val Loss 639.58911
Epoch 58: Val Loss 620.53058
Epoch 59: Val Loss 599.87213
Epoch 60: Val Loss 594.27759
Epoch 61: Val Loss 593.26959
Epoch 62: Val Loss 582.08752
Epoch 63: Val Loss 578.38794
Epoch 64: Val Loss 558.44818
Epoch 65: Val Loss 552.94287
Epoch 66: Val Loss 551.29456
Epoch 67: Val Loss 551.82434
Epoch 68: Val Loss 547.99890
Epoch 69: Val Loss 546.82227
Epoch 70: Val Loss 535.93469
Epoch 71: Val Loss 529.92767
Epoch 72: Val Loss 520.56189
Epoch 73: Val Loss 518.96313
Epoch 74: Val Loss 521.56116
Epoch 75: Val Loss 517.32227
Epoch 76: Val Loss 509.19418
Epoch 77: Val Loss 503.34354
Epoch 78: Val Loss 493.28809
Epoch 79: Val Loss 481.22186
Epoch 80: Val Loss 479.11893
Epoch 81: Val Loss 478.52646
Epoch 82: Val Loss 481.63730
Epoch 83: Val Loss 473.95999
Epoch 84: Val Loss 469.87143
Epoch 85: Val Loss 476.67209
Epoch 86: Val Loss 473.16125
Epoch 87: Val Loss 463.92896
Epoch 88: Val Loss 460.79276
Epoch 89: Val Loss 453.27014
Epoch 90: Val Loss 446.50702
Epoch 91: Val Loss 446.72723
Epoch 92: Val Loss 440.18817
Epoch 93: Val Loss 424.24323
Epoch 94: Val Loss 413.30841
Epoch 95: Val Loss 412.09619
Epoch 96: Val Loss 417.91519
Epoch 97: Val Loss 420.61990
Epoch 98: Val Loss 422.12088
Epoch 99: Val Loss 415.52652
{'MSE - mean': 287.7271906777183, 'MSE - std': 91.92747908529935, 'R2 - mean': 0.7991658077971252, 'R2 - std': 0.03298165412352897} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2132.41162
Epoch 1: Val Loss 2127.64160
Epoch 2: Val Loss 2122.09937
Epoch 3: Val Loss 2115.07202
Epoch 4: Val Loss 2105.76489
Epoch 5: Val Loss 2092.76270
Epoch 6: Val Loss 2075.57520
Epoch 7: Val Loss 2052.57422
Epoch 8: Val Loss 2023.52429
Epoch 9: Val Loss 1985.37915
Epoch 10: Val Loss 1938.90503
Epoch 11: Val Loss 1881.55432
Epoch 12: Val Loss 1815.84521
Epoch 13: Val Loss 1743.95239
Epoch 14: Val Loss 1664.31824
Epoch 15: Val Loss 1580.01379
Epoch 16: Val Loss 1498.77539
Epoch 17: Val Loss 1417.29980
Epoch 18: Val Loss 1340.27173
Epoch 19: Val Loss 1278.77490
Epoch 20: Val Loss 1225.50671
Epoch 21: Val Loss 1181.14197
Epoch 22: Val Loss 1138.22510
Epoch 23: Val Loss 1098.95447
Epoch 24: Val Loss 1061.29504
Epoch 25: Val Loss 1025.33679
Epoch 26: Val Loss 988.00964
Epoch 27: Val Loss 946.52136
Epoch 28: Val Loss 905.86975
Epoch 29: Val Loss 867.78003
Epoch 30: Val Loss 832.07831
Epoch 31: Val Loss 797.50500
Epoch 32: Val Loss 765.38580
Epoch 33: Val Loss 732.49402
Epoch 34: Val Loss 702.07208
Epoch 35: Val Loss 674.36920
Epoch 36: Val Loss 649.36987
Epoch 37: Val Loss 624.87750
Epoch 38: Val Loss 603.74799
Epoch 39: Val Loss 586.14056
Epoch 40: Val Loss 569.89514
Epoch 41: Val Loss 555.78040
Epoch 42: Val Loss 542.21405
Epoch 43: Val Loss 530.95398
Epoch 44: Val Loss 521.74939
Epoch 45: Val Loss 511.80212
Epoch 46: Val Loss 500.98560
Epoch 47: Val Loss 491.78186
Epoch 48: Val Loss 485.34323
Epoch 49: Val Loss 477.75745
Epoch 50: Val Loss 471.50937
Epoch 51: Val Loss 463.08707
Epoch 52: Val Loss 454.69650
Epoch 53: Val Loss 446.18478
Epoch 54: Val Loss 440.76013
Epoch 55: Val Loss 435.92831
Epoch 56: Val Loss 431.47916
Epoch 57: Val Loss 428.11655
Epoch 58: Val Loss 425.62170
Epoch 59: Val Loss 427.02362
Epoch 60: Val Loss 421.53244
Epoch 61: Val Loss 412.16974
Epoch 62: Val Loss 405.01807
Epoch 63: Val Loss 403.96524
Epoch 64: Val Loss 404.86292
Epoch 65: Val Loss 400.52625
Epoch 66: Val Loss 393.53217
Epoch 67: Val Loss 392.26559
Epoch 68: Val Loss 386.66464
Epoch 69: Val Loss 388.42728
Epoch 70: Val Loss 389.63150
Epoch 71: Val Loss 388.33740
Epoch 72: Val Loss 382.74991
Epoch 73: Val Loss 376.78424
Epoch 74: Val Loss 373.09912
Epoch 75: Val Loss 368.45074
Epoch 76: Val Loss 368.68784
Epoch 77: Val Loss 367.73053
Epoch 78: Val Loss 365.57349
Epoch 79: Val Loss 371.46854
Epoch 80: Val Loss 365.66782
Epoch 81: Val Loss 359.37836
Epoch 82: Val Loss 353.93445
Epoch 83: Val Loss 352.66330
Epoch 84: Val Loss 354.28503
Epoch 85: Val Loss 355.15372
Epoch 86: Val Loss 351.18872
Epoch 87: Val Loss 345.54599
Epoch 88: Val Loss 344.37039
Epoch 89: Val Loss 341.97781
Epoch 90: Val Loss 345.71060
Epoch 91: Val Loss 347.93192
Epoch 92: Val Loss 341.97882
Epoch 93: Val Loss 336.02261
Epoch 94: Val Loss 329.29993
Epoch 95: Val Loss 326.72162
Epoch 96: Val Loss 329.06519
Epoch 97: Val Loss 333.34430
Epoch 98: Val Loss 330.20676
Epoch 99: Val Loss 329.49805
{'MSE - mean': 297.4758011263474, 'MSE - std': 81.38244451520842, 'R2 - mean': 0.8019793459942077, 'R2 - std': 0.028975681581326053} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2531.64355
Epoch 1: Val Loss 2529.84424
Epoch 2: Val Loss 2528.11694
Epoch 3: Val Loss 2526.16968
Epoch 4: Val Loss 2523.76050
Epoch 5: Val Loss 2520.83911
Epoch 6: Val Loss 2517.27026
Epoch 7: Val Loss 2513.00732
Epoch 8: Val Loss 2507.86694
Epoch 9: Val Loss 2501.43896
Epoch 10: Val Loss 2493.02002
Epoch 11: Val Loss 2481.73657
Epoch 12: Val Loss 2467.75171
Epoch 13: Val Loss 2450.66089
Epoch 14: Val Loss 2429.76611
Epoch 15: Val Loss 2404.74780
Epoch 16: Val Loss 2374.39795
Epoch 17: Val Loss 2338.41553
Epoch 18: Val Loss 2296.44775
Epoch 19: Val Loss 2246.73438
Epoch 20: Val Loss 2193.26099
Epoch 21: Val Loss 2132.26733
Epoch 22: Val Loss 2070.22314
Epoch 23: Val Loss 2003.81250
Epoch 24: Val Loss 1940.51294
Epoch 25: Val Loss 1876.23218
Epoch 26: Val Loss 1815.16748
Epoch 27: Val Loss 1760.08215
Epoch 28: Val Loss 1711.32581
Epoch 29: Val Loss 1665.57983
Epoch 30: Val Loss 1625.66370
Epoch 31: Val Loss 1587.32373
Epoch 32: Val Loss 1546.94214
Epoch 33: Val Loss 1507.27026
Epoch 34: Val Loss 1465.04431
Epoch 35: Val Loss 1423.38232
Epoch 36: Val Loss 1385.10376
Epoch 37: Val Loss 1346.91785
Epoch 38: Val Loss 1306.10901
Epoch 39: Val Loss 1269.05505
Epoch 40: Val Loss 1225.35461
Epoch 41: Val Loss 1183.10901
Epoch 42: Val Loss 1143.47888
Epoch 43: Val Loss 1113.60876
Epoch 44: Val Loss 1080.96167
Epoch 45: Val Loss 1051.17395
Epoch 46: Val Loss 1023.89233
Epoch 47: Val Loss 999.20587
Epoch 48: Val Loss 975.27625
Epoch 49: Val Loss 957.08160
Epoch 50: Val Loss 928.99188
Epoch 51: Val Loss 901.80585
Epoch 52: Val Loss 884.03223
Epoch 53: Val Loss 866.48248
Epoch 54: Val Loss 854.16461
Epoch 55: Val Loss 836.88458
Epoch 56: Val Loss 819.07672
Epoch 57: Val Loss 804.26520
Epoch 58: Val Loss 788.65723
Epoch 59: Val Loss 775.39941
Epoch 60: Val Loss 757.79816
Epoch 61: Val Loss 739.30670
Epoch 62: Val Loss 728.78558
Epoch 63: Val Loss 713.48114
Epoch 64: Val Loss 705.79004
Epoch 65: Val Loss 705.68042
Epoch 66: Val Loss 697.31226
Epoch 67: Val Loss 685.20496
Epoch 68: Val Loss 661.82336
Epoch 69: Val Loss 655.93359
Epoch 70: Val Loss 649.72900
Epoch 71: Val Loss 644.29150
Epoch 72: Val Loss 641.35876
Epoch 73: Val Loss 632.48010
Epoch 74: Val Loss 618.19336
Epoch 75: Val Loss 611.88361
Epoch 76: Val Loss 612.10486
Epoch 77: Val Loss 609.47913
Epoch 78: Val Loss 603.26544
Epoch 79: Val Loss 592.56653
Epoch 80: Val Loss 586.87830
Epoch 81: Val Loss 583.47046
Epoch 82: Val Loss 584.38818
Epoch 83: Val Loss 579.25513
Epoch 84: Val Loss 575.93134
Epoch 85: Val Loss 571.37140
Epoch 86: Val Loss 564.61469
Epoch 87: Val Loss 559.49213
Epoch 88: Val Loss 555.80225
Epoch 89: Val Loss 554.58838
Epoch 90: Val Loss 550.04126
Epoch 91: Val Loss 548.15851
Epoch 92: Val Loss 545.32520
Epoch 93: Val Loss 535.12262
Epoch 94: Val Loss 529.34338
Epoch 95: Val Loss 529.20081
Epoch 96: Val Loss 529.38525
Epoch 97: Val Loss 528.70490
Epoch 98: Val Loss 521.94116
Epoch 99: Val Loss 519.10712
{'MSE - mean': 341.80207815824525, 'MSE - std': 114.70726725997356, 'R2 - mean': 0.7930478971442422, 'R2 - std': 0.031476264275166525} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 341.80207815824525 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 12 with value: 341.80207815824525.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1288.97156
Epoch 1: Val Loss 1285.58813
Epoch 2: Val Loss 1281.64404
Epoch 3: Val Loss 1276.93481
Epoch 4: Val Loss 1271.42761
Epoch 5: Val Loss 1265.30103
Epoch 6: Val Loss 1258.13672
Epoch 7: Val Loss 1249.16003
Epoch 8: Val Loss 1237.36719
Epoch 9: Val Loss 1222.53369
Epoch 10: Val Loss 1203.56238
Epoch 11: Val Loss 1180.75940
Epoch 12: Val Loss 1153.55945
Epoch 13: Val Loss 1120.36462
Epoch 14: Val Loss 1083.06384
Epoch 15: Val Loss 1042.43860
Epoch 16: Val Loss 997.03540
Epoch 17: Val Loss 947.43878
Epoch 18: Val Loss 895.15247
Epoch 19: Val Loss 844.29968
Epoch 20: Val Loss 797.51160
Epoch 21: Val Loss 756.37933
Epoch 22: Val Loss 718.12781
Epoch 23: Val Loss 685.96802
Epoch 24: Val Loss 657.46686
Epoch 25: Val Loss 631.89825
Epoch 26: Val Loss 608.44482
Epoch 27: Val Loss 584.02148
Epoch 28: Val Loss 558.45764
Epoch 29: Val Loss 532.28229
Epoch 30: Val Loss 504.32462
Epoch 31: Val Loss 476.38861
Epoch 32: Val Loss 448.59192
Epoch 33: Val Loss 424.13351
Epoch 34: Val Loss 402.56824
Epoch 35: Val Loss 381.94730
Epoch 36: Val Loss 363.21198
Epoch 37: Val Loss 347.58240
Epoch 38: Val Loss 333.90323
Epoch 39: Val Loss 322.42624
Epoch 40: Val Loss 309.00876
Epoch 41: Val Loss 298.49182
Epoch 42: Val Loss 287.99213
Epoch 43: Val Loss 279.16458
Epoch 44: Val Loss 272.12411
Epoch 45: Val Loss 264.21518
Epoch 46: Val Loss 257.46234
Epoch 47: Val Loss 250.20016
Epoch 48: Val Loss 242.24532
Epoch 49: Val Loss 237.39317
Epoch 50: Val Loss 232.05756
Epoch 51: Val Loss 225.77960
Epoch 52: Val Loss 221.83868
Epoch 53: Val Loss 220.61540
Epoch 54: Val Loss 218.65311
Epoch 55: Val Loss 213.27138
Epoch 56: Val Loss 210.27908
Epoch 57: Val Loss 208.30762
Epoch 58: Val Loss 207.06775
Epoch 59: Val Loss 207.09972
Epoch 60: Val Loss 211.75381
Epoch 61: Val Loss 205.50294
Epoch 62: Val Loss 201.36447
Epoch 63: Val Loss 198.72911
Epoch 64: Val Loss 197.91707
Epoch 65: Val Loss 197.23476
Epoch 66: Val Loss 197.49384
Epoch 67: Val Loss 196.25938
Epoch 68: Val Loss 198.44170
Epoch 69: Val Loss 200.69341
Epoch 70: Val Loss 199.67447
Epoch 71: Val Loss 196.01852
Epoch 72: Val Loss 194.20181
Epoch 73: Val Loss 195.89702
Epoch 74: Val Loss 198.69699
Epoch 75: Val Loss 200.28304
Epoch 76: Val Loss 199.50856
Epoch 77: Val Loss 193.53728
Epoch 78: Val Loss 192.29861
Epoch 79: Val Loss 191.20319
Epoch 80: Val Loss 191.41118
Epoch 81: Val Loss 191.47012
Epoch 82: Val Loss 190.87405
Epoch 83: Val Loss 190.82323
Epoch 84: Val Loss 192.77650
Epoch 85: Val Loss 190.84927
Epoch 86: Val Loss 191.85271
Epoch 87: Val Loss 189.37433
Epoch 88: Val Loss 187.67726
Epoch 89: Val Loss 186.73839
Epoch 90: Val Loss 184.52765
Epoch 91: Val Loss 183.59233
Epoch 92: Val Loss 185.34163
Epoch 93: Val Loss 182.97957
Epoch 94: Val Loss 182.38448
Epoch 95: Val Loss 184.10765
Epoch 96: Val Loss 185.85893
Epoch 97: Val Loss 182.57315
Epoch 98: Val Loss 181.15393
Epoch 99: Val Loss 180.97995
{'MSE - mean': 180.97995430232467, 'MSE - std': 0.0, 'R2 - mean': 0.823216620244005, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1305.96594
Epoch 1: Val Loss 1303.40161
Epoch 2: Val Loss 1300.80200
Epoch 3: Val Loss 1297.86926
Epoch 4: Val Loss 1294.40527
Epoch 5: Val Loss 1289.14746
Epoch 6: Val Loss 1281.86926
Epoch 7: Val Loss 1273.08655
Epoch 8: Val Loss 1261.95862
Epoch 9: Val Loss 1247.95117
Epoch 10: Val Loss 1230.46399
Epoch 11: Val Loss 1209.81641
Epoch 12: Val Loss 1185.10876
Epoch 13: Val Loss 1156.27283
Epoch 14: Val Loss 1122.86609
Epoch 15: Val Loss 1083.84790
Epoch 16: Val Loss 1039.27844
Epoch 17: Val Loss 990.27881
Epoch 18: Val Loss 938.38379
Epoch 19: Val Loss 885.30737
Epoch 20: Val Loss 833.81616
Epoch 21: Val Loss 789.09363
Epoch 22: Val Loss 752.38770
Epoch 23: Val Loss 721.86523
Epoch 24: Val Loss 698.58386
Epoch 25: Val Loss 678.28082
Epoch 26: Val Loss 659.08301
Epoch 27: Val Loss 637.49231
Epoch 28: Val Loss 615.27179
Epoch 29: Val Loss 593.94830
Epoch 30: Val Loss 573.40161
Epoch 31: Val Loss 553.15015
Epoch 32: Val Loss 534.91730
Epoch 33: Val Loss 517.81647
Epoch 34: Val Loss 502.17322
Epoch 35: Val Loss 484.92654
Epoch 36: Val Loss 467.22430
Epoch 37: Val Loss 451.25998
Epoch 38: Val Loss 440.90192
Epoch 39: Val Loss 430.21518
Epoch 40: Val Loss 420.05139
Epoch 41: Val Loss 413.81433
Epoch 42: Val Loss 409.05646
Epoch 43: Val Loss 398.62650
Epoch 44: Val Loss 400.46292
Epoch 45: Val Loss 389.44489
Epoch 46: Val Loss 380.99841
Epoch 47: Val Loss 374.13260
Epoch 48: Val Loss 364.38684
Epoch 49: Val Loss 361.44901
Epoch 50: Val Loss 354.11935
Epoch 51: Val Loss 350.73273
Epoch 52: Val Loss 356.95779
Epoch 53: Val Loss 360.98492
Epoch 54: Val Loss 353.74164
Epoch 55: Val Loss 342.14612
Epoch 56: Val Loss 336.44043
Epoch 57: Val Loss 333.51819
Epoch 58: Val Loss 339.96225
Epoch 59: Val Loss 334.16733
Epoch 60: Val Loss 326.94025
Epoch 61: Val Loss 327.84293
Epoch 62: Val Loss 327.51135
Epoch 63: Val Loss 320.42670
Epoch 64: Val Loss 318.16632
Epoch 65: Val Loss 311.74222
Epoch 66: Val Loss 317.53687
Epoch 67: Val Loss 314.49377
Epoch 68: Val Loss 310.23975
Epoch 69: Val Loss 302.13147
Epoch 70: Val Loss 308.30225
Epoch 71: Val Loss 311.66373
Epoch 72: Val Loss 308.58240
Epoch 73: Val Loss 301.32080
Epoch 74: Val Loss 297.19998
Epoch 75: Val Loss 298.12827
Epoch 76: Val Loss 298.02991
Epoch 77: Val Loss 290.18784
Epoch 78: Val Loss 282.64569
Epoch 79: Val Loss 283.35910
Epoch 80: Val Loss 288.04272
Epoch 81: Val Loss 285.68118
Epoch 82: Val Loss 289.68134
Epoch 83: Val Loss 288.63821
Epoch 84: Val Loss 278.74435
Epoch 85: Val Loss 270.82462
Epoch 86: Val Loss 268.23718
Epoch 87: Val Loss 275.17038
Epoch 88: Val Loss 268.52573
Epoch 89: Val Loss 265.93848
Epoch 90: Val Loss 271.52808
Epoch 91: Val Loss 268.91434
Epoch 92: Val Loss 276.15952
Epoch 93: Val Loss 264.49210
Epoch 94: Val Loss 258.18463
Epoch 95: Val Loss 258.61334
Epoch 96: Val Loss 260.49762
Epoch 97: Val Loss 257.09012
Epoch 98: Val Loss 253.05049
Epoch 99: Val Loss 256.04318
{'MSE - mean': 217.01521756448903, 'MSE - std': 36.03526326216438, 'R2 - mean': 0.7911169016302627, 'R2 - std': 0.03209971861374228} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2813.82837
Epoch 1: Val Loss 2810.32959
Epoch 2: Val Loss 2805.77173
Epoch 3: Val Loss 2799.94263
Epoch 4: Val Loss 2791.80005
Epoch 5: Val Loss 2780.75708
Epoch 6: Val Loss 2765.63208
Epoch 7: Val Loss 2744.63501
Epoch 8: Val Loss 2716.57300
Epoch 9: Val Loss 2680.69824
Epoch 10: Val Loss 2636.03320
Epoch 11: Val Loss 2580.73340
Epoch 12: Val Loss 2515.09302
Epoch 13: Val Loss 2440.03809
Epoch 14: Val Loss 2357.15503
Epoch 15: Val Loss 2268.79102
Epoch 16: Val Loss 2181.91260
Epoch 17: Val Loss 2095.14575
Epoch 18: Val Loss 2017.12854
Epoch 19: Val Loss 1941.45447
Epoch 20: Val Loss 1876.37585
Epoch 21: Val Loss 1813.52283
Epoch 22: Val Loss 1748.56628
Epoch 23: Val Loss 1688.40503
Epoch 24: Val Loss 1621.16833
Epoch 25: Val Loss 1552.35779
Epoch 26: Val Loss 1493.49402
Epoch 27: Val Loss 1445.58411
Epoch 28: Val Loss 1396.54126
Epoch 29: Val Loss 1339.80139
Epoch 30: Val Loss 1288.09521
Epoch 31: Val Loss 1246.42725
Epoch 32: Val Loss 1197.76477
Epoch 33: Val Loss 1157.22705
Epoch 34: Val Loss 1119.21021
Epoch 35: Val Loss 1087.43994
Epoch 36: Val Loss 1050.96228
Epoch 37: Val Loss 1015.94653
Epoch 38: Val Loss 990.64246
Epoch 39: Val Loss 969.53326
Epoch 40: Val Loss 941.47876
Epoch 41: Val Loss 920.18799
Epoch 42: Val Loss 888.48981
Epoch 43: Val Loss 859.63623
Epoch 44: Val Loss 843.72125
Epoch 45: Val Loss 828.54980
Epoch 46: Val Loss 816.96552
Epoch 47: Val Loss 799.59399
Epoch 48: Val Loss 773.72546
Epoch 49: Val Loss 763.87195
Epoch 50: Val Loss 755.43243
Epoch 51: Val Loss 746.32861
Epoch 52: Val Loss 737.98541
Epoch 53: Val Loss 726.67188
Epoch 54: Val Loss 713.53571
Epoch 55: Val Loss 702.78125
Epoch 56: Val Loss 686.34540
Epoch 57: Val Loss 676.51648
Epoch 58: Val Loss 657.40839
Epoch 59: Val Loss 648.72675
Epoch 60: Val Loss 645.74457
Epoch 61: Val Loss 645.58301
Epoch 62: Val Loss 625.20941
Epoch 63: Val Loss 610.94653
Epoch 64: Val Loss 604.35455
Epoch 65: Val Loss 594.36725
Epoch 66: Val Loss 589.60712
Epoch 67: Val Loss 586.52271
Epoch 68: Val Loss 583.41571
Epoch 69: Val Loss 574.37378
Epoch 70: Val Loss 570.05341
Epoch 71: Val Loss 567.68146
Epoch 72: Val Loss 567.12598
Epoch 73: Val Loss 564.84277
Epoch 74: Val Loss 556.76678
Epoch 75: Val Loss 556.36603
Epoch 76: Val Loss 555.57288
Epoch 77: Val Loss 548.80450
Epoch 78: Val Loss 533.38196
Epoch 79: Val Loss 527.81287
Epoch 80: Val Loss 526.09161
Epoch 81: Val Loss 523.50775
Epoch 82: Val Loss 531.27069
Epoch 83: Val Loss 529.84912
Epoch 84: Val Loss 520.13257
Epoch 85: Val Loss 509.81433
Epoch 86: Val Loss 507.92621
Epoch 87: Val Loss 505.40167
Epoch 88: Val Loss 500.98810
Epoch 89: Val Loss 488.80103
Epoch 90: Val Loss 476.03577
Epoch 91: Val Loss 475.01212
Epoch 92: Val Loss 478.78085
Epoch 93: Val Loss 475.75778
Epoch 94: Val Loss 470.44284
Epoch 95: Val Loss 459.73138
Epoch 96: Val Loss 457.70459
Epoch 97: Val Loss 457.60526
Epoch 98: Val Loss 453.11826
Epoch 99: Val Loss 447.09576
{'MSE - mean': 293.7087370217047, 'MSE - std': 112.38098290658859, 'R2 - mean': 0.7999138640067501, 'R2 - std': 0.029012084565360394} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2121.63989
Epoch 1: Val Loss 2117.61426
Epoch 2: Val Loss 2113.18604
Epoch 3: Val Loss 2108.26196
Epoch 4: Val Loss 2102.30884
Epoch 5: Val Loss 2094.83496
Epoch 6: Val Loss 2085.63574
Epoch 7: Val Loss 2073.43823
Epoch 8: Val Loss 2057.21899
Epoch 9: Val Loss 2035.03149
Epoch 10: Val Loss 2004.87280
Epoch 11: Val Loss 1966.12915
Epoch 12: Val Loss 1920.91479
Epoch 13: Val Loss 1864.86841
Epoch 14: Val Loss 1798.18213
Epoch 15: Val Loss 1720.53003
Epoch 16: Val Loss 1635.92932
Epoch 17: Val Loss 1550.84790
Epoch 18: Val Loss 1467.02222
Epoch 19: Val Loss 1388.90637
Epoch 20: Val Loss 1319.15588
Epoch 21: Val Loss 1257.25830
Epoch 22: Val Loss 1201.76721
Epoch 23: Val Loss 1149.07605
Epoch 24: Val Loss 1097.75195
Epoch 25: Val Loss 1046.90015
Epoch 26: Val Loss 996.45428
Epoch 27: Val Loss 949.28223
Epoch 28: Val Loss 902.90131
Epoch 29: Val Loss 862.21338
Epoch 30: Val Loss 817.22394
Epoch 31: Val Loss 780.77411
Epoch 32: Val Loss 747.25146
Epoch 33: Val Loss 716.29327
Epoch 34: Val Loss 689.35443
Epoch 35: Val Loss 664.84998
Epoch 36: Val Loss 643.04565
Epoch 37: Val Loss 623.32733
Epoch 38: Val Loss 602.22955
Epoch 39: Val Loss 583.23486
Epoch 40: Val Loss 566.04102
Epoch 41: Val Loss 551.69879
Epoch 42: Val Loss 540.18243
Epoch 43: Val Loss 525.24115
Epoch 44: Val Loss 512.88214
Epoch 45: Val Loss 498.76050
Epoch 46: Val Loss 490.47287
Epoch 47: Val Loss 480.95258
Epoch 48: Val Loss 472.41321
Epoch 49: Val Loss 465.42337
Epoch 50: Val Loss 459.17651
Epoch 51: Val Loss 450.51920
Epoch 52: Val Loss 445.67294
Epoch 53: Val Loss 442.04913
Epoch 54: Val Loss 436.45648
Epoch 55: Val Loss 432.57773
Epoch 56: Val Loss 427.85886
Epoch 57: Val Loss 426.80399
Epoch 58: Val Loss 423.83026
Epoch 59: Val Loss 414.62253
Epoch 60: Val Loss 409.14407
Epoch 61: Val Loss 406.49414
Epoch 62: Val Loss 408.71185
Epoch 63: Val Loss 406.54446
Epoch 64: Val Loss 401.47336
Epoch 65: Val Loss 396.76120
Epoch 66: Val Loss 393.79770
Epoch 67: Val Loss 390.57947
Epoch 68: Val Loss 392.74622
Epoch 69: Val Loss 386.39581
Epoch 70: Val Loss 384.09030
Epoch 71: Val Loss 379.71927
Epoch 72: Val Loss 373.83597
Epoch 73: Val Loss 379.73117
Epoch 74: Val Loss 380.50974
Epoch 75: Val Loss 375.25589
Epoch 76: Val Loss 367.43121
Epoch 77: Val Loss 365.74557
Epoch 78: Val Loss 361.57071
Epoch 79: Val Loss 358.08151
Epoch 80: Val Loss 357.93286
Epoch 81: Val Loss 359.66086
Epoch 82: Val Loss 362.70724
Epoch 83: Val Loss 358.00146
Epoch 84: Val Loss 361.63351
Epoch 85: Val Loss 353.35541
Epoch 86: Val Loss 350.22098
Epoch 87: Val Loss 352.04688
Epoch 88: Val Loss 349.30264
Epoch 89: Val Loss 344.51791
Epoch 90: Val Loss 341.39957
Epoch 91: Val Loss 336.84888
Epoch 92: Val Loss 334.25665
Epoch 93: Val Loss 332.06686
Epoch 94: Val Loss 333.68863
Epoch 95: Val Loss 331.60492
Epoch 96: Val Loss 329.23355
Epoch 97: Val Loss 325.10071
Epoch 98: Val Loss 325.89963
Epoch 99: Val Loss 328.19290
{'MSE - mean': 301.5567427528804, 'MSE - std': 98.26946408298926, 'R2 - mean': 0.8027755157392502, 'R2 - std': 0.025609430685493203} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2508.60449
Epoch 1: Val Loss 2507.57739
Epoch 2: Val Loss 2506.99341
Epoch 3: Val Loss 2506.53589
Epoch 4: Val Loss 2506.06104
Epoch 5: Val Loss 2505.41602
Epoch 6: Val Loss 2504.26172
Epoch 7: Val Loss 2501.80811
Epoch 8: Val Loss 2497.74048
Epoch 9: Val Loss 2491.47412
Epoch 10: Val Loss 2482.04761
Epoch 11: Val Loss 2468.10059
Epoch 12: Val Loss 2448.50049
Epoch 13: Val Loss 2421.90430
Epoch 14: Val Loss 2385.95190
Epoch 15: Val Loss 2340.62939
Epoch 16: Val Loss 2284.90723
Epoch 17: Val Loss 2220.16748
Epoch 18: Val Loss 2144.97876
Epoch 19: Val Loss 2062.58350
Epoch 20: Val Loss 1978.10925
Epoch 21: Val Loss 1895.99109
Epoch 22: Val Loss 1817.64343
Epoch 23: Val Loss 1747.43835
Epoch 24: Val Loss 1684.06165
Epoch 25: Val Loss 1627.17749
Epoch 26: Val Loss 1578.34863
Epoch 27: Val Loss 1537.20129
Epoch 28: Val Loss 1501.52527
Epoch 29: Val Loss 1462.51782
Epoch 30: Val Loss 1416.64075
Epoch 31: Val Loss 1371.24109
Epoch 32: Val Loss 1332.33826
Epoch 33: Val Loss 1292.36499
Epoch 34: Val Loss 1249.94116
Epoch 35: Val Loss 1210.81018
Epoch 36: Val Loss 1171.29663
Epoch 37: Val Loss 1136.82983
Epoch 38: Val Loss 1104.06946
Epoch 39: Val Loss 1066.23901
Epoch 40: Val Loss 1027.85876
Epoch 41: Val Loss 989.30981
Epoch 42: Val Loss 959.63599
Epoch 43: Val Loss 928.01379
Epoch 44: Val Loss 905.39691
Epoch 45: Val Loss 884.99701
Epoch 46: Val Loss 867.99475
Epoch 47: Val Loss 849.24951
Epoch 48: Val Loss 821.74976
Epoch 49: Val Loss 802.02106
Epoch 50: Val Loss 786.12427
Epoch 51: Val Loss 776.11078
Epoch 52: Val Loss 758.67834
Epoch 53: Val Loss 747.52136
Epoch 54: Val Loss 728.21039
Epoch 55: Val Loss 704.93164
Epoch 56: Val Loss 693.53363
Epoch 57: Val Loss 684.07117
Epoch 58: Val Loss 670.21643
Epoch 59: Val Loss 661.69147
Epoch 60: Val Loss 657.00671
Epoch 61: Val Loss 645.65363
Epoch 62: Val Loss 636.85522
Epoch 63: Val Loss 628.46942
Epoch 64: Val Loss 618.63446
Epoch 65: Val Loss 612.77393
Epoch 66: Val Loss 606.01044
Epoch 67: Val Loss 604.37036
Epoch 68: Val Loss 599.21387
Epoch 69: Val Loss 593.29291
Epoch 70: Val Loss 587.12573
Epoch 71: Val Loss 577.91541
Epoch 72: Val Loss 572.47394
Epoch 73: Val Loss 564.29224
Epoch 74: Val Loss 561.77179
Epoch 75: Val Loss 552.50287
Epoch 76: Val Loss 547.43378
Epoch 77: Val Loss 543.08813
Epoch 78: Val Loss 536.51227
Epoch 79: Val Loss 531.35651
Epoch 80: Val Loss 523.41327
Epoch 81: Val Loss 512.56055
Epoch 82: Val Loss 508.70840
Epoch 83: Val Loss 509.19730
Epoch 84: Val Loss 504.14224
Epoch 85: Val Loss 503.95877
Epoch 86: Val Loss 503.18155
Epoch 87: Val Loss 500.38977
Epoch 88: Val Loss 496.88974
Epoch 89: Val Loss 492.82034
Epoch 90: Val Loss 486.12744
Epoch 91: Val Loss 480.39362
Epoch 92: Val Loss 479.42560
Epoch 93: Val Loss 471.20288
Epoch 94: Val Loss 469.15887
Epoch 95: Val Loss 468.76849
Epoch 96: Val Loss 465.82211
Epoch 97: Val Loss 460.85330
Epoch 98: Val Loss 457.48593
Epoch 99: Val Loss 456.61011
{'MSE - mean': 332.567417648879, 'MSE - std': 107.57396472635749, 'R2 - mean': 0.7995281954540665, 'R2 - std': 0.02380871076237277} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 332.567417648879 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 13 with value: 332.567417648879.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1271.69348
Epoch 1: Val Loss 1267.49878
Epoch 2: Val Loss 1262.68213
Epoch 3: Val Loss 1256.98071
Epoch 4: Val Loss 1249.65356
Epoch 5: Val Loss 1240.17639
Epoch 6: Val Loss 1228.13904
Epoch 7: Val Loss 1212.26868
Epoch 8: Val Loss 1191.67627
Epoch 9: Val Loss 1165.82593
Epoch 10: Val Loss 1134.08508
Epoch 11: Val Loss 1096.40857
Epoch 12: Val Loss 1051.33386
Epoch 13: Val Loss 998.81616
Epoch 14: Val Loss 943.09772
Epoch 15: Val Loss 879.93610
Epoch 16: Val Loss 817.32190
Epoch 17: Val Loss 761.55463
Epoch 18: Val Loss 714.96887
Epoch 19: Val Loss 677.46564
Epoch 20: Val Loss 647.73938
Epoch 21: Val Loss 621.80328
Epoch 22: Val Loss 596.34991
Epoch 23: Val Loss 569.79755
Epoch 24: Val Loss 541.85101
Epoch 25: Val Loss 510.92841
Epoch 26: Val Loss 482.96964
Epoch 27: Val Loss 453.52618
Epoch 28: Val Loss 426.64954
Epoch 29: Val Loss 404.03625
Epoch 30: Val Loss 387.21915
Epoch 31: Val Loss 371.42444
Epoch 32: Val Loss 351.49905
Epoch 33: Val Loss 338.84711
Epoch 34: Val Loss 325.85501
Epoch 35: Val Loss 315.97321
Epoch 36: Val Loss 308.61551
Epoch 37: Val Loss 305.39987
Epoch 38: Val Loss 292.47507
Epoch 39: Val Loss 280.48285
Epoch 40: Val Loss 270.48660
Epoch 41: Val Loss 266.40955
Epoch 42: Val Loss 260.81046
Epoch 43: Val Loss 251.56651
Epoch 44: Val Loss 246.04913
Epoch 45: Val Loss 242.96301
Epoch 46: Val Loss 237.60439
Epoch 47: Val Loss 233.25148
Epoch 48: Val Loss 228.81914
Epoch 49: Val Loss 224.72598
Epoch 50: Val Loss 222.32224
Epoch 51: Val Loss 220.34787
Epoch 52: Val Loss 219.12207
Epoch 53: Val Loss 214.78156
Epoch 54: Val Loss 210.48729
Epoch 55: Val Loss 207.22943
Epoch 56: Val Loss 206.19989
Epoch 57: Val Loss 206.69662
Epoch 58: Val Loss 205.72427
Epoch 59: Val Loss 203.10460
Epoch 60: Val Loss 203.68945
Epoch 61: Val Loss 202.86523
Epoch 62: Val Loss 201.46266
Epoch 63: Val Loss 200.43594
Epoch 64: Val Loss 198.19083
Epoch 65: Val Loss 199.16695
Epoch 66: Val Loss 196.75340
Epoch 67: Val Loss 195.57155
Epoch 68: Val Loss 196.06749
Epoch 69: Val Loss 195.54855
Epoch 70: Val Loss 194.14392
Epoch 71: Val Loss 195.17087
Epoch 72: Val Loss 194.24408
Epoch 73: Val Loss 192.20244
Epoch 74: Val Loss 192.04108
Epoch 75: Val Loss 195.94934
Epoch 76: Val Loss 195.42410
Epoch 77: Val Loss 190.66080
Epoch 78: Val Loss 188.90340
Epoch 79: Val Loss 188.49490
Epoch 80: Val Loss 187.83369
Epoch 81: Val Loss 188.98634
Epoch 82: Val Loss 188.49397
Epoch 83: Val Loss 185.53560
Epoch 84: Val Loss 183.49339
Epoch 85: Val Loss 185.72540
Epoch 86: Val Loss 186.59923
Epoch 87: Val Loss 183.64099
Epoch 88: Val Loss 181.44725
Epoch 89: Val Loss 180.67711
Epoch 90: Val Loss 179.17854
Epoch 91: Val Loss 179.60394
Epoch 92: Val Loss 177.18881
Epoch 93: Val Loss 179.44539
Epoch 94: Val Loss 176.78917
Epoch 95: Val Loss 173.27246
Epoch 96: Val Loss 172.13170
Epoch 97: Val Loss 171.65770
Epoch 98: Val Loss 172.69618
Epoch 99: Val Loss 170.96259
{'MSE - mean': 170.96258905002017, 'MSE - std': 0.0, 'R2 - mean': 0.8330017022017249, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1303.76880
Epoch 1: Val Loss 1300.35632
Epoch 2: Val Loss 1296.59790
Epoch 3: Val Loss 1292.13733
Epoch 4: Val Loss 1286.55896
Epoch 5: Val Loss 1279.28516
Epoch 6: Val Loss 1270.27295
Epoch 7: Val Loss 1258.88635
Epoch 8: Val Loss 1243.88550
Epoch 9: Val Loss 1225.27429
Epoch 10: Val Loss 1202.21399
Epoch 11: Val Loss 1174.69470
Epoch 12: Val Loss 1140.20154
Epoch 13: Val Loss 1100.88721
Epoch 14: Val Loss 1055.32874
Epoch 15: Val Loss 1004.19897
Epoch 16: Val Loss 950.39044
Epoch 17: Val Loss 894.24377
Epoch 18: Val Loss 837.37115
Epoch 19: Val Loss 785.99585
Epoch 20: Val Loss 742.71381
Epoch 21: Val Loss 708.54535
Epoch 22: Val Loss 683.30859
Epoch 23: Val Loss 661.94519
Epoch 24: Val Loss 639.29639
Epoch 25: Val Loss 614.69397
Epoch 26: Val Loss 589.81665
Epoch 27: Val Loss 560.76837
Epoch 28: Val Loss 533.72766
Epoch 29: Val Loss 505.40286
Epoch 30: Val Loss 487.26926
Epoch 31: Val Loss 470.85217
Epoch 32: Val Loss 449.57162
Epoch 33: Val Loss 431.61011
Epoch 34: Val Loss 417.72363
Epoch 35: Val Loss 407.30316
Epoch 36: Val Loss 398.43283
Epoch 37: Val Loss 394.97693
Epoch 38: Val Loss 388.16586
Epoch 39: Val Loss 379.93600
Epoch 40: Val Loss 365.79871
Epoch 41: Val Loss 353.84146
Epoch 42: Val Loss 350.00760
Epoch 43: Val Loss 344.51663
Epoch 44: Val Loss 341.37460
Epoch 45: Val Loss 343.35052
Epoch 46: Val Loss 337.13483
Epoch 47: Val Loss 333.42303
Epoch 48: Val Loss 334.90598
Epoch 49: Val Loss 335.52301
Epoch 50: Val Loss 332.57425
Epoch 51: Val Loss 326.95312
Epoch 52: Val Loss 322.88785
Epoch 53: Val Loss 321.63318
Epoch 54: Val Loss 314.51895
Epoch 55: Val Loss 324.27591
Epoch 56: Val Loss 335.66245
Epoch 57: Val Loss 332.85001
Epoch 58: Val Loss 325.10495
Epoch 59: Val Loss 313.77490
Epoch 60: Val Loss 312.97858
Epoch 61: Val Loss 313.84274
Epoch 62: Val Loss 310.73239
Epoch 63: Val Loss 321.12195
Epoch 64: Val Loss 309.42862
Epoch 65: Val Loss 305.81570
Epoch 66: Val Loss 303.01257
Epoch 67: Val Loss 299.65884
Epoch 68: Val Loss 301.37711
Epoch 69: Val Loss 308.16397
Epoch 70: Val Loss 306.74185
Epoch 71: Val Loss 308.38596
Epoch 72: Val Loss 299.97510
Epoch 73: Val Loss 299.80850
Epoch 74: Val Loss 296.15134
Epoch 75: Val Loss 300.74759
Epoch 76: Val Loss 293.75754
Epoch 77: Val Loss 286.06293
Epoch 78: Val Loss 280.74734
Epoch 79: Val Loss 283.36899
Epoch 80: Val Loss 287.84354
Epoch 81: Val Loss 282.47202
Epoch 82: Val Loss 276.04782
Epoch 83: Val Loss 281.15768
Epoch 84: Val Loss 285.49545
Epoch 85: Val Loss 280.20288
Epoch 86: Val Loss 273.81335
Epoch 87: Val Loss 271.65259
Epoch 88: Val Loss 277.11041
Epoch 89: Val Loss 276.33478
Epoch 90: Val Loss 268.35089
Epoch 91: Val Loss 268.04935
Epoch 92: Val Loss 281.03992
Epoch 93: Val Loss 268.04715
Epoch 94: Val Loss 264.38196
Epoch 95: Val Loss 260.18665
Epoch 96: Val Loss 260.00482
Epoch 97: Val Loss 260.18176
Epoch 98: Val Loss 277.45184
Epoch 99: Val Loss 270.45386
{'MSE - mean': 215.48370986843344, 'MSE - std': 44.52112081841325, 'R2 - mean': 0.792698089842427, 'R2 - std': 0.04030361235929791} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2824.40332
Epoch 1: Val Loss 2820.17896
Epoch 2: Val Loss 2814.71069
Epoch 3: Val Loss 2807.69409
Epoch 4: Val Loss 2799.00928
Epoch 5: Val Loss 2787.97998
Epoch 6: Val Loss 2774.13892
Epoch 7: Val Loss 2756.76294
Epoch 8: Val Loss 2734.39062
Epoch 9: Val Loss 2706.76025
Epoch 10: Val Loss 2672.87329
Epoch 11: Val Loss 2631.09302
Epoch 12: Val Loss 2582.88574
Epoch 13: Val Loss 2526.61035
Epoch 14: Val Loss 2464.83105
Epoch 15: Val Loss 2390.06006
Epoch 16: Val Loss 2311.35693
Epoch 17: Val Loss 2227.49585
Epoch 18: Val Loss 2142.00439
Epoch 19: Val Loss 2063.53809
Epoch 20: Val Loss 1991.60010
Epoch 21: Val Loss 1925.28723
Epoch 22: Val Loss 1866.80249
Epoch 23: Val Loss 1810.77490
Epoch 24: Val Loss 1754.61475
Epoch 25: Val Loss 1699.75574
Epoch 26: Val Loss 1648.33032
Epoch 27: Val Loss 1590.30090
Epoch 28: Val Loss 1536.99280
Epoch 29: Val Loss 1486.60071
Epoch 30: Val Loss 1438.88232
Epoch 31: Val Loss 1392.93396
Epoch 32: Val Loss 1336.03223
Epoch 33: Val Loss 1289.11304
Epoch 34: Val Loss 1252.82080
Epoch 35: Val Loss 1200.37451
Epoch 36: Val Loss 1161.39539
Epoch 37: Val Loss 1118.93726
Epoch 38: Val Loss 1087.27905
Epoch 39: Val Loss 1060.68091
Epoch 40: Val Loss 1035.42578
Epoch 41: Val Loss 1009.85400
Epoch 42: Val Loss 984.72711
Epoch 43: Val Loss 960.84015
Epoch 44: Val Loss 937.76227
Epoch 45: Val Loss 917.06512
Epoch 46: Val Loss 895.40149
Epoch 47: Val Loss 879.64038
Epoch 48: Val Loss 863.20135
Epoch 49: Val Loss 840.88477
Epoch 50: Val Loss 817.93304
Epoch 51: Val Loss 798.81586
Epoch 52: Val Loss 789.97815
Epoch 53: Val Loss 768.43652
Epoch 54: Val Loss 755.77826
Epoch 55: Val Loss 742.09076
Epoch 56: Val Loss 728.31171
Epoch 57: Val Loss 717.08252
Epoch 58: Val Loss 707.27039
Epoch 59: Val Loss 693.15894
Epoch 60: Val Loss 683.50354
Epoch 61: Val Loss 675.17993
Epoch 62: Val Loss 671.39825
Epoch 63: Val Loss 666.83691
Epoch 64: Val Loss 644.35016
Epoch 65: Val Loss 635.82892
Epoch 66: Val Loss 631.49200
Epoch 67: Val Loss 623.23859
Epoch 68: Val Loss 615.27466
Epoch 69: Val Loss 609.36578
Epoch 70: Val Loss 604.30035
Epoch 71: Val Loss 602.48224
Epoch 72: Val Loss 590.99695
Epoch 73: Val Loss 585.38770
Epoch 74: Val Loss 580.70575
Epoch 75: Val Loss 570.61865
Epoch 76: Val Loss 558.91632
Epoch 77: Val Loss 553.45264
Epoch 78: Val Loss 544.80524
Epoch 79: Val Loss 551.35046
Epoch 80: Val Loss 549.57434
Epoch 81: Val Loss 540.58868
Epoch 82: Val Loss 536.89716
Epoch 83: Val Loss 536.07941
Epoch 84: Val Loss 531.38977
Epoch 85: Val Loss 525.32196
Epoch 86: Val Loss 521.53625
Epoch 87: Val Loss 516.09113
Epoch 88: Val Loss 508.41028
Epoch 89: Val Loss 504.16492
Epoch 90: Val Loss 496.10193
Epoch 91: Val Loss 490.01590
Epoch 92: Val Loss 488.50381
Epoch 93: Val Loss 471.99689
Epoch 94: Val Loss 463.60214
Epoch 95: Val Loss 464.18799
Epoch 96: Val Loss 463.07230
Epoch 97: Val Loss 461.24167
Epoch 98: Val Loss 461.20840
Epoch 99: Val Loss 455.95996
{'MSE - mean': 295.64244555266816, 'MSE - std': 119.04732649500582, 'R2 - mean': 0.799761956549163, 'R2 - std': 0.03439065578283487} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2114.77002
Epoch 1: Val Loss 2111.15601
Epoch 2: Val Loss 2107.08105
Epoch 3: Val Loss 2102.45874
Epoch 4: Val Loss 2096.96680
Epoch 5: Val Loss 2089.87231
Epoch 6: Val Loss 2080.96484
Epoch 7: Val Loss 2069.86865
Epoch 8: Val Loss 2055.51367
Epoch 9: Val Loss 2036.59204
Epoch 10: Val Loss 2012.39990
Epoch 11: Val Loss 1979.05933
Epoch 12: Val Loss 1935.43408
Epoch 13: Val Loss 1881.61182
Epoch 14: Val Loss 1820.12769
Epoch 15: Val Loss 1751.97290
Epoch 16: Val Loss 1677.66040
Epoch 17: Val Loss 1598.28918
Epoch 18: Val Loss 1517.33118
Epoch 19: Val Loss 1437.87598
Epoch 20: Val Loss 1361.85889
Epoch 21: Val Loss 1291.87036
Epoch 22: Val Loss 1226.03076
Epoch 23: Val Loss 1167.63623
Epoch 24: Val Loss 1110.68518
Epoch 25: Val Loss 1060.29602
Epoch 26: Val Loss 1013.66516
Epoch 27: Val Loss 969.68433
Epoch 28: Val Loss 927.69153
Epoch 29: Val Loss 884.83264
Epoch 30: Val Loss 846.46027
Epoch 31: Val Loss 812.26929
Epoch 32: Val Loss 779.16803
Epoch 33: Val Loss 749.09460
Epoch 34: Val Loss 721.78198
Epoch 35: Val Loss 696.09418
Epoch 36: Val Loss 671.82941
Epoch 37: Val Loss 650.27368
Epoch 38: Val Loss 630.34558
Epoch 39: Val Loss 611.70898
Epoch 40: Val Loss 594.02820
Epoch 41: Val Loss 576.17053
Epoch 42: Val Loss 561.10645
Epoch 43: Val Loss 547.93982
Epoch 44: Val Loss 536.67230
Epoch 45: Val Loss 526.62500
Epoch 46: Val Loss 516.45288
Epoch 47: Val Loss 506.44354
Epoch 48: Val Loss 498.22119
Epoch 49: Val Loss 490.10574
Epoch 50: Val Loss 482.95291
Epoch 51: Val Loss 473.27094
Epoch 52: Val Loss 468.43384
Epoch 53: Val Loss 462.41498
Epoch 54: Val Loss 453.98322
Epoch 55: Val Loss 448.03375
Epoch 56: Val Loss 444.04443
Epoch 57: Val Loss 438.64944
Epoch 58: Val Loss 432.03220
Epoch 59: Val Loss 425.71021
Epoch 60: Val Loss 417.11118
Epoch 61: Val Loss 414.36215
Epoch 62: Val Loss 410.61615
Epoch 63: Val Loss 406.04715
Epoch 64: Val Loss 399.28186
Epoch 65: Val Loss 394.56473
Epoch 66: Val Loss 389.39801
Epoch 67: Val Loss 383.35147
Epoch 68: Val Loss 382.59491
Epoch 69: Val Loss 377.62875
Epoch 70: Val Loss 373.39941
Epoch 71: Val Loss 370.59583
Epoch 72: Val Loss 369.86459
Epoch 73: Val Loss 366.51315
Epoch 74: Val Loss 366.81912
Epoch 75: Val Loss 371.01920
Epoch 76: Val Loss 361.15845
Epoch 77: Val Loss 351.20679
Epoch 78: Val Loss 346.78278
Epoch 79: Val Loss 343.92773
Epoch 80: Val Loss 342.25229
Epoch 81: Val Loss 340.09164
Epoch 82: Val Loss 335.28409
Epoch 83: Val Loss 336.22144
Epoch 84: Val Loss 332.85477
Epoch 85: Val Loss 329.49573
Epoch 86: Val Loss 328.64111
Epoch 87: Val Loss 331.78787
Epoch 88: Val Loss 334.29968
Epoch 89: Val Loss 325.03714
Epoch 90: Val Loss 317.17776
Epoch 91: Val Loss 314.30273
Epoch 92: Val Loss 318.67648
Epoch 93: Val Loss 317.08737
Epoch 94: Val Loss 319.77502
Epoch 95: Val Loss 322.20282
Epoch 96: Val Loss 315.76929
Epoch 97: Val Loss 304.01947
Epoch 98: Val Loss 304.29749
Epoch 99: Val Loss 300.52805
{'MSE - mean': 296.86384578422775, 'MSE - std': 103.11971157244064, 'R2 - mean': 0.8062261608618162, 'R2 - std': 0.03181816644728683} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2513.41455
Epoch 1: Val Loss 2511.26245
Epoch 2: Val Loss 2509.13354
Epoch 3: Val Loss 2506.73730
Epoch 4: Val Loss 2503.60059
Epoch 5: Val Loss 2499.15723
Epoch 6: Val Loss 2492.54956
Epoch 7: Val Loss 2483.18677
Epoch 8: Val Loss 2470.34009
Epoch 9: Val Loss 2453.40649
Epoch 10: Val Loss 2431.54858
Epoch 11: Val Loss 2403.14600
Epoch 12: Val Loss 2368.69434
Epoch 13: Val Loss 2327.54077
Epoch 14: Val Loss 2280.80908
Epoch 15: Val Loss 2226.31567
Epoch 16: Val Loss 2164.86987
Epoch 17: Val Loss 2097.06909
Epoch 18: Val Loss 2026.66992
Epoch 19: Val Loss 1948.80701
Epoch 20: Val Loss 1877.12036
Epoch 21: Val Loss 1808.98975
Epoch 22: Val Loss 1749.09387
Epoch 23: Val Loss 1692.80640
Epoch 24: Val Loss 1639.01709
Epoch 25: Val Loss 1586.96265
Epoch 26: Val Loss 1543.68042
Epoch 27: Val Loss 1506.04639
Epoch 28: Val Loss 1464.85339
Epoch 29: Val Loss 1419.23108
Epoch 30: Val Loss 1378.15100
Epoch 31: Val Loss 1331.80957
Epoch 32: Val Loss 1285.87305
Epoch 33: Val Loss 1232.61353
Epoch 34: Val Loss 1188.27234
Epoch 35: Val Loss 1147.58337
Epoch 36: Val Loss 1111.07776
Epoch 37: Val Loss 1077.15100
Epoch 38: Val Loss 1035.60510
Epoch 39: Val Loss 1000.10175
Epoch 40: Val Loss 963.44147
Epoch 41: Val Loss 932.93628
Epoch 42: Val Loss 916.02869
Epoch 43: Val Loss 892.15259
Epoch 44: Val Loss 869.41443
Epoch 45: Val Loss 850.62054
Epoch 46: Val Loss 826.09454
Epoch 47: Val Loss 804.76544
Epoch 48: Val Loss 788.09894
Epoch 49: Val Loss 775.96216
Epoch 50: Val Loss 769.33185
Epoch 51: Val Loss 756.89514
Epoch 52: Val Loss 743.91962
Epoch 53: Val Loss 722.88129
Epoch 54: Val Loss 710.89209
Epoch 55: Val Loss 703.13666
Epoch 56: Val Loss 689.63556
Epoch 57: Val Loss 684.47351
Epoch 58: Val Loss 673.14636
Epoch 59: Val Loss 661.16541
Epoch 60: Val Loss 652.68817
Epoch 61: Val Loss 647.33264
Epoch 62: Val Loss 634.68597
Epoch 63: Val Loss 625.54858
Epoch 64: Val Loss 617.77612
Epoch 65: Val Loss 607.03052
Epoch 66: Val Loss 599.87091
Epoch 67: Val Loss 600.11133
Epoch 68: Val Loss 590.74591
Epoch 69: Val Loss 586.79419
Epoch 70: Val Loss 585.25623
Epoch 71: Val Loss 570.42102
Epoch 72: Val Loss 562.79980
Epoch 73: Val Loss 561.09790
Epoch 74: Val Loss 555.73663
Epoch 75: Val Loss 549.98767
Epoch 76: Val Loss 544.71576
Epoch 77: Val Loss 540.64941
Epoch 78: Val Loss 533.34662
Epoch 79: Val Loss 527.64038
Epoch 80: Val Loss 524.65430
Epoch 81: Val Loss 523.86835
Epoch 82: Val Loss 519.29773
Epoch 83: Val Loss 516.99835
Epoch 84: Val Loss 513.23492
Epoch 85: Val Loss 503.96112
Epoch 86: Val Loss 500.03821
Epoch 87: Val Loss 493.48697
Epoch 88: Val Loss 493.08759
Epoch 89: Val Loss 493.77921
Epoch 90: Val Loss 486.68845
Epoch 91: Val Loss 483.96637
Epoch 92: Val Loss 479.29770
Epoch 93: Val Loss 476.92786
Epoch 94: Val Loss 473.27280
Epoch 95: Val Loss 470.55365
Epoch 96: Val Loss 467.01840
Epoch 97: Val Loss 467.04965
Epoch 98: Val Loss 463.93304
Epoch 99: Val Loss 458.87366
{'MSE - mean': 329.26580985652555, 'MSE - std': 112.72306345551729, 'R2 - mean': 0.8020770738374894, 'R2 - std': 0.029644160730159765} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 329.26580985652555 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 14 with value: 329.26580985652555.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1283.27747
Epoch 1: Val Loss 1280.63440
Epoch 2: Val Loss 1277.01831
Epoch 3: Val Loss 1272.01624
Epoch 4: Val Loss 1264.67664
Epoch 5: Val Loss 1253.85535
Epoch 6: Val Loss 1238.55176
Epoch 7: Val Loss 1217.93677
Epoch 8: Val Loss 1191.71252
Epoch 9: Val Loss 1158.47839
Epoch 10: Val Loss 1117.43994
Epoch 11: Val Loss 1064.19690
Epoch 12: Val Loss 1002.04272
Epoch 13: Val Loss 933.61554
Epoch 14: Val Loss 866.46124
Epoch 15: Val Loss 807.08942
Epoch 16: Val Loss 752.06787
Epoch 17: Val Loss 708.15027
Epoch 18: Val Loss 674.25378
Epoch 19: Val Loss 646.39722
Epoch 20: Val Loss 618.84186
Epoch 21: Val Loss 590.15247
Epoch 22: Val Loss 559.39288
Epoch 23: Val Loss 529.23706
Epoch 24: Val Loss 498.69052
Epoch 25: Val Loss 470.59592
Epoch 26: Val Loss 444.08115
Epoch 27: Val Loss 419.44022
Epoch 28: Val Loss 400.24911
Epoch 29: Val Loss 385.06485
Epoch 30: Val Loss 363.09476
Epoch 31: Val Loss 340.77432
Epoch 32: Val Loss 321.33212
Epoch 33: Val Loss 308.42352
Epoch 34: Val Loss 295.05557
Epoch 35: Val Loss 283.34494
Epoch 36: Val Loss 271.56308
Epoch 37: Val Loss 261.39746
Epoch 38: Val Loss 253.02025
Epoch 39: Val Loss 245.46538
Epoch 40: Val Loss 238.41078
Epoch 41: Val Loss 232.47768
Epoch 42: Val Loss 236.14864
Epoch 43: Val Loss 233.09906
Epoch 44: Val Loss 223.31169
Epoch 45: Val Loss 216.38898
Epoch 46: Val Loss 211.77899
Epoch 47: Val Loss 209.59093
Epoch 48: Val Loss 206.57809
Epoch 49: Val Loss 205.56265
Epoch 50: Val Loss 205.01378
Epoch 51: Val Loss 204.93527
Epoch 52: Val Loss 200.67769
Epoch 53: Val Loss 197.87994
Epoch 54: Val Loss 195.61298
Epoch 55: Val Loss 196.19891
Epoch 56: Val Loss 196.90968
Epoch 57: Val Loss 197.83412
Epoch 58: Val Loss 198.11314
Epoch 59: Val Loss 196.82735
Epoch 60: Val Loss 192.76216
Epoch 61: Val Loss 191.63536
Epoch 62: Val Loss 192.61722
Epoch 63: Val Loss 196.11557
Epoch 64: Val Loss 198.47021
Epoch 65: Val Loss 195.31918
Epoch 66: Val Loss 193.47475
Epoch 67: Val Loss 191.16774
Epoch 68: Val Loss 190.55237
Epoch 69: Val Loss 189.63504
Epoch 70: Val Loss 190.66315
Epoch 71: Val Loss 191.11311
Epoch 72: Val Loss 191.48151
Epoch 73: Val Loss 185.20673
Epoch 74: Val Loss 183.33826
Epoch 75: Val Loss 182.99255
Epoch 76: Val Loss 188.44640
Epoch 77: Val Loss 194.01973
Epoch 78: Val Loss 189.44972
Epoch 79: Val Loss 185.79549
Epoch 80: Val Loss 185.23354
Epoch 81: Val Loss 184.05357
Epoch 82: Val Loss 181.67638
Epoch 83: Val Loss 180.49849
Epoch 84: Val Loss 179.04172
Epoch 85: Val Loss 178.09645
Epoch 86: Val Loss 182.12273
Epoch 87: Val Loss 186.11635
Epoch 88: Val Loss 185.24838
Epoch 89: Val Loss 181.28906
Epoch 90: Val Loss 179.68130
Epoch 91: Val Loss 176.13646
Epoch 92: Val Loss 176.21094
Epoch 93: Val Loss 176.73465
Epoch 94: Val Loss 172.96486
Epoch 95: Val Loss 170.38954
Epoch 96: Val Loss 169.57498
Epoch 97: Val Loss 170.27054
Epoch 98: Val Loss 168.55890
Epoch 99: Val Loss 166.40588
{'MSE - mean': 166.40589612196206, 'MSE - std': 0.0, 'R2 - mean': 0.8374527342479962, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1305.97351
Epoch 1: Val Loss 1304.08362
Epoch 2: Val Loss 1301.82251
Epoch 3: Val Loss 1298.87134
Epoch 4: Val Loss 1294.84412
Epoch 5: Val Loss 1289.33984
Epoch 6: Val Loss 1281.83411
Epoch 7: Val Loss 1271.90759
Epoch 8: Val Loss 1258.48035
Epoch 9: Val Loss 1240.62988
Epoch 10: Val Loss 1217.30322
Epoch 11: Val Loss 1186.10071
Epoch 12: Val Loss 1146.08447
Epoch 13: Val Loss 1098.12769
Epoch 14: Val Loss 1042.06970
Epoch 15: Val Loss 978.74744
Epoch 16: Val Loss 911.14227
Epoch 17: Val Loss 845.74921
Epoch 18: Val Loss 788.94238
Epoch 19: Val Loss 744.58112
Epoch 20: Val Loss 712.39294
Epoch 21: Val Loss 688.37421
Epoch 22: Val Loss 666.57660
Epoch 23: Val Loss 643.42017
Epoch 24: Val Loss 619.51892
Epoch 25: Val Loss 591.28369
Epoch 26: Val Loss 562.58838
Epoch 27: Val Loss 536.87207
Epoch 28: Val Loss 512.94611
Epoch 29: Val Loss 488.68176
Epoch 30: Val Loss 469.72675
Epoch 31: Val Loss 459.28836
Epoch 32: Val Loss 444.88757
Epoch 33: Val Loss 429.16876
Epoch 34: Val Loss 416.71899
Epoch 35: Val Loss 411.67755
Epoch 36: Val Loss 397.10602
Epoch 37: Val Loss 386.10101
Epoch 38: Val Loss 377.87344
Epoch 39: Val Loss 383.04153
Epoch 40: Val Loss 377.35052
Epoch 41: Val Loss 365.81784
Epoch 42: Val Loss 365.80505
Epoch 43: Val Loss 370.09723
Epoch 44: Val Loss 358.68243
Epoch 45: Val Loss 346.36456
Epoch 46: Val Loss 348.87997
Epoch 47: Val Loss 351.71387
Epoch 48: Val Loss 347.73196
Epoch 49: Val Loss 337.73627
Epoch 50: Val Loss 333.74933
Epoch 51: Val Loss 326.70938
Epoch 52: Val Loss 330.71814
Epoch 53: Val Loss 347.03430
Epoch 54: Val Loss 338.90936
Epoch 55: Val Loss 326.28647
Epoch 56: Val Loss 320.61850
Epoch 57: Val Loss 326.49478
Epoch 58: Val Loss 325.01102
Epoch 59: Val Loss 325.95764
Epoch 60: Val Loss 321.53186
Epoch 61: Val Loss 321.29282
Epoch 62: Val Loss 313.90674
Epoch 63: Val Loss 310.69553
Epoch 64: Val Loss 307.53809
Epoch 65: Val Loss 312.89908
Epoch 66: Val Loss 307.16748
Epoch 67: Val Loss 302.65482
Epoch 68: Val Loss 303.71036
Epoch 69: Val Loss 309.39291
Epoch 70: Val Loss 306.62216
Epoch 71: Val Loss 304.17307
Epoch 72: Val Loss 294.61884
Epoch 73: Val Loss 289.42911
Epoch 74: Val Loss 289.89603
Epoch 75: Val Loss 297.04553
Epoch 76: Val Loss 298.08096
Epoch 77: Val Loss 297.75375
Epoch 78: Val Loss 289.60959
Epoch 79: Val Loss 286.91165
Epoch 80: Val Loss 277.74197
Epoch 81: Val Loss 280.87711
Epoch 82: Val Loss 274.02899
Epoch 83: Val Loss 273.94138
Epoch 84: Val Loss 281.43240
Epoch 85: Val Loss 291.51163
Epoch 86: Val Loss 283.08926
Epoch 87: Val Loss 274.13492
Epoch 88: Val Loss 272.38431
Epoch 89: Val Loss 272.41959
Epoch 90: Val Loss 274.70963
Epoch 91: Val Loss 277.61377
Epoch 92: Val Loss 266.22067
Epoch 93: Val Loss 262.39038
Epoch 94: Val Loss 270.93396
Epoch 95: Val Loss 261.25024
Epoch 96: Val Loss 257.64008
Epoch 97: Val Loss 260.76312
Epoch 98: Val Loss 262.58328
Epoch 99: Val Loss 256.12378
{'MSE - mean': 211.26482570529618, 'MSE - std': 44.85892958333413, 'R2 - mean': 0.7967716017265705, 'R2 - std': 0.040681132521425634} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2802.82202
Epoch 1: Val Loss 2798.18311
Epoch 2: Val Loss 2793.11475
Epoch 3: Val Loss 2786.97510
Epoch 4: Val Loss 2778.83667
Epoch 5: Val Loss 2768.42798
Epoch 6: Val Loss 2754.64404
Epoch 7: Val Loss 2737.21484
Epoch 8: Val Loss 2714.65991
Epoch 9: Val Loss 2685.55249
Epoch 10: Val Loss 2648.99463
Epoch 11: Val Loss 2604.55786
Epoch 12: Val Loss 2553.23901
Epoch 13: Val Loss 2494.50342
Epoch 14: Val Loss 2426.95654
Epoch 15: Val Loss 2353.33008
Epoch 16: Val Loss 2270.37646
Epoch 17: Val Loss 2184.88330
Epoch 18: Val Loss 2096.65820
Epoch 19: Val Loss 2016.24426
Epoch 20: Val Loss 1936.47754
Epoch 21: Val Loss 1867.84241
Epoch 22: Val Loss 1805.52209
Epoch 23: Val Loss 1748.21777
Epoch 24: Val Loss 1690.86389
Epoch 25: Val Loss 1629.66797
Epoch 26: Val Loss 1579.87988
Epoch 27: Val Loss 1532.88855
Epoch 28: Val Loss 1481.06396
Epoch 29: Val Loss 1433.21008
Epoch 30: Val Loss 1386.22461
Epoch 31: Val Loss 1329.58740
Epoch 32: Val Loss 1286.21533
Epoch 33: Val Loss 1245.31006
Epoch 34: Val Loss 1205.27820
Epoch 35: Val Loss 1172.05847
Epoch 36: Val Loss 1141.17590
Epoch 37: Val Loss 1105.93298
Epoch 38: Val Loss 1075.05469
Epoch 39: Val Loss 1040.48987
Epoch 40: Val Loss 1017.22839
Epoch 41: Val Loss 990.67633
Epoch 42: Val Loss 967.37524
Epoch 43: Val Loss 937.37683
Epoch 44: Val Loss 920.81262
Epoch 45: Val Loss 909.30042
Epoch 46: Val Loss 883.49011
Epoch 47: Val Loss 867.63123
Epoch 48: Val Loss 851.59583
Epoch 49: Val Loss 837.56458
Epoch 50: Val Loss 814.59418
Epoch 51: Val Loss 805.39185
Epoch 52: Val Loss 795.44684
Epoch 53: Val Loss 788.04004
Epoch 54: Val Loss 774.03027
Epoch 55: Val Loss 754.87689
Epoch 56: Val Loss 744.69440
Epoch 57: Val Loss 725.39587
Epoch 58: Val Loss 722.21991
Epoch 59: Val Loss 705.05566
Epoch 60: Val Loss 696.34167
Epoch 61: Val Loss 691.80048
Epoch 62: Val Loss 681.80261
Epoch 63: Val Loss 669.46051
Epoch 64: Val Loss 663.37946
Epoch 65: Val Loss 659.37396
Epoch 66: Val Loss 644.45941
Epoch 67: Val Loss 640.00311
Epoch 68: Val Loss 627.78052
Epoch 69: Val Loss 619.60431
Epoch 70: Val Loss 612.31671
Epoch 71: Val Loss 608.71954
Epoch 72: Val Loss 601.65985
Epoch 73: Val Loss 598.77612
Epoch 74: Val Loss 591.96143
Epoch 75: Val Loss 586.69464
Epoch 76: Val Loss 582.17395
Epoch 77: Val Loss 576.88745
Epoch 78: Val Loss 568.53058
Epoch 79: Val Loss 558.54340
Epoch 80: Val Loss 558.25452
Epoch 81: Val Loss 557.59686
Epoch 82: Val Loss 548.74335
Epoch 83: Val Loss 536.27167
Epoch 84: Val Loss 531.14136
Epoch 85: Val Loss 528.69556
Epoch 86: Val Loss 528.42175
Epoch 87: Val Loss 522.97498
Epoch 88: Val Loss 520.11139
Epoch 89: Val Loss 514.93042
Epoch 90: Val Loss 510.17416
Epoch 91: Val Loss 509.02066
Epoch 92: Val Loss 504.96848
Epoch 93: Val Loss 498.15500
Epoch 94: Val Loss 482.62790
Epoch 95: Val Loss 478.10706
Epoch 96: Val Loss 481.36133
Epoch 97: Val Loss 480.14514
Epoch 98: Val Loss 463.11694
Epoch 99: Val Loss 455.57782
{'MSE - mean': 292.70248030392094, 'MSE - std': 120.85417748382343, 'R2 - mean': 0.8025296224430646, 'R2 - std': 0.03419959990936072} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2113.01440
Epoch 1: Val Loss 2108.78320
Epoch 2: Val Loss 2104.18579
Epoch 3: Val Loss 2098.32593
Epoch 4: Val Loss 2090.31104
Epoch 5: Val Loss 2080.30786
Epoch 6: Val Loss 2067.83569
Epoch 7: Val Loss 2051.29614
Epoch 8: Val Loss 2029.68884
Epoch 9: Val Loss 2002.76379
Epoch 10: Val Loss 1967.98462
Epoch 11: Val Loss 1924.94470
Epoch 12: Val Loss 1875.17285
Epoch 13: Val Loss 1816.10498
Epoch 14: Val Loss 1747.91248
Epoch 15: Val Loss 1669.06763
Epoch 16: Val Loss 1586.37512
Epoch 17: Val Loss 1505.64075
Epoch 18: Val Loss 1423.48022
Epoch 19: Val Loss 1347.99084
Epoch 20: Val Loss 1278.68762
Epoch 21: Val Loss 1220.60071
Epoch 22: Val Loss 1166.45605
Epoch 23: Val Loss 1113.52283
Epoch 24: Val Loss 1062.64355
Epoch 25: Val Loss 1010.59692
Epoch 26: Val Loss 958.24860
Epoch 27: Val Loss 913.07245
Epoch 28: Val Loss 869.18958
Epoch 29: Val Loss 827.32434
Epoch 30: Val Loss 788.59125
Epoch 31: Val Loss 754.61487
Epoch 32: Val Loss 725.72394
Epoch 33: Val Loss 699.11255
Epoch 34: Val Loss 674.97278
Epoch 35: Val Loss 651.42572
Epoch 36: Val Loss 630.94739
Epoch 37: Val Loss 613.37549
Epoch 38: Val Loss 595.58899
Epoch 39: Val Loss 579.57251
Epoch 40: Val Loss 564.58435
Epoch 41: Val Loss 549.02197
Epoch 42: Val Loss 534.11542
Epoch 43: Val Loss 524.77948
Epoch 44: Val Loss 517.68652
Epoch 45: Val Loss 512.95239
Epoch 46: Val Loss 501.92606
Epoch 47: Val Loss 491.67188
Epoch 48: Val Loss 484.97284
Epoch 49: Val Loss 473.99512
Epoch 50: Val Loss 467.16885
Epoch 51: Val Loss 464.22098
Epoch 52: Val Loss 455.02106
Epoch 53: Val Loss 450.21896
Epoch 54: Val Loss 444.97549
Epoch 55: Val Loss 440.20370
Epoch 56: Val Loss 436.31293
Epoch 57: Val Loss 431.51303
Epoch 58: Val Loss 428.10904
Epoch 59: Val Loss 427.82288
Epoch 60: Val Loss 420.40463
Epoch 61: Val Loss 411.44684
Epoch 62: Val Loss 418.95645
Epoch 63: Val Loss 412.95801
Epoch 64: Val Loss 406.54678
Epoch 65: Val Loss 396.72800
Epoch 66: Val Loss 392.16400
Epoch 67: Val Loss 389.17526
Epoch 68: Val Loss 386.87772
Epoch 69: Val Loss 384.62503
Epoch 70: Val Loss 383.11719
Epoch 71: Val Loss 381.99197
Epoch 72: Val Loss 378.20700
Epoch 73: Val Loss 374.11832
Epoch 74: Val Loss 374.46808
Epoch 75: Val Loss 369.89185
Epoch 76: Val Loss 363.46436
Epoch 77: Val Loss 362.57748
Epoch 78: Val Loss 360.38303
Epoch 79: Val Loss 363.10010
Epoch 80: Val Loss 359.87033
Epoch 81: Val Loss 352.90591
Epoch 82: Val Loss 352.39215
Epoch 83: Val Loss 355.99420
Epoch 84: Val Loss 354.71957
Epoch 85: Val Loss 348.97696
Epoch 86: Val Loss 347.60635
Epoch 87: Val Loss 349.04468
Epoch 88: Val Loss 348.32385
Epoch 89: Val Loss 348.08615
Epoch 90: Val Loss 346.11508
Epoch 91: Val Loss 339.73239
Epoch 92: Val Loss 334.75443
Epoch 93: Val Loss 330.70480
Epoch 94: Val Loss 332.66837
Epoch 95: Val Loss 338.32410
Epoch 96: Val Loss 333.28427
Epoch 97: Val Loss 337.88666
Epoch 98: Val Loss 333.98700
Epoch 99: Val Loss 336.59308
{'MSE - mean': 302.2030665411492, 'MSE - std': 105.94849021114769, 'R2 - mean': 0.8039243956483964, 'R2 - std': 0.029716084068281835} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2532.55200
Epoch 1: Val Loss 2530.39038
Epoch 2: Val Loss 2527.99951
Epoch 3: Val Loss 2524.99097
Epoch 4: Val Loss 2520.96191
Epoch 5: Val Loss 2515.43774
Epoch 6: Val Loss 2508.16846
Epoch 7: Val Loss 2499.29297
Epoch 8: Val Loss 2487.52979
Epoch 9: Val Loss 2473.14771
Epoch 10: Val Loss 2455.19580
Epoch 11: Val Loss 2432.60107
Epoch 12: Val Loss 2405.20264
Epoch 13: Val Loss 2371.45239
Epoch 14: Val Loss 2331.49072
Epoch 15: Val Loss 2286.18506
Epoch 16: Val Loss 2235.12622
Epoch 17: Val Loss 2176.34302
Epoch 18: Val Loss 2110.41235
Epoch 19: Val Loss 2046.10498
Epoch 20: Val Loss 1978.41772
Epoch 21: Val Loss 1907.66455
Epoch 22: Val Loss 1839.33459
Epoch 23: Val Loss 1781.86133
Epoch 24: Val Loss 1731.03625
Epoch 25: Val Loss 1680.86670
Epoch 26: Val Loss 1634.95508
Epoch 27: Val Loss 1596.19800
Epoch 28: Val Loss 1560.14856
Epoch 29: Val Loss 1530.68323
Epoch 30: Val Loss 1491.84839
Epoch 31: Val Loss 1458.46350
Epoch 32: Val Loss 1421.02783
Epoch 33: Val Loss 1389.41223
Epoch 34: Val Loss 1357.72632
Epoch 35: Val Loss 1323.84521
Epoch 36: Val Loss 1284.34631
Epoch 37: Val Loss 1245.25500
Epoch 38: Val Loss 1212.07776
Epoch 39: Val Loss 1179.12610
Epoch 40: Val Loss 1143.05334
Epoch 41: Val Loss 1103.80334
Epoch 42: Val Loss 1064.19641
Epoch 43: Val Loss 1032.79626
Epoch 44: Val Loss 1004.93250
Epoch 45: Val Loss 979.87079
Epoch 46: Val Loss 953.38202
Epoch 47: Val Loss 924.10089
Epoch 48: Val Loss 900.08313
Epoch 49: Val Loss 883.59589
Epoch 50: Val Loss 860.42426
Epoch 51: Val Loss 846.25922
Epoch 52: Val Loss 825.94495
Epoch 53: Val Loss 802.62073
Epoch 54: Val Loss 785.47308
Epoch 55: Val Loss 774.43732
Epoch 56: Val Loss 758.37506
Epoch 57: Val Loss 745.66071
Epoch 58: Val Loss 735.19586
Epoch 59: Val Loss 711.46252
Epoch 60: Val Loss 700.18304
Epoch 61: Val Loss 695.51294
Epoch 62: Val Loss 672.70129
Epoch 63: Val Loss 664.20258
Epoch 64: Val Loss 656.89069
Epoch 65: Val Loss 649.57355
Epoch 66: Val Loss 641.63373
Epoch 67: Val Loss 633.54236
Epoch 68: Val Loss 628.92798
Epoch 69: Val Loss 616.92426
Epoch 70: Val Loss 599.37421
Epoch 71: Val Loss 598.30182
Epoch 72: Val Loss 591.71295
Epoch 73: Val Loss 582.64026
Epoch 74: Val Loss 570.00580
Epoch 75: Val Loss 567.84631
Epoch 76: Val Loss 563.81354
Epoch 77: Val Loss 551.95184
Epoch 78: Val Loss 547.83868
Epoch 79: Val Loss 545.18903
Epoch 80: Val Loss 542.15552
Epoch 81: Val Loss 532.17175
Epoch 82: Val Loss 523.21161
Epoch 83: Val Loss 517.20239
Epoch 84: Val Loss 508.78848
Epoch 85: Val Loss 507.02063
Epoch 86: Val Loss 504.86337
Epoch 87: Val Loss 500.43661
Epoch 88: Val Loss 495.79404
Epoch 89: Val Loss 492.21793
Epoch 90: Val Loss 488.38345
Epoch 91: Val Loss 482.79199
Epoch 92: Val Loss 479.51880
Epoch 93: Val Loss 481.07901
Epoch 94: Val Loss 473.30743
Epoch 95: Val Loss 474.14206
Epoch 96: Val Loss 466.70255
Epoch 97: Val Loss 461.50131
Epoch 98: Val Loss 455.26385
Epoch 99: Val Loss 450.70529
{'MSE - mean': 331.9035010561821, 'MSE - std': 111.84153657717853, 'R2 - mean': 0.8009993947669377, 'R2 - std': 0.027215051763843652} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 331.9035010561821 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 14 with value: 329.26580985652555.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1281.49927
Epoch 1: Val Loss 1281.22607
Epoch 2: Val Loss 1280.96313
Epoch 3: Val Loss 1280.69897
Epoch 4: Val Loss 1280.44202
Epoch 5: Val Loss 1280.19775
Epoch 6: Val Loss 1279.95203
Epoch 7: Val Loss 1279.71179
Epoch 8: Val Loss 1279.47327
Epoch 9: Val Loss 1279.24390
Epoch 10: Val Loss 1279.01611
Epoch 11: Val Loss 1278.79932
Epoch 12: Val Loss 1278.58459
Epoch 13: Val Loss 1278.36938
Epoch 14: Val Loss 1278.15942
Epoch 15: Val Loss 1277.95532
Epoch 16: Val Loss 1277.75281
Epoch 17: Val Loss 1277.54468
Epoch 18: Val Loss 1277.33948
Epoch 19: Val Loss 1277.13953
Epoch 20: Val Loss 1276.94299
Epoch 21: Val Loss 1276.73718
Epoch 22: Val Loss 1276.52173
Epoch 23: Val Loss 1276.30249
Epoch 24: Val Loss 1276.09387
Epoch 25: Val Loss 1275.88367
Epoch 26: Val Loss 1275.66675
Epoch 27: Val Loss 1275.45312
Epoch 28: Val Loss 1275.23389
Epoch 29: Val Loss 1275.02185
Epoch 30: Val Loss 1274.79773
Epoch 31: Val Loss 1274.57568
Epoch 32: Val Loss 1274.35107
Epoch 33: Val Loss 1274.12646
Epoch 34: Val Loss 1273.89600
Epoch 35: Val Loss 1273.66931
Epoch 36: Val Loss 1273.42493
Epoch 37: Val Loss 1273.17688
Epoch 38: Val Loss 1272.92822
Epoch 39: Val Loss 1272.68274
Epoch 40: Val Loss 1272.43176
Epoch 41: Val Loss 1272.18286
Epoch 42: Val Loss 1271.93103
Epoch 43: Val Loss 1271.66528
Epoch 44: Val Loss 1271.37952
Epoch 45: Val Loss 1271.08765
Epoch 46: Val Loss 1270.78735
Epoch 47: Val Loss 1270.48865
Epoch 48: Val Loss 1270.19128
Epoch 49: Val Loss 1269.89233
Epoch 50: Val Loss 1269.58105
Epoch 51: Val Loss 1269.25964
Epoch 52: Val Loss 1268.91882
Epoch 53: Val Loss 1268.57959
Epoch 54: Val Loss 1268.23279
Epoch 55: Val Loss 1267.87549
Epoch 56: Val Loss 1267.50916
Epoch 57: Val Loss 1267.14807
Epoch 58: Val Loss 1266.76294
Epoch 59: Val Loss 1266.37097
Epoch 60: Val Loss 1265.97644
Epoch 61: Val Loss 1265.59485
Epoch 62: Val Loss 1265.19531
Epoch 63: Val Loss 1264.77234
Epoch 64: Val Loss 1264.36609
Epoch 65: Val Loss 1263.94690
Epoch 66: Val Loss 1263.51587
Epoch 67: Val Loss 1263.07410
Epoch 68: Val Loss 1262.61926
Epoch 69: Val Loss 1262.14050
Epoch 70: Val Loss 1261.66614
Epoch 71: Val Loss 1261.18372
Epoch 72: Val Loss 1260.69482
Epoch 73: Val Loss 1260.19458
Epoch 74: Val Loss 1259.68982
Epoch 75: Val Loss 1259.18530
Epoch 76: Val Loss 1258.67834
Epoch 77: Val Loss 1258.16516
Epoch 78: Val Loss 1257.61829
Epoch 79: Val Loss 1257.01965
Epoch 80: Val Loss 1256.40515
Epoch 81: Val Loss 1255.76514
Epoch 82: Val Loss 1255.10950
Epoch 83: Val Loss 1254.40076
Epoch 84: Val Loss 1253.68396
Epoch 85: Val Loss 1252.93835
Epoch 86: Val Loss 1252.15674
Epoch 87: Val Loss 1251.30762
Epoch 88: Val Loss 1250.39819
Epoch 89: Val Loss 1249.47632
Epoch 90: Val Loss 1248.48779
Epoch 91: Val Loss 1247.43799
Epoch 92: Val Loss 1246.31677
Epoch 93: Val Loss 1245.21448
Epoch 94: Val Loss 1244.07117
Epoch 95: Val Loss 1242.91516
Epoch 96: Val Loss 1241.75513
Epoch 97: Val Loss 1240.54346
Epoch 98: Val Loss 1239.32153
Epoch 99: Val Loss 1238.06140
{'MSE - mean': 1238.061418466917, 'MSE - std': 0.0, 'R2 - mean': -0.20935317254233343, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1309.57251
Epoch 1: Val Loss 1309.27246
Epoch 2: Val Loss 1308.98401
Epoch 3: Val Loss 1308.70288
Epoch 4: Val Loss 1308.44580
Epoch 5: Val Loss 1308.18896
Epoch 6: Val Loss 1307.94287
Epoch 7: Val Loss 1307.69604
Epoch 8: Val Loss 1307.45886
Epoch 9: Val Loss 1307.22095
Epoch 10: Val Loss 1306.98975
Epoch 11: Val Loss 1306.74854
Epoch 12: Val Loss 1306.50757
Epoch 13: Val Loss 1306.27405
Epoch 14: Val Loss 1306.03772
Epoch 15: Val Loss 1305.80090
Epoch 16: Val Loss 1305.57031
Epoch 17: Val Loss 1305.33716
Epoch 18: Val Loss 1305.10254
Epoch 19: Val Loss 1304.85620
Epoch 20: Val Loss 1304.60913
Epoch 21: Val Loss 1304.34949
Epoch 22: Val Loss 1304.08472
Epoch 23: Val Loss 1303.81860
Epoch 24: Val Loss 1303.54053
Epoch 25: Val Loss 1303.25757
Epoch 26: Val Loss 1302.97546
Epoch 27: Val Loss 1302.68774
Epoch 28: Val Loss 1302.38110
Epoch 29: Val Loss 1302.04919
Epoch 30: Val Loss 1301.71521
Epoch 31: Val Loss 1301.34253
Epoch 32: Val Loss 1300.96533
Epoch 33: Val Loss 1300.56018
Epoch 34: Val Loss 1300.12585
Epoch 35: Val Loss 1299.66272
Epoch 36: Val Loss 1299.18518
Epoch 37: Val Loss 1298.70166
Epoch 38: Val Loss 1298.19519
Epoch 39: Val Loss 1297.67224
Epoch 40: Val Loss 1297.09583
Epoch 41: Val Loss 1296.50476
Epoch 42: Val Loss 1295.88196
Epoch 43: Val Loss 1295.21082
Epoch 44: Val Loss 1294.55786
Epoch 45: Val Loss 1293.87097
Epoch 46: Val Loss 1293.14050
Epoch 47: Val Loss 1292.40039
Epoch 48: Val Loss 1291.61133
Epoch 49: Val Loss 1290.79773
Epoch 50: Val Loss 1289.98743
Epoch 51: Val Loss 1289.14392
Epoch 52: Val Loss 1288.26196
Epoch 53: Val Loss 1287.31824
Epoch 54: Val Loss 1286.34875
Epoch 55: Val Loss 1285.34448
Epoch 56: Val Loss 1284.32446
Epoch 57: Val Loss 1283.26746
Epoch 58: Val Loss 1282.16833
Epoch 59: Val Loss 1281.03931
Epoch 60: Val Loss 1279.84009
Epoch 61: Val Loss 1278.58008
Epoch 62: Val Loss 1277.33411
Epoch 63: Val Loss 1276.01221
Epoch 64: Val Loss 1274.67017
Epoch 65: Val Loss 1273.26794
Epoch 66: Val Loss 1271.80359
Epoch 67: Val Loss 1270.34851
Epoch 68: Val Loss 1268.87720
Epoch 69: Val Loss 1267.39600
Epoch 70: Val Loss 1265.84363
Epoch 71: Val Loss 1264.13196
Epoch 72: Val Loss 1262.46594
Epoch 73: Val Loss 1260.78455
Epoch 74: Val Loss 1259.02917
Epoch 75: Val Loss 1257.21704
Epoch 76: Val Loss 1255.22705
Epoch 77: Val Loss 1253.22180
Epoch 78: Val Loss 1251.14978
Epoch 79: Val Loss 1248.96777
Epoch 80: Val Loss 1246.83740
Epoch 81: Val Loss 1244.46716
Epoch 82: Val Loss 1242.02612
Epoch 83: Val Loss 1239.39355
Epoch 84: Val Loss 1236.62207
Epoch 85: Val Loss 1233.72375
Epoch 86: Val Loss 1230.73010
Epoch 87: Val Loss 1227.48511
Epoch 88: Val Loss 1224.24951
Epoch 89: Val Loss 1220.88464
Epoch 90: Val Loss 1217.47864
Epoch 91: Val Loss 1214.08875
Epoch 92: Val Loss 1210.76575
Epoch 93: Val Loss 1207.43042
Epoch 94: Val Loss 1203.93896
Epoch 95: Val Loss 1200.38782
Epoch 96: Val Loss 1196.65637
Epoch 97: Val Loss 1192.87500
Epoch 98: Val Loss 1188.97351
Epoch 99: Val Loss 1184.94287
{'MSE - mean': 1211.5021255783563, 'MSE - std': 26.559292888560776, 'R2 - mean': -0.16889377497823166, 'R2 - std': 0.04045939756410177} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2822.96167
Epoch 1: Val Loss 2822.75659
Epoch 2: Val Loss 2822.54565
Epoch 3: Val Loss 2822.33398
Epoch 4: Val Loss 2822.10986
Epoch 5: Val Loss 2821.88428
Epoch 6: Val Loss 2821.63257
Epoch 7: Val Loss 2821.36206
Epoch 8: Val Loss 2821.09521
Epoch 9: Val Loss 2820.80518
Epoch 10: Val Loss 2820.50269
Epoch 11: Val Loss 2820.17163
Epoch 12: Val Loss 2819.82300
Epoch 13: Val Loss 2819.47510
Epoch 14: Val Loss 2819.10864
Epoch 15: Val Loss 2818.72803
Epoch 16: Val Loss 2818.35425
Epoch 17: Val Loss 2817.96265
Epoch 18: Val Loss 2817.54761
Epoch 19: Val Loss 2817.14014
Epoch 20: Val Loss 2816.70996
Epoch 21: Val Loss 2816.26050
Epoch 22: Val Loss 2815.80908
Epoch 23: Val Loss 2815.30396
Epoch 24: Val Loss 2814.77808
Epoch 25: Val Loss 2814.24805
Epoch 26: Val Loss 2813.71484
Epoch 27: Val Loss 2813.18066
Epoch 28: Val Loss 2812.63281
Epoch 29: Val Loss 2812.04907
Epoch 30: Val Loss 2811.41040
Epoch 31: Val Loss 2810.75269
Epoch 32: Val Loss 2810.09424
Epoch 33: Val Loss 2809.35522
Epoch 34: Val Loss 2808.57764
Epoch 35: Val Loss 2807.78760
Epoch 36: Val Loss 2806.96973
Epoch 37: Val Loss 2806.12671
Epoch 38: Val Loss 2805.22729
Epoch 39: Val Loss 2804.29688
Epoch 40: Val Loss 2803.34668
Epoch 41: Val Loss 2802.38208
Epoch 42: Val Loss 2801.35986
Epoch 43: Val Loss 2800.34253
Epoch 44: Val Loss 2799.26270
Epoch 45: Val Loss 2798.09351
Epoch 46: Val Loss 2796.89722
Epoch 47: Val Loss 2795.71191
Epoch 48: Val Loss 2794.48047
Epoch 49: Val Loss 2793.19165
Epoch 50: Val Loss 2791.87109
Epoch 51: Val Loss 2790.50806
Epoch 52: Val Loss 2788.97754
Epoch 53: Val Loss 2787.47192
Epoch 54: Val Loss 2785.91040
Epoch 55: Val Loss 2784.26782
Epoch 56: Val Loss 2782.41602
Epoch 57: Val Loss 2780.57031
Epoch 58: Val Loss 2778.65894
Epoch 59: Val Loss 2776.66772
Epoch 60: Val Loss 2774.61255
Epoch 61: Val Loss 2772.57300
Epoch 62: Val Loss 2770.50708
Epoch 63: Val Loss 2768.29150
Epoch 64: Val Loss 2765.93701
Epoch 65: Val Loss 2763.53418
Epoch 66: Val Loss 2761.15454
Epoch 67: Val Loss 2758.66479
Epoch 68: Val Loss 2756.01709
Epoch 69: Val Loss 2753.41113
Epoch 70: Val Loss 2750.65112
Epoch 71: Val Loss 2747.84717
Epoch 72: Val Loss 2745.07300
Epoch 73: Val Loss 2742.14673
Epoch 74: Val Loss 2739.12671
Epoch 75: Val Loss 2735.94800
Epoch 76: Val Loss 2732.66357
Epoch 77: Val Loss 2729.25439
Epoch 78: Val Loss 2725.84351
Epoch 79: Val Loss 2722.24390
Epoch 80: Val Loss 2718.71069
Epoch 81: Val Loss 2714.87427
Epoch 82: Val Loss 2711.16821
Epoch 83: Val Loss 2707.32642
Epoch 84: Val Loss 2703.59009
Epoch 85: Val Loss 2699.37427
Epoch 86: Val Loss 2695.20435
Epoch 87: Val Loss 2691.15820
Epoch 88: Val Loss 2686.85718
Epoch 89: Val Loss 2682.52100
Epoch 90: Val Loss 2678.05151
Epoch 91: Val Loss 2673.52490
Epoch 92: Val Loss 2668.93066
Epoch 93: Val Loss 2664.11499
Epoch 94: Val Loss 2658.97925
Epoch 95: Val Loss 2653.71167
Epoch 96: Val Loss 2648.36401
Epoch 97: Val Loss 2643.19897
Epoch 98: Val Loss 2637.92090
Epoch 99: Val Loss 2632.43457
{'MSE - mean': 1685.1462930531222, 'MSE - std': 670.1849437219214, 'R2 - mean': -0.13742501192795928, 'R2 - std': 0.05542449515627911} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2129.46240
Epoch 1: Val Loss 2129.13501
Epoch 2: Val Loss 2128.80054
Epoch 3: Val Loss 2128.44263
Epoch 4: Val Loss 2128.07959
Epoch 5: Val Loss 2127.68335
Epoch 6: Val Loss 2127.29224
Epoch 7: Val Loss 2126.89429
Epoch 8: Val Loss 2126.45874
Epoch 9: Val Loss 2126.00317
Epoch 10: Val Loss 2125.52197
Epoch 11: Val Loss 2125.04346
Epoch 12: Val Loss 2124.54321
Epoch 13: Val Loss 2124.01978
Epoch 14: Val Loss 2123.47778
Epoch 15: Val Loss 2122.92358
Epoch 16: Val Loss 2122.36475
Epoch 17: Val Loss 2121.79834
Epoch 18: Val Loss 2121.23486
Epoch 19: Val Loss 2120.64258
Epoch 20: Val Loss 2120.04785
Epoch 21: Val Loss 2119.43408
Epoch 22: Val Loss 2118.77368
Epoch 23: Val Loss 2118.08154
Epoch 24: Val Loss 2117.37939
Epoch 25: Val Loss 2116.67529
Epoch 26: Val Loss 2115.96606
Epoch 27: Val Loss 2115.24707
Epoch 28: Val Loss 2114.50830
Epoch 29: Val Loss 2113.73047
Epoch 30: Val Loss 2112.89966
Epoch 31: Val Loss 2112.09033
Epoch 32: Val Loss 2111.25269
Epoch 33: Val Loss 2110.36670
Epoch 34: Val Loss 2109.47095
Epoch 35: Val Loss 2108.56177
Epoch 36: Val Loss 2107.63525
Epoch 37: Val Loss 2106.67798
Epoch 38: Val Loss 2105.69922
Epoch 39: Val Loss 2104.68457
Epoch 40: Val Loss 2103.64404
Epoch 41: Val Loss 2102.57544
Epoch 42: Val Loss 2101.44922
Epoch 43: Val Loss 2100.20068
Epoch 44: Val Loss 2098.97998
Epoch 45: Val Loss 2097.79736
Epoch 46: Val Loss 2096.56616
Epoch 47: Val Loss 2095.29272
Epoch 48: Val Loss 2093.96533
Epoch 49: Val Loss 2092.62305
Epoch 50: Val Loss 2091.18750
Epoch 51: Val Loss 2089.66284
Epoch 52: Val Loss 2088.00293
Epoch 53: Val Loss 2086.42944
Epoch 54: Val Loss 2084.79102
Epoch 55: Val Loss 2083.19482
Epoch 56: Val Loss 2081.41479
Epoch 57: Val Loss 2079.56982
Epoch 58: Val Loss 2077.70044
Epoch 59: Val Loss 2075.71240
Epoch 60: Val Loss 2073.71851
Epoch 61: Val Loss 2071.70898
Epoch 62: Val Loss 2069.67700
Epoch 63: Val Loss 2067.54443
Epoch 64: Val Loss 2065.45630
Epoch 65: Val Loss 2063.22339
Epoch 66: Val Loss 2060.93530
Epoch 67: Val Loss 2058.53564
Epoch 68: Val Loss 2056.06592
Epoch 69: Val Loss 2053.53101
Epoch 70: Val Loss 2050.88208
Epoch 71: Val Loss 2048.18677
Epoch 72: Val Loss 2045.24707
Epoch 73: Val Loss 2042.38513
Epoch 74: Val Loss 2039.37292
Epoch 75: Val Loss 2036.24329
Epoch 76: Val Loss 2033.18933
Epoch 77: Val Loss 2029.92676
Epoch 78: Val Loss 2026.65723
Epoch 79: Val Loss 2023.22461
Epoch 80: Val Loss 2019.77368
Epoch 81: Val Loss 2016.28674
Epoch 82: Val Loss 2012.77136
Epoch 83: Val Loss 2009.11682
Epoch 84: Val Loss 2005.40173
Epoch 85: Val Loss 2001.43372
Epoch 86: Val Loss 1997.41052
Epoch 87: Val Loss 1993.12305
Epoch 88: Val Loss 1988.86169
Epoch 89: Val Loss 1984.69202
Epoch 90: Val Loss 1980.23425
Epoch 91: Val Loss 1975.38696
Epoch 92: Val Loss 1970.70142
Epoch 93: Val Loss 1966.01575
Epoch 94: Val Loss 1961.36279
Epoch 95: Val Loss 1956.54297
Epoch 96: Val Loss 1951.73010
Epoch 97: Val Loss 1946.88708
Epoch 98: Val Loss 1941.73926
Epoch 99: Val Loss 1936.68762
{'MSE - mean': 1748.0316274871827, 'MSE - std': 590.5290762988027, 'R2 - mean': -0.1340092300440307, 'R2 - std': 0.04836226519852052} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2532.36255
Epoch 1: Val Loss 2532.17456
Epoch 2: Val Loss 2531.98169
Epoch 3: Val Loss 2531.78223
Epoch 4: Val Loss 2531.57080
Epoch 5: Val Loss 2531.35669
Epoch 6: Val Loss 2531.14062
Epoch 7: Val Loss 2530.92310
Epoch 8: Val Loss 2530.69702
Epoch 9: Val Loss 2530.46826
Epoch 10: Val Loss 2530.22437
Epoch 11: Val Loss 2529.98657
Epoch 12: Val Loss 2529.72949
Epoch 13: Val Loss 2529.46973
Epoch 14: Val Loss 2529.18945
Epoch 15: Val Loss 2528.90063
Epoch 16: Val Loss 2528.60596
Epoch 17: Val Loss 2528.31714
Epoch 18: Val Loss 2528.02075
Epoch 19: Val Loss 2527.72437
Epoch 20: Val Loss 2527.42432
Epoch 21: Val Loss 2527.11450
Epoch 22: Val Loss 2526.78955
Epoch 23: Val Loss 2526.45190
Epoch 24: Val Loss 2526.11426
Epoch 25: Val Loss 2525.77393
Epoch 26: Val Loss 2525.42090
Epoch 27: Val Loss 2525.03467
Epoch 28: Val Loss 2524.63940
Epoch 29: Val Loss 2524.22754
Epoch 30: Val Loss 2523.80737
Epoch 31: Val Loss 2523.38354
Epoch 32: Val Loss 2522.96313
Epoch 33: Val Loss 2522.50659
Epoch 34: Val Loss 2522.05005
Epoch 35: Val Loss 2521.57520
Epoch 36: Val Loss 2521.07495
Epoch 37: Val Loss 2520.52686
Epoch 38: Val Loss 2519.98608
Epoch 39: Val Loss 2519.41553
Epoch 40: Val Loss 2518.84351
Epoch 41: Val Loss 2518.21362
Epoch 42: Val Loss 2517.55957
Epoch 43: Val Loss 2516.90332
Epoch 44: Val Loss 2516.20947
Epoch 45: Val Loss 2515.50903
Epoch 46: Val Loss 2514.77905
Epoch 47: Val Loss 2514.02808
Epoch 48: Val Loss 2513.27661
Epoch 49: Val Loss 2512.48608
Epoch 50: Val Loss 2511.66724
Epoch 51: Val Loss 2510.84814
Epoch 52: Val Loss 2510.01782
Epoch 53: Val Loss 2509.12256
Epoch 54: Val Loss 2508.17529
Epoch 55: Val Loss 2507.23145
Epoch 56: Val Loss 2506.24390
Epoch 57: Val Loss 2505.27563
Epoch 58: Val Loss 2504.28003
Epoch 59: Val Loss 2503.22461
Epoch 60: Val Loss 2502.14014
Epoch 61: Val Loss 2500.97290
Epoch 62: Val Loss 2499.83643
Epoch 63: Val Loss 2498.64746
Epoch 64: Val Loss 2497.38257
Epoch 65: Val Loss 2496.07959
Epoch 66: Val Loss 2494.77612
Epoch 67: Val Loss 2493.45239
Epoch 68: Val Loss 2492.11572
Epoch 69: Val Loss 2490.73511
Epoch 70: Val Loss 2489.32690
Epoch 71: Val Loss 2487.88110
Epoch 72: Val Loss 2486.35083
Epoch 73: Val Loss 2484.68018
Epoch 74: Val Loss 2483.05054
Epoch 75: Val Loss 2481.42090
Epoch 76: Val Loss 2479.77319
Epoch 77: Val Loss 2478.08765
Epoch 78: Val Loss 2476.36450
Epoch 79: Val Loss 2474.60864
Epoch 80: Val Loss 2472.83984
Epoch 81: Val Loss 2471.04761
Epoch 82: Val Loss 2469.18188
Epoch 83: Val Loss 2467.29834
Epoch 84: Val Loss 2465.33716
Epoch 85: Val Loss 2463.30151
Epoch 86: Val Loss 2461.05762
Epoch 87: Val Loss 2458.88013
Epoch 88: Val Loss 2456.63257
Epoch 89: Val Loss 2454.30713
Epoch 90: Val Loss 2451.91577
Epoch 91: Val Loss 2449.55640
Epoch 92: Val Loss 2447.12231
Epoch 93: Val Loss 2444.71265
Epoch 94: Val Loss 2442.36792
Epoch 95: Val Loss 2439.74341
Epoch 96: Val Loss 2437.24048
Epoch 97: Val Loss 2434.66748
Epoch 98: Val Loss 2431.95288
Epoch 99: Val Loss 2429.34473
{'MSE - mean': 1884.2942427565606, 'MSE - std': 594.3481077984987, 'R2 - mean': -0.1343467041756517, 'R2 - std': 0.04326179043062916} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 1884.2942427565606 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -5, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 14 with value: 329.26580985652555.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1271.76392
Epoch 1: Val Loss 1271.54260
Epoch 2: Val Loss 1271.32605
Epoch 3: Val Loss 1271.10791
Epoch 4: Val Loss 1270.86865
Epoch 5: Val Loss 1270.63013
Epoch 6: Val Loss 1270.37512
Epoch 7: Val Loss 1270.12622
Epoch 8: Val Loss 1269.86768
Epoch 9: Val Loss 1269.60168
Epoch 10: Val Loss 1269.33484
Epoch 11: Val Loss 1269.05688
Epoch 12: Val Loss 1268.77368
Epoch 13: Val Loss 1268.48438
Epoch 14: Val Loss 1268.18506
Epoch 15: Val Loss 1267.86389
Epoch 16: Val Loss 1267.53638
Epoch 17: Val Loss 1267.19226
Epoch 18: Val Loss 1266.83093
Epoch 19: Val Loss 1266.46008
Epoch 20: Val Loss 1266.08838
Epoch 21: Val Loss 1265.71362
Epoch 22: Val Loss 1265.31360
Epoch 23: Val Loss 1264.88916
Epoch 24: Val Loss 1264.46252
Epoch 25: Val Loss 1264.03479
Epoch 26: Val Loss 1263.56616
Epoch 27: Val Loss 1263.08203
Epoch 28: Val Loss 1262.57861
Epoch 29: Val Loss 1262.08289
Epoch 30: Val Loss 1261.57202
Epoch 31: Val Loss 1261.02612
Epoch 32: Val Loss 1260.47351
Epoch 33: Val Loss 1259.91296
Epoch 34: Val Loss 1259.29065
Epoch 35: Val Loss 1258.67603
Epoch 36: Val Loss 1258.01880
Epoch 37: Val Loss 1257.35315
Epoch 38: Val Loss 1256.65186
Epoch 39: Val Loss 1255.89380
Epoch 40: Val Loss 1255.09106
Epoch 41: Val Loss 1254.20984
Epoch 42: Val Loss 1253.33569
Epoch 43: Val Loss 1252.39331
Epoch 44: Val Loss 1251.41724
Epoch 45: Val Loss 1250.37476
Epoch 46: Val Loss 1249.26526
Epoch 47: Val Loss 1248.05811
Epoch 48: Val Loss 1246.75049
Epoch 49: Val Loss 1245.32812
Epoch 50: Val Loss 1243.84619
Epoch 51: Val Loss 1242.30762
Epoch 52: Val Loss 1240.66443
Epoch 53: Val Loss 1238.92456
Epoch 54: Val Loss 1237.18262
Epoch 55: Val Loss 1235.38904
Epoch 56: Val Loss 1233.53674
Epoch 57: Val Loss 1231.68213
Epoch 58: Val Loss 1229.68372
Epoch 59: Val Loss 1227.49719
Epoch 60: Val Loss 1225.26318
Epoch 61: Val Loss 1222.91956
Epoch 62: Val Loss 1220.38135
Epoch 63: Val Loss 1217.76038
Epoch 64: Val Loss 1215.07373
Epoch 65: Val Loss 1212.32812
Epoch 66: Val Loss 1209.59619
Epoch 67: Val Loss 1206.89697
Epoch 68: Val Loss 1203.94348
Epoch 69: Val Loss 1200.81250
Epoch 70: Val Loss 1197.63074
Epoch 71: Val Loss 1194.20215
Epoch 72: Val Loss 1190.72217
Epoch 73: Val Loss 1187.19275
Epoch 74: Val Loss 1183.50171
Epoch 75: Val Loss 1179.68726
Epoch 76: Val Loss 1175.86963
Epoch 77: Val Loss 1171.88770
Epoch 78: Val Loss 1167.81384
Epoch 79: Val Loss 1163.46399
Epoch 80: Val Loss 1159.02356
Epoch 81: Val Loss 1154.59766
Epoch 82: Val Loss 1150.17883
Epoch 83: Val Loss 1145.67761
Epoch 84: Val Loss 1140.84692
Epoch 85: Val Loss 1136.20825
Epoch 86: Val Loss 1131.43274
Epoch 87: Val Loss 1126.48279
Epoch 88: Val Loss 1121.43811
Epoch 89: Val Loss 1116.47229
Epoch 90: Val Loss 1111.53479
Epoch 91: Val Loss 1106.35596
Epoch 92: Val Loss 1101.37646
Epoch 93: Val Loss 1096.35352
Epoch 94: Val Loss 1090.97620
Epoch 95: Val Loss 1085.55920
Epoch 96: Val Loss 1080.13757
Epoch 97: Val Loss 1074.34436
Epoch 98: Val Loss 1068.86975
Epoch 99: Val Loss 1062.85962
{'MSE - mean': 1062.8596510428274, 'MSE - std': 0.0, 'R2 - mean': -0.03821399470435738, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1298.53845
Epoch 1: Val Loss 1298.38025
Epoch 2: Val Loss 1298.23108
Epoch 3: Val Loss 1298.07959
Epoch 4: Val Loss 1297.93030
Epoch 5: Val Loss 1297.78198
Epoch 6: Val Loss 1297.63635
Epoch 7: Val Loss 1297.48462
Epoch 8: Val Loss 1297.33472
Epoch 9: Val Loss 1297.18469
Epoch 10: Val Loss 1297.03625
Epoch 11: Val Loss 1296.88806
Epoch 12: Val Loss 1296.72461
Epoch 13: Val Loss 1296.55750
Epoch 14: Val Loss 1296.38794
Epoch 15: Val Loss 1296.21948
Epoch 16: Val Loss 1296.05029
Epoch 17: Val Loss 1295.87720
Epoch 18: Val Loss 1295.69409
Epoch 19: Val Loss 1295.50830
Epoch 20: Val Loss 1295.31360
Epoch 21: Val Loss 1295.10852
Epoch 22: Val Loss 1294.90076
Epoch 23: Val Loss 1294.68237
Epoch 24: Val Loss 1294.44946
Epoch 25: Val Loss 1294.21265
Epoch 26: Val Loss 1293.95886
Epoch 27: Val Loss 1293.68848
Epoch 28: Val Loss 1293.41528
Epoch 29: Val Loss 1293.12341
Epoch 30: Val Loss 1292.82361
Epoch 31: Val Loss 1292.50049
Epoch 32: Val Loss 1292.16614
Epoch 33: Val Loss 1291.81116
Epoch 34: Val Loss 1291.42773
Epoch 35: Val Loss 1290.99573
Epoch 36: Val Loss 1290.54175
Epoch 37: Val Loss 1290.07751
Epoch 38: Val Loss 1289.59412
Epoch 39: Val Loss 1289.10071
Epoch 40: Val Loss 1288.58521
Epoch 41: Val Loss 1288.07629
Epoch 42: Val Loss 1287.54663
Epoch 43: Val Loss 1286.99536
Epoch 44: Val Loss 1286.39856
Epoch 45: Val Loss 1285.79468
Epoch 46: Val Loss 1285.19531
Epoch 47: Val Loss 1284.56274
Epoch 48: Val Loss 1283.87512
Epoch 49: Val Loss 1283.15100
Epoch 50: Val Loss 1282.39905
Epoch 51: Val Loss 1281.66516
Epoch 52: Val Loss 1280.88489
Epoch 53: Val Loss 1280.02759
Epoch 54: Val Loss 1279.14014
Epoch 55: Val Loss 1278.22595
Epoch 56: Val Loss 1277.26013
Epoch 57: Val Loss 1276.27698
Epoch 58: Val Loss 1275.22913
Epoch 59: Val Loss 1274.14294
Epoch 60: Val Loss 1273.08374
Epoch 61: Val Loss 1271.86902
Epoch 62: Val Loss 1270.64746
Epoch 63: Val Loss 1269.29126
Epoch 64: Val Loss 1267.91370
Epoch 65: Val Loss 1266.52734
Epoch 66: Val Loss 1265.03040
Epoch 67: Val Loss 1263.44604
Epoch 68: Val Loss 1261.82983
Epoch 69: Val Loss 1260.15320
Epoch 70: Val Loss 1258.34253
Epoch 71: Val Loss 1256.42615
Epoch 72: Val Loss 1254.54126
Epoch 73: Val Loss 1252.58398
Epoch 74: Val Loss 1250.63647
Epoch 75: Val Loss 1248.61145
Epoch 76: Val Loss 1246.37903
Epoch 77: Val Loss 1244.21423
Epoch 78: Val Loss 1241.80200
Epoch 79: Val Loss 1239.38989
Epoch 80: Val Loss 1236.99182
Epoch 81: Val Loss 1234.47778
Epoch 82: Val Loss 1231.90491
Epoch 83: Val Loss 1229.19641
Epoch 84: Val Loss 1226.36523
Epoch 85: Val Loss 1223.39697
Epoch 86: Val Loss 1220.36804
Epoch 87: Val Loss 1217.21045
Epoch 88: Val Loss 1213.92993
Epoch 89: Val Loss 1210.73047
Epoch 90: Val Loss 1207.05383
Epoch 91: Val Loss 1203.37927
Epoch 92: Val Loss 1199.70703
Epoch 93: Val Loss 1195.95105
Epoch 94: Val Loss 1191.90710
Epoch 95: Val Loss 1187.87891
Epoch 96: Val Loss 1183.80225
Epoch 97: Val Loss 1179.55859
Epoch 98: Val Loss 1175.14282
Epoch 99: Val Loss 1171.01160
{'MSE - mean': 1116.93558568676, 'MSE - std': 54.075934643932555, 'R2 - mean': -0.07669071339085298, 'R2 - std': 0.0384767186864956} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2826.05835
Epoch 1: Val Loss 2825.75293
Epoch 2: Val Loss 2825.44019
Epoch 3: Val Loss 2825.13647
Epoch 4: Val Loss 2824.81836
Epoch 5: Val Loss 2824.49951
Epoch 6: Val Loss 2824.18311
Epoch 7: Val Loss 2823.86279
Epoch 8: Val Loss 2823.54150
Epoch 9: Val Loss 2823.18896
Epoch 10: Val Loss 2822.83105
Epoch 11: Val Loss 2822.48096
Epoch 12: Val Loss 2822.16357
Epoch 13: Val Loss 2821.85498
Epoch 14: Val Loss 2821.53564
Epoch 15: Val Loss 2821.21582
Epoch 16: Val Loss 2820.89526
Epoch 17: Val Loss 2820.57202
Epoch 18: Val Loss 2820.24585
Epoch 19: Val Loss 2819.91357
Epoch 20: Val Loss 2819.57202
Epoch 21: Val Loss 2819.22681
Epoch 22: Val Loss 2818.86963
Epoch 23: Val Loss 2818.50708
Epoch 24: Val Loss 2818.11719
Epoch 25: Val Loss 2817.70605
Epoch 26: Val Loss 2817.30835
Epoch 27: Val Loss 2816.87305
Epoch 28: Val Loss 2816.44800
Epoch 29: Val Loss 2816.02612
Epoch 30: Val Loss 2815.59717
Epoch 31: Val Loss 2815.15430
Epoch 32: Val Loss 2814.67993
Epoch 33: Val Loss 2814.19482
Epoch 34: Val Loss 2813.69580
Epoch 35: Val Loss 2813.19824
Epoch 36: Val Loss 2812.66504
Epoch 37: Val Loss 2812.09302
Epoch 38: Val Loss 2811.51514
Epoch 39: Val Loss 2810.93555
Epoch 40: Val Loss 2810.30029
Epoch 41: Val Loss 2809.66650
Epoch 42: Val Loss 2809.03418
Epoch 43: Val Loss 2808.37500
Epoch 44: Val Loss 2807.64551
Epoch 45: Val Loss 2806.92505
Epoch 46: Val Loss 2806.20825
Epoch 47: Val Loss 2805.43896
Epoch 48: Val Loss 2804.70239
Epoch 49: Val Loss 2803.93994
Epoch 50: Val Loss 2803.14453
Epoch 51: Val Loss 2802.35596
Epoch 52: Val Loss 2801.50000
Epoch 53: Val Loss 2800.66772
Epoch 54: Val Loss 2799.78833
Epoch 55: Val Loss 2798.89014
Epoch 56: Val Loss 2797.98438
Epoch 57: Val Loss 2797.06812
Epoch 58: Val Loss 2796.16064
Epoch 59: Val Loss 2795.19165
Epoch 60: Val Loss 2794.22046
Epoch 61: Val Loss 2793.16528
Epoch 62: Val Loss 2792.03394
Epoch 63: Val Loss 2790.88525
Epoch 64: Val Loss 2789.75391
Epoch 65: Val Loss 2788.57690
Epoch 66: Val Loss 2787.34546
Epoch 67: Val Loss 2786.10815
Epoch 68: Val Loss 2784.82959
Epoch 69: Val Loss 2783.52319
Epoch 70: Val Loss 2782.20825
Epoch 71: Val Loss 2780.80420
Epoch 72: Val Loss 2779.36597
Epoch 73: Val Loss 2777.90674
Epoch 74: Val Loss 2776.42017
Epoch 75: Val Loss 2774.87402
Epoch 76: Val Loss 2773.35986
Epoch 77: Val Loss 2771.79810
Epoch 78: Val Loss 2770.13916
Epoch 79: Val Loss 2768.38525
Epoch 80: Val Loss 2766.54565
Epoch 81: Val Loss 2764.71582
Epoch 82: Val Loss 2762.82520
Epoch 83: Val Loss 2760.85791
Epoch 84: Val Loss 2759.01685
Epoch 85: Val Loss 2757.04321
Epoch 86: Val Loss 2754.99609
Epoch 87: Val Loss 2752.84351
Epoch 88: Val Loss 2750.65649
Epoch 89: Val Loss 2748.40674
Epoch 90: Val Loss 2746.16187
Epoch 91: Val Loss 2743.80493
Epoch 92: Val Loss 2741.41309
Epoch 93: Val Loss 2738.98169
Epoch 94: Val Loss 2736.43433
Epoch 95: Val Loss 2733.84204
Epoch 96: Val Loss 2731.11060
Epoch 97: Val Loss 2728.43018
Epoch 98: Val Loss 2725.56812
Epoch 99: Val Loss 2722.61328
{'MSE - mean': 1652.1614748883658, 'MSE - std': 758.2103771920489, 'R2 - mean': -0.08822578322844292, 'R2 - std': 0.035398977286111445} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2129.19458
Epoch 1: Val Loss 2128.99487
Epoch 2: Val Loss 2128.78906
Epoch 3: Val Loss 2128.59106
Epoch 4: Val Loss 2128.39404
Epoch 5: Val Loss 2128.19995
Epoch 6: Val Loss 2127.99536
Epoch 7: Val Loss 2127.79736
Epoch 8: Val Loss 2127.59839
Epoch 9: Val Loss 2127.38647
Epoch 10: Val Loss 2127.17603
Epoch 11: Val Loss 2126.96167
Epoch 12: Val Loss 2126.74072
Epoch 13: Val Loss 2126.51831
Epoch 14: Val Loss 2126.29346
Epoch 15: Val Loss 2126.04980
Epoch 16: Val Loss 2125.78027
Epoch 17: Val Loss 2125.48779
Epoch 18: Val Loss 2125.18750
Epoch 19: Val Loss 2124.85229
Epoch 20: Val Loss 2124.48218
Epoch 21: Val Loss 2124.09106
Epoch 22: Val Loss 2123.68018
Epoch 23: Val Loss 2123.27246
Epoch 24: Val Loss 2122.84253
Epoch 25: Val Loss 2122.41431
Epoch 26: Val Loss 2121.99414
Epoch 27: Val Loss 2121.58179
Epoch 28: Val Loss 2121.17505
Epoch 29: Val Loss 2120.73706
Epoch 30: Val Loss 2120.29810
Epoch 31: Val Loss 2119.85327
Epoch 32: Val Loss 2119.42065
Epoch 33: Val Loss 2118.97070
Epoch 34: Val Loss 2118.51489
Epoch 35: Val Loss 2118.05225
Epoch 36: Val Loss 2117.56641
Epoch 37: Val Loss 2117.05933
Epoch 38: Val Loss 2116.52612
Epoch 39: Val Loss 2115.99048
Epoch 40: Val Loss 2115.43091
Epoch 41: Val Loss 2114.86694
Epoch 42: Val Loss 2114.28418
Epoch 43: Val Loss 2113.70947
Epoch 44: Val Loss 2113.10645
Epoch 45: Val Loss 2112.45190
Epoch 46: Val Loss 2111.74829
Epoch 47: Val Loss 2111.04541
Epoch 48: Val Loss 2110.33423
Epoch 49: Val Loss 2109.57104
Epoch 50: Val Loss 2108.77686
Epoch 51: Val Loss 2107.99390
Epoch 52: Val Loss 2107.19702
Epoch 53: Val Loss 2106.32471
Epoch 54: Val Loss 2105.38428
Epoch 55: Val Loss 2104.38110
Epoch 56: Val Loss 2103.37964
Epoch 57: Val Loss 2102.34961
Epoch 58: Val Loss 2101.21021
Epoch 59: Val Loss 2100.02563
Epoch 60: Val Loss 2098.73755
Epoch 61: Val Loss 2097.41699
Epoch 62: Val Loss 2096.08691
Epoch 63: Val Loss 2094.68896
Epoch 64: Val Loss 2093.17627
Epoch 65: Val Loss 2091.63037
Epoch 66: Val Loss 2090.05322
Epoch 67: Val Loss 2088.38208
Epoch 68: Val Loss 2086.65088
Epoch 69: Val Loss 2084.89551
Epoch 70: Val Loss 2082.92993
Epoch 71: Val Loss 2081.04736
Epoch 72: Val Loss 2079.06226
Epoch 73: Val Loss 2077.03320
Epoch 74: Val Loss 2074.95312
Epoch 75: Val Loss 2072.72046
Epoch 76: Val Loss 2070.47583
Epoch 77: Val Loss 2068.12305
Epoch 78: Val Loss 2065.66846
Epoch 79: Val Loss 2063.15796
Epoch 80: Val Loss 2060.61914
Epoch 81: Val Loss 2058.10767
Epoch 82: Val Loss 2055.44702
Epoch 83: Val Loss 2052.75488
Epoch 84: Val Loss 2050.09839
Epoch 85: Val Loss 2047.27808
Epoch 86: Val Loss 2044.48364
Epoch 87: Val Loss 2041.63354
Epoch 88: Val Loss 2038.65881
Epoch 89: Val Loss 2035.58301
Epoch 90: Val Loss 2032.36169
Epoch 91: Val Loss 2029.11572
Epoch 92: Val Loss 2025.91125
Epoch 93: Val Loss 2022.48010
Epoch 94: Val Loss 2019.03491
Epoch 95: Val Loss 2015.31335
Epoch 96: Val Loss 2011.77393
Epoch 97: Val Loss 2008.21863
Epoch 98: Val Loss 2004.50049
Epoch 99: Val Loss 2000.68396
{'MSE - mean': 1739.292095456396, 'MSE - std': 673.748816090234, 'R2 - mean': -0.1063932664159673, 'R2 - std': 0.04393162904769091} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2517.60840
Epoch 1: Val Loss 2517.37793
Epoch 2: Val Loss 2517.14478
Epoch 3: Val Loss 2516.90576
Epoch 4: Val Loss 2516.66968
Epoch 5: Val Loss 2516.42944
Epoch 6: Val Loss 2516.18213
Epoch 7: Val Loss 2515.93530
Epoch 8: Val Loss 2515.68359
Epoch 9: Val Loss 2515.42432
Epoch 10: Val Loss 2515.15479
Epoch 11: Val Loss 2514.88110
Epoch 12: Val Loss 2514.60596
Epoch 13: Val Loss 2514.32910
Epoch 14: Val Loss 2514.04102
Epoch 15: Val Loss 2513.74707
Epoch 16: Val Loss 2513.42285
Epoch 17: Val Loss 2513.10400
Epoch 18: Val Loss 2512.78271
Epoch 19: Val Loss 2512.46411
Epoch 20: Val Loss 2512.13037
Epoch 21: Val Loss 2511.78101
Epoch 22: Val Loss 2511.41675
Epoch 23: Val Loss 2511.05151
Epoch 24: Val Loss 2510.66943
Epoch 25: Val Loss 2510.28271
Epoch 26: Val Loss 2509.86157
Epoch 27: Val Loss 2509.44458
Epoch 28: Val Loss 2509.04321
Epoch 29: Val Loss 2508.62939
Epoch 30: Val Loss 2508.19263
Epoch 31: Val Loss 2507.74243
Epoch 32: Val Loss 2507.28833
Epoch 33: Val Loss 2506.80835
Epoch 34: Val Loss 2506.29053
Epoch 35: Val Loss 2505.78833
Epoch 36: Val Loss 2505.24951
Epoch 37: Val Loss 2504.71240
Epoch 38: Val Loss 2504.09448
Epoch 39: Val Loss 2503.47192
Epoch 40: Val Loss 2502.79834
Epoch 41: Val Loss 2502.10449
Epoch 42: Val Loss 2501.40674
Epoch 43: Val Loss 2500.64111
Epoch 44: Val Loss 2499.83813
Epoch 45: Val Loss 2498.97437
Epoch 46: Val Loss 2498.12305
Epoch 47: Val Loss 2497.24707
Epoch 48: Val Loss 2496.32666
Epoch 49: Val Loss 2495.35571
Epoch 50: Val Loss 2494.34277
Epoch 51: Val Loss 2493.25439
Epoch 52: Val Loss 2492.15259
Epoch 53: Val Loss 2491.02441
Epoch 54: Val Loss 2489.91333
Epoch 55: Val Loss 2488.75708
Epoch 56: Val Loss 2487.53198
Epoch 57: Val Loss 2486.28613
Epoch 58: Val Loss 2484.93677
Epoch 59: Val Loss 2483.57935
Epoch 60: Val Loss 2482.14600
Epoch 61: Val Loss 2480.72046
Epoch 62: Val Loss 2479.22900
Epoch 63: Val Loss 2477.70898
Epoch 64: Val Loss 2476.10571
Epoch 65: Val Loss 2474.40649
Epoch 66: Val Loss 2472.65967
Epoch 67: Val Loss 2470.91553
Epoch 68: Val Loss 2469.16675
Epoch 69: Val Loss 2467.34229
Epoch 70: Val Loss 2465.43506
Epoch 71: Val Loss 2463.43457
Epoch 72: Val Loss 2461.34058
Epoch 73: Val Loss 2459.17773
Epoch 74: Val Loss 2457.02075
Epoch 75: Val Loss 2454.62964
Epoch 76: Val Loss 2452.29688
Epoch 77: Val Loss 2449.88696
Epoch 78: Val Loss 2447.40820
Epoch 79: Val Loss 2444.81519
Epoch 80: Val Loss 2442.12549
Epoch 81: Val Loss 2439.49780
Epoch 82: Val Loss 2436.68408
Epoch 83: Val Loss 2433.77026
Epoch 84: Val Loss 2430.95068
Epoch 85: Val Loss 2428.04541
Epoch 86: Val Loss 2424.83643
Epoch 87: Val Loss 2421.50488
Epoch 88: Val Loss 2418.21313
Epoch 89: Val Loss 2414.70312
Epoch 90: Val Loss 2411.25391
Epoch 91: Val Loss 2407.71484
Epoch 92: Val Loss 2404.29346
Epoch 93: Val Loss 2400.36938
Epoch 94: Val Loss 2396.72583
Epoch 95: Val Loss 2392.76294
Epoch 96: Val Loss 2388.79224
Epoch 97: Val Loss 2384.57910
Epoch 98: Val Loss 2380.16602
Epoch 99: Val Loss 2376.00684
{'MSE - mean': 1866.6350445653127, 'MSE - std': 654.2284620049926, 'R2 - mean': -0.10726693991547295, 'R2 - std': 0.03933247571869954} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 1866.6350445653127 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -5, 'learning_rate': -4, 'dropout': 0.3}. Best is trial 14 with value: 329.26580985652555.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1282.23352
Epoch 1: Val Loss 1281.03040
Epoch 2: Val Loss 1280.09644
Epoch 3: Val Loss 1279.28015
Epoch 4: Val Loss 1278.45178
Epoch 5: Val Loss 1277.22095
Epoch 6: Val Loss 1275.32983
Epoch 7: Val Loss 1272.79443
Epoch 8: Val Loss 1269.47388
Epoch 9: Val Loss 1264.94238
Epoch 10: Val Loss 1258.97156
Epoch 11: Val Loss 1251.22217
Epoch 12: Val Loss 1241.23889
Epoch 13: Val Loss 1228.88550
Epoch 14: Val Loss 1213.63672
Epoch 15: Val Loss 1195.76721
Epoch 16: Val Loss 1173.81238
Epoch 17: Val Loss 1148.49341
Epoch 18: Val Loss 1119.22668
Epoch 19: Val Loss 1087.56409
Epoch 20: Val Loss 1050.30762
Epoch 21: Val Loss 1010.46741
Epoch 22: Val Loss 966.94507
Epoch 23: Val Loss 921.77087
Epoch 24: Val Loss 875.59698
Epoch 25: Val Loss 829.06720
Epoch 26: Val Loss 788.63849
Epoch 27: Val Loss 752.74231
Epoch 28: Val Loss 726.02234
Epoch 29: Val Loss 704.49219
Epoch 30: Val Loss 684.67065
Epoch 31: Val Loss 665.18744
Epoch 32: Val Loss 645.08667
Epoch 33: Val Loss 624.39130
Epoch 34: Val Loss 601.41382
Epoch 35: Val Loss 578.92664
Epoch 36: Val Loss 556.58124
Epoch 37: Val Loss 534.94757
Epoch 38: Val Loss 514.61212
Epoch 39: Val Loss 492.25461
Epoch 40: Val Loss 471.54675
Epoch 41: Val Loss 452.92648
Epoch 42: Val Loss 433.16125
Epoch 43: Val Loss 413.17383
Epoch 44: Val Loss 396.63785
Epoch 45: Val Loss 380.96088
Epoch 46: Val Loss 368.67114
Epoch 47: Val Loss 357.49295
Epoch 48: Val Loss 350.93665
Epoch 49: Val Loss 340.73367
Epoch 50: Val Loss 330.22690
Epoch 51: Val Loss 319.49374
Epoch 52: Val Loss 309.93039
Epoch 53: Val Loss 303.73569
Epoch 54: Val Loss 295.27054
Epoch 55: Val Loss 283.55020
Epoch 56: Val Loss 274.69214
Epoch 57: Val Loss 268.58682
Epoch 58: Val Loss 262.64432
Epoch 59: Val Loss 256.73209
Epoch 60: Val Loss 252.05206
Epoch 61: Val Loss 246.97360
Epoch 62: Val Loss 243.17349
Epoch 63: Val Loss 237.94333
Epoch 64: Val Loss 231.36829
Epoch 65: Val Loss 227.35489
Epoch 66: Val Loss 227.61937
Epoch 67: Val Loss 224.08211
Epoch 68: Val Loss 221.80443
Epoch 69: Val Loss 218.59190
Epoch 70: Val Loss 216.48006
Epoch 71: Val Loss 212.25453
Epoch 72: Val Loss 209.89090
Epoch 73: Val Loss 207.93925
Epoch 74: Val Loss 206.29411
Epoch 75: Val Loss 203.32637
Epoch 76: Val Loss 205.17827
Epoch 77: Val Loss 205.99983
Epoch 78: Val Loss 202.65538
Epoch 79: Val Loss 198.98973
Epoch 80: Val Loss 196.82265
Epoch 81: Val Loss 193.85603
Epoch 82: Val Loss 192.60345
Epoch 83: Val Loss 192.80334
Epoch 84: Val Loss 191.89935
Epoch 85: Val Loss 191.50240
Epoch 86: Val Loss 193.13742
Epoch 87: Val Loss 190.22034
Epoch 88: Val Loss 186.82439
Epoch 89: Val Loss 186.39493
Epoch 90: Val Loss 184.83086
Epoch 91: Val Loss 184.57207
Epoch 92: Val Loss 184.42018
Epoch 93: Val Loss 182.95721
Epoch 94: Val Loss 183.40114
Epoch 95: Val Loss 182.56105
Epoch 96: Val Loss 180.71124
Epoch 97: Val Loss 177.70349
Epoch 98: Val Loss 177.97969
Epoch 99: Val Loss 177.86134
{'MSE - mean': 177.7034957928135, 'MSE - std': 0.0, 'R2 - mean': 0.8264171040278283, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1308.44641
Epoch 1: Val Loss 1306.27698
Epoch 2: Val Loss 1303.93103
Epoch 3: Val Loss 1301.28223
Epoch 4: Val Loss 1298.08850
Epoch 5: Val Loss 1294.08875
Epoch 6: Val Loss 1288.73071
Epoch 7: Val Loss 1281.49402
Epoch 8: Val Loss 1271.93652
Epoch 9: Val Loss 1258.85681
Epoch 10: Val Loss 1241.82495
Epoch 11: Val Loss 1219.02710
Epoch 12: Val Loss 1189.98340
Epoch 13: Val Loss 1155.42285
Epoch 14: Val Loss 1114.11621
Epoch 15: Val Loss 1066.63110
Epoch 16: Val Loss 1013.27063
Epoch 17: Val Loss 957.09851
Epoch 18: Val Loss 903.74567
Epoch 19: Val Loss 849.45398
Epoch 20: Val Loss 802.31561
Epoch 21: Val Loss 763.66425
Epoch 22: Val Loss 732.84320
Epoch 23: Val Loss 709.00677
Epoch 24: Val Loss 687.39325
Epoch 25: Val Loss 663.84174
Epoch 26: Val Loss 639.32361
Epoch 27: Val Loss 610.82996
Epoch 28: Val Loss 581.87640
Epoch 29: Val Loss 555.34003
Epoch 30: Val Loss 529.95154
Epoch 31: Val Loss 505.69635
Epoch 32: Val Loss 484.97345
Epoch 33: Val Loss 464.93961
Epoch 34: Val Loss 448.05527
Epoch 35: Val Loss 433.53430
Epoch 36: Val Loss 423.91992
Epoch 37: Val Loss 411.55542
Epoch 38: Val Loss 399.55035
Epoch 39: Val Loss 392.32095
Epoch 40: Val Loss 379.26575
Epoch 41: Val Loss 367.01456
Epoch 42: Val Loss 360.01968
Epoch 43: Val Loss 355.56256
Epoch 44: Val Loss 346.52887
Epoch 45: Val Loss 340.21655
Epoch 46: Val Loss 329.49460
Epoch 47: Val Loss 321.64896
Epoch 48: Val Loss 320.41226
Epoch 49: Val Loss 326.08789
Epoch 50: Val Loss 319.03055
Epoch 51: Val Loss 311.56912
Epoch 52: Val Loss 306.18658
Epoch 53: Val Loss 313.95355
Epoch 54: Val Loss 310.98593
Epoch 55: Val Loss 308.79431
Epoch 56: Val Loss 304.74365
Epoch 57: Val Loss 300.05304
Epoch 58: Val Loss 296.23010
Epoch 59: Val Loss 286.20276
Epoch 60: Val Loss 288.21777
Epoch 61: Val Loss 295.85449
Epoch 62: Val Loss 289.12439
Epoch 63: Val Loss 280.96613
Epoch 64: Val Loss 280.05594
Epoch 65: Val Loss 286.14059
Epoch 66: Val Loss 283.65781
Epoch 67: Val Loss 284.85492
Epoch 68: Val Loss 291.08435
Epoch 69: Val Loss 284.02957
Epoch 70: Val Loss 284.81335
Epoch 71: Val Loss 271.78134
Epoch 72: Val Loss 272.60333
Epoch 73: Val Loss 271.98163
Epoch 74: Val Loss 282.83380
Epoch 75: Val Loss 290.16187
Epoch 76: Val Loss 279.87930
Epoch 77: Val Loss 268.09256
Epoch 78: Val Loss 258.39767
Epoch 79: Val Loss 253.42281
Epoch 80: Val Loss 252.80688
Epoch 81: Val Loss 261.92227
Epoch 82: Val Loss 260.01312
Epoch 83: Val Loss 254.60223
Epoch 84: Val Loss 257.00385
Epoch 85: Val Loss 249.79082
Epoch 86: Val Loss 238.38341
Epoch 87: Val Loss 233.70229
Epoch 88: Val Loss 235.69748
Epoch 89: Val Loss 237.27489
Epoch 90: Val Loss 239.09175
Epoch 91: Val Loss 245.65733
Epoch 92: Val Loss 242.94540
Epoch 93: Val Loss 231.91873
Epoch 94: Val Loss 233.73491
Epoch 95: Val Loss 246.77104
Epoch 96: Val Loss 238.79135
Epoch 97: Val Loss 227.25075
Epoch 98: Val Loss 223.73448
Epoch 99: Val Loss 220.96808
{'MSE - mean': 199.33577901852786, 'MSE - std': 21.63228322571436, 'R2 - mean': 0.8079933674210112, 'R2 - std': 0.018423736606817065} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2818.60962
Epoch 1: Val Loss 2816.36353
Epoch 2: Val Loss 2814.24609
Epoch 3: Val Loss 2812.10669
Epoch 4: Val Loss 2809.63086
Epoch 5: Val Loss 2806.52930
Epoch 6: Val Loss 2802.72949
Epoch 7: Val Loss 2797.69482
Epoch 8: Val Loss 2791.22339
Epoch 9: Val Loss 2783.34033
Epoch 10: Val Loss 2772.98608
Epoch 11: Val Loss 2760.35303
Epoch 12: Val Loss 2744.23975
Epoch 13: Val Loss 2724.15576
Epoch 14: Val Loss 2699.29590
Epoch 15: Val Loss 2668.31274
Epoch 16: Val Loss 2630.68945
Epoch 17: Val Loss 2585.36401
Epoch 18: Val Loss 2532.08594
Epoch 19: Val Loss 2471.80420
Epoch 20: Val Loss 2408.50684
Epoch 21: Val Loss 2337.45850
Epoch 22: Val Loss 2264.73047
Epoch 23: Val Loss 2194.93457
Epoch 24: Val Loss 2129.93140
Epoch 25: Val Loss 2067.12695
Epoch 26: Val Loss 2007.89734
Epoch 27: Val Loss 1958.55396
Epoch 28: Val Loss 1914.21777
Epoch 29: Val Loss 1867.16382
Epoch 30: Val Loss 1822.38965
Epoch 31: Val Loss 1775.88159
Epoch 32: Val Loss 1734.86743
Epoch 33: Val Loss 1693.33740
Epoch 34: Val Loss 1642.05664
Epoch 35: Val Loss 1597.60229
Epoch 36: Val Loss 1556.27246
Epoch 37: Val Loss 1512.38098
Epoch 38: Val Loss 1472.46985
Epoch 39: Val Loss 1429.25354
Epoch 40: Val Loss 1394.58508
Epoch 41: Val Loss 1360.14075
Epoch 42: Val Loss 1328.58923
Epoch 43: Val Loss 1294.97864
Epoch 44: Val Loss 1260.62146
Epoch 45: Val Loss 1228.73254
Epoch 46: Val Loss 1199.29419
Epoch 47: Val Loss 1167.79517
Epoch 48: Val Loss 1140.55139
Epoch 49: Val Loss 1114.47168
Epoch 50: Val Loss 1093.07227
Epoch 51: Val Loss 1078.06677
Epoch 52: Val Loss 1057.48926
Epoch 53: Val Loss 1045.88538
Epoch 54: Val Loss 1026.77930
Epoch 55: Val Loss 1001.76489
Epoch 56: Val Loss 986.28168
Epoch 57: Val Loss 966.00623
Epoch 58: Val Loss 949.11200
Epoch 59: Val Loss 941.65393
Epoch 60: Val Loss 932.62524
Epoch 61: Val Loss 911.26190
Epoch 62: Val Loss 900.15735
Epoch 63: Val Loss 886.78571
Epoch 64: Val Loss 872.67004
Epoch 65: Val Loss 861.51550
Epoch 66: Val Loss 849.18585
Epoch 67: Val Loss 835.82990
Epoch 68: Val Loss 822.66840
Epoch 69: Val Loss 809.27289
Epoch 70: Val Loss 801.03894
Epoch 71: Val Loss 794.15271
Epoch 72: Val Loss 784.97882
Epoch 73: Val Loss 778.08716
Epoch 74: Val Loss 771.06415
Epoch 75: Val Loss 758.67194
Epoch 76: Val Loss 752.00214
Epoch 77: Val Loss 747.79980
Epoch 78: Val Loss 724.29388
Epoch 79: Val Loss 717.47864
Epoch 80: Val Loss 709.88123
Epoch 81: Val Loss 699.31049
Epoch 82: Val Loss 692.34863
Epoch 83: Val Loss 691.94604
Epoch 84: Val Loss 689.66870
Epoch 85: Val Loss 683.38959
Epoch 86: Val Loss 675.54895
Epoch 87: Val Loss 674.25189
Epoch 88: Val Loss 665.30054
Epoch 89: Val Loss 659.08862
Epoch 90: Val Loss 656.67371
Epoch 91: Val Loss 652.38098
Epoch 92: Val Loss 650.03693
Epoch 93: Val Loss 647.39893
Epoch 94: Val Loss 638.97357
Epoch 95: Val Loss 636.84900
Epoch 96: Val Loss 629.85797
Epoch 97: Val Loss 622.40582
Epoch 98: Val Loss 618.84021
Epoch 99: Val Loss 616.03058
{'MSE - mean': 338.23402182889413, 'MSE - std': 197.22427376801252, 'R2 - mean': 0.788180002627961, 'R2 - std': 0.03180295944116768} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2114.54346
Epoch 1: Val Loss 2111.95068
Epoch 2: Val Loss 2107.93506
Epoch 3: Val Loss 2102.85400
Epoch 4: Val Loss 2096.58813
Epoch 5: Val Loss 2088.86011
Epoch 6: Val Loss 2079.04224
Epoch 7: Val Loss 2065.99805
Epoch 8: Val Loss 2049.67700
Epoch 9: Val Loss 2029.01794
Epoch 10: Val Loss 2003.37219
Epoch 11: Val Loss 1971.06982
Epoch 12: Val Loss 1932.41553
Epoch 13: Val Loss 1886.42261
Epoch 14: Val Loss 1830.95081
Epoch 15: Val Loss 1765.52271
Epoch 16: Val Loss 1694.43628
Epoch 17: Val Loss 1616.35950
Epoch 18: Val Loss 1538.81958
Epoch 19: Val Loss 1462.89099
Epoch 20: Val Loss 1393.06030
Epoch 21: Val Loss 1323.26111
Epoch 22: Val Loss 1265.91455
Epoch 23: Val Loss 1218.77893
Epoch 24: Val Loss 1172.25476
Epoch 25: Val Loss 1130.82239
Epoch 26: Val Loss 1089.24988
Epoch 27: Val Loss 1049.29907
Epoch 28: Val Loss 1010.82104
Epoch 29: Val Loss 971.93927
Epoch 30: Val Loss 932.82422
Epoch 31: Val Loss 896.69794
Epoch 32: Val Loss 859.57538
Epoch 33: Val Loss 823.63599
Epoch 34: Val Loss 790.46808
Epoch 35: Val Loss 760.08301
Epoch 36: Val Loss 731.69586
Epoch 37: Val Loss 704.66602
Epoch 38: Val Loss 681.15179
Epoch 39: Val Loss 659.90741
Epoch 40: Val Loss 640.06543
Epoch 41: Val Loss 620.09955
Epoch 42: Val Loss 599.49152
Epoch 43: Val Loss 582.36255
Epoch 44: Val Loss 567.45599
Epoch 45: Val Loss 552.97382
Epoch 46: Val Loss 539.28229
Epoch 47: Val Loss 527.68762
Epoch 48: Val Loss 516.52509
Epoch 49: Val Loss 506.43616
Epoch 50: Val Loss 495.32660
Epoch 51: Val Loss 485.73505
Epoch 52: Val Loss 474.29398
Epoch 53: Val Loss 460.37933
Epoch 54: Val Loss 450.08145
Epoch 55: Val Loss 441.99066
Epoch 56: Val Loss 431.26910
Epoch 57: Val Loss 423.10065
Epoch 58: Val Loss 414.80676
Epoch 59: Val Loss 410.70007
Epoch 60: Val Loss 403.22519
Epoch 61: Val Loss 394.38312
Epoch 62: Val Loss 389.25595
Epoch 63: Val Loss 385.49295
Epoch 64: Val Loss 377.41211
Epoch 65: Val Loss 376.00562
Epoch 66: Val Loss 371.72925
Epoch 67: Val Loss 370.16409
Epoch 68: Val Loss 362.92242
Epoch 69: Val Loss 360.19653
Epoch 70: Val Loss 357.02246
Epoch 71: Val Loss 351.98398
Epoch 72: Val Loss 348.08997
Epoch 73: Val Loss 343.53821
Epoch 74: Val Loss 342.88193
Epoch 75: Val Loss 341.31137
Epoch 76: Val Loss 344.42551
Epoch 77: Val Loss 342.32260
Epoch 78: Val Loss 329.17175
Epoch 79: Val Loss 327.36200
Epoch 80: Val Loss 326.89862
Epoch 81: Val Loss 319.70129
Epoch 82: Val Loss 315.23587
Epoch 83: Val Loss 311.26514
Epoch 84: Val Loss 304.98837
Epoch 85: Val Loss 300.16388
Epoch 86: Val Loss 302.75128
Epoch 87: Val Loss 304.98276
Epoch 88: Val Loss 302.44363
Epoch 89: Val Loss 296.76151
Epoch 90: Val Loss 302.61652
Epoch 91: Val Loss 304.68088
Epoch 92: Val Loss 294.21503
Epoch 93: Val Loss 291.07065
Epoch 94: Val Loss 288.87650
Epoch 95: Val Loss 282.58627
Epoch 96: Val Loss 281.90918
Epoch 97: Val Loss 277.71198
Epoch 98: Val Loss 284.80511
Epoch 99: Val Loss 280.26981
{'MSE - mean': 323.10351468019593, 'MSE - std': 172.80005023789892, 'R2 - mean': 0.8008494458655857, 'R2 - std': 0.035215274353135595} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2504.67798
Epoch 1: Val Loss 2500.22974
Epoch 2: Val Loss 2495.04053
Epoch 3: Val Loss 2488.91772
Epoch 4: Val Loss 2481.02930
Epoch 5: Val Loss 2470.92114
Epoch 6: Val Loss 2457.40332
Epoch 7: Val Loss 2440.34912
Epoch 8: Val Loss 2417.51318
Epoch 9: Val Loss 2387.80249
Epoch 10: Val Loss 2351.30078
Epoch 11: Val Loss 2306.85449
Epoch 12: Val Loss 2254.45654
Epoch 13: Val Loss 2195.71582
Epoch 14: Val Loss 2126.34229
Epoch 15: Val Loss 2048.07568
Epoch 16: Val Loss 1967.15991
Epoch 17: Val Loss 1888.65771
Epoch 18: Val Loss 1811.14148
Epoch 19: Val Loss 1737.08997
Epoch 20: Val Loss 1674.02393
Epoch 21: Val Loss 1608.56274
Epoch 22: Val Loss 1552.82361
Epoch 23: Val Loss 1496.21667
Epoch 24: Val Loss 1442.03870
Epoch 25: Val Loss 1387.17029
Epoch 26: Val Loss 1331.45667
Epoch 27: Val Loss 1260.52271
Epoch 28: Val Loss 1194.74731
Epoch 29: Val Loss 1134.66272
Epoch 30: Val Loss 1087.62231
Epoch 31: Val Loss 1039.14233
Epoch 32: Val Loss 989.58551
Epoch 33: Val Loss 939.33630
Epoch 34: Val Loss 901.96326
Epoch 35: Val Loss 876.12000
Epoch 36: Val Loss 847.97559
Epoch 37: Val Loss 814.03192
Epoch 38: Val Loss 781.38269
Epoch 39: Val Loss 753.08362
Epoch 40: Val Loss 737.25732
Epoch 41: Val Loss 716.70734
Epoch 42: Val Loss 693.81763
Epoch 43: Val Loss 669.39215
Epoch 44: Val Loss 655.55682
Epoch 45: Val Loss 646.15723
Epoch 46: Val Loss 633.62994
Epoch 47: Val Loss 621.26190
Epoch 48: Val Loss 613.55811
Epoch 49: Val Loss 593.02087
Epoch 50: Val Loss 579.27704
Epoch 51: Val Loss 569.68195
Epoch 52: Val Loss 563.91479
Epoch 53: Val Loss 544.06036
Epoch 54: Val Loss 538.28943
Epoch 55: Val Loss 529.36554
Epoch 56: Val Loss 520.43286
Epoch 57: Val Loss 515.08148
Epoch 58: Val Loss 513.18567
Epoch 59: Val Loss 508.03741
Epoch 60: Val Loss 505.39447
Epoch 61: Val Loss 497.47327
Epoch 62: Val Loss 484.17438
Epoch 63: Val Loss 483.96622
Epoch 64: Val Loss 480.73779
Epoch 65: Val Loss 476.78445
Epoch 66: Val Loss 468.82285
Epoch 67: Val Loss 461.69620
Epoch 68: Val Loss 464.26840
Epoch 69: Val Loss 455.72247
Epoch 70: Val Loss 442.81000
Epoch 71: Val Loss 441.72708
Epoch 72: Val Loss 433.05179
Epoch 73: Val Loss 422.03647
Epoch 74: Val Loss 418.86011
Epoch 75: Val Loss 416.80405
Epoch 76: Val Loss 420.35748
Epoch 77: Val Loss 417.96869
Epoch 78: Val Loss 411.67813
Epoch 79: Val Loss 406.86789
Epoch 80: Val Loss 400.88232
Epoch 81: Val Loss 397.55447
Epoch 82: Val Loss 396.99594
Epoch 83: Val Loss 389.89191
Epoch 84: Val Loss 382.22662
Epoch 85: Val Loss 379.38489
Epoch 86: Val Loss 382.68857
Epoch 87: Val Loss 381.38943
Epoch 88: Val Loss 376.84305
Epoch 89: Val Loss 374.11722
Epoch 90: Val Loss 375.03278
Epoch 91: Val Loss 371.14273
Epoch 92: Val Loss 368.52255
Epoch 93: Val Loss 367.63791
Epoch 94: Val Loss 362.57324
Epoch 95: Val Loss 358.23999
Epoch 96: Val Loss 357.45941
Epoch 97: Val Loss 355.55170
Epoch 98: Val Loss 353.27960
Epoch 99: Val Loss 347.87717
{'MSE - mean': 328.058245650853, 'MSE - std': 154.8744114621349, 'R2 - mean': 0.8081536727964928, 'R2 - std': 0.034720301877230755} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 328.058245650853 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 18 with value: 328.058245650853.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1292.50891
Epoch 1: Val Loss 1292.26868
Epoch 2: Val Loss 1292.03015
Epoch 3: Val Loss 1291.79565
Epoch 4: Val Loss 1291.55701
Epoch 5: Val Loss 1291.32300
Epoch 6: Val Loss 1291.08496
Epoch 7: Val Loss 1290.85828
Epoch 8: Val Loss 1290.63306
Epoch 9: Val Loss 1290.41101
Epoch 10: Val Loss 1290.18848
Epoch 11: Val Loss 1289.96973
Epoch 12: Val Loss 1289.74487
Epoch 13: Val Loss 1289.52600
Epoch 14: Val Loss 1289.30420
Epoch 15: Val Loss 1289.07324
Epoch 16: Val Loss 1288.84460
Epoch 17: Val Loss 1288.61755
Epoch 18: Val Loss 1288.38684
Epoch 19: Val Loss 1288.14697
Epoch 20: Val Loss 1287.89575
Epoch 21: Val Loss 1287.66675
Epoch 22: Val Loss 1287.43677
Epoch 23: Val Loss 1287.20068
Epoch 24: Val Loss 1286.96899
Epoch 25: Val Loss 1286.73425
Epoch 26: Val Loss 1286.49536
Epoch 27: Val Loss 1286.24634
Epoch 28: Val Loss 1285.99463
Epoch 29: Val Loss 1285.74243
Epoch 30: Val Loss 1285.49268
Epoch 31: Val Loss 1285.25000
Epoch 32: Val Loss 1284.97449
Epoch 33: Val Loss 1284.70398
Epoch 34: Val Loss 1284.43384
Epoch 35: Val Loss 1284.16809
Epoch 36: Val Loss 1283.89844
Epoch 37: Val Loss 1283.61633
Epoch 38: Val Loss 1283.31958
Epoch 39: Val Loss 1283.02734
Epoch 40: Val Loss 1282.72119
Epoch 41: Val Loss 1282.41785
Epoch 42: Val Loss 1282.09558
Epoch 43: Val Loss 1281.76855
Epoch 44: Val Loss 1281.42407
Epoch 45: Val Loss 1281.08301
Epoch 46: Val Loss 1280.71899
Epoch 47: Val Loss 1280.34155
Epoch 48: Val Loss 1279.94824
Epoch 49: Val Loss 1279.55444
Epoch 50: Val Loss 1279.14575
Epoch 51: Val Loss 1278.71521
Epoch 52: Val Loss 1278.26917
Epoch 53: Val Loss 1277.81763
Epoch 54: Val Loss 1277.33655
Epoch 55: Val Loss 1276.84265
Epoch 56: Val Loss 1276.31604
Epoch 57: Val Loss 1275.76660
Epoch 58: Val Loss 1275.19641
Epoch 59: Val Loss 1274.59741
Epoch 60: Val Loss 1273.93884
Epoch 61: Val Loss 1273.25623
Epoch 62: Val Loss 1272.57544
Epoch 63: Val Loss 1271.84827
Epoch 64: Val Loss 1271.05139
Epoch 65: Val Loss 1270.25378
Epoch 66: Val Loss 1269.48169
Epoch 67: Val Loss 1268.65491
Epoch 68: Val Loss 1267.79407
Epoch 69: Val Loss 1266.93396
Epoch 70: Val Loss 1265.99512
Epoch 71: Val Loss 1265.04797
Epoch 72: Val Loss 1264.02917
Epoch 73: Val Loss 1262.95068
Epoch 74: Val Loss 1261.88599
Epoch 75: Val Loss 1260.74854
Epoch 76: Val Loss 1259.59351
Epoch 77: Val Loss 1258.33179
Epoch 78: Val Loss 1257.11340
Epoch 79: Val Loss 1255.79028
Epoch 80: Val Loss 1254.41565
Epoch 81: Val Loss 1252.98059
Epoch 82: Val Loss 1251.54822
Epoch 83: Val Loss 1250.13000
Epoch 84: Val Loss 1248.61963
Epoch 85: Val Loss 1247.07202
Epoch 86: Val Loss 1245.45044
Epoch 87: Val Loss 1243.79590
Epoch 88: Val Loss 1242.06824
Epoch 89: Val Loss 1240.28638
Epoch 90: Val Loss 1238.38330
Epoch 91: Val Loss 1236.48682
Epoch 92: Val Loss 1234.55310
Epoch 93: Val Loss 1232.62476
Epoch 94: Val Loss 1230.62207
Epoch 95: Val Loss 1228.60864
Epoch 96: Val Loss 1226.46729
Epoch 97: Val Loss 1224.26855
Epoch 98: Val Loss 1221.83948
Epoch 99: Val Loss 1219.43262
{'MSE - mean': 1219.4327533835844, 'MSE - std': 0.0, 'R2 - mean': -0.19115647011487713, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1296.39001
Epoch 1: Val Loss 1296.17932
Epoch 2: Val Loss 1295.96179
Epoch 3: Val Loss 1295.75122
Epoch 4: Val Loss 1295.54541
Epoch 5: Val Loss 1295.33337
Epoch 6: Val Loss 1295.11304
Epoch 7: Val Loss 1294.88721
Epoch 8: Val Loss 1294.66150
Epoch 9: Val Loss 1294.42334
Epoch 10: Val Loss 1294.18079
Epoch 11: Val Loss 1293.92993
Epoch 12: Val Loss 1293.66406
Epoch 13: Val Loss 1293.38928
Epoch 14: Val Loss 1293.11206
Epoch 15: Val Loss 1292.82166
Epoch 16: Val Loss 1292.50891
Epoch 17: Val Loss 1292.19287
Epoch 18: Val Loss 1291.85095
Epoch 19: Val Loss 1291.48572
Epoch 20: Val Loss 1291.11414
Epoch 21: Val Loss 1290.72119
Epoch 22: Val Loss 1290.31213
Epoch 23: Val Loss 1289.86230
Epoch 24: Val Loss 1289.38232
Epoch 25: Val Loss 1288.87695
Epoch 26: Val Loss 1288.32471
Epoch 27: Val Loss 1287.73511
Epoch 28: Val Loss 1287.10535
Epoch 29: Val Loss 1286.42725
Epoch 30: Val Loss 1285.69641
Epoch 31: Val Loss 1284.90015
Epoch 32: Val Loss 1284.09021
Epoch 33: Val Loss 1283.26013
Epoch 34: Val Loss 1282.34998
Epoch 35: Val Loss 1281.39270
Epoch 36: Val Loss 1280.38892
Epoch 37: Val Loss 1279.37390
Epoch 38: Val Loss 1278.32520
Epoch 39: Val Loss 1277.17529
Epoch 40: Val Loss 1276.03198
Epoch 41: Val Loss 1274.91235
Epoch 42: Val Loss 1273.73621
Epoch 43: Val Loss 1272.54114
Epoch 44: Val Loss 1271.32971
Epoch 45: Val Loss 1270.06201
Epoch 46: Val Loss 1268.65820
Epoch 47: Val Loss 1267.27869
Epoch 48: Val Loss 1265.81909
Epoch 49: Val Loss 1264.34497
Epoch 50: Val Loss 1262.86987
Epoch 51: Val Loss 1261.32019
Epoch 52: Val Loss 1259.70044
Epoch 53: Val Loss 1258.11230
Epoch 54: Val Loss 1256.41504
Epoch 55: Val Loss 1254.64392
Epoch 56: Val Loss 1252.88013
Epoch 57: Val Loss 1251.00781
Epoch 58: Val Loss 1249.22229
Epoch 59: Val Loss 1247.39197
Epoch 60: Val Loss 1245.46021
Epoch 61: Val Loss 1243.42627
Epoch 62: Val Loss 1241.47107
Epoch 63: Val Loss 1239.40723
Epoch 64: Val Loss 1237.20496
Epoch 65: Val Loss 1234.89307
Epoch 66: Val Loss 1232.59216
Epoch 67: Val Loss 1230.27734
Epoch 68: Val Loss 1227.92725
Epoch 69: Val Loss 1225.53125
Epoch 70: Val Loss 1223.03870
Epoch 71: Val Loss 1220.59399
Epoch 72: Val Loss 1218.00635
Epoch 73: Val Loss 1215.30493
Epoch 74: Val Loss 1212.58228
Epoch 75: Val Loss 1209.80371
Epoch 76: Val Loss 1207.03711
Epoch 77: Val Loss 1204.09082
Epoch 78: Val Loss 1201.24402
Epoch 79: Val Loss 1198.24341
Epoch 80: Val Loss 1195.18225
Epoch 81: Val Loss 1192.04736
Epoch 82: Val Loss 1188.64832
Epoch 83: Val Loss 1185.30383
Epoch 84: Val Loss 1181.97400
Epoch 85: Val Loss 1178.35657
Epoch 86: Val Loss 1174.68677
Epoch 87: Val Loss 1170.85339
Epoch 88: Val Loss 1166.89832
Epoch 89: Val Loss 1162.88770
Epoch 90: Val Loss 1158.85144
Epoch 91: Val Loss 1154.88965
Epoch 92: Val Loss 1150.82446
Epoch 93: Val Loss 1146.71899
Epoch 94: Val Loss 1142.45679
Epoch 95: Val Loss 1138.19348
Epoch 96: Val Loss 1133.66504
Epoch 97: Val Loss 1129.07239
Epoch 98: Val Loss 1124.60864
Epoch 99: Val Loss 1120.05640
{'MSE - mean': 1169.7445476705107, 'MSE - std': 49.68820571307367, 'R2 - mean': -0.12889935656523255, 'R2 - std': 0.06225711354964458} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2816.11230
Epoch 1: Val Loss 2815.71533
Epoch 2: Val Loss 2815.31055
Epoch 3: Val Loss 2814.92261
Epoch 4: Val Loss 2814.51758
Epoch 5: Val Loss 2814.11255
Epoch 6: Val Loss 2813.70776
Epoch 7: Val Loss 2813.30518
Epoch 8: Val Loss 2812.87549
Epoch 9: Val Loss 2812.43921
Epoch 10: Val Loss 2811.96899
Epoch 11: Val Loss 2811.47607
Epoch 12: Val Loss 2810.98926
Epoch 13: Val Loss 2810.49194
Epoch 14: Val Loss 2809.98438
Epoch 15: Val Loss 2809.45605
Epoch 16: Val Loss 2808.91821
Epoch 17: Val Loss 2808.36914
Epoch 18: Val Loss 2807.78687
Epoch 19: Val Loss 2807.20410
Epoch 20: Val Loss 2806.60278
Epoch 21: Val Loss 2805.99829
Epoch 22: Val Loss 2805.38037
Epoch 23: Val Loss 2804.74170
Epoch 24: Val Loss 2804.10181
Epoch 25: Val Loss 2803.38159
Epoch 26: Val Loss 2802.64941
Epoch 27: Val Loss 2801.89453
Epoch 28: Val Loss 2801.09277
Epoch 29: Val Loss 2800.25024
Epoch 30: Val Loss 2799.41284
Epoch 31: Val Loss 2798.53149
Epoch 32: Val Loss 2797.63477
Epoch 33: Val Loss 2796.66772
Epoch 34: Val Loss 2795.73755
Epoch 35: Val Loss 2794.69189
Epoch 36: Val Loss 2793.64282
Epoch 37: Val Loss 2792.53955
Epoch 38: Val Loss 2791.35278
Epoch 39: Val Loss 2790.06812
Epoch 40: Val Loss 2788.81104
Epoch 41: Val Loss 2787.47461
Epoch 42: Val Loss 2786.04102
Epoch 43: Val Loss 2784.52222
Epoch 44: Val Loss 2782.93604
Epoch 45: Val Loss 2781.21167
Epoch 46: Val Loss 2779.54712
Epoch 47: Val Loss 2777.95166
Epoch 48: Val Loss 2776.24609
Epoch 49: Val Loss 2774.48218
Epoch 50: Val Loss 2772.67725
Epoch 51: Val Loss 2770.82837
Epoch 52: Val Loss 2768.95361
Epoch 53: Val Loss 2767.08545
Epoch 54: Val Loss 2765.24023
Epoch 55: Val Loss 2763.20679
Epoch 56: Val Loss 2761.09424
Epoch 57: Val Loss 2758.88965
Epoch 58: Val Loss 2756.72559
Epoch 59: Val Loss 2754.42896
Epoch 60: Val Loss 2752.01318
Epoch 61: Val Loss 2749.58545
Epoch 62: Val Loss 2747.09912
Epoch 63: Val Loss 2744.46729
Epoch 64: Val Loss 2741.66553
Epoch 65: Val Loss 2738.78809
Epoch 66: Val Loss 2735.79492
Epoch 67: Val Loss 2732.49609
Epoch 68: Val Loss 2728.99805
Epoch 69: Val Loss 2725.21533
Epoch 70: Val Loss 2721.26392
Epoch 71: Val Loss 2717.26562
Epoch 72: Val Loss 2713.26147
Epoch 73: Val Loss 2709.11035
Epoch 74: Val Loss 2704.96240
Epoch 75: Val Loss 2700.73169
Epoch 76: Val Loss 2696.44141
Epoch 77: Val Loss 2692.16528
Epoch 78: Val Loss 2687.66455
Epoch 79: Val Loss 2683.03857
Epoch 80: Val Loss 2678.58179
Epoch 81: Val Loss 2673.94800
Epoch 82: Val Loss 2669.10229
Epoch 83: Val Loss 2664.23242
Epoch 84: Val Loss 2659.27441
Epoch 85: Val Loss 2654.36792
Epoch 86: Val Loss 2649.53027
Epoch 87: Val Loss 2644.12451
Epoch 88: Val Loss 2638.70801
Epoch 89: Val Loss 2633.33789
Epoch 90: Val Loss 2627.91089
Epoch 91: Val Loss 2622.53687
Epoch 92: Val Loss 2616.93994
Epoch 93: Val Loss 2611.20532
Epoch 94: Val Loss 2605.22021
Epoch 95: Val Loss 2599.35010
Epoch 96: Val Loss 2592.75415
Epoch 97: Val Loss 2586.41797
Epoch 98: Val Loss 2580.23657
Epoch 99: Val Loss 2573.75000
{'MSE - mean': 1637.7463486546721, 'MSE - std': 663.0967626524507, 'R2 - mean': -0.1027775753613156, 'R2 - std': 0.06283836698137674} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2118.40332
Epoch 1: Val Loss 2118.14209
Epoch 2: Val Loss 2117.89502
Epoch 3: Val Loss 2117.66162
Epoch 4: Val Loss 2117.42407
Epoch 5: Val Loss 2117.18896
Epoch 6: Val Loss 2116.96240
Epoch 7: Val Loss 2116.74243
Epoch 8: Val Loss 2116.52393
Epoch 9: Val Loss 2116.30566
Epoch 10: Val Loss 2116.08838
Epoch 11: Val Loss 2115.87524
Epoch 12: Val Loss 2115.66455
Epoch 13: Val Loss 2115.45288
Epoch 14: Val Loss 2115.25220
Epoch 15: Val Loss 2115.06104
Epoch 16: Val Loss 2114.87695
Epoch 17: Val Loss 2114.70410
Epoch 18: Val Loss 2114.53906
Epoch 19: Val Loss 2114.38452
Epoch 20: Val Loss 2114.23218
Epoch 21: Val Loss 2114.08789
Epoch 22: Val Loss 2113.94824
Epoch 23: Val Loss 2113.82056
Epoch 24: Val Loss 2113.69019
Epoch 25: Val Loss 2113.56641
Epoch 26: Val Loss 2113.43726
Epoch 27: Val Loss 2113.31006
Epoch 28: Val Loss 2113.18311
Epoch 29: Val Loss 2113.06055
Epoch 30: Val Loss 2112.93604
Epoch 31: Val Loss 2112.81348
Epoch 32: Val Loss 2112.68384
Epoch 33: Val Loss 2112.55298
Epoch 34: Val Loss 2112.42261
Epoch 35: Val Loss 2112.29077
Epoch 36: Val Loss 2112.16064
Epoch 37: Val Loss 2112.02710
Epoch 38: Val Loss 2111.89722
Epoch 39: Val Loss 2111.76025
Epoch 40: Val Loss 2111.62622
Epoch 41: Val Loss 2111.48438
Epoch 42: Val Loss 2111.33740
Epoch 43: Val Loss 2111.18726
Epoch 44: Val Loss 2111.02393
Epoch 45: Val Loss 2110.86401
Epoch 46: Val Loss 2110.70337
Epoch 47: Val Loss 2110.53149
Epoch 48: Val Loss 2110.35181
Epoch 49: Val Loss 2110.17041
Epoch 50: Val Loss 2109.98511
Epoch 51: Val Loss 2109.79199
Epoch 52: Val Loss 2109.59082
Epoch 53: Val Loss 2109.37988
Epoch 54: Val Loss 2109.15723
Epoch 55: Val Loss 2108.92700
Epoch 56: Val Loss 2108.68164
Epoch 57: Val Loss 2108.42578
Epoch 58: Val Loss 2108.16040
Epoch 59: Val Loss 2107.88672
Epoch 60: Val Loss 2107.60181
Epoch 61: Val Loss 2107.30640
Epoch 62: Val Loss 2106.99731
Epoch 63: Val Loss 2106.67627
Epoch 64: Val Loss 2106.33960
Epoch 65: Val Loss 2105.98242
Epoch 66: Val Loss 2105.61499
Epoch 67: Val Loss 2105.22241
Epoch 68: Val Loss 2104.82837
Epoch 69: Val Loss 2104.39648
Epoch 70: Val Loss 2103.95312
Epoch 71: Val Loss 2103.48120
Epoch 72: Val Loss 2102.97583
Epoch 73: Val Loss 2102.46411
Epoch 74: Val Loss 2101.92896
Epoch 75: Val Loss 2101.38452
Epoch 76: Val Loss 2100.82275
Epoch 77: Val Loss 2100.23486
Epoch 78: Val Loss 2099.60425
Epoch 79: Val Loss 2098.95288
Epoch 80: Val Loss 2098.28491
Epoch 81: Val Loss 2097.55688
Epoch 82: Val Loss 2096.78833
Epoch 83: Val Loss 2095.99023
Epoch 84: Val Loss 2095.14941
Epoch 85: Val Loss 2094.28809
Epoch 86: Val Loss 2093.44580
Epoch 87: Val Loss 2092.50562
Epoch 88: Val Loss 2091.52393
Epoch 89: Val Loss 2090.56812
Epoch 90: Val Loss 2089.52563
Epoch 91: Val Loss 2088.47388
Epoch 92: Val Loss 2087.36450
Epoch 93: Val Loss 2086.19800
Epoch 94: Val Loss 2084.96362
Epoch 95: Val Loss 2083.69189
Epoch 96: Val Loss 2082.41528
Epoch 97: Val Loss 2081.03784
Epoch 98: Val Loss 2079.58105
Epoch 99: Val Loss 2078.13428
{'MSE - mean': 1747.8433115807038, 'MSE - std': 605.0925641723986, 'R2 - mean': -0.1285422252883251, 'R2 - std': 0.07037717549922477} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2510.74316
Epoch 1: Val Loss 2510.47144
Epoch 2: Val Loss 2510.19604
Epoch 3: Val Loss 2509.92822
Epoch 4: Val Loss 2509.65918
Epoch 5: Val Loss 2509.39600
Epoch 6: Val Loss 2509.13232
Epoch 7: Val Loss 2508.86792
Epoch 8: Val Loss 2508.60669
Epoch 9: Val Loss 2508.33813
Epoch 10: Val Loss 2508.05957
Epoch 11: Val Loss 2507.78467
Epoch 12: Val Loss 2507.50049
Epoch 13: Val Loss 2507.20825
Epoch 14: Val Loss 2506.92822
Epoch 15: Val Loss 2506.63550
Epoch 16: Val Loss 2506.33960
Epoch 17: Val Loss 2506.02515
Epoch 18: Val Loss 2505.69604
Epoch 19: Val Loss 2505.35425
Epoch 20: Val Loss 2505.00977
Epoch 21: Val Loss 2504.65088
Epoch 22: Val Loss 2504.29395
Epoch 23: Val Loss 2503.93359
Epoch 24: Val Loss 2503.53809
Epoch 25: Val Loss 2503.14771
Epoch 26: Val Loss 2502.75439
Epoch 27: Val Loss 2502.33960
Epoch 28: Val Loss 2501.91675
Epoch 29: Val Loss 2501.48315
Epoch 30: Val Loss 2501.01831
Epoch 31: Val Loss 2500.55103
Epoch 32: Val Loss 2500.05542
Epoch 33: Val Loss 2499.56689
Epoch 34: Val Loss 2499.05151
Epoch 35: Val Loss 2498.53345
Epoch 36: Val Loss 2498.00684
Epoch 37: Val Loss 2497.47974
Epoch 38: Val Loss 2496.93457
Epoch 39: Val Loss 2496.34644
Epoch 40: Val Loss 2495.75513
Epoch 41: Val Loss 2495.15332
Epoch 42: Val Loss 2494.51099
Epoch 43: Val Loss 2493.87891
Epoch 44: Val Loss 2493.21777
Epoch 45: Val Loss 2492.54443
Epoch 46: Val Loss 2491.81982
Epoch 47: Val Loss 2491.07788
Epoch 48: Val Loss 2490.31470
Epoch 49: Val Loss 2489.51050
Epoch 50: Val Loss 2488.70239
Epoch 51: Val Loss 2487.84790
Epoch 52: Val Loss 2486.96436
Epoch 53: Val Loss 2486.05151
Epoch 54: Val Loss 2485.10718
Epoch 55: Val Loss 2484.14917
Epoch 56: Val Loss 2483.14038
Epoch 57: Val Loss 2482.11426
Epoch 58: Val Loss 2481.06494
Epoch 59: Val Loss 2479.97437
Epoch 60: Val Loss 2478.88672
Epoch 61: Val Loss 2477.74854
Epoch 62: Val Loss 2476.59351
Epoch 63: Val Loss 2475.38550
Epoch 64: Val Loss 2474.05469
Epoch 65: Val Loss 2472.65991
Epoch 66: Val Loss 2471.25757
Epoch 67: Val Loss 2469.78320
Epoch 68: Val Loss 2468.32275
Epoch 69: Val Loss 2466.70190
Epoch 70: Val Loss 2465.07275
Epoch 71: Val Loss 2463.23926
Epoch 72: Val Loss 2461.38794
Epoch 73: Val Loss 2459.53247
Epoch 74: Val Loss 2457.55249
Epoch 75: Val Loss 2455.61304
Epoch 76: Val Loss 2453.55469
Epoch 77: Val Loss 2451.44971
Epoch 78: Val Loss 2449.18335
Epoch 79: Val Loss 2446.79663
Epoch 80: Val Loss 2444.50781
Epoch 81: Val Loss 2442.12183
Epoch 82: Val Loss 2439.58057
Epoch 83: Val Loss 2436.87500
Epoch 84: Val Loss 2434.15430
Epoch 85: Val Loss 2431.34912
Epoch 86: Val Loss 2428.46191
Epoch 87: Val Loss 2425.40918
Epoch 88: Val Loss 2422.45654
Epoch 89: Val Loss 2419.36572
Epoch 90: Val Loss 2416.33105
Epoch 91: Val Loss 2412.95557
Epoch 92: Val Loss 2409.45825
Epoch 93: Val Loss 2405.86450
Epoch 94: Val Loss 2402.13477
Epoch 95: Val Loss 2398.18140
Epoch 96: Val Loss 2394.23755
Epoch 97: Val Loss 2390.44922
Epoch 98: Val Loss 2386.58667
Epoch 99: Val Loss 2382.72046
{'MSE - mean': 1874.8187239375227, 'MSE - std': 597.8299343008171, 'R2 - mean': -0.125613810103189, 'R2 - std': 0.0632191420930804} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 1874.8187239375227 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0.3}. Best is trial 18 with value: 328.058245650853.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1288.10999
Epoch 1: Val Loss 1284.74866
Epoch 2: Val Loss 1280.45557
Epoch 3: Val Loss 1274.88464
Epoch 4: Val Loss 1267.36536
Epoch 5: Val Loss 1257.22571
Epoch 6: Val Loss 1244.20300
Epoch 7: Val Loss 1227.15649
Epoch 8: Val Loss 1205.78784
Epoch 9: Val Loss 1178.43091
Epoch 10: Val Loss 1145.65747
Epoch 11: Val Loss 1105.00879
Epoch 12: Val Loss 1058.05310
Epoch 13: Val Loss 1006.80200
Epoch 14: Val Loss 951.12451
Epoch 15: Val Loss 895.20557
Epoch 16: Val Loss 841.84973
Epoch 17: Val Loss 791.61444
Epoch 18: Val Loss 750.20258
Epoch 19: Val Loss 716.75482
Epoch 20: Val Loss 688.62769
Epoch 21: Val Loss 663.14014
Epoch 22: Val Loss 638.34674
Epoch 23: Val Loss 610.84949
Epoch 24: Val Loss 581.74420
Epoch 25: Val Loss 552.45844
Epoch 26: Val Loss 525.27258
Epoch 27: Val Loss 497.65802
Epoch 28: Val Loss 471.29559
Epoch 29: Val Loss 447.57611
Epoch 30: Val Loss 423.70740
Epoch 31: Val Loss 401.07153
Epoch 32: Val Loss 381.29376
Epoch 33: Val Loss 366.38855
Epoch 34: Val Loss 349.28952
Epoch 35: Val Loss 336.41339
Epoch 36: Val Loss 323.37317
Epoch 37: Val Loss 310.59213
Epoch 38: Val Loss 299.39694
Epoch 39: Val Loss 288.40469
Epoch 40: Val Loss 281.27869
Epoch 41: Val Loss 275.18561
Epoch 42: Val Loss 268.79871
Epoch 43: Val Loss 261.47879
Epoch 44: Val Loss 257.75089
Epoch 45: Val Loss 252.74071
Epoch 46: Val Loss 249.73950
Epoch 47: Val Loss 244.18251
Epoch 48: Val Loss 236.29691
Epoch 49: Val Loss 230.48792
Epoch 50: Val Loss 229.16566
Epoch 51: Val Loss 226.94319
Epoch 52: Val Loss 225.94119
Epoch 53: Val Loss 223.90213
Epoch 54: Val Loss 219.75136
Epoch 55: Val Loss 216.25186
Epoch 56: Val Loss 214.36601
Epoch 57: Val Loss 211.88097
Epoch 58: Val Loss 210.07782
Epoch 59: Val Loss 208.57957
Epoch 60: Val Loss 206.53035
Epoch 61: Val Loss 207.57906
Epoch 62: Val Loss 204.96495
Epoch 63: Val Loss 204.16113
Epoch 64: Val Loss 203.49544
Epoch 65: Val Loss 202.55130
Epoch 66: Val Loss 202.28549
Epoch 67: Val Loss 200.25465
Epoch 68: Val Loss 197.90608
Epoch 69: Val Loss 198.56615
Epoch 70: Val Loss 199.99841
Epoch 71: Val Loss 200.45798
Epoch 72: Val Loss 199.80446
Epoch 73: Val Loss 198.26755
Epoch 74: Val Loss 200.63571
Epoch 75: Val Loss 198.83772
Epoch 76: Val Loss 197.57726
Epoch 77: Val Loss 194.56557
Epoch 78: Val Loss 193.99364
Epoch 79: Val Loss 195.30058
Epoch 80: Val Loss 196.43289
Epoch 81: Val Loss 196.96796
Epoch 82: Val Loss 195.99350
Epoch 83: Val Loss 192.04601
Epoch 84: Val Loss 191.42612
Epoch 85: Val Loss 190.16901
Epoch 86: Val Loss 189.32623
Epoch 87: Val Loss 188.61273
Epoch 88: Val Loss 187.56734
Epoch 89: Val Loss 187.71329
Epoch 90: Val Loss 184.74657
Epoch 91: Val Loss 183.73715
Epoch 92: Val Loss 182.87717
Epoch 93: Val Loss 183.53740
Epoch 94: Val Loss 186.32222
Epoch 95: Val Loss 186.55013
Epoch 96: Val Loss 184.10443
Epoch 97: Val Loss 181.30873
Epoch 98: Val Loss 180.02896
Epoch 99: Val Loss 180.77165
{'MSE - mean': 180.0289688377055, 'MSE - std': 0.0, 'R2 - mean': 0.8241455541979464, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1310.09375
Epoch 1: Val Loss 1307.56384
Epoch 2: Val Loss 1304.59131
Epoch 3: Val Loss 1301.17554
Epoch 4: Val Loss 1296.93640
Epoch 5: Val Loss 1291.18494
Epoch 6: Val Loss 1283.78491
Epoch 7: Val Loss 1273.94128
Epoch 8: Val Loss 1261.53821
Epoch 9: Val Loss 1245.29431
Epoch 10: Val Loss 1224.47583
Epoch 11: Val Loss 1198.27771
Epoch 12: Val Loss 1165.96216
Epoch 13: Val Loss 1127.89624
Epoch 14: Val Loss 1083.29797
Epoch 15: Val Loss 1033.20349
Epoch 16: Val Loss 980.25018
Epoch 17: Val Loss 923.20319
Epoch 18: Val Loss 869.86591
Epoch 19: Val Loss 818.67822
Epoch 20: Val Loss 776.10730
Epoch 21: Val Loss 740.48505
Epoch 22: Val Loss 710.91321
Epoch 23: Val Loss 687.12079
Epoch 24: Val Loss 663.69690
Epoch 25: Val Loss 639.57849
Epoch 26: Val Loss 614.07617
Epoch 27: Val Loss 588.07544
Epoch 28: Val Loss 561.11102
Epoch 29: Val Loss 537.63782
Epoch 30: Val Loss 514.46722
Epoch 31: Val Loss 493.17413
Epoch 32: Val Loss 473.43756
Epoch 33: Val Loss 460.04080
Epoch 34: Val Loss 443.63690
Epoch 35: Val Loss 430.41260
Epoch 36: Val Loss 412.62051
Epoch 37: Val Loss 398.10614
Epoch 38: Val Loss 385.71301
Epoch 39: Val Loss 380.73749
Epoch 40: Val Loss 371.14841
Epoch 41: Val Loss 361.62524
Epoch 42: Val Loss 352.96490
Epoch 43: Val Loss 344.61676
Epoch 44: Val Loss 341.38333
Epoch 45: Val Loss 336.42236
Epoch 46: Val Loss 336.77942
Epoch 47: Val Loss 334.28464
Epoch 48: Val Loss 331.45190
Epoch 49: Val Loss 325.72708
Epoch 50: Val Loss 315.22903
Epoch 51: Val Loss 319.34225
Epoch 52: Val Loss 320.95978
Epoch 53: Val Loss 307.51871
Epoch 54: Val Loss 303.26794
Epoch 55: Val Loss 309.21591
Epoch 56: Val Loss 313.96689
Epoch 57: Val Loss 308.77237
Epoch 58: Val Loss 308.16388
Epoch 59: Val Loss 312.53174
Epoch 60: Val Loss 320.37762
Epoch 61: Val Loss 309.48917
Epoch 62: Val Loss 307.62347
Epoch 63: Val Loss 300.59790
Epoch 64: Val Loss 307.76941
Epoch 65: Val Loss 299.73099
Epoch 66: Val Loss 303.42892
Epoch 67: Val Loss 300.01822
Epoch 68: Val Loss 293.56979
Epoch 69: Val Loss 284.55020
Epoch 70: Val Loss 293.69528
Epoch 71: Val Loss 300.97562
Epoch 72: Val Loss 297.14096
Epoch 73: Val Loss 283.87244
Epoch 74: Val Loss 283.18976
Epoch 75: Val Loss 284.54294
Epoch 76: Val Loss 278.59378
Epoch 77: Val Loss 277.04825
Epoch 78: Val Loss 275.57291
Epoch 79: Val Loss 282.69403
Epoch 80: Val Loss 276.39523
Epoch 81: Val Loss 275.60806
Epoch 82: Val Loss 268.69705
Epoch 83: Val Loss 272.54620
Epoch 84: Val Loss 264.85724
Epoch 85: Val Loss 261.11133
Epoch 86: Val Loss 260.71729
Epoch 87: Val Loss 263.49243
Epoch 88: Val Loss 270.24844
Epoch 89: Val Loss 267.40552
Epoch 90: Val Loss 267.65347
Epoch 91: Val Loss 244.57957
Epoch 92: Val Loss 240.80652
Epoch 93: Val Loss 251.28487
Epoch 94: Val Loss 257.74979
Epoch 95: Val Loss 241.57858
Epoch 96: Val Loss 243.13557
Epoch 97: Val Loss 231.46576
Epoch 98: Val Loss 248.37598
Epoch 99: Val Loss 247.52087
{'MSE - mean': 205.74736037674398, 'MSE - std': 25.718391539038507, 'R2 - mean': 0.8018590585759983, 'R2 - std': 0.022286495621948177} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2824.84790
Epoch 1: Val Loss 2822.05737
Epoch 2: Val Loss 2818.62207
Epoch 3: Val Loss 2814.44214
Epoch 4: Val Loss 2808.96802
Epoch 5: Val Loss 2801.76221
Epoch 6: Val Loss 2792.01978
Epoch 7: Val Loss 2779.27979
Epoch 8: Val Loss 2762.87061
Epoch 9: Val Loss 2742.15039
Epoch 10: Val Loss 2716.88086
Epoch 11: Val Loss 2685.89673
Epoch 12: Val Loss 2649.49561
Epoch 13: Val Loss 2604.88037
Epoch 14: Val Loss 2554.73706
Epoch 15: Val Loss 2498.86646
Epoch 16: Val Loss 2436.34814
Epoch 17: Val Loss 2368.08984
Epoch 18: Val Loss 2298.34717
Epoch 19: Val Loss 2227.85303
Epoch 20: Val Loss 2157.88892
Epoch 21: Val Loss 2086.81055
Epoch 22: Val Loss 2018.15479
Epoch 23: Val Loss 1952.38635
Epoch 24: Val Loss 1894.14282
Epoch 25: Val Loss 1839.18018
Epoch 26: Val Loss 1783.18701
Epoch 27: Val Loss 1733.50977
Epoch 28: Val Loss 1680.08655
Epoch 29: Val Loss 1625.65588
Epoch 30: Val Loss 1571.58447
Epoch 31: Val Loss 1522.11365
Epoch 32: Val Loss 1470.97339
Epoch 33: Val Loss 1417.17834
Epoch 34: Val Loss 1365.36877
Epoch 35: Val Loss 1319.95972
Epoch 36: Val Loss 1266.41321
Epoch 37: Val Loss 1222.49756
Epoch 38: Val Loss 1186.30542
Epoch 39: Val Loss 1149.59119
Epoch 40: Val Loss 1117.03589
Epoch 41: Val Loss 1088.21790
Epoch 42: Val Loss 1058.69324
Epoch 43: Val Loss 1017.79993
Epoch 44: Val Loss 986.49475
Epoch 45: Val Loss 963.66901
Epoch 46: Val Loss 941.32007
Epoch 47: Val Loss 921.28711
Epoch 48: Val Loss 888.50507
Epoch 49: Val Loss 867.87634
Epoch 50: Val Loss 848.07428
Epoch 51: Val Loss 831.68964
Epoch 52: Val Loss 813.00238
Epoch 53: Val Loss 800.10590
Epoch 54: Val Loss 786.94995
Epoch 55: Val Loss 769.83063
Epoch 56: Val Loss 755.46698
Epoch 57: Val Loss 742.86444
Epoch 58: Val Loss 725.33411
Epoch 59: Val Loss 713.94696
Epoch 60: Val Loss 696.65802
Epoch 61: Val Loss 688.10864
Epoch 62: Val Loss 687.71582
Epoch 63: Val Loss 685.15063
Epoch 64: Val Loss 681.68390
Epoch 65: Val Loss 663.02356
Epoch 66: Val Loss 648.60632
Epoch 67: Val Loss 639.51801
Epoch 68: Val Loss 634.16663
Epoch 69: Val Loss 633.52509
Epoch 70: Val Loss 625.14868
Epoch 71: Val Loss 613.28748
Epoch 72: Val Loss 602.85760
Epoch 73: Val Loss 598.07330
Epoch 74: Val Loss 580.27142
Epoch 75: Val Loss 568.58466
Epoch 76: Val Loss 559.50427
Epoch 77: Val Loss 555.04126
Epoch 78: Val Loss 543.36267
Epoch 79: Val Loss 538.05609
Epoch 80: Val Loss 537.35229
Epoch 81: Val Loss 532.70935
Epoch 82: Val Loss 521.06891
Epoch 83: Val Loss 517.40118
Epoch 84: Val Loss 506.66879
Epoch 85: Val Loss 492.19598
Epoch 86: Val Loss 492.55835
Epoch 87: Val Loss 501.24377
Epoch 88: Val Loss 499.35715
Epoch 89: Val Loss 490.54010
Epoch 90: Val Loss 477.91809
Epoch 91: Val Loss 460.14011
Epoch 92: Val Loss 464.54529
Epoch 93: Val Loss 462.67459
Epoch 94: Val Loss 466.64563
Epoch 95: Val Loss 460.50284
Epoch 96: Val Loss 451.97052
Epoch 97: Val Loss 443.14365
Epoch 98: Val Loss 438.94357
Epoch 99: Val Loss 436.52325
{'MSE - mean': 282.6726527627017, 'MSE - std': 110.79692381214241, 'R2 - mean': 0.8085137751979832, 'R2 - std': 0.020486477606198063} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2127.46606
Epoch 1: Val Loss 2123.36108
Epoch 2: Val Loss 2117.68579
Epoch 3: Val Loss 2109.81592
Epoch 4: Val Loss 2099.35059
Epoch 5: Val Loss 2085.19214
Epoch 6: Val Loss 2066.29785
Epoch 7: Val Loss 2041.85522
Epoch 8: Val Loss 2010.23535
Epoch 9: Val Loss 1970.65198
Epoch 10: Val Loss 1921.37915
Epoch 11: Val Loss 1862.14453
Epoch 12: Val Loss 1792.72571
Epoch 13: Val Loss 1714.13892
Epoch 14: Val Loss 1631.46204
Epoch 15: Val Loss 1543.82202
Epoch 16: Val Loss 1461.75574
Epoch 17: Val Loss 1385.04883
Epoch 18: Val Loss 1315.17798
Epoch 19: Val Loss 1249.22241
Epoch 20: Val Loss 1192.06982
Epoch 21: Val Loss 1132.95203
Epoch 22: Val Loss 1079.99329
Epoch 23: Val Loss 1029.87415
Epoch 24: Val Loss 981.58087
Epoch 25: Val Loss 936.90326
Epoch 26: Val Loss 888.52637
Epoch 27: Val Loss 841.38855
Epoch 28: Val Loss 800.33624
Epoch 29: Val Loss 764.77557
Epoch 30: Val Loss 732.54742
Epoch 31: Val Loss 703.63208
Epoch 32: Val Loss 677.26636
Epoch 33: Val Loss 652.94562
Epoch 34: Val Loss 629.87701
Epoch 35: Val Loss 607.91040
Epoch 36: Val Loss 588.90546
Epoch 37: Val Loss 573.17761
Epoch 38: Val Loss 557.88062
Epoch 39: Val Loss 543.87604
Epoch 40: Val Loss 532.98615
Epoch 41: Val Loss 521.43713
Epoch 42: Val Loss 508.57346
Epoch 43: Val Loss 499.36148
Epoch 44: Val Loss 490.42053
Epoch 45: Val Loss 481.68878
Epoch 46: Val Loss 473.74338
Epoch 47: Val Loss 466.26385
Epoch 48: Val Loss 459.35294
Epoch 49: Val Loss 455.60437
Epoch 50: Val Loss 450.72226
Epoch 51: Val Loss 443.46558
Epoch 52: Val Loss 439.42972
Epoch 53: Val Loss 435.58813
Epoch 54: Val Loss 432.11826
Epoch 55: Val Loss 426.54099
Epoch 56: Val Loss 419.81747
Epoch 57: Val Loss 420.63455
Epoch 58: Val Loss 414.66415
Epoch 59: Val Loss 408.22635
Epoch 60: Val Loss 407.59308
Epoch 61: Val Loss 403.18808
Epoch 62: Val Loss 393.69037
Epoch 63: Val Loss 386.64270
Epoch 64: Val Loss 383.09961
Epoch 65: Val Loss 379.99820
Epoch 66: Val Loss 379.64502
Epoch 67: Val Loss 380.71136
Epoch 68: Val Loss 372.75342
Epoch 69: Val Loss 372.47385
Epoch 70: Val Loss 374.74091
Epoch 71: Val Loss 373.16537
Epoch 72: Val Loss 365.74939
Epoch 73: Val Loss 359.31689
Epoch 74: Val Loss 354.93033
Epoch 75: Val Loss 359.36972
Epoch 76: Val Loss 353.10809
Epoch 77: Val Loss 345.64716
Epoch 78: Val Loss 353.16833
Epoch 79: Val Loss 345.86292
Epoch 80: Val Loss 343.50961
Epoch 81: Val Loss 342.44467
Epoch 82: Val Loss 347.55444
Epoch 83: Val Loss 338.58862
Epoch 84: Val Loss 332.81119
Epoch 85: Val Loss 331.92004
Epoch 86: Val Loss 333.51880
Epoch 87: Val Loss 331.90039
Epoch 88: Val Loss 326.06555
Epoch 89: Val Loss 322.58533
Epoch 90: Val Loss 324.98917
Epoch 91: Val Loss 324.45224
Epoch 92: Val Loss 320.67606
Epoch 93: Val Loss 324.30261
Epoch 94: Val Loss 316.91666
Epoch 95: Val Loss 311.94522
Epoch 96: Val Loss 308.09940
Epoch 97: Val Loss 314.46811
Epoch 98: Val Loss 316.50534
Epoch 99: Val Loss 314.77625
{'MSE - mean': 289.02934432701227, 'MSE - std': 96.58256223359679, 'R2 - mean': 0.8116917037073649, 'R2 - std': 0.01857604673670273} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2506.85669
Epoch 1: Val Loss 2504.77393
Epoch 2: Val Loss 2502.44531
Epoch 3: Val Loss 2499.34790
Epoch 4: Val Loss 2495.04736
Epoch 5: Val Loss 2488.93555
Epoch 6: Val Loss 2479.92432
Epoch 7: Val Loss 2467.15332
Epoch 8: Val Loss 2448.47656
Epoch 9: Val Loss 2422.65552
Epoch 10: Val Loss 2388.18018
Epoch 11: Val Loss 2343.31812
Epoch 12: Val Loss 2290.62598
Epoch 13: Val Loss 2224.50049
Epoch 14: Val Loss 2145.34351
Epoch 15: Val Loss 2051.65918
Epoch 16: Val Loss 1950.01196
Epoch 17: Val Loss 1849.86853
Epoch 18: Val Loss 1753.87878
Epoch 19: Val Loss 1664.45349
Epoch 20: Val Loss 1582.50806
Epoch 21: Val Loss 1511.92590
Epoch 22: Val Loss 1450.45471
Epoch 23: Val Loss 1395.02368
Epoch 24: Val Loss 1347.53918
Epoch 25: Val Loss 1309.38599
Epoch 26: Val Loss 1263.40881
Epoch 27: Val Loss 1214.11304
Epoch 28: Val Loss 1161.10608
Epoch 29: Val Loss 1119.37598
Epoch 30: Val Loss 1084.70361
Epoch 31: Val Loss 1049.91821
Epoch 32: Val Loss 1010.24200
Epoch 33: Val Loss 969.47644
Epoch 34: Val Loss 929.25391
Epoch 35: Val Loss 901.23053
Epoch 36: Val Loss 881.69019
Epoch 37: Val Loss 859.47430
Epoch 38: Val Loss 842.18445
Epoch 39: Val Loss 823.50061
Epoch 40: Val Loss 809.28827
Epoch 41: Val Loss 787.90015
Epoch 42: Val Loss 773.60211
Epoch 43: Val Loss 758.94214
Epoch 44: Val Loss 744.08600
Epoch 45: Val Loss 726.48920
Epoch 46: Val Loss 713.92773
Epoch 47: Val Loss 694.02991
Epoch 48: Val Loss 683.74982
Epoch 49: Val Loss 658.87823
Epoch 50: Val Loss 646.01013
Epoch 51: Val Loss 639.53265
Epoch 52: Val Loss 638.89899
Epoch 53: Val Loss 638.90607
Epoch 54: Val Loss 628.10645
Epoch 55: Val Loss 618.91840
Epoch 56: Val Loss 608.90771
Epoch 57: Val Loss 599.56396
Epoch 58: Val Loss 593.15814
Epoch 59: Val Loss 586.91840
Epoch 60: Val Loss 582.49268
Epoch 61: Val Loss 573.65369
Epoch 62: Val Loss 563.35577
Epoch 63: Val Loss 557.79120
Epoch 64: Val Loss 558.21814
Epoch 65: Val Loss 553.81476
Epoch 66: Val Loss 546.48871
Epoch 67: Val Loss 545.59888
Epoch 68: Val Loss 543.03638
Epoch 69: Val Loss 535.25500
Epoch 70: Val Loss 527.19116
Epoch 71: Val Loss 523.38232
Epoch 72: Val Loss 511.58795
Epoch 73: Val Loss 510.10779
Epoch 74: Val Loss 510.92307
Epoch 75: Val Loss 513.21051
Epoch 76: Val Loss 510.18011
Epoch 77: Val Loss 502.48184
Epoch 78: Val Loss 496.57956
Epoch 79: Val Loss 494.92169
Epoch 80: Val Loss 492.91980
Epoch 81: Val Loss 488.88007
Epoch 82: Val Loss 483.20847
Epoch 83: Val Loss 476.85406
Epoch 84: Val Loss 475.14365
Epoch 85: Val Loss 472.32507
Epoch 86: Val Loss 470.33353
Epoch 87: Val Loss 466.76935
Epoch 88: Val Loss 464.81189
Epoch 89: Val Loss 464.81979
Epoch 90: Val Loss 462.46555
Epoch 91: Val Loss 463.08368
Epoch 92: Val Loss 458.22949
Epoch 93: Val Loss 448.30426
Epoch 94: Val Loss 443.77966
Epoch 95: Val Loss 440.28018
Epoch 96: Val Loss 439.46945
Epoch 97: Val Loss 439.71313
Epoch 98: Val Loss 436.63055
Epoch 99: Val Loss 434.04471
{'MSE - mean': 318.03241536023376, 'MSE - std': 104.0541473441479, 'R2 - mean': 0.8087709714272077, 'R2 - std': 0.017611880027434326} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 318.03241536023376 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 20 with value: 318.03241536023376.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1291.43958
Epoch 1: Val Loss 1291.23462
Epoch 2: Val Loss 1291.02722
Epoch 3: Val Loss 1290.82544
Epoch 4: Val Loss 1290.62915
Epoch 5: Val Loss 1290.44153
Epoch 6: Val Loss 1290.25110
Epoch 7: Val Loss 1290.06067
Epoch 8: Val Loss 1289.85828
Epoch 9: Val Loss 1289.65576
Epoch 10: Val Loss 1289.45068
Epoch 11: Val Loss 1289.25134
Epoch 12: Val Loss 1289.04858
Epoch 13: Val Loss 1288.83435
Epoch 14: Val Loss 1288.61450
Epoch 15: Val Loss 1288.37817
Epoch 16: Val Loss 1288.14661
Epoch 17: Val Loss 1287.89978
Epoch 18: Val Loss 1287.63818
Epoch 19: Val Loss 1287.34521
Epoch 20: Val Loss 1287.06396
Epoch 21: Val Loss 1286.75757
Epoch 22: Val Loss 1286.42554
Epoch 23: Val Loss 1286.07190
Epoch 24: Val Loss 1285.70349
Epoch 25: Val Loss 1285.31250
Epoch 26: Val Loss 1284.91711
Epoch 27: Val Loss 1284.51160
Epoch 28: Val Loss 1284.07385
Epoch 29: Val Loss 1283.61450
Epoch 30: Val Loss 1283.13281
Epoch 31: Val Loss 1282.62659
Epoch 32: Val Loss 1282.09888
Epoch 33: Val Loss 1281.56030
Epoch 34: Val Loss 1280.97900
Epoch 35: Val Loss 1280.39343
Epoch 36: Val Loss 1279.80286
Epoch 37: Val Loss 1279.17432
Epoch 38: Val Loss 1278.51587
Epoch 39: Val Loss 1277.83508
Epoch 40: Val Loss 1277.11340
Epoch 41: Val Loss 1276.36743
Epoch 42: Val Loss 1275.58215
Epoch 43: Val Loss 1274.79138
Epoch 44: Val Loss 1273.97375
Epoch 45: Val Loss 1273.09668
Epoch 46: Val Loss 1272.18567
Epoch 47: Val Loss 1271.24988
Epoch 48: Val Loss 1270.30200
Epoch 49: Val Loss 1269.35535
Epoch 50: Val Loss 1268.30896
Epoch 51: Val Loss 1267.21460
Epoch 52: Val Loss 1266.09875
Epoch 53: Val Loss 1264.97400
Epoch 54: Val Loss 1263.76611
Epoch 55: Val Loss 1262.53467
Epoch 56: Val Loss 1261.24756
Epoch 57: Val Loss 1259.91858
Epoch 58: Val Loss 1258.50037
Epoch 59: Val Loss 1256.98975
Epoch 60: Val Loss 1255.45483
Epoch 61: Val Loss 1253.92566
Epoch 62: Val Loss 1252.28015
Epoch 63: Val Loss 1250.64478
Epoch 64: Val Loss 1248.93213
Epoch 65: Val Loss 1247.05811
Epoch 66: Val Loss 1245.23230
Epoch 67: Val Loss 1243.39417
Epoch 68: Val Loss 1241.46619
Epoch 69: Val Loss 1239.54822
Epoch 70: Val Loss 1237.53491
Epoch 71: Val Loss 1235.34351
Epoch 72: Val Loss 1233.13623
Epoch 73: Val Loss 1230.90710
Epoch 74: Val Loss 1228.65112
Epoch 75: Val Loss 1226.34985
Epoch 76: Val Loss 1223.99353
Epoch 77: Val Loss 1221.65845
Epoch 78: Val Loss 1219.23157
Epoch 79: Val Loss 1216.57385
Epoch 80: Val Loss 1213.95190
Epoch 81: Val Loss 1211.38440
Epoch 82: Val Loss 1208.67175
Epoch 83: Val Loss 1205.87891
Epoch 84: Val Loss 1202.97095
Epoch 85: Val Loss 1199.97693
Epoch 86: Val Loss 1196.86975
Epoch 87: Val Loss 1193.77844
Epoch 88: Val Loss 1190.69348
Epoch 89: Val Loss 1187.57166
Epoch 90: Val Loss 1184.39124
Epoch 91: Val Loss 1181.11646
Epoch 92: Val Loss 1177.78003
Epoch 93: Val Loss 1174.39111
Epoch 94: Val Loss 1170.97644
Epoch 95: Val Loss 1167.36804
Epoch 96: Val Loss 1163.67480
Epoch 97: Val Loss 1159.74878
Epoch 98: Val Loss 1155.98438
Epoch 99: Val Loss 1152.31274
{'MSE - mean': 1152.3127747167136, 'MSE - std': 0.0, 'R2 - mean': -0.1255928737286265, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1305.58398
Epoch 1: Val Loss 1305.31714
Epoch 2: Val Loss 1305.05286
Epoch 3: Val Loss 1304.78345
Epoch 4: Val Loss 1304.49548
Epoch 5: Val Loss 1304.19800
Epoch 6: Val Loss 1303.88635
Epoch 7: Val Loss 1303.57800
Epoch 8: Val Loss 1303.26733
Epoch 9: Val Loss 1302.94983
Epoch 10: Val Loss 1302.62805
Epoch 11: Val Loss 1302.30798
Epoch 12: Val Loss 1301.96484
Epoch 13: Val Loss 1301.62158
Epoch 14: Val Loss 1301.27795
Epoch 15: Val Loss 1300.93103
Epoch 16: Val Loss 1300.58789
Epoch 17: Val Loss 1300.23352
Epoch 18: Val Loss 1299.88379
Epoch 19: Val Loss 1299.54431
Epoch 20: Val Loss 1299.19507
Epoch 21: Val Loss 1298.83105
Epoch 22: Val Loss 1298.47009
Epoch 23: Val Loss 1298.09583
Epoch 24: Val Loss 1297.69238
Epoch 25: Val Loss 1297.27832
Epoch 26: Val Loss 1296.86633
Epoch 27: Val Loss 1296.46155
Epoch 28: Val Loss 1296.05469
Epoch 29: Val Loss 1295.62207
Epoch 30: Val Loss 1295.19727
Epoch 31: Val Loss 1294.75647
Epoch 32: Val Loss 1294.27832
Epoch 33: Val Loss 1293.81384
Epoch 34: Val Loss 1293.32642
Epoch 35: Val Loss 1292.83960
Epoch 36: Val Loss 1292.33765
Epoch 37: Val Loss 1291.84302
Epoch 38: Val Loss 1291.32971
Epoch 39: Val Loss 1290.77759
Epoch 40: Val Loss 1290.23889
Epoch 41: Val Loss 1289.66345
Epoch 42: Val Loss 1289.07007
Epoch 43: Val Loss 1288.45642
Epoch 44: Val Loss 1287.82825
Epoch 45: Val Loss 1287.15613
Epoch 46: Val Loss 1286.48096
Epoch 47: Val Loss 1285.79651
Epoch 48: Val Loss 1285.06738
Epoch 49: Val Loss 1284.29663
Epoch 50: Val Loss 1283.51331
Epoch 51: Val Loss 1282.60889
Epoch 52: Val Loss 1281.68970
Epoch 53: Val Loss 1280.76782
Epoch 54: Val Loss 1279.77002
Epoch 55: Val Loss 1278.66040
Epoch 56: Val Loss 1277.54138
Epoch 57: Val Loss 1276.35352
Epoch 58: Val Loss 1275.04004
Epoch 59: Val Loss 1273.66980
Epoch 60: Val Loss 1272.25513
Epoch 61: Val Loss 1270.82104
Epoch 62: Val Loss 1269.35095
Epoch 63: Val Loss 1267.84265
Epoch 64: Val Loss 1266.22241
Epoch 65: Val Loss 1264.56921
Epoch 66: Val Loss 1262.91138
Epoch 67: Val Loss 1261.10083
Epoch 68: Val Loss 1259.31885
Epoch 69: Val Loss 1257.50232
Epoch 70: Val Loss 1255.66406
Epoch 71: Val Loss 1253.71741
Epoch 72: Val Loss 1251.73889
Epoch 73: Val Loss 1249.70911
Epoch 74: Val Loss 1247.71509
Epoch 75: Val Loss 1245.67358
Epoch 76: Val Loss 1243.52405
Epoch 77: Val Loss 1241.27014
Epoch 78: Val Loss 1238.98743
Epoch 79: Val Loss 1236.70044
Epoch 80: Val Loss 1234.40173
Epoch 81: Val Loss 1231.98169
Epoch 82: Val Loss 1229.54651
Epoch 83: Val Loss 1226.94470
Epoch 84: Val Loss 1224.29749
Epoch 85: Val Loss 1221.69885
Epoch 86: Val Loss 1219.07178
Epoch 87: Val Loss 1216.38660
Epoch 88: Val Loss 1213.68701
Epoch 89: Val Loss 1210.80457
Epoch 90: Val Loss 1207.94263
Epoch 91: Val Loss 1204.94238
Epoch 92: Val Loss 1201.90674
Epoch 93: Val Loss 1198.90979
Epoch 94: Val Loss 1195.91174
Epoch 95: Val Loss 1192.69666
Epoch 96: Val Loss 1189.51782
Epoch 97: Val Loss 1186.01794
Epoch 98: Val Loss 1182.53979
Epoch 99: Val Loss 1179.23730
{'MSE - mean': 1165.774982495494, 'MSE - std': 13.462207778780225, 'R2 - mean': -0.12429685187171724, 'R2 - std': 0.0012960218569092463} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2823.15186
Epoch 1: Val Loss 2822.75000
Epoch 2: Val Loss 2822.35498
Epoch 3: Val Loss 2821.96045
Epoch 4: Val Loss 2821.57080
Epoch 5: Val Loss 2821.18701
Epoch 6: Val Loss 2820.82935
Epoch 7: Val Loss 2820.47607
Epoch 8: Val Loss 2820.14307
Epoch 9: Val Loss 2819.81763
Epoch 10: Val Loss 2819.50439
Epoch 11: Val Loss 2819.19287
Epoch 12: Val Loss 2818.88354
Epoch 13: Val Loss 2818.59570
Epoch 14: Val Loss 2818.31396
Epoch 15: Val Loss 2818.02954
Epoch 16: Val Loss 2817.75659
Epoch 17: Val Loss 2817.48364
Epoch 18: Val Loss 2817.21289
Epoch 19: Val Loss 2816.95239
Epoch 20: Val Loss 2816.68042
Epoch 21: Val Loss 2816.40479
Epoch 22: Val Loss 2816.14185
Epoch 23: Val Loss 2815.86841
Epoch 24: Val Loss 2815.59473
Epoch 25: Val Loss 2815.29761
Epoch 26: Val Loss 2814.99414
Epoch 27: Val Loss 2814.68994
Epoch 28: Val Loss 2814.34937
Epoch 29: Val Loss 2813.98364
Epoch 30: Val Loss 2813.57666
Epoch 31: Val Loss 2813.15234
Epoch 32: Val Loss 2812.74243
Epoch 33: Val Loss 2812.29102
Epoch 34: Val Loss 2811.84106
Epoch 35: Val Loss 2811.40576
Epoch 36: Val Loss 2810.96558
Epoch 37: Val Loss 2810.50146
Epoch 38: Val Loss 2810.03198
Epoch 39: Val Loss 2809.55688
Epoch 40: Val Loss 2809.05835
Epoch 41: Val Loss 2808.53442
Epoch 42: Val Loss 2808.02051
Epoch 43: Val Loss 2807.49292
Epoch 44: Val Loss 2806.96289
Epoch 45: Val Loss 2806.40479
Epoch 46: Val Loss 2805.81714
Epoch 47: Val Loss 2805.24951
Epoch 48: Val Loss 2804.64282
Epoch 49: Val Loss 2803.99146
Epoch 50: Val Loss 2803.34204
Epoch 51: Val Loss 2802.66064
Epoch 52: Val Loss 2801.96704
Epoch 53: Val Loss 2801.25317
Epoch 54: Val Loss 2800.51270
Epoch 55: Val Loss 2799.72974
Epoch 56: Val Loss 2798.94702
Epoch 57: Val Loss 2798.16943
Epoch 58: Val Loss 2797.31519
Epoch 59: Val Loss 2796.42847
Epoch 60: Val Loss 2795.53174
Epoch 61: Val Loss 2794.67651
Epoch 62: Val Loss 2793.73682
Epoch 63: Val Loss 2792.73584
Epoch 64: Val Loss 2791.73096
Epoch 65: Val Loss 2790.69531
Epoch 66: Val Loss 2789.64722
Epoch 67: Val Loss 2788.61548
Epoch 68: Val Loss 2787.55908
Epoch 69: Val Loss 2786.44263
Epoch 70: Val Loss 2785.30664
Epoch 71: Val Loss 2784.05713
Epoch 72: Val Loss 2782.76514
Epoch 73: Val Loss 2781.45190
Epoch 74: Val Loss 2780.14575
Epoch 75: Val Loss 2778.74683
Epoch 76: Val Loss 2777.27710
Epoch 77: Val Loss 2775.75757
Epoch 78: Val Loss 2774.19922
Epoch 79: Val Loss 2772.64551
Epoch 80: Val Loss 2771.01147
Epoch 81: Val Loss 2769.35645
Epoch 82: Val Loss 2767.63159
Epoch 83: Val Loss 2765.89453
Epoch 84: Val Loss 2764.10986
Epoch 85: Val Loss 2762.19214
Epoch 86: Val Loss 2760.31323
Epoch 87: Val Loss 2758.44434
Epoch 88: Val Loss 2756.46655
Epoch 89: Val Loss 2754.33813
Epoch 90: Val Loss 2752.08179
Epoch 91: Val Loss 2749.88550
Epoch 92: Val Loss 2747.71729
Epoch 93: Val Loss 2745.35107
Epoch 94: Val Loss 2742.90479
Epoch 95: Val Loss 2740.37012
Epoch 96: Val Loss 2737.90015
Epoch 97: Val Loss 2735.11523
Epoch 98: Val Loss 2732.46436
Epoch 99: Val Loss 2729.78345
{'MSE - mean': 1687.1111563951315, 'MSE - std': 737.3626198315264, 'R2 - mean': -0.12093877535142727, 'R2 - std': 0.004865504866023996} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2136.24292
Epoch 1: Val Loss 2135.87891
Epoch 2: Val Loss 2135.51465
Epoch 3: Val Loss 2135.14307
Epoch 4: Val Loss 2134.76318
Epoch 5: Val Loss 2134.38599
Epoch 6: Val Loss 2133.99463
Epoch 7: Val Loss 2133.59814
Epoch 8: Val Loss 2133.16455
Epoch 9: Val Loss 2132.72754
Epoch 10: Val Loss 2132.28027
Epoch 11: Val Loss 2131.82056
Epoch 12: Val Loss 2131.35156
Epoch 13: Val Loss 2130.85059
Epoch 14: Val Loss 2130.32544
Epoch 15: Val Loss 2129.75146
Epoch 16: Val Loss 2129.18677
Epoch 17: Val Loss 2128.58154
Epoch 18: Val Loss 2127.99438
Epoch 19: Val Loss 2127.38696
Epoch 20: Val Loss 2126.73193
Epoch 21: Val Loss 2126.08716
Epoch 22: Val Loss 2125.43433
Epoch 23: Val Loss 2124.77832
Epoch 24: Val Loss 2124.07471
Epoch 25: Val Loss 2123.33032
Epoch 26: Val Loss 2122.56567
Epoch 27: Val Loss 2121.77734
Epoch 28: Val Loss 2120.96265
Epoch 29: Val Loss 2120.07104
Epoch 30: Val Loss 2119.17041
Epoch 31: Val Loss 2118.24805
Epoch 32: Val Loss 2117.28540
Epoch 33: Val Loss 2116.30737
Epoch 34: Val Loss 2115.26221
Epoch 35: Val Loss 2114.17554
Epoch 36: Val Loss 2113.09351
Epoch 37: Val Loss 2111.98901
Epoch 38: Val Loss 2110.82568
Epoch 39: Val Loss 2109.61938
Epoch 40: Val Loss 2108.34814
Epoch 41: Val Loss 2107.04663
Epoch 42: Val Loss 2105.71655
Epoch 43: Val Loss 2104.30298
Epoch 44: Val Loss 2102.78516
Epoch 45: Val Loss 2101.25537
Epoch 46: Val Loss 2099.66455
Epoch 47: Val Loss 2097.90601
Epoch 48: Val Loss 2096.20605
Epoch 49: Val Loss 2094.51782
Epoch 50: Val Loss 2092.73438
Epoch 51: Val Loss 2090.89282
Epoch 52: Val Loss 2088.95532
Epoch 53: Val Loss 2086.90234
Epoch 54: Val Loss 2084.79102
Epoch 55: Val Loss 2082.63062
Epoch 56: Val Loss 2080.39551
Epoch 57: Val Loss 2078.16650
Epoch 58: Val Loss 2075.72729
Epoch 59: Val Loss 2073.26465
Epoch 60: Val Loss 2070.68384
Epoch 61: Val Loss 2068.16821
Epoch 62: Val Loss 2065.55347
Epoch 63: Val Loss 2062.79175
Epoch 64: Val Loss 2060.00586
Epoch 65: Val Loss 2057.02002
Epoch 66: Val Loss 2054.06714
Epoch 67: Val Loss 2051.07983
Epoch 68: Val Loss 2048.02466
Epoch 69: Val Loss 2044.71875
Epoch 70: Val Loss 2041.47803
Epoch 71: Val Loss 2038.00525
Epoch 72: Val Loss 2034.48999
Epoch 73: Val Loss 2030.93433
Epoch 74: Val Loss 2027.27209
Epoch 75: Val Loss 2023.70532
Epoch 76: Val Loss 2020.01428
Epoch 77: Val Loss 2016.35535
Epoch 78: Val Loss 2012.48779
Epoch 79: Val Loss 2008.37390
Epoch 80: Val Loss 2004.27332
Epoch 81: Val Loss 1999.97900
Epoch 82: Val Loss 1995.73621
Epoch 83: Val Loss 1991.44727
Epoch 84: Val Loss 1986.92969
Epoch 85: Val Loss 1982.40039
Epoch 86: Val Loss 1977.36609
Epoch 87: Val Loss 1972.64636
Epoch 88: Val Loss 1967.71887
Epoch 89: Val Loss 1962.90588
Epoch 90: Val Loss 1957.74133
Epoch 91: Val Loss 1952.43774
Epoch 92: Val Loss 1947.14624
Epoch 93: Val Loss 1941.73157
Epoch 94: Val Loss 1936.18750
Epoch 95: Val Loss 1930.64526
Epoch 96: Val Loss 1925.06519
Epoch 97: Val Loss 1919.38794
Epoch 98: Val Loss 1913.76868
Epoch 99: Val Loss 1907.84460
{'MSE - mean': 1742.2945275536063, 'MSE - std': 645.6882670459879, 'R2 - mean': -0.1174605205548761, 'R2 - std': 0.0073518448358334595} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2530.95361
Epoch 1: Val Loss 2530.80713
Epoch 2: Val Loss 2530.66187
Epoch 3: Val Loss 2530.51733
Epoch 4: Val Loss 2530.36523
Epoch 5: Val Loss 2530.21338
Epoch 6: Val Loss 2530.06519
Epoch 7: Val Loss 2529.91162
Epoch 8: Val Loss 2529.75659
Epoch 9: Val Loss 2529.59961
Epoch 10: Val Loss 2529.43677
Epoch 11: Val Loss 2529.26416
Epoch 12: Val Loss 2529.08252
Epoch 13: Val Loss 2528.89673
Epoch 14: Val Loss 2528.70532
Epoch 15: Val Loss 2528.50757
Epoch 16: Val Loss 2528.31714
Epoch 17: Val Loss 2528.10864
Epoch 18: Val Loss 2527.90039
Epoch 19: Val Loss 2527.68042
Epoch 20: Val Loss 2527.46045
Epoch 21: Val Loss 2527.22412
Epoch 22: Val Loss 2526.97510
Epoch 23: Val Loss 2526.72046
Epoch 24: Val Loss 2526.45898
Epoch 25: Val Loss 2526.18335
Epoch 26: Val Loss 2525.89331
Epoch 27: Val Loss 2525.58301
Epoch 28: Val Loss 2525.26953
Epoch 29: Val Loss 2524.93896
Epoch 30: Val Loss 2524.58105
Epoch 31: Val Loss 2524.20508
Epoch 32: Val Loss 2523.78833
Epoch 33: Val Loss 2523.36499
Epoch 34: Val Loss 2522.92114
Epoch 35: Val Loss 2522.46973
Epoch 36: Val Loss 2521.95996
Epoch 37: Val Loss 2521.45605
Epoch 38: Val Loss 2520.91724
Epoch 39: Val Loss 2520.36523
Epoch 40: Val Loss 2519.78589
Epoch 41: Val Loss 2519.15112
Epoch 42: Val Loss 2518.48901
Epoch 43: Val Loss 2517.78223
Epoch 44: Val Loss 2517.08350
Epoch 45: Val Loss 2516.34326
Epoch 46: Val Loss 2515.59155
Epoch 47: Val Loss 2514.78540
Epoch 48: Val Loss 2513.94043
Epoch 49: Val Loss 2513.01343
Epoch 50: Val Loss 2512.05957
Epoch 51: Val Loss 2511.10938
Epoch 52: Val Loss 2510.12109
Epoch 53: Val Loss 2509.06152
Epoch 54: Val Loss 2507.84595
Epoch 55: Val Loss 2506.63428
Epoch 56: Val Loss 2505.31104
Epoch 57: Val Loss 2503.90308
Epoch 58: Val Loss 2502.41821
Epoch 59: Val Loss 2500.81006
Epoch 60: Val Loss 2499.21216
Epoch 61: Val Loss 2497.52319
Epoch 62: Val Loss 2495.73169
Epoch 63: Val Loss 2493.86304
Epoch 64: Val Loss 2491.93091
Epoch 65: Val Loss 2490.02832
Epoch 66: Val Loss 2487.96973
Epoch 67: Val Loss 2485.79590
Epoch 68: Val Loss 2483.62842
Epoch 69: Val Loss 2481.37598
Epoch 70: Val Loss 2479.09473
Epoch 71: Val Loss 2476.68604
Epoch 72: Val Loss 2474.07520
Epoch 73: Val Loss 2471.37012
Epoch 74: Val Loss 2468.61841
Epoch 75: Val Loss 2465.88916
Epoch 76: Val Loss 2463.02393
Epoch 77: Val Loss 2460.08203
Epoch 78: Val Loss 2456.91064
Epoch 79: Val Loss 2453.78687
Epoch 80: Val Loss 2450.63159
Epoch 81: Val Loss 2447.27417
Epoch 82: Val Loss 2443.90698
Epoch 83: Val Loss 2440.28223
Epoch 84: Val Loss 2436.70703
Epoch 85: Val Loss 2433.02930
Epoch 86: Val Loss 2429.31323
Epoch 87: Val Loss 2425.47241
Epoch 88: Val Loss 2421.41504
Epoch 89: Val Loss 2417.32959
Epoch 90: Val Loss 2413.21484
Epoch 91: Val Loss 2409.18994
Epoch 92: Val Loss 2404.78735
Epoch 93: Val Loss 2400.12769
Epoch 94: Val Loss 2395.53076
Epoch 95: Val Loss 2391.01953
Epoch 96: Val Loss 2386.27661
Epoch 97: Val Loss 2381.41821
Epoch 98: Val Loss 2376.05664
Epoch 99: Val Loss 2370.81714
{'MSE - mean': 1867.9990038280866, 'MSE - std': 629.8707264053843, 'R2 - mean': -0.11563549399818726, 'R2 - std': 0.007520810177275777} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 1867.9990038280866 and parameters: {'dim': 64, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 20 with value: 318.03241536023376.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1272.11157
Epoch 1: Val Loss 1267.81384
Epoch 2: Val Loss 1262.67712
Epoch 3: Val Loss 1256.48462
Epoch 4: Val Loss 1249.36780
Epoch 5: Val Loss 1240.35559
Epoch 6: Val Loss 1229.09192
Epoch 7: Val Loss 1214.43787
Epoch 8: Val Loss 1195.29260
Epoch 9: Val Loss 1170.03735
Epoch 10: Val Loss 1138.97705
Epoch 11: Val Loss 1102.08044
Epoch 12: Val Loss 1060.07288
Epoch 13: Val Loss 1011.99402
Epoch 14: Val Loss 955.92163
Epoch 15: Val Loss 898.08466
Epoch 16: Val Loss 840.42181
Epoch 17: Val Loss 785.42926
Epoch 18: Val Loss 735.41705
Epoch 19: Val Loss 695.29541
Epoch 20: Val Loss 664.03522
Epoch 21: Val Loss 637.64917
Epoch 22: Val Loss 612.88336
Epoch 23: Val Loss 589.43958
Epoch 24: Val Loss 560.56927
Epoch 25: Val Loss 532.86792
Epoch 26: Val Loss 505.08136
Epoch 27: Val Loss 478.02701
Epoch 28: Val Loss 453.42081
Epoch 29: Val Loss 428.45856
Epoch 30: Val Loss 408.44601
Epoch 31: Val Loss 392.31152
Epoch 32: Val Loss 374.20105
Epoch 33: Val Loss 351.78030
Epoch 34: Val Loss 335.11371
Epoch 35: Val Loss 319.61914
Epoch 36: Val Loss 307.33981
Epoch 37: Val Loss 297.27960
Epoch 38: Val Loss 290.06009
Epoch 39: Val Loss 280.10977
Epoch 40: Val Loss 272.00519
Epoch 41: Val Loss 262.04178
Epoch 42: Val Loss 252.25635
Epoch 43: Val Loss 245.31906
Epoch 44: Val Loss 240.59474
Epoch 45: Val Loss 237.07925
Epoch 46: Val Loss 238.01111
Epoch 47: Val Loss 233.24101
Epoch 48: Val Loss 235.28175
Epoch 49: Val Loss 231.24693
Epoch 50: Val Loss 220.98976
Epoch 51: Val Loss 214.51822
Epoch 52: Val Loss 211.18709
Epoch 53: Val Loss 209.40581
Epoch 54: Val Loss 207.78279
Epoch 55: Val Loss 208.31885
Epoch 56: Val Loss 207.46695
Epoch 57: Val Loss 209.90506
Epoch 58: Val Loss 207.63126
Epoch 59: Val Loss 204.36729
Epoch 60: Val Loss 201.36203
Epoch 61: Val Loss 200.02496
Epoch 62: Val Loss 199.94630
Epoch 63: Val Loss 201.83904
Epoch 64: Val Loss 205.29164
Epoch 65: Val Loss 205.08403
Epoch 66: Val Loss 204.37880
Epoch 67: Val Loss 202.14317
Epoch 68: Val Loss 201.63028
Epoch 69: Val Loss 202.44469
Epoch 70: Val Loss 202.71024
Epoch 71: Val Loss 201.36537
Epoch 72: Val Loss 200.64127
Epoch 73: Val Loss 200.34052
Epoch 74: Val Loss 202.18414
Epoch 75: Val Loss 200.89601
Epoch 76: Val Loss 200.20961
Epoch 77: Val Loss 200.62369
Epoch 78: Val Loss 201.61160
Epoch 79: Val Loss 202.06705
Epoch 80: Val Loss 201.86559
Epoch 81: Val Loss 199.30539
Epoch 82: Val Loss 198.00246
Epoch 83: Val Loss 198.68263
Epoch 84: Val Loss 198.25667
Epoch 85: Val Loss 198.82785
Epoch 86: Val Loss 199.08490
Epoch 87: Val Loss 197.91473
Epoch 88: Val Loss 196.64005
Epoch 89: Val Loss 195.41328
Epoch 90: Val Loss 194.30956
Epoch 91: Val Loss 192.73666
Epoch 92: Val Loss 192.29819
Epoch 93: Val Loss 191.72969
Epoch 94: Val Loss 191.55766
Epoch 95: Val Loss 191.32045
Epoch 96: Val Loss 190.61584
Epoch 97: Val Loss 191.90561
Epoch 98: Val Loss 192.48532
Epoch 99: Val Loss 193.24522
{'MSE - mean': 190.61585064148383, 'MSE - std': 0.0, 'R2 - mean': 0.8138041616743152, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1295.69446
Epoch 1: Val Loss 1293.11011
Epoch 2: Val Loss 1290.09985
Epoch 3: Val Loss 1286.33630
Epoch 4: Val Loss 1281.42017
Epoch 5: Val Loss 1274.83997
Epoch 6: Val Loss 1265.82544
Epoch 7: Val Loss 1253.66040
Epoch 8: Val Loss 1236.74292
Epoch 9: Val Loss 1213.79407
Epoch 10: Val Loss 1185.55396
Epoch 11: Val Loss 1150.84302
Epoch 12: Val Loss 1109.60657
Epoch 13: Val Loss 1058.49597
Epoch 14: Val Loss 1002.17303
Epoch 15: Val Loss 941.19165
Epoch 16: Val Loss 879.65540
Epoch 17: Val Loss 820.88116
Epoch 18: Val Loss 770.31622
Epoch 19: Val Loss 728.57654
Epoch 20: Val Loss 697.47424
Epoch 21: Val Loss 671.95709
Epoch 22: Val Loss 648.32263
Epoch 23: Val Loss 623.57507
Epoch 24: Val Loss 597.34161
Epoch 25: Val Loss 571.10785
Epoch 26: Val Loss 550.52509
Epoch 27: Val Loss 527.36023
Epoch 28: Val Loss 503.15396
Epoch 29: Val Loss 484.44482
Epoch 30: Val Loss 466.48688
Epoch 31: Val Loss 447.21320
Epoch 32: Val Loss 437.23914
Epoch 33: Val Loss 422.90768
Epoch 34: Val Loss 410.90280
Epoch 35: Val Loss 399.37741
Epoch 36: Val Loss 394.74091
Epoch 37: Val Loss 384.37601
Epoch 38: Val Loss 377.73450
Epoch 39: Val Loss 374.79343
Epoch 40: Val Loss 374.20905
Epoch 41: Val Loss 366.95593
Epoch 42: Val Loss 366.03366
Epoch 43: Val Loss 366.62494
Epoch 44: Val Loss 368.10471
Epoch 45: Val Loss 360.28494
Epoch 46: Val Loss 354.44705
Epoch 47: Val Loss 351.68567
Epoch 48: Val Loss 347.04059
Epoch 49: Val Loss 340.41562
Epoch 50: Val Loss 344.57285
Epoch 51: Val Loss 343.58496
Epoch 52: Val Loss 339.46637
Epoch 53: Val Loss 332.12628
Epoch 54: Val Loss 326.86331
Epoch 55: Val Loss 330.81979
Epoch 56: Val Loss 335.85080
Epoch 57: Val Loss 335.88629
Epoch 58: Val Loss 339.75818
Epoch 59: Val Loss 338.37357
Epoch 60: Val Loss 325.27258
Epoch 61: Val Loss 316.64685
Epoch 62: Val Loss 316.68179
Epoch 63: Val Loss 315.95566
Epoch 64: Val Loss 326.28870
Epoch 65: Val Loss 322.03302
Epoch 66: Val Loss 316.15137
Epoch 67: Val Loss 307.30264
Epoch 68: Val Loss 305.16489
Epoch 69: Val Loss 308.07526
Epoch 70: Val Loss 311.55145
Epoch 71: Val Loss 307.75742
Epoch 72: Val Loss 304.75751
Epoch 73: Val Loss 302.87967
Epoch 74: Val Loss 295.03540
Epoch 75: Val Loss 303.33902
Epoch 76: Val Loss 304.89264
Epoch 77: Val Loss 298.19095
Epoch 78: Val Loss 293.94095
Epoch 79: Val Loss 289.36411
Epoch 80: Val Loss 290.93939
Epoch 81: Val Loss 295.61581
Epoch 82: Val Loss 295.80038
Epoch 83: Val Loss 296.52246
Epoch 84: Val Loss 294.88663
Epoch 85: Val Loss 287.72513
Epoch 86: Val Loss 286.67145
Epoch 87: Val Loss 292.83502
Epoch 88: Val Loss 284.47284
Epoch 89: Val Loss 290.08786
Epoch 90: Val Loss 286.54810
Epoch 91: Val Loss 285.65259
Epoch 92: Val Loss 281.74316
Epoch 93: Val Loss 277.94717
Epoch 94: Val Loss 281.29849
Epoch 95: Val Loss 275.69348
Epoch 96: Val Loss 272.10318
Epoch 97: Val Loss 271.49384
Epoch 98: Val Loss 279.74701
Epoch 99: Val Loss 267.92014
{'MSE - mean': 229.26799997471755, 'MSE - std': 38.65214933323371, 'R2 - mean': 0.7793303961362446, 'R2 - std': 0.03447376553807069} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2820.83105
Epoch 1: Val Loss 2816.93262
Epoch 2: Val Loss 2813.13086
Epoch 3: Val Loss 2808.95850
Epoch 4: Val Loss 2803.79663
Epoch 5: Val Loss 2797.22559
Epoch 6: Val Loss 2789.12744
Epoch 7: Val Loss 2778.56201
Epoch 8: Val Loss 2764.92358
Epoch 9: Val Loss 2747.97339
Epoch 10: Val Loss 2725.86426
Epoch 11: Val Loss 2696.97583
Epoch 12: Val Loss 2660.99023
Epoch 13: Val Loss 2616.76025
Epoch 14: Val Loss 2562.89722
Epoch 15: Val Loss 2498.45850
Epoch 16: Val Loss 2425.34570
Epoch 17: Val Loss 2345.86255
Epoch 18: Val Loss 2263.73315
Epoch 19: Val Loss 2176.69727
Epoch 20: Val Loss 2092.03223
Epoch 21: Val Loss 2012.16663
Epoch 22: Val Loss 1943.24377
Epoch 23: Val Loss 1882.48987
Epoch 24: Val Loss 1813.59900
Epoch 25: Val Loss 1756.79089
Epoch 26: Val Loss 1700.86682
Epoch 27: Val Loss 1643.44946
Epoch 28: Val Loss 1587.04285
Epoch 29: Val Loss 1543.60254
Epoch 30: Val Loss 1498.87451
Epoch 31: Val Loss 1455.97046
Epoch 32: Val Loss 1400.49841
Epoch 33: Val Loss 1341.21179
Epoch 34: Val Loss 1297.59607
Epoch 35: Val Loss 1251.96997
Epoch 36: Val Loss 1205.92847
Epoch 37: Val Loss 1168.84473
Epoch 38: Val Loss 1135.10425
Epoch 39: Val Loss 1099.50635
Epoch 40: Val Loss 1059.46411
Epoch 41: Val Loss 1033.05176
Epoch 42: Val Loss 1004.22260
Epoch 43: Val Loss 976.63922
Epoch 44: Val Loss 955.36615
Epoch 45: Val Loss 931.11053
Epoch 46: Val Loss 911.46704
Epoch 47: Val Loss 886.18396
Epoch 48: Val Loss 868.30981
Epoch 49: Val Loss 855.38123
Epoch 50: Val Loss 839.44818
Epoch 51: Val Loss 820.76007
Epoch 52: Val Loss 810.43103
Epoch 53: Val Loss 788.38177
Epoch 54: Val Loss 773.44373
Epoch 55: Val Loss 762.38080
Epoch 56: Val Loss 746.38190
Epoch 57: Val Loss 733.97485
Epoch 58: Val Loss 722.96942
Epoch 59: Val Loss 716.22449
Epoch 60: Val Loss 699.56885
Epoch 61: Val Loss 690.68671
Epoch 62: Val Loss 678.58160
Epoch 63: Val Loss 666.60883
Epoch 64: Val Loss 655.09576
Epoch 65: Val Loss 649.36591
Epoch 66: Val Loss 637.28241
Epoch 67: Val Loss 634.11743
Epoch 68: Val Loss 637.82465
Epoch 69: Val Loss 621.26318
Epoch 70: Val Loss 612.69086
Epoch 71: Val Loss 610.70984
Epoch 72: Val Loss 604.43335
Epoch 73: Val Loss 595.60297
Epoch 74: Val Loss 591.78412
Epoch 75: Val Loss 588.87482
Epoch 76: Val Loss 581.79425
Epoch 77: Val Loss 576.24054
Epoch 78: Val Loss 566.85980
Epoch 79: Val Loss 551.54724
Epoch 80: Val Loss 547.15875
Epoch 81: Val Loss 546.20129
Epoch 82: Val Loss 543.12457
Epoch 83: Val Loss 539.71051
Epoch 84: Val Loss 542.18890
Epoch 85: Val Loss 533.68402
Epoch 86: Val Loss 525.72113
Epoch 87: Val Loss 513.39160
Epoch 88: Val Loss 510.40094
Epoch 89: Val Loss 514.32062
Epoch 90: Val Loss 513.37567
Epoch 91: Val Loss 512.07288
Epoch 92: Val Loss 505.81772
Epoch 93: Val Loss 498.35724
Epoch 94: Val Loss 492.95792
Epoch 95: Val Loss 487.48538
Epoch 96: Val Loss 481.55957
Epoch 97: Val Loss 479.37338
Epoch 98: Val Loss 472.17633
Epoch 99: Val Loss 471.03949
{'MSE - mean': 309.85849759668525, 'MSE - std': 118.2609362636926, 'R2 - mean': 0.7887984712247889, 'R2 - std': 0.031170219205953684} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2113.48364
Epoch 1: Val Loss 2109.47339
Epoch 2: Val Loss 2103.88940
Epoch 3: Val Loss 2096.30835
Epoch 4: Val Loss 2086.55347
Epoch 5: Val Loss 2073.12549
Epoch 6: Val Loss 2055.07666
Epoch 7: Val Loss 2031.77710
Epoch 8: Val Loss 2000.64282
Epoch 9: Val Loss 1958.62280
Epoch 10: Val Loss 1902.83801
Epoch 11: Val Loss 1836.06494
Epoch 12: Val Loss 1756.06885
Epoch 13: Val Loss 1665.29041
Epoch 14: Val Loss 1567.22827
Epoch 15: Val Loss 1467.51624
Epoch 16: Val Loss 1375.59961
Epoch 17: Val Loss 1299.30359
Epoch 18: Val Loss 1234.26917
Epoch 19: Val Loss 1178.10242
Epoch 20: Val Loss 1127.66150
Epoch 21: Val Loss 1078.06580
Epoch 22: Val Loss 1029.23438
Epoch 23: Val Loss 980.27924
Epoch 24: Val Loss 934.09985
Epoch 25: Val Loss 891.06915
Epoch 26: Val Loss 850.49603
Epoch 27: Val Loss 811.63043
Epoch 28: Val Loss 775.34363
Epoch 29: Val Loss 741.22174
Epoch 30: Val Loss 708.01666
Epoch 31: Val Loss 677.55444
Epoch 32: Val Loss 649.43207
Epoch 33: Val Loss 624.25354
Epoch 34: Val Loss 602.28186
Epoch 35: Val Loss 582.03357
Epoch 36: Val Loss 564.95184
Epoch 37: Val Loss 550.55542
Epoch 38: Val Loss 537.85425
Epoch 39: Val Loss 522.98309
Epoch 40: Val Loss 511.22009
Epoch 41: Val Loss 499.12805
Epoch 42: Val Loss 489.18842
Epoch 43: Val Loss 480.52496
Epoch 44: Val Loss 473.75012
Epoch 45: Val Loss 465.67645
Epoch 46: Val Loss 458.40524
Epoch 47: Val Loss 451.44910
Epoch 48: Val Loss 444.52698
Epoch 49: Val Loss 440.85760
Epoch 50: Val Loss 435.50531
Epoch 51: Val Loss 432.21463
Epoch 52: Val Loss 432.60544
Epoch 53: Val Loss 428.17688
Epoch 54: Val Loss 419.05005
Epoch 55: Val Loss 414.05667
Epoch 56: Val Loss 410.00156
Epoch 57: Val Loss 402.51230
Epoch 58: Val Loss 399.87668
Epoch 59: Val Loss 399.01050
Epoch 60: Val Loss 396.39969
Epoch 61: Val Loss 391.08664
Epoch 62: Val Loss 389.47147
Epoch 63: Val Loss 385.65948
Epoch 64: Val Loss 382.05096
Epoch 65: Val Loss 380.34851
Epoch 66: Val Loss 376.00562
Epoch 67: Val Loss 375.20166
Epoch 68: Val Loss 376.11768
Epoch 69: Val Loss 376.98956
Epoch 70: Val Loss 372.86105
Epoch 71: Val Loss 370.26978
Epoch 72: Val Loss 362.34280
Epoch 73: Val Loss 362.80472
Epoch 74: Val Loss 359.25955
Epoch 75: Val Loss 365.24701
Epoch 76: Val Loss 364.44073
Epoch 77: Val Loss 355.54556
Epoch 78: Val Loss 360.77130
Epoch 79: Val Loss 367.23245
Epoch 80: Val Loss 356.26691
Epoch 81: Val Loss 357.67407
Epoch 82: Val Loss 344.25125
Epoch 83: Val Loss 342.04306
Epoch 84: Val Loss 341.41150
Epoch 85: Val Loss 342.69073
Epoch 86: Val Loss 338.64532
Epoch 87: Val Loss 336.72037
Epoch 88: Val Loss 338.53061
Epoch 89: Val Loss 337.48581
Epoch 90: Val Loss 333.17841
Epoch 91: Val Loss 328.57123
Epoch 92: Val Loss 326.13489
Epoch 93: Val Loss 326.90530
Epoch 94: Val Loss 342.22440
Epoch 95: Val Loss 334.39969
Epoch 96: Val Loss 321.60599
Epoch 97: Val Loss 316.03619
Epoch 98: Val Loss 318.22025
Epoch 99: Val Loss 321.92590
{'MSE - mean': 311.4029254748681, 'MSE - std': 102.45190362744168, 'R2 - mean': 0.795753896255801, 'R2 - std': 0.029560458996715423} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2504.84351
Epoch 1: Val Loss 2501.00757
Epoch 2: Val Loss 2496.16724
Epoch 3: Val Loss 2489.78296
Epoch 4: Val Loss 2481.11694
Epoch 5: Val Loss 2469.26270
Epoch 6: Val Loss 2452.64404
Epoch 7: Val Loss 2431.26001
Epoch 8: Val Loss 2403.07251
Epoch 9: Val Loss 2367.44067
Epoch 10: Val Loss 2322.41309
Epoch 11: Val Loss 2268.07568
Epoch 12: Val Loss 2205.23682
Epoch 13: Val Loss 2131.25537
Epoch 14: Val Loss 2048.57056
Epoch 15: Val Loss 1958.98267
Epoch 16: Val Loss 1868.91467
Epoch 17: Val Loss 1782.41614
Epoch 18: Val Loss 1707.09058
Epoch 19: Val Loss 1633.88342
Epoch 20: Val Loss 1579.82654
Epoch 21: Val Loss 1525.09363
Epoch 22: Val Loss 1473.40430
Epoch 23: Val Loss 1418.31091
Epoch 24: Val Loss 1362.27966
Epoch 25: Val Loss 1313.30200
Epoch 26: Val Loss 1266.36035
Epoch 27: Val Loss 1217.07410
Epoch 28: Val Loss 1173.92383
Epoch 29: Val Loss 1126.97314
Epoch 30: Val Loss 1086.79736
Epoch 31: Val Loss 1050.49817
Epoch 32: Val Loss 1015.29047
Epoch 33: Val Loss 971.84314
Epoch 34: Val Loss 936.92212
Epoch 35: Val Loss 907.82275
Epoch 36: Val Loss 874.22113
Epoch 37: Val Loss 849.65021
Epoch 38: Val Loss 829.51324
Epoch 39: Val Loss 807.02924
Epoch 40: Val Loss 779.55084
Epoch 41: Val Loss 762.61517
Epoch 42: Val Loss 741.01959
Epoch 43: Val Loss 719.93848
Epoch 44: Val Loss 707.91003
Epoch 45: Val Loss 699.83209
Epoch 46: Val Loss 674.91510
Epoch 47: Val Loss 659.75244
Epoch 48: Val Loss 640.08209
Epoch 49: Val Loss 626.95209
Epoch 50: Val Loss 611.67334
Epoch 51: Val Loss 612.38763
Epoch 52: Val Loss 608.46942
Epoch 53: Val Loss 601.08215
Epoch 54: Val Loss 593.43921
Epoch 55: Val Loss 572.32233
Epoch 56: Val Loss 562.63953
Epoch 57: Val Loss 555.88654
Epoch 58: Val Loss 550.01678
Epoch 59: Val Loss 537.47369
Epoch 60: Val Loss 529.23376
Epoch 61: Val Loss 520.53876
Epoch 62: Val Loss 519.21265
Epoch 63: Val Loss 515.79382
Epoch 64: Val Loss 514.18512
Epoch 65: Val Loss 511.56635
Epoch 66: Val Loss 495.84027
Epoch 67: Val Loss 485.81650
Epoch 68: Val Loss 480.74313
Epoch 69: Val Loss 480.01508
Epoch 70: Val Loss 477.74521
Epoch 71: Val Loss 474.41635
Epoch 72: Val Loss 462.32196
Epoch 73: Val Loss 461.03082
Epoch 74: Val Loss 459.89340
Epoch 75: Val Loss 455.80426
Epoch 76: Val Loss 452.12021
Epoch 77: Val Loss 449.54364
Epoch 78: Val Loss 448.91208
Epoch 79: Val Loss 445.80154
Epoch 80: Val Loss 441.81393
Epoch 81: Val Loss 436.83463
Epoch 82: Val Loss 435.84991
Epoch 83: Val Loss 433.28894
Epoch 84: Val Loss 434.24155
Epoch 85: Val Loss 430.20367
Epoch 86: Val Loss 425.47754
Epoch 87: Val Loss 424.23187
Epoch 88: Val Loss 424.66153
Epoch 89: Val Loss 424.49719
Epoch 90: Val Loss 416.83456
Epoch 91: Val Loss 412.31375
Epoch 92: Val Loss 406.63184
Epoch 93: Val Loss 407.22430
Epoch 94: Val Loss 407.53806
Epoch 95: Val Loss 405.39505
Epoch 96: Val Loss 398.34253
Epoch 97: Val Loss 395.45752
Epoch 98: Val Loss 391.00458
Epoch 99: Val Loss 390.43039
{'MSE - mean': 327.20842656824055, 'MSE - std': 96.93487249055251, 'R2 - mean': 0.8000985804082996, 'R2 - std': 0.0278309487875833} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 327.20842656824055 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 20 with value: 318.03241536023376.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1290.12805
Epoch 1: Val Loss 1288.88208
Epoch 2: Val Loss 1287.33936
Epoch 3: Val Loss 1285.45581
Epoch 4: Val Loss 1283.24243
Epoch 5: Val Loss 1280.77686
Epoch 6: Val Loss 1277.68762
Epoch 7: Val Loss 1274.08691
Epoch 8: Val Loss 1269.67957
Epoch 9: Val Loss 1263.90674
Epoch 10: Val Loss 1256.21729
Epoch 11: Val Loss 1246.28931
Epoch 12: Val Loss 1233.40076
Epoch 13: Val Loss 1215.77271
Epoch 14: Val Loss 1193.70312
Epoch 15: Val Loss 1164.74854
Epoch 16: Val Loss 1129.18066
Epoch 17: Val Loss 1090.02124
Epoch 18: Val Loss 1045.53992
Epoch 19: Val Loss 995.13782
Epoch 20: Val Loss 940.62402
Epoch 21: Val Loss 885.86865
Epoch 22: Val Loss 834.84821
Epoch 23: Val Loss 790.76477
Epoch 24: Val Loss 753.01172
Epoch 25: Val Loss 722.93793
Epoch 26: Val Loss 696.46747
Epoch 27: Val Loss 669.61920
Epoch 28: Val Loss 642.06909
Epoch 29: Val Loss 612.71954
Epoch 30: Val Loss 580.74445
Epoch 31: Val Loss 545.59686
Epoch 32: Val Loss 514.98175
Epoch 33: Val Loss 484.57315
Epoch 34: Val Loss 455.67905
Epoch 35: Val Loss 431.40674
Epoch 36: Val Loss 410.35916
Epoch 37: Val Loss 392.15466
Epoch 38: Val Loss 378.24741
Epoch 39: Val Loss 363.04605
Epoch 40: Val Loss 350.75620
Epoch 41: Val Loss 342.92285
Epoch 42: Val Loss 329.91763
Epoch 43: Val Loss 323.66187
Epoch 44: Val Loss 314.45190
Epoch 45: Val Loss 310.72308
Epoch 46: Val Loss 301.89359
Epoch 47: Val Loss 293.82156
Epoch 48: Val Loss 286.69495
Epoch 49: Val Loss 278.34192
Epoch 50: Val Loss 268.08606
Epoch 51: Val Loss 261.32532
Epoch 52: Val Loss 253.80467
Epoch 53: Val Loss 249.76294
Epoch 54: Val Loss 245.73882
Epoch 55: Val Loss 245.91800
Epoch 56: Val Loss 234.87665
Epoch 57: Val Loss 230.40648
Epoch 58: Val Loss 226.04738
Epoch 59: Val Loss 226.13190
Epoch 60: Val Loss 220.44135
Epoch 61: Val Loss 216.48160
Epoch 62: Val Loss 213.53569
Epoch 63: Val Loss 211.61655
Epoch 64: Val Loss 213.70226
Epoch 65: Val Loss 209.91112
Epoch 66: Val Loss 207.83696
Epoch 67: Val Loss 205.89107
Epoch 68: Val Loss 204.71628
Epoch 69: Val Loss 203.61539
Epoch 70: Val Loss 202.70039
Epoch 71: Val Loss 201.53610
Epoch 72: Val Loss 199.69965
Epoch 73: Val Loss 200.09308
Epoch 74: Val Loss 203.82547
Epoch 75: Val Loss 201.62437
Epoch 76: Val Loss 197.99783
Epoch 77: Val Loss 196.75899
Epoch 78: Val Loss 197.05550
Epoch 79: Val Loss 197.77800
Epoch 80: Val Loss 196.37570
Epoch 81: Val Loss 197.32469
Epoch 82: Val Loss 200.03091
Epoch 83: Val Loss 197.19983
Epoch 84: Val Loss 196.16081
Epoch 85: Val Loss 196.53543
Epoch 86: Val Loss 195.57439
Epoch 87: Val Loss 195.24007
Epoch 88: Val Loss 197.57681
Epoch 89: Val Loss 204.93979
Epoch 90: Val Loss 201.76228
Epoch 91: Val Loss 201.79730
Epoch 92: Val Loss 197.75078
Epoch 93: Val Loss 197.40027
Epoch 94: Val Loss 196.92165
Epoch 95: Val Loss 199.34816
Epoch 96: Val Loss 198.77431
Epoch 97: Val Loss 197.19221
Epoch 98: Val Loss 194.21161
Epoch 99: Val Loss 192.57161
{'MSE - mean': 192.57161261990154, 'MSE - std': 0.0, 'R2 - mean': 0.8118937500274795, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1314.63574
Epoch 1: Val Loss 1313.32507
Epoch 2: Val Loss 1311.46033
Epoch 3: Val Loss 1308.77002
Epoch 4: Val Loss 1304.79871
Epoch 5: Val Loss 1299.21484
Epoch 6: Val Loss 1291.43213
Epoch 7: Val Loss 1280.57910
Epoch 8: Val Loss 1265.70227
Epoch 9: Val Loss 1245.02222
Epoch 10: Val Loss 1218.18909
Epoch 11: Val Loss 1184.56091
Epoch 12: Val Loss 1144.36536
Epoch 13: Val Loss 1094.99829
Epoch 14: Val Loss 1038.22754
Epoch 15: Val Loss 976.58966
Epoch 16: Val Loss 909.48999
Epoch 17: Val Loss 847.42694
Epoch 18: Val Loss 789.47742
Epoch 19: Val Loss 741.97876
Epoch 20: Val Loss 707.75433
Epoch 21: Val Loss 684.03723
Epoch 22: Val Loss 662.74390
Epoch 23: Val Loss 636.93146
Epoch 24: Val Loss 609.16083
Epoch 25: Val Loss 583.57568
Epoch 26: Val Loss 556.60962
Epoch 27: Val Loss 533.56610
Epoch 28: Val Loss 514.61542
Epoch 29: Val Loss 491.60498
Epoch 30: Val Loss 472.58029
Epoch 31: Val Loss 454.29126
Epoch 32: Val Loss 437.64914
Epoch 33: Val Loss 423.47858
Epoch 34: Val Loss 414.47726
Epoch 35: Val Loss 408.44263
Epoch 36: Val Loss 397.14990
Epoch 37: Val Loss 384.90530
Epoch 38: Val Loss 380.48529
Epoch 39: Val Loss 376.86560
Epoch 40: Val Loss 368.56787
Epoch 41: Val Loss 353.49878
Epoch 42: Val Loss 344.89664
Epoch 43: Val Loss 343.93750
Epoch 44: Val Loss 366.85718
Epoch 45: Val Loss 367.02264
Epoch 46: Val Loss 347.75055
Epoch 47: Val Loss 333.44629
Epoch 48: Val Loss 341.72040
Epoch 49: Val Loss 341.06155
Epoch 50: Val Loss 335.33337
Epoch 51: Val Loss 333.05588
Epoch 52: Val Loss 328.92087
Epoch 53: Val Loss 324.18628
Epoch 54: Val Loss 321.45346
Epoch 55: Val Loss 324.69632
Epoch 56: Val Loss 317.72717
Epoch 57: Val Loss 314.87805
Epoch 58: Val Loss 310.87268
Epoch 59: Val Loss 313.91003
Epoch 60: Val Loss 312.79532
Epoch 61: Val Loss 312.33575
Epoch 62: Val Loss 310.30661
Epoch 63: Val Loss 308.89740
Epoch 64: Val Loss 313.12469
Epoch 65: Val Loss 313.43372
Epoch 66: Val Loss 308.16660
Epoch 67: Val Loss 303.50980
Epoch 68: Val Loss 302.63022
Epoch 69: Val Loss 307.59943
Epoch 70: Val Loss 305.82779
Epoch 71: Val Loss 303.63699
Epoch 72: Val Loss 306.67834
Epoch 73: Val Loss 312.71313
Epoch 74: Val Loss 305.78009
Epoch 75: Val Loss 302.42538
Epoch 76: Val Loss 303.31128
Epoch 77: Val Loss 297.14709
Epoch 78: Val Loss 296.74710
Epoch 79: Val Loss 297.42120
Epoch 80: Val Loss 300.61935
Epoch 81: Val Loss 292.80594
Epoch 82: Val Loss 318.02728
Epoch 83: Val Loss 312.13101
Epoch 84: Val Loss 298.61459
Epoch 85: Val Loss 286.29138
Epoch 86: Val Loss 291.52695
Epoch 87: Val Loss 291.21344
Epoch 88: Val Loss 285.03711
Epoch 89: Val Loss 282.03625
Epoch 90: Val Loss 271.63147
Epoch 91: Val Loss 270.18069
Epoch 92: Val Loss 272.06772
Epoch 93: Val Loss 275.76871
Epoch 94: Val Loss 276.59277
Epoch 95: Val Loss 279.54205
Epoch 96: Val Loss 275.20682
Epoch 97: Val Loss 266.20560
Epoch 98: Val Loss 257.92142
Epoch 99: Val Loss 252.51576
{'MSE - mean': 222.54369150388555, 'MSE - std': 29.972078883984025, 'R2 - mean': 0.7857100719022934, 'R2 - std': 0.026183678125186127} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2820.20459
Epoch 1: Val Loss 2817.37769
Epoch 2: Val Loss 2813.18188
Epoch 3: Val Loss 2806.73608
Epoch 4: Val Loss 2797.69580
Epoch 5: Val Loss 2786.17310
Epoch 6: Val Loss 2771.06104
Epoch 7: Val Loss 2750.96582
Epoch 8: Val Loss 2724.10815
Epoch 9: Val Loss 2688.13477
Epoch 10: Val Loss 2643.17969
Epoch 11: Val Loss 2585.86353
Epoch 12: Val Loss 2515.94580
Epoch 13: Val Loss 2437.82178
Epoch 14: Val Loss 2349.88281
Epoch 15: Val Loss 2256.67749
Epoch 16: Val Loss 2159.69824
Epoch 17: Val Loss 2064.50562
Epoch 18: Val Loss 1984.53870
Epoch 19: Val Loss 1912.69373
Epoch 20: Val Loss 1842.77429
Epoch 21: Val Loss 1778.43982
Epoch 22: Val Loss 1725.26746
Epoch 23: Val Loss 1674.93970
Epoch 24: Val Loss 1622.33582
Epoch 25: Val Loss 1573.06665
Epoch 26: Val Loss 1522.62341
Epoch 27: Val Loss 1466.20605
Epoch 28: Val Loss 1413.24121
Epoch 29: Val Loss 1367.66345
Epoch 30: Val Loss 1316.44531
Epoch 31: Val Loss 1269.26575
Epoch 32: Val Loss 1210.36182
Epoch 33: Val Loss 1170.46533
Epoch 34: Val Loss 1133.25623
Epoch 35: Val Loss 1097.01428
Epoch 36: Val Loss 1064.36230
Epoch 37: Val Loss 1032.73999
Epoch 38: Val Loss 999.63611
Epoch 39: Val Loss 952.36005
Epoch 40: Val Loss 919.95483
Epoch 41: Val Loss 893.13702
Epoch 42: Val Loss 863.19696
Epoch 43: Val Loss 850.04266
Epoch 44: Val Loss 832.92181
Epoch 45: Val Loss 814.44702
Epoch 46: Val Loss 798.98578
Epoch 47: Val Loss 784.29724
Epoch 48: Val Loss 767.76862
Epoch 49: Val Loss 751.76440
Epoch 50: Val Loss 733.12292
Epoch 51: Val Loss 721.38770
Epoch 52: Val Loss 712.36987
Epoch 53: Val Loss 703.95752
Epoch 54: Val Loss 688.99878
Epoch 55: Val Loss 681.40741
Epoch 56: Val Loss 672.33331
Epoch 57: Val Loss 663.91949
Epoch 58: Val Loss 653.82648
Epoch 59: Val Loss 644.68506
Epoch 60: Val Loss 629.49139
Epoch 61: Val Loss 619.66388
Epoch 62: Val Loss 612.23224
Epoch 63: Val Loss 608.31586
Epoch 64: Val Loss 604.20966
Epoch 65: Val Loss 593.99152
Epoch 66: Val Loss 587.94879
Epoch 67: Val Loss 582.99512
Epoch 68: Val Loss 563.25250
Epoch 69: Val Loss 550.67395
Epoch 70: Val Loss 546.29614
Epoch 71: Val Loss 543.09668
Epoch 72: Val Loss 544.18048
Epoch 73: Val Loss 524.04456
Epoch 74: Val Loss 507.16519
Epoch 75: Val Loss 501.93716
Epoch 76: Val Loss 504.25409
Epoch 77: Val Loss 502.15292
Epoch 78: Val Loss 496.58887
Epoch 79: Val Loss 488.87994
Epoch 80: Val Loss 480.37347
Epoch 81: Val Loss 482.25183
Epoch 82: Val Loss 479.20291
Epoch 83: Val Loss 479.05899
Epoch 84: Val Loss 472.42450
Epoch 85: Val Loss 479.11688
Epoch 86: Val Loss 471.74973
Epoch 87: Val Loss 468.18802
Epoch 88: Val Loss 452.00073
Epoch 89: Val Loss 450.26468
Epoch 90: Val Loss 444.41104
Epoch 91: Val Loss 440.42157
Epoch 92: Val Loss 445.03619
Epoch 93: Val Loss 434.26184
Epoch 94: Val Loss 426.91269
Epoch 95: Val Loss 419.15179
Epoch 96: Val Loss 416.08417
Epoch 97: Val Loss 401.86600
Epoch 98: Val Loss 404.49286
Epoch 99: Val Loss 407.85303
{'MSE - mean': 282.31779668274925, 'MSE - std': 88.00438043048139, 'R2 - mean': 0.802463160494983, 'R2 - std': 0.03191220176616754} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2124.09741
Epoch 1: Val Loss 2120.95190
Epoch 2: Val Loss 2117.39795
Epoch 3: Val Loss 2113.18555
Epoch 4: Val Loss 2108.28125
Epoch 5: Val Loss 2102.18701
Epoch 6: Val Loss 2094.33179
Epoch 7: Val Loss 2083.92896
Epoch 8: Val Loss 2070.52515
Epoch 9: Val Loss 2053.31909
Epoch 10: Val Loss 2031.47021
Epoch 11: Val Loss 2004.18933
Epoch 12: Val Loss 1969.91150
Epoch 13: Val Loss 1927.47827
Epoch 14: Val Loss 1878.62280
Epoch 15: Val Loss 1822.87500
Epoch 16: Val Loss 1763.63550
Epoch 17: Val Loss 1694.12158
Epoch 18: Val Loss 1625.05603
Epoch 19: Val Loss 1558.34961
Epoch 20: Val Loss 1489.94214
Epoch 21: Val Loss 1425.69775
Epoch 22: Val Loss 1372.93726
Epoch 23: Val Loss 1326.55701
Epoch 24: Val Loss 1287.74658
Epoch 25: Val Loss 1249.59900
Epoch 26: Val Loss 1214.20728
Epoch 27: Val Loss 1180.76892
Epoch 28: Val Loss 1146.33618
Epoch 29: Val Loss 1107.91687
Epoch 30: Val Loss 1074.78735
Epoch 31: Val Loss 1035.72339
Epoch 32: Val Loss 999.02612
Epoch 33: Val Loss 962.18884
Epoch 34: Val Loss 924.50299
Epoch 35: Val Loss 889.74890
Epoch 36: Val Loss 857.77765
Epoch 37: Val Loss 825.92194
Epoch 38: Val Loss 793.88214
Epoch 39: Val Loss 765.42792
Epoch 40: Val Loss 737.17535
Epoch 41: Val Loss 710.93054
Epoch 42: Val Loss 685.69232
Epoch 43: Val Loss 663.02991
Epoch 44: Val Loss 641.89508
Epoch 45: Val Loss 623.05231
Epoch 46: Val Loss 604.99292
Epoch 47: Val Loss 587.56390
Epoch 48: Val Loss 572.11340
Epoch 49: Val Loss 558.02570
Epoch 50: Val Loss 544.73877
Epoch 51: Val Loss 531.81140
Epoch 52: Val Loss 521.21515
Epoch 53: Val Loss 511.33542
Epoch 54: Val Loss 503.06992
Epoch 55: Val Loss 495.94748
Epoch 56: Val Loss 485.82788
Epoch 57: Val Loss 479.04944
Epoch 58: Val Loss 472.21970
Epoch 59: Val Loss 465.89914
Epoch 60: Val Loss 458.02322
Epoch 61: Val Loss 451.61490
Epoch 62: Val Loss 445.56757
Epoch 63: Val Loss 441.70840
Epoch 64: Val Loss 440.44345
Epoch 65: Val Loss 434.48181
Epoch 66: Val Loss 429.01712
Epoch 67: Val Loss 423.56494
Epoch 68: Val Loss 417.97430
Epoch 69: Val Loss 415.14191
Epoch 70: Val Loss 413.95374
Epoch 71: Val Loss 412.67529
Epoch 72: Val Loss 408.87845
Epoch 73: Val Loss 403.96753
Epoch 74: Val Loss 403.18430
Epoch 75: Val Loss 400.48724
Epoch 76: Val Loss 397.55624
Epoch 77: Val Loss 397.25928
Epoch 78: Val Loss 394.58542
Epoch 79: Val Loss 390.31772
Epoch 80: Val Loss 391.42972
Epoch 81: Val Loss 390.36194
Epoch 82: Val Loss 389.15103
Epoch 83: Val Loss 387.60434
Epoch 84: Val Loss 391.49057
Epoch 85: Val Loss 386.67566
Epoch 86: Val Loss 382.48743
Epoch 87: Val Loss 375.41223
Epoch 88: Val Loss 370.54666
Epoch 89: Val Loss 369.12338
Epoch 90: Val Loss 369.91168
Epoch 91: Val Loss 377.32291
Epoch 92: Val Loss 377.37274
Epoch 93: Val Loss 376.33002
Epoch 94: Val Loss 380.22742
Epoch 95: Val Loss 380.51230
Epoch 96: Val Loss 374.09711
Epoch 97: Val Loss 366.98425
Epoch 98: Val Loss 361.87521
Epoch 99: Val Loss 358.73486
{'MSE - mean': 301.4220728075643, 'MSE - std': 83.0872934327157, 'R2 - mean': 0.799808440326379, 'R2 - std': 0.02801667510062927} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2517.25244
Epoch 1: Val Loss 2515.34448
Epoch 2: Val Loss 2513.49194
Epoch 3: Val Loss 2511.67603
Epoch 4: Val Loss 2509.50439
Epoch 5: Val Loss 2506.47827
Epoch 6: Val Loss 2502.26099
Epoch 7: Val Loss 2496.10498
Epoch 8: Val Loss 2487.23999
Epoch 9: Val Loss 2474.84058
Epoch 10: Val Loss 2457.57251
Epoch 11: Val Loss 2434.60718
Epoch 12: Val Loss 2403.51514
Epoch 13: Val Loss 2363.78784
Epoch 14: Val Loss 2313.82471
Epoch 15: Val Loss 2252.06055
Epoch 16: Val Loss 2180.86499
Epoch 17: Val Loss 2097.85840
Epoch 18: Val Loss 2004.74670
Epoch 19: Val Loss 1910.71265
Epoch 20: Val Loss 1825.16516
Epoch 21: Val Loss 1746.42297
Epoch 22: Val Loss 1672.05298
Epoch 23: Val Loss 1605.15259
Epoch 24: Val Loss 1551.02490
Epoch 25: Val Loss 1500.52808
Epoch 26: Val Loss 1449.09375
Epoch 27: Val Loss 1396.94031
Epoch 28: Val Loss 1350.55676
Epoch 29: Val Loss 1309.99365
Epoch 30: Val Loss 1276.03345
Epoch 31: Val Loss 1239.43884
Epoch 32: Val Loss 1206.67505
Epoch 33: Val Loss 1174.56409
Epoch 34: Val Loss 1138.28601
Epoch 35: Val Loss 1102.17163
Epoch 36: Val Loss 1068.72388
Epoch 37: Val Loss 1040.03418
Epoch 38: Val Loss 1013.82739
Epoch 39: Val Loss 987.49188
Epoch 40: Val Loss 959.12042
Epoch 41: Val Loss 922.16882
Epoch 42: Val Loss 894.75989
Epoch 43: Val Loss 872.56885
Epoch 44: Val Loss 852.88177
Epoch 45: Val Loss 829.08936
Epoch 46: Val Loss 813.75317
Epoch 47: Val Loss 800.60675
Epoch 48: Val Loss 785.86285
Epoch 49: Val Loss 770.47101
Epoch 50: Val Loss 752.89044
Epoch 51: Val Loss 733.26477
Epoch 52: Val Loss 721.46075
Epoch 53: Val Loss 711.40570
Epoch 54: Val Loss 695.85675
Epoch 55: Val Loss 686.41058
Epoch 56: Val Loss 671.31573
Epoch 57: Val Loss 664.18127
Epoch 58: Val Loss 657.79950
Epoch 59: Val Loss 652.77368
Epoch 60: Val Loss 645.89410
Epoch 61: Val Loss 636.30682
Epoch 62: Val Loss 630.62146
Epoch 63: Val Loss 619.96350
Epoch 64: Val Loss 613.63623
Epoch 65: Val Loss 606.49994
Epoch 66: Val Loss 604.38501
Epoch 67: Val Loss 594.04132
Epoch 68: Val Loss 588.37140
Epoch 69: Val Loss 587.47736
Epoch 70: Val Loss 582.37994
Epoch 71: Val Loss 573.34430
Epoch 72: Val Loss 570.18695
Epoch 73: Val Loss 567.44135
Epoch 74: Val Loss 565.46936
Epoch 75: Val Loss 561.93976
Epoch 76: Val Loss 557.68695
Epoch 77: Val Loss 549.38184
Epoch 78: Val Loss 545.05292
Epoch 79: Val Loss 537.09399
Epoch 80: Val Loss 533.83051
Epoch 81: Val Loss 535.76990
Epoch 82: Val Loss 534.63763
Epoch 83: Val Loss 526.15051
Epoch 84: Val Loss 522.17072
Epoch 85: Val Loss 522.40582
Epoch 86: Val Loss 521.96582
Epoch 87: Val Loss 516.56683
Epoch 88: Val Loss 513.72266
Epoch 89: Val Loss 508.80551
Epoch 90: Val Loss 505.70041
Epoch 91: Val Loss 499.78009
Epoch 92: Val Loss 498.62427
Epoch 93: Val Loss 496.47321
Epoch 94: Val Loss 491.28430
Epoch 95: Val Loss 485.36389
Epoch 96: Val Loss 482.27316
Epoch 97: Val Loss 482.05182
Epoch 98: Val Loss 483.72733
Epoch 99: Val Loss 475.05386
{'MSE - mean': 336.14843105129887, 'MSE - std': 101.71764100424342, 'R2 - mean': 0.7954300782770837, 'R2 - std': 0.026544820283705334} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 336.14843105129887 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 20 with value: 318.03241536023376.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1268.47034
Epoch 1: Val Loss 1265.56006
Epoch 2: Val Loss 1261.95984
Epoch 3: Val Loss 1257.32031
Epoch 4: Val Loss 1251.29602
Epoch 5: Val Loss 1243.02466
Epoch 6: Val Loss 1231.78967
Epoch 7: Val Loss 1216.15442
Epoch 8: Val Loss 1195.88672
Epoch 9: Val Loss 1169.16931
Epoch 10: Val Loss 1135.16162
Epoch 11: Val Loss 1092.46448
Epoch 12: Val Loss 1041.70483
Epoch 13: Val Loss 987.38806
Epoch 14: Val Loss 924.93280
Epoch 15: Val Loss 861.56250
Epoch 16: Val Loss 804.49915
Epoch 17: Val Loss 755.19586
Epoch 18: Val Loss 716.44025
Epoch 19: Val Loss 688.26678
Epoch 20: Val Loss 666.81116
Epoch 21: Val Loss 645.23322
Epoch 22: Val Loss 622.16321
Epoch 23: Val Loss 596.22778
Epoch 24: Val Loss 569.02826
Epoch 25: Val Loss 543.76166
Epoch 26: Val Loss 517.97327
Epoch 27: Val Loss 495.43549
Epoch 28: Val Loss 474.74515
Epoch 29: Val Loss 454.67780
Epoch 30: Val Loss 433.44901
Epoch 31: Val Loss 415.25125
Epoch 32: Val Loss 400.06714
Epoch 33: Val Loss 381.27444
Epoch 34: Val Loss 364.35251
Epoch 35: Val Loss 350.84167
Epoch 36: Val Loss 337.49240
Epoch 37: Val Loss 331.00513
Epoch 38: Val Loss 322.71539
Epoch 39: Val Loss 311.71951
Epoch 40: Val Loss 301.33121
Epoch 41: Val Loss 291.15231
Epoch 42: Val Loss 281.80786
Epoch 43: Val Loss 272.44366
Epoch 44: Val Loss 265.83661
Epoch 45: Val Loss 257.60321
Epoch 46: Val Loss 251.46187
Epoch 47: Val Loss 247.25764
Epoch 48: Val Loss 241.09615
Epoch 49: Val Loss 241.67564
Epoch 50: Val Loss 239.47218
Epoch 51: Val Loss 233.02417
Epoch 52: Val Loss 229.54617
Epoch 53: Val Loss 225.96902
Epoch 54: Val Loss 224.49034
Epoch 55: Val Loss 219.85727
Epoch 56: Val Loss 217.47229
Epoch 57: Val Loss 213.39149
Epoch 58: Val Loss 210.34085
Epoch 59: Val Loss 209.09731
Epoch 60: Val Loss 207.37740
Epoch 61: Val Loss 206.32277
Epoch 62: Val Loss 205.68327
Epoch 63: Val Loss 204.07246
Epoch 64: Val Loss 203.42714
Epoch 65: Val Loss 201.61189
Epoch 66: Val Loss 199.35271
Epoch 67: Val Loss 198.04933
Epoch 68: Val Loss 196.44646
Epoch 69: Val Loss 195.30075
Epoch 70: Val Loss 194.82079
Epoch 71: Val Loss 193.60245
Epoch 72: Val Loss 193.28123
Epoch 73: Val Loss 193.14340
Epoch 74: Val Loss 196.85893
Epoch 75: Val Loss 199.47887
Epoch 76: Val Loss 196.39799
Epoch 77: Val Loss 198.01491
Epoch 78: Val Loss 197.74654
Epoch 79: Val Loss 195.35220
Epoch 80: Val Loss 195.26064
Epoch 81: Val Loss 194.29344
Epoch 82: Val Loss 193.19667
Epoch 83: Val Loss 193.15477
Epoch 84: Val Loss 192.34671
Epoch 85: Val Loss 195.61383
Epoch 86: Val Loss 196.87181
Epoch 87: Val Loss 194.97734
Epoch 88: Val Loss 192.24727
Epoch 89: Val Loss 195.21204
Epoch 90: Val Loss 194.77800
Epoch 91: Val Loss 190.68544
Epoch 92: Val Loss 189.11623
Epoch 93: Val Loss 189.89110
Epoch 94: Val Loss 189.58829
Epoch 95: Val Loss 188.52318
Epoch 96: Val Loss 187.12540
Epoch 97: Val Loss 187.07144
Epoch 98: Val Loss 184.84099
Epoch 99: Val Loss 183.76057
{'MSE - mean': 183.7605877786761, 'MSE - std': 0.0, 'R2 - mean': 0.8205004642713335, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1292.87354
Epoch 1: Val Loss 1290.77271
Epoch 2: Val Loss 1287.88928
Epoch 3: Val Loss 1283.89697
Epoch 4: Val Loss 1278.59570
Epoch 5: Val Loss 1271.59753
Epoch 6: Val Loss 1262.07642
Epoch 7: Val Loss 1249.46143
Epoch 8: Val Loss 1233.50513
Epoch 9: Val Loss 1212.96594
Epoch 10: Val Loss 1186.83496
Epoch 11: Val Loss 1154.27576
Epoch 12: Val Loss 1114.78589
Epoch 13: Val Loss 1068.23279
Epoch 14: Val Loss 1017.23663
Epoch 15: Val Loss 960.77863
Epoch 16: Val Loss 901.90961
Epoch 17: Val Loss 847.03339
Epoch 18: Val Loss 797.99164
Epoch 19: Val Loss 759.53070
Epoch 20: Val Loss 728.72009
Epoch 21: Val Loss 699.48553
Epoch 22: Val Loss 668.84692
Epoch 23: Val Loss 635.37476
Epoch 24: Val Loss 602.21173
Epoch 25: Val Loss 571.83734
Epoch 26: Val Loss 548.25549
Epoch 27: Val Loss 522.74927
Epoch 28: Val Loss 492.95474
Epoch 29: Val Loss 467.31351
Epoch 30: Val Loss 448.22482
Epoch 31: Val Loss 429.98010
Epoch 32: Val Loss 410.73224
Epoch 33: Val Loss 389.37439
Epoch 34: Val Loss 373.28601
Epoch 35: Val Loss 367.55872
Epoch 36: Val Loss 366.55817
Epoch 37: Val Loss 361.32065
Epoch 38: Val Loss 345.56241
Epoch 39: Val Loss 330.98611
Epoch 40: Val Loss 323.99005
Epoch 41: Val Loss 323.79657
Epoch 42: Val Loss 323.32385
Epoch 43: Val Loss 320.50732
Epoch 44: Val Loss 311.26300
Epoch 45: Val Loss 307.13495
Epoch 46: Val Loss 307.86176
Epoch 47: Val Loss 316.43161
Epoch 48: Val Loss 333.34637
Epoch 49: Val Loss 317.80783
Epoch 50: Val Loss 303.88507
Epoch 51: Val Loss 297.47876
Epoch 52: Val Loss 295.17111
Epoch 53: Val Loss 292.87021
Epoch 54: Val Loss 292.98462
Epoch 55: Val Loss 297.40201
Epoch 56: Val Loss 295.22949
Epoch 57: Val Loss 300.58292
Epoch 58: Val Loss 298.33691
Epoch 59: Val Loss 290.35544
Epoch 60: Val Loss 303.41864
Epoch 61: Val Loss 313.35559
Epoch 62: Val Loss 307.97760
Epoch 63: Val Loss 304.00357
Epoch 64: Val Loss 282.89896
Epoch 65: Val Loss 278.37939
Epoch 66: Val Loss 278.07990
Epoch 67: Val Loss 281.78000
Epoch 68: Val Loss 289.84052
Epoch 69: Val Loss 300.86197
Epoch 70: Val Loss 283.14926
Epoch 71: Val Loss 264.32318
Epoch 72: Val Loss 270.44324
Epoch 73: Val Loss 274.59583
Epoch 74: Val Loss 275.40741
Epoch 75: Val Loss 272.84943
Epoch 76: Val Loss 268.76349
Epoch 77: Val Loss 267.29694
Epoch 78: Val Loss 263.88489
Epoch 79: Val Loss 255.94341
Epoch 80: Val Loss 254.97302
Epoch 81: Val Loss 246.03641
Epoch 82: Val Loss 251.57965
Epoch 83: Val Loss 253.72481
Epoch 84: Val Loss 254.77684
Epoch 85: Val Loss 242.57472
Epoch 86: Val Loss 234.33661
Epoch 87: Val Loss 236.75940
Epoch 88: Val Loss 247.89009
Epoch 89: Val Loss 247.08424
Epoch 90: Val Loss 243.43803
Epoch 91: Val Loss 237.97710
Epoch 92: Val Loss 237.79906
Epoch 93: Val Loss 237.64709
Epoch 94: Val Loss 226.73578
Epoch 95: Val Loss 223.21588
Epoch 96: Val Loss 227.99770
Epoch 97: Val Loss 229.25230
Epoch 98: Val Loss 238.93158
Epoch 99: Val Loss 229.45221
{'MSE - mean': 203.4882362031949, 'MSE - std': 19.727648424518804, 'R2 - mean': 0.8039647342829789, 'R2 - std': 0.01653572998835462} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2826.23218
Epoch 1: Val Loss 2822.14062
Epoch 2: Val Loss 2817.09961
Epoch 3: Val Loss 2810.44653
Epoch 4: Val Loss 2801.17090
Epoch 5: Val Loss 2788.85059
Epoch 6: Val Loss 2771.76489
Epoch 7: Val Loss 2749.36646
Epoch 8: Val Loss 2720.71484
Epoch 9: Val Loss 2686.10596
Epoch 10: Val Loss 2642.65479
Epoch 11: Val Loss 2588.02295
Epoch 12: Val Loss 2520.74194
Epoch 13: Val Loss 2439.83545
Epoch 14: Val Loss 2349.00684
Epoch 15: Val Loss 2254.47852
Epoch 16: Val Loss 2155.36084
Epoch 17: Val Loss 2055.46143
Epoch 18: Val Loss 1963.29358
Epoch 19: Val Loss 1874.78198
Epoch 20: Val Loss 1794.93909
Epoch 21: Val Loss 1718.57410
Epoch 22: Val Loss 1653.08557
Epoch 23: Val Loss 1590.68347
Epoch 24: Val Loss 1534.05933
Epoch 25: Val Loss 1473.14832
Epoch 26: Val Loss 1410.11499
Epoch 27: Val Loss 1346.09607
Epoch 28: Val Loss 1297.85852
Epoch 29: Val Loss 1251.87134
Epoch 30: Val Loss 1209.46912
Epoch 31: Val Loss 1172.37842
Epoch 32: Val Loss 1136.31335
Epoch 33: Val Loss 1090.47546
Epoch 34: Val Loss 1049.27405
Epoch 35: Val Loss 1012.43250
Epoch 36: Val Loss 986.73090
Epoch 37: Val Loss 963.79077
Epoch 38: Val Loss 943.25366
Epoch 39: Val Loss 916.43945
Epoch 40: Val Loss 887.52448
Epoch 41: Val Loss 865.39081
Epoch 42: Val Loss 836.15424
Epoch 43: Val Loss 815.50726
Epoch 44: Val Loss 796.33246
Epoch 45: Val Loss 781.02826
Epoch 46: Val Loss 764.07898
Epoch 47: Val Loss 742.36957
Epoch 48: Val Loss 721.90436
Epoch 49: Val Loss 709.67139
Epoch 50: Val Loss 702.37878
Epoch 51: Val Loss 687.36707
Epoch 52: Val Loss 667.29224
Epoch 53: Val Loss 643.62421
Epoch 54: Val Loss 634.35138
Epoch 55: Val Loss 623.72900
Epoch 56: Val Loss 608.84607
Epoch 57: Val Loss 596.54272
Epoch 58: Val Loss 592.37250
Epoch 59: Val Loss 587.67853
Epoch 60: Val Loss 578.90204
Epoch 61: Val Loss 575.74799
Epoch 62: Val Loss 572.74786
Epoch 63: Val Loss 550.91479
Epoch 64: Val Loss 540.09796
Epoch 65: Val Loss 535.92285
Epoch 66: Val Loss 536.07324
Epoch 67: Val Loss 538.87354
Epoch 68: Val Loss 534.29993
Epoch 69: Val Loss 520.38080
Epoch 70: Val Loss 516.24426
Epoch 71: Val Loss 512.25854
Epoch 72: Val Loss 503.71387
Epoch 73: Val Loss 492.38971
Epoch 74: Val Loss 500.15787
Epoch 75: Val Loss 498.47440
Epoch 76: Val Loss 482.44128
Epoch 77: Val Loss 475.93546
Epoch 78: Val Loss 474.55878
Epoch 79: Val Loss 455.25723
Epoch 80: Val Loss 452.10370
Epoch 81: Val Loss 453.57352
Epoch 82: Val Loss 447.70001
Epoch 83: Val Loss 442.27112
Epoch 84: Val Loss 428.50623
Epoch 85: Val Loss 425.62213
Epoch 86: Val Loss 433.49594
Epoch 87: Val Loss 433.00824
Epoch 88: Val Loss 422.30573
Epoch 89: Val Loss 415.14737
Epoch 90: Val Loss 413.75836
Epoch 91: Val Loss 412.20251
Epoch 92: Val Loss 414.11166
Epoch 93: Val Loss 409.59491
Epoch 94: Val Loss 397.54059
Epoch 95: Val Loss 396.43701
Epoch 96: Val Loss 395.12054
Epoch 97: Val Loss 390.01981
Epoch 98: Val Loss 377.37994
Epoch 99: Val Loss 372.46326
{'MSE - mean': 259.8132446584451, 'MSE - std': 81.26786919323628, 'R2 - mean': 0.8186333996906093, 'R2 - std': 0.02475129088580782} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2112.36963
Epoch 1: Val Loss 2108.75586
Epoch 2: Val Loss 2104.25098
Epoch 3: Val Loss 2098.50293
Epoch 4: Val Loss 2090.97681
Epoch 5: Val Loss 2080.54053
Epoch 6: Val Loss 2066.22095
Epoch 7: Val Loss 2046.69971
Epoch 8: Val Loss 2019.67676
Epoch 9: Val Loss 1983.64771
Epoch 10: Val Loss 1938.70251
Epoch 11: Val Loss 1882.10229
Epoch 12: Val Loss 1815.57178
Epoch 13: Val Loss 1740.34607
Epoch 14: Val Loss 1657.61304
Epoch 15: Val Loss 1570.35828
Epoch 16: Val Loss 1483.32397
Epoch 17: Val Loss 1403.99133
Epoch 18: Val Loss 1337.79846
Epoch 19: Val Loss 1281.39600
Epoch 20: Val Loss 1228.56360
Epoch 21: Val Loss 1179.93164
Epoch 22: Val Loss 1134.15417
Epoch 23: Val Loss 1085.78210
Epoch 24: Val Loss 1039.12292
Epoch 25: Val Loss 989.03137
Epoch 26: Val Loss 942.28998
Epoch 27: Val Loss 898.30402
Epoch 28: Val Loss 856.41632
Epoch 29: Val Loss 817.80750
Epoch 30: Val Loss 782.70551
Epoch 31: Val Loss 748.49011
Epoch 32: Val Loss 717.27521
Epoch 33: Val Loss 690.15210
Epoch 34: Val Loss 663.96869
Epoch 35: Val Loss 638.86090
Epoch 36: Val Loss 616.20367
Epoch 37: Val Loss 592.96979
Epoch 38: Val Loss 573.68585
Epoch 39: Val Loss 557.40198
Epoch 40: Val Loss 542.72400
Epoch 41: Val Loss 529.89240
Epoch 42: Val Loss 517.65894
Epoch 43: Val Loss 506.98492
Epoch 44: Val Loss 499.98938
Epoch 45: Val Loss 489.91458
Epoch 46: Val Loss 479.96930
Epoch 47: Val Loss 472.95786
Epoch 48: Val Loss 464.05814
Epoch 49: Val Loss 454.19098
Epoch 50: Val Loss 446.76837
Epoch 51: Val Loss 439.86996
Epoch 52: Val Loss 433.31601
Epoch 53: Val Loss 428.84247
Epoch 54: Val Loss 423.69873
Epoch 55: Val Loss 420.88614
Epoch 56: Val Loss 413.15405
Epoch 57: Val Loss 407.44601
Epoch 58: Val Loss 406.48041
Epoch 59: Val Loss 402.93991
Epoch 60: Val Loss 401.77750
Epoch 61: Val Loss 395.53781
Epoch 62: Val Loss 391.65457
Epoch 63: Val Loss 390.08435
Epoch 64: Val Loss 384.89102
Epoch 65: Val Loss 385.42731
Epoch 66: Val Loss 383.14883
Epoch 67: Val Loss 382.16077
Epoch 68: Val Loss 377.38660
Epoch 69: Val Loss 370.98160
Epoch 70: Val Loss 364.94470
Epoch 71: Val Loss 365.21429
Epoch 72: Val Loss 359.39001
Epoch 73: Val Loss 355.30411
Epoch 74: Val Loss 352.31024
Epoch 75: Val Loss 351.80450
Epoch 76: Val Loss 350.89441
Epoch 77: Val Loss 347.19479
Epoch 78: Val Loss 345.97342
Epoch 79: Val Loss 337.77026
Epoch 80: Val Loss 334.81009
Epoch 81: Val Loss 341.20847
Epoch 82: Val Loss 339.78439
Epoch 83: Val Loss 330.24698
Epoch 84: Val Loss 324.04221
Epoch 85: Val Loss 321.20029
Epoch 86: Val Loss 324.56021
Epoch 87: Val Loss 323.25031
Epoch 88: Val Loss 317.42191
Epoch 89: Val Loss 314.44791
Epoch 90: Val Loss 310.94400
Epoch 91: Val Loss 310.32608
Epoch 92: Val Loss 312.68396
Epoch 93: Val Loss 309.12418
Epoch 94: Val Loss 310.03247
Epoch 95: Val Loss 305.56409
Epoch 96: Val Loss 300.86520
Epoch 97: Val Loss 302.44812
Epoch 98: Val Loss 301.20944
Epoch 99: Val Loss 300.55325
{'MSE - mean': 269.99825396441344, 'MSE - std': 72.5572406179749, 'R2 - mean': 0.8203760825108204, 'R2 - std': 0.021646723322964648} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2512.07568
Epoch 1: Val Loss 2508.69971
Epoch 2: Val Loss 2504.46045
Epoch 3: Val Loss 2498.85498
Epoch 4: Val Loss 2490.89453
Epoch 5: Val Loss 2479.69043
Epoch 6: Val Loss 2464.96777
Epoch 7: Val Loss 2446.32300
Epoch 8: Val Loss 2422.63208
Epoch 9: Val Loss 2393.38330
Epoch 10: Val Loss 2357.31665
Epoch 11: Val Loss 2313.81812
Epoch 12: Val Loss 2260.86914
Epoch 13: Val Loss 2197.90576
Epoch 14: Val Loss 2125.26782
Epoch 15: Val Loss 2041.00378
Epoch 16: Val Loss 1952.39392
Epoch 17: Val Loss 1863.51489
Epoch 18: Val Loss 1781.05554
Epoch 19: Val Loss 1712.23535
Epoch 20: Val Loss 1648.38867
Epoch 21: Val Loss 1589.21741
Epoch 22: Val Loss 1535.16968
Epoch 23: Val Loss 1488.36975
Epoch 24: Val Loss 1443.21838
Epoch 25: Val Loss 1396.53882
Epoch 26: Val Loss 1347.03540
Epoch 27: Val Loss 1284.98071
Epoch 28: Val Loss 1232.37170
Epoch 29: Val Loss 1176.75305
Epoch 30: Val Loss 1128.78992
Epoch 31: Val Loss 1072.55664
Epoch 32: Val Loss 1026.17883
Epoch 33: Val Loss 992.31525
Epoch 34: Val Loss 940.33514
Epoch 35: Val Loss 902.73773
Epoch 36: Val Loss 875.86426
Epoch 37: Val Loss 838.10022
Epoch 38: Val Loss 807.88690
Epoch 39: Val Loss 788.24542
Epoch 40: Val Loss 765.70154
Epoch 41: Val Loss 743.18994
Epoch 42: Val Loss 719.79144
Epoch 43: Val Loss 696.32751
Epoch 44: Val Loss 686.35236
Epoch 45: Val Loss 671.49701
Epoch 46: Val Loss 650.45319
Epoch 47: Val Loss 629.61584
Epoch 48: Val Loss 616.60962
Epoch 49: Val Loss 609.66290
Epoch 50: Val Loss 595.95441
Epoch 51: Val Loss 586.42828
Epoch 52: Val Loss 573.67651
Epoch 53: Val Loss 564.79803
Epoch 54: Val Loss 550.62610
Epoch 55: Val Loss 547.63574
Epoch 56: Val Loss 533.97473
Epoch 57: Val Loss 522.44025
Epoch 58: Val Loss 520.33478
Epoch 59: Val Loss 526.89551
Epoch 60: Val Loss 518.64166
Epoch 61: Val Loss 502.71375
Epoch 62: Val Loss 496.70541
Epoch 63: Val Loss 497.75940
Epoch 64: Val Loss 495.02939
Epoch 65: Val Loss 487.90274
Epoch 66: Val Loss 478.52725
Epoch 67: Val Loss 469.65927
Epoch 68: Val Loss 466.84494
Epoch 69: Val Loss 464.88278
Epoch 70: Val Loss 461.30029
Epoch 71: Val Loss 448.22125
Epoch 72: Val Loss 444.70929
Epoch 73: Val Loss 443.50125
Epoch 74: Val Loss 442.60504
Epoch 75: Val Loss 435.35107
Epoch 76: Val Loss 435.57043
Epoch 77: Val Loss 432.03748
Epoch 78: Val Loss 426.90820
Epoch 79: Val Loss 418.03186
Epoch 80: Val Loss 414.20251
Epoch 81: Val Loss 410.33490
Epoch 82: Val Loss 408.19125
Epoch 83: Val Loss 404.68414
Epoch 84: Val Loss 398.44479
Epoch 85: Val Loss 390.48044
Epoch 86: Val Loss 387.16962
Epoch 87: Val Loss 390.54510
Epoch 88: Val Loss 393.07947
Epoch 89: Val Loss 380.48187
Epoch 90: Val Loss 374.36584
Epoch 91: Val Loss 376.48871
Epoch 92: Val Loss 374.65909
Epoch 93: Val Loss 369.23639
Epoch 94: Val Loss 366.29440
Epoch 95: Val Loss 363.87378
Epoch 96: Val Loss 357.79205
Epoch 97: Val Loss 354.15112
Epoch 98: Val Loss 354.34067
Epoch 99: Val Loss 351.95688
{'MSE - mean': 286.3899802356503, 'MSE - std': 72.70761509966056, 'R2 - mean': 0.8233935360919535, 'R2 - std': 0.020280153080141066} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 286.3899802356503 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1286.36890
Epoch 1: Val Loss 1284.49707
Epoch 2: Val Loss 1282.50940
Epoch 3: Val Loss 1280.17371
Epoch 4: Val Loss 1277.07373
Epoch 5: Val Loss 1272.93506
Epoch 6: Val Loss 1267.52100
Epoch 7: Val Loss 1260.37366
Epoch 8: Val Loss 1250.97412
Epoch 9: Val Loss 1238.81763
Epoch 10: Val Loss 1223.13574
Epoch 11: Val Loss 1203.13098
Epoch 12: Val Loss 1177.22632
Epoch 13: Val Loss 1145.69592
Epoch 14: Val Loss 1108.56311
Epoch 15: Val Loss 1065.25183
Epoch 16: Val Loss 1017.56140
Epoch 17: Val Loss 965.53217
Epoch 18: Val Loss 909.35492
Epoch 19: Val Loss 851.78302
Epoch 20: Val Loss 803.55542
Epoch 21: Val Loss 763.36084
Epoch 22: Val Loss 733.68140
Epoch 23: Val Loss 708.71472
Epoch 24: Val Loss 686.46613
Epoch 25: Val Loss 665.43414
Epoch 26: Val Loss 641.23328
Epoch 27: Val Loss 613.92590
Epoch 28: Val Loss 587.35791
Epoch 29: Val Loss 560.39581
Epoch 30: Val Loss 531.09906
Epoch 31: Val Loss 504.04089
Epoch 32: Val Loss 478.61630
Epoch 33: Val Loss 457.20905
Epoch 34: Val Loss 434.26398
Epoch 35: Val Loss 408.86087
Epoch 36: Val Loss 385.94531
Epoch 37: Val Loss 365.66998
Epoch 38: Val Loss 350.06726
Epoch 39: Val Loss 335.99878
Epoch 40: Val Loss 322.59543
Epoch 41: Val Loss 313.05405
Epoch 42: Val Loss 301.83707
Epoch 43: Val Loss 292.57840
Epoch 44: Val Loss 281.48422
Epoch 45: Val Loss 270.79230
Epoch 46: Val Loss 261.28903
Epoch 47: Val Loss 259.25876
Epoch 48: Val Loss 257.28021
Epoch 49: Val Loss 253.35420
Epoch 50: Val Loss 242.59749
Epoch 51: Val Loss 234.28560
Epoch 52: Val Loss 233.16182
Epoch 53: Val Loss 231.26562
Epoch 54: Val Loss 234.58850
Epoch 55: Val Loss 233.53081
Epoch 56: Val Loss 228.54884
Epoch 57: Val Loss 223.49115
Epoch 58: Val Loss 218.13838
Epoch 59: Val Loss 216.93951
Epoch 60: Val Loss 215.45905
Epoch 61: Val Loss 213.40254
Epoch 62: Val Loss 212.29884
Epoch 63: Val Loss 208.89461
Epoch 64: Val Loss 209.77037
Epoch 65: Val Loss 209.45358
Epoch 66: Val Loss 207.02592
Epoch 67: Val Loss 203.67015
Epoch 68: Val Loss 205.30203
Epoch 69: Val Loss 205.38968
Epoch 70: Val Loss 209.15524
Epoch 71: Val Loss 206.52785
Epoch 72: Val Loss 204.74063
Epoch 73: Val Loss 202.63719
Epoch 74: Val Loss 200.74561
Epoch 75: Val Loss 199.95247
Epoch 76: Val Loss 199.22896
Epoch 77: Val Loss 198.22459
Epoch 78: Val Loss 198.24606
Epoch 79: Val Loss 200.19014
Epoch 80: Val Loss 199.88930
Epoch 81: Val Loss 198.22514
Epoch 82: Val Loss 196.77528
Epoch 83: Val Loss 195.03569
Epoch 84: Val Loss 195.65591
Epoch 85: Val Loss 194.41396
Epoch 86: Val Loss 195.28172
Epoch 87: Val Loss 197.10172
Epoch 88: Val Loss 197.04495
Epoch 89: Val Loss 195.87718
Epoch 90: Val Loss 193.96013
Epoch 91: Val Loss 195.15123
Epoch 92: Val Loss 194.38701
Epoch 93: Val Loss 192.93587
Epoch 94: Val Loss 191.48201
Epoch 95: Val Loss 190.20877
Epoch 96: Val Loss 190.43474
Epoch 97: Val Loss 189.90663
Epoch 98: Val Loss 189.91495
Epoch 99: Val Loss 188.81708
{'MSE - mean': 188.81709677591024, 'MSE - std': 0.0, 'R2 - mean': 0.8155612059222879, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1307.98230
Epoch 1: Val Loss 1304.69080
Epoch 2: Val Loss 1300.87122
Epoch 3: Val Loss 1296.04163
Epoch 4: Val Loss 1289.63525
Epoch 5: Val Loss 1281.00195
Epoch 6: Val Loss 1269.37378
Epoch 7: Val Loss 1253.81763
Epoch 8: Val Loss 1233.61902
Epoch 9: Val Loss 1208.05237
Epoch 10: Val Loss 1176.57874
Epoch 11: Val Loss 1137.14172
Epoch 12: Val Loss 1089.91895
Epoch 13: Val Loss 1036.99878
Epoch 14: Val Loss 979.86853
Epoch 15: Val Loss 921.13068
Epoch 16: Val Loss 866.17505
Epoch 17: Val Loss 815.64429
Epoch 18: Val Loss 774.08740
Epoch 19: Val Loss 741.09125
Epoch 20: Val Loss 714.84998
Epoch 21: Val Loss 691.13550
Epoch 22: Val Loss 665.63519
Epoch 23: Val Loss 639.22498
Epoch 24: Val Loss 613.50677
Epoch 25: Val Loss 582.95288
Epoch 26: Val Loss 555.16864
Epoch 27: Val Loss 527.45709
Epoch 28: Val Loss 502.91092
Epoch 29: Val Loss 479.90714
Epoch 30: Val Loss 460.28577
Epoch 31: Val Loss 446.59320
Epoch 32: Val Loss 433.05576
Epoch 33: Val Loss 416.99432
Epoch 34: Val Loss 406.09171
Epoch 35: Val Loss 391.56790
Epoch 36: Val Loss 376.80176
Epoch 37: Val Loss 363.12839
Epoch 38: Val Loss 359.88239
Epoch 39: Val Loss 364.20068
Epoch 40: Val Loss 358.38965
Epoch 41: Val Loss 358.00629
Epoch 42: Val Loss 344.47754
Epoch 43: Val Loss 334.93689
Epoch 44: Val Loss 326.52771
Epoch 45: Val Loss 323.83575
Epoch 46: Val Loss 310.84381
Epoch 47: Val Loss 307.21024
Epoch 48: Val Loss 307.63440
Epoch 49: Val Loss 314.21646
Epoch 50: Val Loss 331.31940
Epoch 51: Val Loss 330.59436
Epoch 52: Val Loss 304.82346
Epoch 53: Val Loss 298.32602
Epoch 54: Val Loss 298.17389
Epoch 55: Val Loss 298.17850
Epoch 56: Val Loss 301.50784
Epoch 57: Val Loss 298.68674
Epoch 58: Val Loss 296.47214
Epoch 59: Val Loss 297.16913
Epoch 60: Val Loss 293.49570
Epoch 61: Val Loss 287.94870
Epoch 62: Val Loss 278.16556
Epoch 63: Val Loss 277.20004
Epoch 64: Val Loss 292.62494
Epoch 65: Val Loss 287.91113
Epoch 66: Val Loss 299.02930
Epoch 67: Val Loss 298.69785
Epoch 68: Val Loss 289.42538
Epoch 69: Val Loss 281.06192
Epoch 70: Val Loss 279.62387
Epoch 71: Val Loss 281.44583
Epoch 72: Val Loss 281.74988
Epoch 73: Val Loss 297.21756
Epoch 74: Val Loss 300.77087
Epoch 75: Val Loss 291.86621
Epoch 76: Val Loss 284.84042
Epoch 77: Val Loss 284.45178
Epoch 78: Val Loss 291.67557
Epoch 79: Val Loss 276.77182
Epoch 80: Val Loss 274.24905
Epoch 81: Val Loss 272.00208
Epoch 82: Val Loss 274.16129
Epoch 83: Val Loss 268.72931
Epoch 84: Val Loss 264.00150
Epoch 85: Val Loss 259.06012
Epoch 86: Val Loss 265.64420
Epoch 87: Val Loss 269.26575
Epoch 88: Val Loss 257.86920
Epoch 89: Val Loss 257.17975
Epoch 90: Val Loss 269.37439
Epoch 91: Val Loss 272.61945
Epoch 92: Val Loss 267.63947
Epoch 93: Val Loss 259.81302
Epoch 94: Val Loss 244.40146
Epoch 95: Val Loss 242.24994
Epoch 96: Val Loss 235.34729
Epoch 97: Val Loss 238.00832
Epoch 98: Val Loss 235.76163
Epoch 99: Val Loss 240.14847
{'MSE - mean': 212.08218990913764, 'MSE - std': 23.265093133227396, 'R2 - mean': 0.7957186715795385, 'R2 - std': 0.019842534342749407} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2824.42798
Epoch 1: Val Loss 2823.41895
Epoch 2: Val Loss 2822.24463
Epoch 3: Val Loss 2819.91187
Epoch 4: Val Loss 2816.55176
Epoch 5: Val Loss 2812.29272
Epoch 6: Val Loss 2806.99072
Epoch 7: Val Loss 2800.00537
Epoch 8: Val Loss 2790.48657
Epoch 9: Val Loss 2777.92261
Epoch 10: Val Loss 2761.61108
Epoch 11: Val Loss 2741.38330
Epoch 12: Val Loss 2715.39819
Epoch 13: Val Loss 2684.30835
Epoch 14: Val Loss 2647.51562
Epoch 15: Val Loss 2603.20166
Epoch 16: Val Loss 2552.82275
Epoch 17: Val Loss 2498.49707
Epoch 18: Val Loss 2437.10059
Epoch 19: Val Loss 2372.74390
Epoch 20: Val Loss 2307.75562
Epoch 21: Val Loss 2240.91284
Epoch 22: Val Loss 2176.48779
Epoch 23: Val Loss 2112.54443
Epoch 24: Val Loss 2057.88770
Epoch 25: Val Loss 2011.12769
Epoch 26: Val Loss 1968.33411
Epoch 27: Val Loss 1927.70264
Epoch 28: Val Loss 1889.23865
Epoch 29: Val Loss 1850.70947
Epoch 30: Val Loss 1813.35376
Epoch 31: Val Loss 1774.83118
Epoch 32: Val Loss 1732.43091
Epoch 33: Val Loss 1684.91113
Epoch 34: Val Loss 1637.43677
Epoch 35: Val Loss 1591.45178
Epoch 36: Val Loss 1540.03528
Epoch 37: Val Loss 1492.74219
Epoch 38: Val Loss 1438.37476
Epoch 39: Val Loss 1383.58301
Epoch 40: Val Loss 1335.89453
Epoch 41: Val Loss 1294.79578
Epoch 42: Val Loss 1250.50659
Epoch 43: Val Loss 1211.61011
Epoch 44: Val Loss 1173.95239
Epoch 45: Val Loss 1139.90259
Epoch 46: Val Loss 1104.28015
Epoch 47: Val Loss 1071.29260
Epoch 48: Val Loss 1041.67505
Epoch 49: Val Loss 1019.46014
Epoch 50: Val Loss 994.53986
Epoch 51: Val Loss 968.35577
Epoch 52: Val Loss 948.86084
Epoch 53: Val Loss 932.48169
Epoch 54: Val Loss 909.12201
Epoch 55: Val Loss 881.85992
Epoch 56: Val Loss 859.98853
Epoch 57: Val Loss 840.09943
Epoch 58: Val Loss 833.70874
Epoch 59: Val Loss 808.61975
Epoch 60: Val Loss 796.59644
Epoch 61: Val Loss 782.09253
Epoch 62: Val Loss 771.94208
Epoch 63: Val Loss 767.17743
Epoch 64: Val Loss 751.73810
Epoch 65: Val Loss 729.74634
Epoch 66: Val Loss 714.96808
Epoch 67: Val Loss 707.08063
Epoch 68: Val Loss 699.64746
Epoch 69: Val Loss 683.81378
Epoch 70: Val Loss 663.28809
Epoch 71: Val Loss 652.52295
Epoch 72: Val Loss 647.87622
Epoch 73: Val Loss 646.22803
Epoch 74: Val Loss 642.72412
Epoch 75: Val Loss 632.26227
Epoch 76: Val Loss 627.00476
Epoch 77: Val Loss 626.34143
Epoch 78: Val Loss 618.47552
Epoch 79: Val Loss 610.27979
Epoch 80: Val Loss 610.92291
Epoch 81: Val Loss 605.56049
Epoch 82: Val Loss 597.05225
Epoch 83: Val Loss 590.15936
Epoch 84: Val Loss 578.88135
Epoch 85: Val Loss 569.96661
Epoch 86: Val Loss 565.79541
Epoch 87: Val Loss 563.47266
Epoch 88: Val Loss 558.11041
Epoch 89: Val Loss 552.09351
Epoch 90: Val Loss 553.64874
Epoch 91: Val Loss 551.67810
Epoch 92: Val Loss 542.47449
Epoch 93: Val Loss 532.90558
Epoch 94: Val Loss 525.89642
Epoch 95: Val Loss 521.60022
Epoch 96: Val Loss 518.60193
Epoch 97: Val Loss 510.34027
Epoch 98: Val Loss 508.29889
Epoch 99: Val Loss 514.50586
{'MSE - mean': 310.8210900562875, 'MSE - std': 140.92403571973153, 'R2 - mean': 0.7946545674147893, 'R2 - std': 0.016271101625550708} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2102.19580
Epoch 1: Val Loss 2094.94971
Epoch 2: Val Loss 2085.86523
Epoch 3: Val Loss 2073.81689
Epoch 4: Val Loss 2057.98462
Epoch 5: Val Loss 2037.45801
Epoch 6: Val Loss 2010.75842
Epoch 7: Val Loss 1976.01843
Epoch 8: Val Loss 1931.23755
Epoch 9: Val Loss 1874.80005
Epoch 10: Val Loss 1806.49915
Epoch 11: Val Loss 1727.57019
Epoch 12: Val Loss 1638.88171
Epoch 13: Val Loss 1544.68860
Epoch 14: Val Loss 1456.09375
Epoch 15: Val Loss 1371.41284
Epoch 16: Val Loss 1299.75378
Epoch 17: Val Loss 1238.57251
Epoch 18: Val Loss 1185.08325
Epoch 19: Val Loss 1135.80139
Epoch 20: Val Loss 1090.83899
Epoch 21: Val Loss 1044.29797
Epoch 22: Val Loss 998.64069
Epoch 23: Val Loss 954.29993
Epoch 24: Val Loss 908.87854
Epoch 25: Val Loss 866.24756
Epoch 26: Val Loss 826.20050
Epoch 27: Val Loss 787.18372
Epoch 28: Val Loss 750.53333
Epoch 29: Val Loss 716.41815
Epoch 30: Val Loss 685.48816
Epoch 31: Val Loss 657.67883
Epoch 32: Val Loss 630.03876
Epoch 33: Val Loss 606.93463
Epoch 34: Val Loss 588.69550
Epoch 35: Val Loss 571.84491
Epoch 36: Val Loss 556.14966
Epoch 37: Val Loss 542.21692
Epoch 38: Val Loss 529.54089
Epoch 39: Val Loss 520.07068
Epoch 40: Val Loss 508.41064
Epoch 41: Val Loss 493.88144
Epoch 42: Val Loss 481.71817
Epoch 43: Val Loss 473.36365
Epoch 44: Val Loss 466.85825
Epoch 45: Val Loss 460.48468
Epoch 46: Val Loss 455.38956
Epoch 47: Val Loss 448.91324
Epoch 48: Val Loss 445.23886
Epoch 49: Val Loss 440.47202
Epoch 50: Val Loss 436.85553
Epoch 51: Val Loss 432.26050
Epoch 52: Val Loss 425.49493
Epoch 53: Val Loss 421.73557
Epoch 54: Val Loss 417.25345
Epoch 55: Val Loss 413.55176
Epoch 56: Val Loss 409.92664
Epoch 57: Val Loss 405.65442
Epoch 58: Val Loss 399.50739
Epoch 59: Val Loss 396.94745
Epoch 60: Val Loss 395.58075
Epoch 61: Val Loss 401.40350
Epoch 62: Val Loss 397.19864
Epoch 63: Val Loss 387.06616
Epoch 64: Val Loss 384.40375
Epoch 65: Val Loss 378.46103
Epoch 66: Val Loss 377.61902
Epoch 67: Val Loss 384.82318
Epoch 68: Val Loss 382.37595
Epoch 69: Val Loss 374.34595
Epoch 70: Val Loss 368.26669
Epoch 71: Val Loss 363.21042
Epoch 72: Val Loss 361.13794
Epoch 73: Val Loss 364.06912
Epoch 74: Val Loss 362.38251
Epoch 75: Val Loss 357.35620
Epoch 76: Val Loss 352.81772
Epoch 77: Val Loss 354.83881
Epoch 78: Val Loss 355.32947
Epoch 79: Val Loss 352.12463
Epoch 80: Val Loss 347.80072
Epoch 81: Val Loss 347.38208
Epoch 82: Val Loss 349.18118
Epoch 83: Val Loss 351.74777
Epoch 84: Val Loss 349.27841
Epoch 85: Val Loss 341.85437
Epoch 86: Val Loss 331.77325
Epoch 87: Val Loss 331.64752
Epoch 88: Val Loss 333.05856
Epoch 89: Val Loss 335.97113
Epoch 90: Val Loss 334.67093
Epoch 91: Val Loss 330.97433
Epoch 92: Val Loss 326.90036
Epoch 93: Val Loss 326.16415
Epoch 94: Val Loss 321.81647
Epoch 95: Val Loss 326.58948
Epoch 96: Val Loss 325.58398
Epoch 97: Val Loss 323.44751
Epoch 98: Val Loss 318.65100
Epoch 99: Val Loss 320.35742
{'MSE - mean': 312.7785736060192, 'MSE - std': 122.09088052837744, 'R2 - mean': 0.7997666571520188, 'R2 - std': 0.016642173701959972} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2511.44019
Epoch 1: Val Loss 2509.42432
Epoch 2: Val Loss 2507.10718
Epoch 3: Val Loss 2504.33496
Epoch 4: Val Loss 2500.79956
Epoch 5: Val Loss 2496.09741
Epoch 6: Val Loss 2489.63721
Epoch 7: Val Loss 2480.73975
Epoch 8: Val Loss 2469.08276
Epoch 9: Val Loss 2453.71069
Epoch 10: Val Loss 2433.58154
Epoch 11: Val Loss 2408.25537
Epoch 12: Val Loss 2375.60864
Epoch 13: Val Loss 2335.14600
Epoch 14: Val Loss 2287.57153
Epoch 15: Val Loss 2232.35864
Epoch 16: Val Loss 2167.97607
Epoch 17: Val Loss 2093.22559
Epoch 18: Val Loss 2013.71155
Epoch 19: Val Loss 1930.82141
Epoch 20: Val Loss 1851.37488
Epoch 21: Val Loss 1777.72510
Epoch 22: Val Loss 1710.18457
Epoch 23: Val Loss 1654.94775
Epoch 24: Val Loss 1600.04590
Epoch 25: Val Loss 1546.74536
Epoch 26: Val Loss 1496.37524
Epoch 27: Val Loss 1447.14624
Epoch 28: Val Loss 1401.23755
Epoch 29: Val Loss 1363.24048
Epoch 30: Val Loss 1309.26709
Epoch 31: Val Loss 1262.71179
Epoch 32: Val Loss 1215.74756
Epoch 33: Val Loss 1166.03137
Epoch 34: Val Loss 1123.79785
Epoch 35: Val Loss 1085.76721
Epoch 36: Val Loss 1048.56104
Epoch 37: Val Loss 1006.33661
Epoch 38: Val Loss 972.06995
Epoch 39: Val Loss 934.59180
Epoch 40: Val Loss 910.41473
Epoch 41: Val Loss 887.91962
Epoch 42: Val Loss 859.58020
Epoch 43: Val Loss 829.21600
Epoch 44: Val Loss 809.11212
Epoch 45: Val Loss 787.03992
Epoch 46: Val Loss 765.73853
Epoch 47: Val Loss 745.01288
Epoch 48: Val Loss 734.52692
Epoch 49: Val Loss 731.92633
Epoch 50: Val Loss 723.33478
Epoch 51: Val Loss 705.83673
Epoch 52: Val Loss 690.71027
Epoch 53: Val Loss 679.76837
Epoch 54: Val Loss 666.45459
Epoch 55: Val Loss 651.30426
Epoch 56: Val Loss 643.27441
Epoch 57: Val Loss 639.65881
Epoch 58: Val Loss 622.49634
Epoch 59: Val Loss 622.78003
Epoch 60: Val Loss 624.47327
Epoch 61: Val Loss 621.22119
Epoch 62: Val Loss 616.05225
Epoch 63: Val Loss 611.28802
Epoch 64: Val Loss 604.90857
Epoch 65: Val Loss 598.40472
Epoch 66: Val Loss 591.24396
Epoch 67: Val Loss 583.36017
Epoch 68: Val Loss 577.83008
Epoch 69: Val Loss 566.57703
Epoch 70: Val Loss 558.71356
Epoch 71: Val Loss 556.32849
Epoch 72: Val Loss 558.25684
Epoch 73: Val Loss 552.86945
Epoch 74: Val Loss 549.97577
Epoch 75: Val Loss 542.81665
Epoch 76: Val Loss 537.24860
Epoch 77: Val Loss 532.31097
Epoch 78: Val Loss 530.88452
Epoch 79: Val Loss 527.31366
Epoch 80: Val Loss 523.75421
Epoch 81: Val Loss 521.48938
Epoch 82: Val Loss 519.43427
Epoch 83: Val Loss 511.33456
Epoch 84: Val Loss 505.44757
Epoch 85: Val Loss 502.91315
Epoch 86: Val Loss 503.84018
Epoch 87: Val Loss 503.57458
Epoch 88: Val Loss 497.05136
Epoch 89: Val Loss 496.84970
Epoch 90: Val Loss 497.07388
Epoch 91: Val Loss 491.30905
Epoch 92: Val Loss 485.30643
Epoch 93: Val Loss 477.07162
Epoch 94: Val Loss 476.50488
Epoch 95: Val Loss 475.39880
Epoch 96: Val Loss 470.10580
Epoch 97: Val Loss 462.58313
Epoch 98: Val Loss 459.29855
Epoch 99: Val Loss 458.75107
{'MSE - mean': 341.9730896974135, 'MSE - std': 123.83144013455691, 'R2 - mean': 0.7969209255270064, 'R2 - std': 0.015936194977074974} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 341.9730896974135 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1295.66675
Epoch 1: Val Loss 1295.39600
Epoch 2: Val Loss 1295.13684
Epoch 3: Val Loss 1294.87585
Epoch 4: Val Loss 1294.61597
Epoch 5: Val Loss 1294.36426
Epoch 6: Val Loss 1294.11633
Epoch 7: Val Loss 1293.86975
Epoch 8: Val Loss 1293.62744
Epoch 9: Val Loss 1293.39954
Epoch 10: Val Loss 1293.16357
Epoch 11: Val Loss 1292.94348
Epoch 12: Val Loss 1292.72180
Epoch 13: Val Loss 1292.49890
Epoch 14: Val Loss 1292.28955
Epoch 15: Val Loss 1292.07983
Epoch 16: Val Loss 1291.86877
Epoch 17: Val Loss 1291.64795
Epoch 18: Val Loss 1291.42834
Epoch 19: Val Loss 1291.20789
Epoch 20: Val Loss 1290.98657
Epoch 21: Val Loss 1290.76135
Epoch 22: Val Loss 1290.53040
Epoch 23: Val Loss 1290.30750
Epoch 24: Val Loss 1290.09656
Epoch 25: Val Loss 1289.88550
Epoch 26: Val Loss 1289.65784
Epoch 27: Val Loss 1289.43323
Epoch 28: Val Loss 1289.21497
Epoch 29: Val Loss 1288.99353
Epoch 30: Val Loss 1288.75537
Epoch 31: Val Loss 1288.51196
Epoch 32: Val Loss 1288.27307
Epoch 33: Val Loss 1288.01868
Epoch 34: Val Loss 1287.74182
Epoch 35: Val Loss 1287.45264
Epoch 36: Val Loss 1287.17175
Epoch 37: Val Loss 1286.89075
Epoch 38: Val Loss 1286.59998
Epoch 39: Val Loss 1286.29871
Epoch 40: Val Loss 1285.96960
Epoch 41: Val Loss 1285.65637
Epoch 42: Val Loss 1285.31616
Epoch 43: Val Loss 1284.94788
Epoch 44: Val Loss 1284.56250
Epoch 45: Val Loss 1284.15625
Epoch 46: Val Loss 1283.71021
Epoch 47: Val Loss 1283.25928
Epoch 48: Val Loss 1282.82275
Epoch 49: Val Loss 1282.34619
Epoch 50: Val Loss 1281.86292
Epoch 51: Val Loss 1281.39221
Epoch 52: Val Loss 1280.90381
Epoch 53: Val Loss 1280.39734
Epoch 54: Val Loss 1279.85071
Epoch 55: Val Loss 1279.29407
Epoch 56: Val Loss 1278.73303
Epoch 57: Val Loss 1278.15088
Epoch 58: Val Loss 1277.55005
Epoch 59: Val Loss 1276.93799
Epoch 60: Val Loss 1276.30762
Epoch 61: Val Loss 1275.66748
Epoch 62: Val Loss 1275.00159
Epoch 63: Val Loss 1274.31653
Epoch 64: Val Loss 1273.60852
Epoch 65: Val Loss 1272.79395
Epoch 66: Val Loss 1271.96143
Epoch 67: Val Loss 1271.12317
Epoch 68: Val Loss 1270.19983
Epoch 69: Val Loss 1269.27905
Epoch 70: Val Loss 1268.38318
Epoch 71: Val Loss 1267.43518
Epoch 72: Val Loss 1266.45398
Epoch 73: Val Loss 1265.44531
Epoch 74: Val Loss 1264.44104
Epoch 75: Val Loss 1263.37939
Epoch 76: Val Loss 1262.32739
Epoch 77: Val Loss 1261.21228
Epoch 78: Val Loss 1260.07349
Epoch 79: Val Loss 1258.85571
Epoch 80: Val Loss 1257.56543
Epoch 81: Val Loss 1256.27039
Epoch 82: Val Loss 1254.98425
Epoch 83: Val Loss 1253.58655
Epoch 84: Val Loss 1252.20581
Epoch 85: Val Loss 1250.79565
Epoch 86: Val Loss 1249.28540
Epoch 87: Val Loss 1247.71509
Epoch 88: Val Loss 1246.21667
Epoch 89: Val Loss 1244.56946
Epoch 90: Val Loss 1242.90491
Epoch 91: Val Loss 1241.20020
Epoch 92: Val Loss 1239.44690
Epoch 93: Val Loss 1237.67847
Epoch 94: Val Loss 1235.82544
Epoch 95: Val Loss 1233.87500
Epoch 96: Val Loss 1231.85962
Epoch 97: Val Loss 1229.65857
Epoch 98: Val Loss 1227.54773
Epoch 99: Val Loss 1225.36255
{'MSE - mean': 1225.36266300003, 'MSE - std': 0.0, 'R2 - mean': -0.19694887661472427, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1313.53479
Epoch 1: Val Loss 1313.25525
Epoch 2: Val Loss 1312.97046
Epoch 3: Val Loss 1312.69470
Epoch 4: Val Loss 1312.41260
Epoch 5: Val Loss 1312.13965
Epoch 6: Val Loss 1311.86389
Epoch 7: Val Loss 1311.58911
Epoch 8: Val Loss 1311.31055
Epoch 9: Val Loss 1311.02600
Epoch 10: Val Loss 1310.74719
Epoch 11: Val Loss 1310.46130
Epoch 12: Val Loss 1310.17468
Epoch 13: Val Loss 1309.88135
Epoch 14: Val Loss 1309.59351
Epoch 15: Val Loss 1309.30347
Epoch 16: Val Loss 1309.00916
Epoch 17: Val Loss 1308.70923
Epoch 18: Val Loss 1308.39197
Epoch 19: Val Loss 1308.04675
Epoch 20: Val Loss 1307.70593
Epoch 21: Val Loss 1307.37305
Epoch 22: Val Loss 1307.02380
Epoch 23: Val Loss 1306.66968
Epoch 24: Val Loss 1306.30273
Epoch 25: Val Loss 1305.90747
Epoch 26: Val Loss 1305.50049
Epoch 27: Val Loss 1305.06677
Epoch 28: Val Loss 1304.62073
Epoch 29: Val Loss 1304.18860
Epoch 30: Val Loss 1303.69604
Epoch 31: Val Loss 1303.18750
Epoch 32: Val Loss 1302.66394
Epoch 33: Val Loss 1302.14172
Epoch 34: Val Loss 1301.59509
Epoch 35: Val Loss 1301.02087
Epoch 36: Val Loss 1300.43127
Epoch 37: Val Loss 1299.82092
Epoch 38: Val Loss 1299.21875
Epoch 39: Val Loss 1298.55054
Epoch 40: Val Loss 1297.82837
Epoch 41: Val Loss 1297.09326
Epoch 42: Val Loss 1296.33655
Epoch 43: Val Loss 1295.49072
Epoch 44: Val Loss 1294.65173
Epoch 45: Val Loss 1293.77759
Epoch 46: Val Loss 1292.86145
Epoch 47: Val Loss 1291.91248
Epoch 48: Val Loss 1290.90161
Epoch 49: Val Loss 1289.81873
Epoch 50: Val Loss 1288.68408
Epoch 51: Val Loss 1287.47705
Epoch 52: Val Loss 1286.15173
Epoch 53: Val Loss 1284.85510
Epoch 54: Val Loss 1283.53003
Epoch 55: Val Loss 1282.19946
Epoch 56: Val Loss 1280.83008
Epoch 57: Val Loss 1279.37451
Epoch 58: Val Loss 1277.84363
Epoch 59: Val Loss 1276.34888
Epoch 60: Val Loss 1274.78699
Epoch 61: Val Loss 1273.18591
Epoch 62: Val Loss 1271.46228
Epoch 63: Val Loss 1269.77222
Epoch 64: Val Loss 1267.99084
Epoch 65: Val Loss 1266.12659
Epoch 66: Val Loss 1264.26648
Epoch 67: Val Loss 1262.29297
Epoch 68: Val Loss 1260.30396
Epoch 69: Val Loss 1258.17346
Epoch 70: Val Loss 1255.99646
Epoch 71: Val Loss 1253.79382
Epoch 72: Val Loss 1251.62451
Epoch 73: Val Loss 1249.39551
Epoch 74: Val Loss 1247.15857
Epoch 75: Val Loss 1244.88123
Epoch 76: Val Loss 1242.47986
Epoch 77: Val Loss 1240.01294
Epoch 78: Val Loss 1237.51355
Epoch 79: Val Loss 1234.93103
Epoch 80: Val Loss 1232.17993
Epoch 81: Val Loss 1229.39453
Epoch 82: Val Loss 1226.58594
Epoch 83: Val Loss 1223.72009
Epoch 84: Val Loss 1220.74597
Epoch 85: Val Loss 1217.61707
Epoch 86: Val Loss 1214.47400
Epoch 87: Val Loss 1211.33252
Epoch 88: Val Loss 1208.01709
Epoch 89: Val Loss 1204.60669
Epoch 90: Val Loss 1201.00110
Epoch 91: Val Loss 1197.36523
Epoch 92: Val Loss 1193.82300
Epoch 93: Val Loss 1190.05200
Epoch 94: Val Loss 1186.15662
Epoch 95: Val Loss 1182.12671
Epoch 96: Val Loss 1178.12000
Epoch 97: Val Loss 1174.17322
Epoch 98: Val Loss 1170.22217
Epoch 99: Val Loss 1166.15857
{'MSE - mean': 1195.7605704363025, 'MSE - std': 29.60209256372741, 'R2 - mean': -0.15374735090737646, 'R2 - std': 0.04320152570734781} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2808.53149
Epoch 1: Val Loss 2808.14990
Epoch 2: Val Loss 2807.76074
Epoch 3: Val Loss 2807.35327
Epoch 4: Val Loss 2806.92358
Epoch 5: Val Loss 2806.50659
Epoch 6: Val Loss 2806.09082
Epoch 7: Val Loss 2805.66528
Epoch 8: Val Loss 2805.23267
Epoch 9: Val Loss 2804.76562
Epoch 10: Val Loss 2804.28931
Epoch 11: Val Loss 2803.80029
Epoch 12: Val Loss 2803.29395
Epoch 13: Val Loss 2802.76465
Epoch 14: Val Loss 2802.21191
Epoch 15: Val Loss 2801.65796
Epoch 16: Val Loss 2801.09644
Epoch 17: Val Loss 2800.51147
Epoch 18: Val Loss 2799.92432
Epoch 19: Val Loss 2799.33716
Epoch 20: Val Loss 2798.72656
Epoch 21: Val Loss 2798.13159
Epoch 22: Val Loss 2797.53516
Epoch 23: Val Loss 2796.91113
Epoch 24: Val Loss 2796.27441
Epoch 25: Val Loss 2795.57788
Epoch 26: Val Loss 2794.88013
Epoch 27: Val Loss 2794.14551
Epoch 28: Val Loss 2793.41504
Epoch 29: Val Loss 2792.67114
Epoch 30: Val Loss 2791.92334
Epoch 31: Val Loss 2791.14673
Epoch 32: Val Loss 2790.34814
Epoch 33: Val Loss 2789.52930
Epoch 34: Val Loss 2788.67651
Epoch 35: Val Loss 2787.82080
Epoch 36: Val Loss 2786.96533
Epoch 37: Val Loss 2786.06006
Epoch 38: Val Loss 2785.12109
Epoch 39: Val Loss 2784.17773
Epoch 40: Val Loss 2783.14551
Epoch 41: Val Loss 2782.11499
Epoch 42: Val Loss 2781.07593
Epoch 43: Val Loss 2780.03784
Epoch 44: Val Loss 2778.96802
Epoch 45: Val Loss 2777.86572
Epoch 46: Val Loss 2776.76685
Epoch 47: Val Loss 2775.60059
Epoch 48: Val Loss 2774.37720
Epoch 49: Val Loss 2773.13745
Epoch 50: Val Loss 2771.88892
Epoch 51: Val Loss 2770.58496
Epoch 52: Val Loss 2769.27246
Epoch 53: Val Loss 2767.80737
Epoch 54: Val Loss 2766.44165
Epoch 55: Val Loss 2765.06006
Epoch 56: Val Loss 2763.54932
Epoch 57: Val Loss 2762.04614
Epoch 58: Val Loss 2760.49780
Epoch 59: Val Loss 2758.92651
Epoch 60: Val Loss 2757.36987
Epoch 61: Val Loss 2755.72339
Epoch 62: Val Loss 2754.03467
Epoch 63: Val Loss 2752.31787
Epoch 64: Val Loss 2750.61914
Epoch 65: Val Loss 2748.82202
Epoch 66: Val Loss 2746.94312
Epoch 67: Val Loss 2745.01978
Epoch 68: Val Loss 2743.07104
Epoch 69: Val Loss 2741.04150
Epoch 70: Val Loss 2738.95752
Epoch 71: Val Loss 2736.91675
Epoch 72: Val Loss 2734.75586
Epoch 73: Val Loss 2732.64722
Epoch 74: Val Loss 2730.36523
Epoch 75: Val Loss 2728.16455
Epoch 76: Val Loss 2725.79443
Epoch 77: Val Loss 2723.34570
Epoch 78: Val Loss 2720.87549
Epoch 79: Val Loss 2718.36255
Epoch 80: Val Loss 2715.74341
Epoch 81: Val Loss 2712.92847
Epoch 82: Val Loss 2709.86206
Epoch 83: Val Loss 2706.81445
Epoch 84: Val Loss 2703.69092
Epoch 85: Val Loss 2700.59204
Epoch 86: Val Loss 2697.39893
Epoch 87: Val Loss 2694.13159
Epoch 88: Val Loss 2690.77808
Epoch 89: Val Loss 2687.35962
Epoch 90: Val Loss 2683.97607
Epoch 91: Val Loss 2680.24561
Epoch 92: Val Loss 2676.55566
Epoch 93: Val Loss 2672.85059
Epoch 94: Val Loss 2669.03906
Epoch 95: Val Loss 2665.17700
Epoch 96: Val Loss 2661.06006
Epoch 97: Val Loss 2656.91113
Epoch 98: Val Loss 2652.69434
Epoch 99: Val Loss 2648.44409
{'MSE - mean': 1679.9884035752777, 'MSE - std': 685.2279752236007, 'R2 - mean': -0.12950560064226865, 'R2 - std': 0.0491891532162} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2131.77637
Epoch 1: Val Loss 2131.59424
Epoch 2: Val Loss 2131.42578
Epoch 3: Val Loss 2131.26489
Epoch 4: Val Loss 2131.11499
Epoch 5: Val Loss 2130.96216
Epoch 6: Val Loss 2130.80957
Epoch 7: Val Loss 2130.65771
Epoch 8: Val Loss 2130.49463
Epoch 9: Val Loss 2130.34180
Epoch 10: Val Loss 2130.18921
Epoch 11: Val Loss 2130.02930
Epoch 12: Val Loss 2129.86963
Epoch 13: Val Loss 2129.69971
Epoch 14: Val Loss 2129.54028
Epoch 15: Val Loss 2129.38501
Epoch 16: Val Loss 2129.22461
Epoch 17: Val Loss 2129.06665
Epoch 18: Val Loss 2128.91064
Epoch 19: Val Loss 2128.74780
Epoch 20: Val Loss 2128.57422
Epoch 21: Val Loss 2128.39258
Epoch 22: Val Loss 2128.20459
Epoch 23: Val Loss 2128.01147
Epoch 24: Val Loss 2127.81030
Epoch 25: Val Loss 2127.61328
Epoch 26: Val Loss 2127.40527
Epoch 27: Val Loss 2127.18848
Epoch 28: Val Loss 2126.96802
Epoch 29: Val Loss 2126.74683
Epoch 30: Val Loss 2126.51123
Epoch 31: Val Loss 2126.27661
Epoch 32: Val Loss 2126.02466
Epoch 33: Val Loss 2125.77344
Epoch 34: Val Loss 2125.51074
Epoch 35: Val Loss 2125.23901
Epoch 36: Val Loss 2124.96436
Epoch 37: Val Loss 2124.66602
Epoch 38: Val Loss 2124.34668
Epoch 39: Val Loss 2124.01685
Epoch 40: Val Loss 2123.68945
Epoch 41: Val Loss 2123.33911
Epoch 42: Val Loss 2122.96753
Epoch 43: Val Loss 2122.58130
Epoch 44: Val Loss 2122.18237
Epoch 45: Val Loss 2121.75244
Epoch 46: Val Loss 2121.30347
Epoch 47: Val Loss 2120.85229
Epoch 48: Val Loss 2120.34155
Epoch 49: Val Loss 2119.80664
Epoch 50: Val Loss 2119.24170
Epoch 51: Val Loss 2118.64307
Epoch 52: Val Loss 2117.99487
Epoch 53: Val Loss 2117.30542
Epoch 54: Val Loss 2116.57910
Epoch 55: Val Loss 2115.82007
Epoch 56: Val Loss 2115.03857
Epoch 57: Val Loss 2114.22998
Epoch 58: Val Loss 2113.38306
Epoch 59: Val Loss 2112.51172
Epoch 60: Val Loss 2111.54834
Epoch 61: Val Loss 2110.50708
Epoch 62: Val Loss 2109.44629
Epoch 63: Val Loss 2108.32812
Epoch 64: Val Loss 2107.12573
Epoch 65: Val Loss 2105.86157
Epoch 66: Val Loss 2104.47485
Epoch 67: Val Loss 2102.98218
Epoch 68: Val Loss 2101.51074
Epoch 69: Val Loss 2100.02295
Epoch 70: Val Loss 2098.45093
Epoch 71: Val Loss 2096.81274
Epoch 72: Val Loss 2095.17432
Epoch 73: Val Loss 2093.41968
Epoch 74: Val Loss 2091.57031
Epoch 75: Val Loss 2089.56323
Epoch 76: Val Loss 2087.55811
Epoch 77: Val Loss 2085.50806
Epoch 78: Val Loss 2083.38794
Epoch 79: Val Loss 2081.12744
Epoch 80: Val Loss 2078.76221
Epoch 81: Val Loss 2076.41577
Epoch 82: Val Loss 2073.90112
Epoch 83: Val Loss 2071.27124
Epoch 84: Val Loss 2068.58960
Epoch 85: Val Loss 2065.73584
Epoch 86: Val Loss 2062.75586
Epoch 87: Val Loss 2059.75928
Epoch 88: Val Loss 2056.75415
Epoch 89: Val Loss 2053.56592
Epoch 90: Val Loss 2050.44092
Epoch 91: Val Loss 2047.23962
Epoch 92: Val Loss 2043.84521
Epoch 93: Val Loss 2040.13489
Epoch 94: Val Loss 2036.50513
Epoch 95: Val Loss 2032.71387
Epoch 96: Val Loss 2028.72217
Epoch 97: Val Loss 2024.51233
Epoch 98: Val Loss 2020.04993
Epoch 99: Val Loss 2015.73254
{'MSE - mean': 1763.9244566386947, 'MSE - std': 610.9736627166271, 'R2 - mean': -0.13953612336161664, 'R2 - std': 0.04600558394005197} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2529.82739
Epoch 1: Val Loss 2529.50464
Epoch 2: Val Loss 2529.19092
Epoch 3: Val Loss 2528.89014
Epoch 4: Val Loss 2528.59937
Epoch 5: Val Loss 2528.29590
Epoch 6: Val Loss 2527.99268
Epoch 7: Val Loss 2527.69507
Epoch 8: Val Loss 2527.41260
Epoch 9: Val Loss 2527.12915
Epoch 10: Val Loss 2526.84814
Epoch 11: Val Loss 2526.55103
Epoch 12: Val Loss 2526.25659
Epoch 13: Val Loss 2525.95239
Epoch 14: Val Loss 2525.65210
Epoch 15: Val Loss 2525.34033
Epoch 16: Val Loss 2525.02246
Epoch 17: Val Loss 2524.69531
Epoch 18: Val Loss 2524.35107
Epoch 19: Val Loss 2523.99243
Epoch 20: Val Loss 2523.61523
Epoch 21: Val Loss 2523.22021
Epoch 22: Val Loss 2522.80200
Epoch 23: Val Loss 2522.35474
Epoch 24: Val Loss 2521.88208
Epoch 25: Val Loss 2521.37256
Epoch 26: Val Loss 2520.83325
Epoch 27: Val Loss 2520.25024
Epoch 28: Val Loss 2519.65015
Epoch 29: Val Loss 2518.98755
Epoch 30: Val Loss 2518.31494
Epoch 31: Val Loss 2517.57544
Epoch 32: Val Loss 2516.81445
Epoch 33: Val Loss 2516.01514
Epoch 34: Val Loss 2515.16064
Epoch 35: Val Loss 2514.22607
Epoch 36: Val Loss 2513.27466
Epoch 37: Val Loss 2512.30396
Epoch 38: Val Loss 2511.27246
Epoch 39: Val Loss 2510.22559
Epoch 40: Val Loss 2509.07910
Epoch 41: Val Loss 2507.93896
Epoch 42: Val Loss 2506.74683
Epoch 43: Val Loss 2505.48340
Epoch 44: Val Loss 2504.17554
Epoch 45: Val Loss 2502.83179
Epoch 46: Val Loss 2501.50537
Epoch 47: Val Loss 2500.06055
Epoch 48: Val Loss 2498.58203
Epoch 49: Val Loss 2496.99390
Epoch 50: Val Loss 2495.38525
Epoch 51: Val Loss 2493.74487
Epoch 52: Val Loss 2492.05200
Epoch 53: Val Loss 2490.36890
Epoch 54: Val Loss 2488.50195
Epoch 55: Val Loss 2486.60547
Epoch 56: Val Loss 2484.68701
Epoch 57: Val Loss 2482.81592
Epoch 58: Val Loss 2480.82251
Epoch 59: Val Loss 2478.70654
Epoch 60: Val Loss 2476.54663
Epoch 61: Val Loss 2474.26807
Epoch 62: Val Loss 2471.85352
Epoch 63: Val Loss 2469.39819
Epoch 64: Val Loss 2466.90112
Epoch 65: Val Loss 2464.43994
Epoch 66: Val Loss 2461.90356
Epoch 67: Val Loss 2459.18896
Epoch 68: Val Loss 2456.34106
Epoch 69: Val Loss 2453.38940
Epoch 70: Val Loss 2450.34814
Epoch 71: Val Loss 2447.22046
Epoch 72: Val Loss 2444.03418
Epoch 73: Val Loss 2440.73438
Epoch 74: Val Loss 2437.44580
Epoch 75: Val Loss 2434.14160
Epoch 76: Val Loss 2430.75049
Epoch 77: Val Loss 2427.01953
Epoch 78: Val Loss 2423.31860
Epoch 79: Val Loss 2419.50806
Epoch 80: Val Loss 2415.37793
Epoch 81: Val Loss 2411.30811
Epoch 82: Val Loss 2407.30493
Epoch 83: Val Loss 2403.17847
Epoch 84: Val Loss 2398.86108
Epoch 85: Val Loss 2394.34204
Epoch 86: Val Loss 2389.92432
Epoch 87: Val Loss 2385.19507
Epoch 88: Val Loss 2380.52197
Epoch 89: Val Loss 2375.79102
Epoch 90: Val Loss 2370.83716
Epoch 91: Val Loss 2365.85010
Epoch 92: Val Loss 2360.87939
Epoch 93: Val Loss 2355.78906
Epoch 94: Val Loss 2350.15894
Epoch 95: Val Loss 2344.71313
Epoch 96: Val Loss 2339.27734
Epoch 97: Val Loss 2333.38843
Epoch 98: Val Loss 2327.39551
Epoch 99: Val Loss 2321.39575
{'MSE - mean': 1875.4187394185028, 'MSE - std': 590.2160228230271, 'R2 - mean': -0.12867519886174925, 'R2 - std': 0.046530094851605586} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 1875.4187394185028 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1284.78162
Epoch 1: Val Loss 1281.52954
Epoch 2: Val Loss 1278.00793
Epoch 3: Val Loss 1273.96143
Epoch 4: Val Loss 1269.35913
Epoch 5: Val Loss 1263.81335
Epoch 6: Val Loss 1257.07019
Epoch 7: Val Loss 1248.62024
Epoch 8: Val Loss 1237.62097
Epoch 9: Val Loss 1223.29688
Epoch 10: Val Loss 1205.26538
Epoch 11: Val Loss 1182.05042
Epoch 12: Val Loss 1154.29602
Epoch 13: Val Loss 1120.97998
Epoch 14: Val Loss 1080.90833
Epoch 15: Val Loss 1033.15942
Epoch 16: Val Loss 979.83789
Epoch 17: Val Loss 923.58984
Epoch 18: Val Loss 866.28271
Epoch 19: Val Loss 809.45221
Epoch 20: Val Loss 761.60150
Epoch 21: Val Loss 720.97644
Epoch 22: Val Loss 685.88385
Epoch 23: Val Loss 660.20325
Epoch 24: Val Loss 636.73071
Epoch 25: Val Loss 616.02039
Epoch 26: Val Loss 593.88074
Epoch 27: Val Loss 571.67175
Epoch 28: Val Loss 547.50012
Epoch 29: Val Loss 525.51178
Epoch 30: Val Loss 501.83478
Epoch 31: Val Loss 481.39328
Epoch 32: Val Loss 459.34305
Epoch 33: Val Loss 439.72784
Epoch 34: Val Loss 424.70718
Epoch 35: Val Loss 409.99670
Epoch 36: Val Loss 396.57205
Epoch 37: Val Loss 381.53159
Epoch 38: Val Loss 367.50546
Epoch 39: Val Loss 357.42667
Epoch 40: Val Loss 348.79373
Epoch 41: Val Loss 338.29193
Epoch 42: Val Loss 330.39972
Epoch 43: Val Loss 320.22064
Epoch 44: Val Loss 311.77142
Epoch 45: Val Loss 303.70190
Epoch 46: Val Loss 296.18707
Epoch 47: Val Loss 290.85690
Epoch 48: Val Loss 284.22885
Epoch 49: Val Loss 285.01068
Epoch 50: Val Loss 279.23431
Epoch 51: Val Loss 268.08582
Epoch 52: Val Loss 259.91888
Epoch 53: Val Loss 255.67757
Epoch 54: Val Loss 248.72202
Epoch 55: Val Loss 243.80379
Epoch 56: Val Loss 240.23294
Epoch 57: Val Loss 235.79465
Epoch 58: Val Loss 232.20172
Epoch 59: Val Loss 229.24701
Epoch 60: Val Loss 224.69484
Epoch 61: Val Loss 219.73587
Epoch 62: Val Loss 217.29880
Epoch 63: Val Loss 217.65234
Epoch 64: Val Loss 215.04492
Epoch 65: Val Loss 212.34389
Epoch 66: Val Loss 209.96632
Epoch 67: Val Loss 205.90659
Epoch 68: Val Loss 203.93463
Epoch 69: Val Loss 201.58354
Epoch 70: Val Loss 201.49562
Epoch 71: Val Loss 200.90498
Epoch 72: Val Loss 203.48521
Epoch 73: Val Loss 200.59396
Epoch 74: Val Loss 198.22333
Epoch 75: Val Loss 194.51984
Epoch 76: Val Loss 192.27002
Epoch 77: Val Loss 192.02756
Epoch 78: Val Loss 193.21205
Epoch 79: Val Loss 192.61911
Epoch 80: Val Loss 192.80894
Epoch 81: Val Loss 188.56618
Epoch 82: Val Loss 186.16641
Epoch 83: Val Loss 184.23264
Epoch 84: Val Loss 183.95564
Epoch 85: Val Loss 181.95937
Epoch 86: Val Loss 182.77565
Epoch 87: Val Loss 182.84102
Epoch 88: Val Loss 180.44518
Epoch 89: Val Loss 177.98802
Epoch 90: Val Loss 177.48103
Epoch 91: Val Loss 177.90349
Epoch 92: Val Loss 175.94176
Epoch 93: Val Loss 174.22597
Epoch 94: Val Loss 177.30864
Epoch 95: Val Loss 180.52112
Epoch 96: Val Loss 178.80438
Epoch 97: Val Loss 176.05881
Epoch 98: Val Loss 174.07530
Epoch 99: Val Loss 172.66104
{'MSE - mean': 172.6610539678168, 'MSE - std': 0.0, 'R2 - mean': 0.8313426213951097, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1292.69214
Epoch 1: Val Loss 1288.69592
Epoch 2: Val Loss 1283.83899
Epoch 3: Val Loss 1277.79871
Epoch 4: Val Loss 1270.23096
Epoch 5: Val Loss 1260.63208
Epoch 6: Val Loss 1248.29626
Epoch 7: Val Loss 1231.77039
Epoch 8: Val Loss 1209.64258
Epoch 9: Val Loss 1182.06519
Epoch 10: Val Loss 1145.87378
Epoch 11: Val Loss 1103.15637
Epoch 12: Val Loss 1053.68933
Epoch 13: Val Loss 997.39856
Epoch 14: Val Loss 938.37982
Epoch 15: Val Loss 878.37628
Epoch 16: Val Loss 817.29126
Epoch 17: Val Loss 762.49182
Epoch 18: Val Loss 720.37244
Epoch 19: Val Loss 688.98755
Epoch 20: Val Loss 663.42938
Epoch 21: Val Loss 641.29120
Epoch 22: Val Loss 618.24487
Epoch 23: Val Loss 594.52106
Epoch 24: Val Loss 567.00739
Epoch 25: Val Loss 542.86652
Epoch 26: Val Loss 519.44489
Epoch 27: Val Loss 496.95496
Epoch 28: Val Loss 479.23065
Epoch 29: Val Loss 460.16788
Epoch 30: Val Loss 441.40848
Epoch 31: Val Loss 426.30856
Epoch 32: Val Loss 417.27408
Epoch 33: Val Loss 412.00406
Epoch 34: Val Loss 402.74524
Epoch 35: Val Loss 392.79376
Epoch 36: Val Loss 385.32379
Epoch 37: Val Loss 371.47577
Epoch 38: Val Loss 368.14877
Epoch 39: Val Loss 355.21246
Epoch 40: Val Loss 345.57935
Epoch 41: Val Loss 344.44650
Epoch 42: Val Loss 342.98132
Epoch 43: Val Loss 347.79025
Epoch 44: Val Loss 339.90366
Epoch 45: Val Loss 329.29782
Epoch 46: Val Loss 328.22684
Epoch 47: Val Loss 315.79483
Epoch 48: Val Loss 301.40237
Epoch 49: Val Loss 304.27472
Epoch 50: Val Loss 300.07510
Epoch 51: Val Loss 303.94696
Epoch 52: Val Loss 303.75693
Epoch 53: Val Loss 306.31351
Epoch 54: Val Loss 309.11395
Epoch 55: Val Loss 304.71448
Epoch 56: Val Loss 294.79639
Epoch 57: Val Loss 294.96121
Epoch 58: Val Loss 308.49783
Epoch 59: Val Loss 304.08731
Epoch 60: Val Loss 301.86105
Epoch 61: Val Loss 297.05753
Epoch 62: Val Loss 287.28210
Epoch 63: Val Loss 284.96460
Epoch 64: Val Loss 281.22940
Epoch 65: Val Loss 284.89740
Epoch 66: Val Loss 287.73187
Epoch 67: Val Loss 294.93195
Epoch 68: Val Loss 292.64529
Epoch 69: Val Loss 286.58069
Epoch 70: Val Loss 281.89221
Epoch 71: Val Loss 292.68375
Epoch 72: Val Loss 289.36279
Epoch 73: Val Loss 279.22333
Epoch 74: Val Loss 277.17664
Epoch 75: Val Loss 277.75769
Epoch 76: Val Loss 280.47263
Epoch 77: Val Loss 278.90100
Epoch 78: Val Loss 264.74466
Epoch 79: Val Loss 261.68988
Epoch 80: Val Loss 262.94733
Epoch 81: Val Loss 269.43295
Epoch 82: Val Loss 271.39481
Epoch 83: Val Loss 259.15540
Epoch 84: Val Loss 258.64557
Epoch 85: Val Loss 275.20596
Epoch 86: Val Loss 274.12946
Epoch 87: Val Loss 258.66858
Epoch 88: Val Loss 247.50226
Epoch 89: Val Loss 247.63306
Epoch 90: Val Loss 244.77614
Epoch 91: Val Loss 249.46686
Epoch 92: Val Loss 271.02216
Epoch 93: Val Loss 261.83194
Epoch 94: Val Loss 240.25793
Epoch 95: Val Loss 233.69444
Epoch 96: Val Loss 238.81395
Epoch 97: Val Loss 240.42444
Epoch 98: Val Loss 233.63849
Epoch 99: Val Loss 239.48022
{'MSE - mean': 203.1497693535274, 'MSE - std': 30.48871538571062, 'R2 - mean': 0.8044230332307709, 'R2 - std': 0.02691958816433887} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2808.73535
Epoch 1: Val Loss 2805.84204
Epoch 2: Val Loss 2802.51660
Epoch 3: Val Loss 2798.36011
Epoch 4: Val Loss 2793.17944
Epoch 5: Val Loss 2786.22290
Epoch 6: Val Loss 2777.31909
Epoch 7: Val Loss 2765.44824
Epoch 8: Val Loss 2750.06226
Epoch 9: Val Loss 2729.91675
Epoch 10: Val Loss 2704.10303
Epoch 11: Val Loss 2670.33447
Epoch 12: Val Loss 2629.84473
Epoch 13: Val Loss 2583.20508
Epoch 14: Val Loss 2526.41187
Epoch 15: Val Loss 2462.05566
Epoch 16: Val Loss 2390.92261
Epoch 17: Val Loss 2313.86523
Epoch 18: Val Loss 2234.29688
Epoch 19: Val Loss 2155.47705
Epoch 20: Val Loss 2080.71899
Epoch 21: Val Loss 2005.95911
Epoch 22: Val Loss 1927.82849
Epoch 23: Val Loss 1855.55164
Epoch 24: Val Loss 1782.25195
Epoch 25: Val Loss 1716.07141
Epoch 26: Val Loss 1649.80786
Epoch 27: Val Loss 1591.19507
Epoch 28: Val Loss 1538.48108
Epoch 29: Val Loss 1484.93091
Epoch 30: Val Loss 1433.55725
Epoch 31: Val Loss 1386.07056
Epoch 32: Val Loss 1346.47534
Epoch 33: Val Loss 1300.48572
Epoch 34: Val Loss 1259.28149
Epoch 35: Val Loss 1217.88013
Epoch 36: Val Loss 1187.25171
Epoch 37: Val Loss 1153.63953
Epoch 38: Val Loss 1122.66968
Epoch 39: Val Loss 1097.22498
Epoch 40: Val Loss 1073.94299
Epoch 41: Val Loss 1054.69800
Epoch 42: Val Loss 1026.36011
Epoch 43: Val Loss 993.81818
Epoch 44: Val Loss 973.67145
Epoch 45: Val Loss 955.68750
Epoch 46: Val Loss 938.12390
Epoch 47: Val Loss 903.85974
Epoch 48: Val Loss 886.64966
Epoch 49: Val Loss 876.72296
Epoch 50: Val Loss 866.14801
Epoch 51: Val Loss 841.03461
Epoch 52: Val Loss 819.78113
Epoch 53: Val Loss 799.00684
Epoch 54: Val Loss 790.13440
Epoch 55: Val Loss 777.90204
Epoch 56: Val Loss 769.87988
Epoch 57: Val Loss 759.15875
Epoch 58: Val Loss 744.25958
Epoch 59: Val Loss 736.07355
Epoch 60: Val Loss 727.29431
Epoch 61: Val Loss 714.27924
Epoch 62: Val Loss 703.91809
Epoch 63: Val Loss 694.70685
Epoch 64: Val Loss 680.15100
Epoch 65: Val Loss 654.86389
Epoch 66: Val Loss 643.81580
Epoch 67: Val Loss 647.34399
Epoch 68: Val Loss 646.04126
Epoch 69: Val Loss 623.86395
Epoch 70: Val Loss 612.92511
Epoch 71: Val Loss 609.36951
Epoch 72: Val Loss 611.92560
Epoch 73: Val Loss 607.90314
Epoch 74: Val Loss 602.67914
Epoch 75: Val Loss 592.40851
Epoch 76: Val Loss 581.98248
Epoch 77: Val Loss 575.18420
Epoch 78: Val Loss 560.82660
Epoch 79: Val Loss 558.83893
Epoch 80: Val Loss 563.09674
Epoch 81: Val Loss 558.98553
Epoch 82: Val Loss 545.48761
Epoch 83: Val Loss 535.65546
Epoch 84: Val Loss 538.79987
Epoch 85: Val Loss 538.04016
Epoch 86: Val Loss 527.23511
Epoch 87: Val Loss 514.43805
Epoch 88: Val Loss 515.56384
Epoch 89: Val Loss 517.27057
Epoch 90: Val Loss 512.73956
Epoch 91: Val Loss 513.89886
Epoch 92: Val Loss 500.07178
Epoch 93: Val Loss 491.08627
Epoch 94: Val Loss 487.55615
Epoch 95: Val Loss 488.33899
Epoch 96: Val Loss 487.71240
Epoch 97: Val Loss 487.95496
Epoch 98: Val Loss 477.70862
Epoch 99: Val Loss 463.70108
{'MSE - mean': 290.0001959736827, 'MSE - std': 125.32238847117411, 'R2 - mean': 0.8065253463063998, 'R2 - std': 0.02217992166715195} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2123.05933
Epoch 1: Val Loss 2121.03149
Epoch 2: Val Loss 2118.76245
Epoch 3: Val Loss 2116.00098
Epoch 4: Val Loss 2112.48071
Epoch 5: Val Loss 2107.88672
Epoch 6: Val Loss 2101.76929
Epoch 7: Val Loss 2093.82861
Epoch 8: Val Loss 2083.53467
Epoch 9: Val Loss 2070.25024
Epoch 10: Val Loss 2053.11597
Epoch 11: Val Loss 2031.79358
Epoch 12: Val Loss 2005.41504
Epoch 13: Val Loss 1973.31348
Epoch 14: Val Loss 1935.05371
Epoch 15: Val Loss 1890.48999
Epoch 16: Val Loss 1838.23340
Epoch 17: Val Loss 1780.20776
Epoch 18: Val Loss 1718.90369
Epoch 19: Val Loss 1654.91125
Epoch 20: Val Loss 1586.54724
Epoch 21: Val Loss 1519.91699
Epoch 22: Val Loss 1456.71875
Epoch 23: Val Loss 1395.10999
Epoch 24: Val Loss 1342.88574
Epoch 25: Val Loss 1291.08789
Epoch 26: Val Loss 1243.65979
Epoch 27: Val Loss 1200.01624
Epoch 28: Val Loss 1153.42102
Epoch 29: Val Loss 1110.11230
Epoch 30: Val Loss 1064.44104
Epoch 31: Val Loss 1017.95288
Epoch 32: Val Loss 974.71558
Epoch 33: Val Loss 931.84991
Epoch 34: Val Loss 890.99854
Epoch 35: Val Loss 853.13550
Epoch 36: Val Loss 820.04706
Epoch 37: Val Loss 787.48627
Epoch 38: Val Loss 759.83386
Epoch 39: Val Loss 731.19855
Epoch 40: Val Loss 706.48822
Epoch 41: Val Loss 684.67450
Epoch 42: Val Loss 659.64648
Epoch 43: Val Loss 636.94128
Epoch 44: Val Loss 616.67249
Epoch 45: Val Loss 602.99164
Epoch 46: Val Loss 587.16089
Epoch 47: Val Loss 571.48114
Epoch 48: Val Loss 553.32007
Epoch 49: Val Loss 541.92249
Epoch 50: Val Loss 533.07483
Epoch 51: Val Loss 518.73419
Epoch 52: Val Loss 505.77179
Epoch 53: Val Loss 493.72147
Epoch 54: Val Loss 486.57639
Epoch 55: Val Loss 476.01306
Epoch 56: Val Loss 467.52194
Epoch 57: Val Loss 463.35165
Epoch 58: Val Loss 456.48920
Epoch 59: Val Loss 449.10556
Epoch 60: Val Loss 443.10269
Epoch 61: Val Loss 437.73071
Epoch 62: Val Loss 433.79446
Epoch 63: Val Loss 433.19724
Epoch 64: Val Loss 419.34125
Epoch 65: Val Loss 415.47037
Epoch 66: Val Loss 411.40060
Epoch 67: Val Loss 405.43158
Epoch 68: Val Loss 401.74942
Epoch 69: Val Loss 398.68692
Epoch 70: Val Loss 395.28668
Epoch 71: Val Loss 392.31088
Epoch 72: Val Loss 389.13779
Epoch 73: Val Loss 384.85159
Epoch 74: Val Loss 382.82657
Epoch 75: Val Loss 381.95480
Epoch 76: Val Loss 381.48721
Epoch 77: Val Loss 380.67313
Epoch 78: Val Loss 370.99496
Epoch 79: Val Loss 367.16183
Epoch 80: Val Loss 368.55103
Epoch 81: Val Loss 365.84348
Epoch 82: Val Loss 360.19592
Epoch 83: Val Loss 355.97464
Epoch 84: Val Loss 353.47626
Epoch 85: Val Loss 356.31369
Epoch 86: Val Loss 354.20139
Epoch 87: Val Loss 349.72064
Epoch 88: Val Loss 348.20761
Epoch 89: Val Loss 346.26614
Epoch 90: Val Loss 343.33496
Epoch 91: Val Loss 341.72534
Epoch 92: Val Loss 340.29477
Epoch 93: Val Loss 336.05627
Epoch 94: Val Loss 334.76569
Epoch 95: Val Loss 335.60925
Epoch 96: Val Loss 333.47064
Epoch 97: Val Loss 329.08115
Epoch 98: Val Loss 326.72491
Epoch 99: Val Loss 326.70621
{'MSE - mean': 299.17669581899173, 'MSE - std': 109.69001885915952, 'R2 - mean': 0.8075012392207757, 'R2 - std': 0.019282603424116113} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2508.24683
Epoch 1: Val Loss 2504.77148
Epoch 2: Val Loss 2500.04419
Epoch 3: Val Loss 2493.92773
Epoch 4: Val Loss 2486.03296
Epoch 5: Val Loss 2476.17578
Epoch 6: Val Loss 2463.25928
Epoch 7: Val Loss 2446.56763
Epoch 8: Val Loss 2423.60962
Epoch 9: Val Loss 2392.16577
Epoch 10: Val Loss 2349.93506
Epoch 11: Val Loss 2297.24341
Epoch 12: Val Loss 2232.36572
Epoch 13: Val Loss 2156.84985
Epoch 14: Val Loss 2074.72974
Epoch 15: Val Loss 1980.99451
Epoch 16: Val Loss 1880.36035
Epoch 17: Val Loss 1780.54102
Epoch 18: Val Loss 1682.32898
Epoch 19: Val Loss 1607.15784
Epoch 20: Val Loss 1539.49268
Epoch 21: Val Loss 1478.02844
Epoch 22: Val Loss 1418.01636
Epoch 23: Val Loss 1364.80676
Epoch 24: Val Loss 1308.46716
Epoch 25: Val Loss 1249.00525
Epoch 26: Val Loss 1201.98132
Epoch 27: Val Loss 1159.21655
Epoch 28: Val Loss 1118.33728
Epoch 29: Val Loss 1061.20654
Epoch 30: Val Loss 1018.98370
Epoch 31: Val Loss 989.96350
Epoch 32: Val Loss 962.39679
Epoch 33: Val Loss 933.51447
Epoch 34: Val Loss 904.53009
Epoch 35: Val Loss 883.20294
Epoch 36: Val Loss 856.28876
Epoch 37: Val Loss 832.33624
Epoch 38: Val Loss 806.47778
Epoch 39: Val Loss 778.01666
Epoch 40: Val Loss 753.67902
Epoch 41: Val Loss 741.38934
Epoch 42: Val Loss 732.41754
Epoch 43: Val Loss 714.57849
Epoch 44: Val Loss 693.19629
Epoch 45: Val Loss 676.42670
Epoch 46: Val Loss 668.43304
Epoch 47: Val Loss 652.71008
Epoch 48: Val Loss 642.32025
Epoch 49: Val Loss 628.07001
Epoch 50: Val Loss 607.22357
Epoch 51: Val Loss 600.55682
Epoch 52: Val Loss 593.31976
Epoch 53: Val Loss 587.26569
Epoch 54: Val Loss 577.82184
Epoch 55: Val Loss 567.68817
Epoch 56: Val Loss 551.27997
Epoch 57: Val Loss 548.02460
Epoch 58: Val Loss 548.55054
Epoch 59: Val Loss 541.64069
Epoch 60: Val Loss 540.85797
Epoch 61: Val Loss 532.99683
Epoch 62: Val Loss 520.74921
Epoch 63: Val Loss 514.55725
Epoch 64: Val Loss 512.92712
Epoch 65: Val Loss 515.10236
Epoch 66: Val Loss 507.74512
Epoch 67: Val Loss 501.31467
Epoch 68: Val Loss 497.05426
Epoch 69: Val Loss 494.56281
Epoch 70: Val Loss 489.51227
Epoch 71: Val Loss 484.22272
Epoch 72: Val Loss 482.21002
Epoch 73: Val Loss 478.81006
Epoch 74: Val Loss 472.67456
Epoch 75: Val Loss 468.15338
Epoch 76: Val Loss 467.01373
Epoch 77: Val Loss 460.44101
Epoch 78: Val Loss 456.73355
Epoch 79: Val Loss 454.69040
Epoch 80: Val Loss 453.70996
Epoch 81: Val Loss 448.36179
Epoch 82: Val Loss 441.50446
Epoch 83: Val Loss 439.05002
Epoch 84: Val Loss 436.65826
Epoch 85: Val Loss 432.43671
Epoch 86: Val Loss 427.47137
Epoch 87: Val Loss 422.59760
Epoch 88: Val Loss 420.24728
Epoch 89: Val Loss 418.15131
Epoch 90: Val Loss 413.12302
Epoch 91: Val Loss 411.16019
Epoch 92: Val Loss 407.62909
Epoch 93: Val Loss 404.65616
Epoch 94: Val Loss 402.07919
Epoch 95: Val Loss 397.58975
Epoch 96: Val Loss 395.28769
Epoch 97: Val Loss 395.76712
Epoch 98: Val Loss 389.12552
Epoch 99: Val Loss 386.16180
{'MSE - mean': 316.57372721259844, 'MSE - std': 104.09681548971017, 'R2 - mean': 0.809895559089559, 'R2 - std': 0.017899332568159936} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 316.57372721259844 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1281.62842
Epoch 1: Val Loss 1281.46106
Epoch 2: Val Loss 1281.29333
Epoch 3: Val Loss 1281.12561
Epoch 4: Val Loss 1280.96985
Epoch 5: Val Loss 1280.81311
Epoch 6: Val Loss 1280.65784
Epoch 7: Val Loss 1280.51294
Epoch 8: Val Loss 1280.36694
Epoch 9: Val Loss 1280.22900
Epoch 10: Val Loss 1280.08679
Epoch 11: Val Loss 1279.94983
Epoch 12: Val Loss 1279.81787
Epoch 13: Val Loss 1279.68237
Epoch 14: Val Loss 1279.54614
Epoch 15: Val Loss 1279.40894
Epoch 16: Val Loss 1279.27502
Epoch 17: Val Loss 1279.14307
Epoch 18: Val Loss 1279.00635
Epoch 19: Val Loss 1278.87317
Epoch 20: Val Loss 1278.73975
Epoch 21: Val Loss 1278.60425
Epoch 22: Val Loss 1278.46069
Epoch 23: Val Loss 1278.32166
Epoch 24: Val Loss 1278.17700
Epoch 25: Val Loss 1278.02771
Epoch 26: Val Loss 1277.88025
Epoch 27: Val Loss 1277.73035
Epoch 28: Val Loss 1277.57141
Epoch 29: Val Loss 1277.41052
Epoch 30: Val Loss 1277.24841
Epoch 31: Val Loss 1277.07947
Epoch 32: Val Loss 1276.89758
Epoch 33: Val Loss 1276.71240
Epoch 34: Val Loss 1276.52783
Epoch 35: Val Loss 1276.32825
Epoch 36: Val Loss 1276.13208
Epoch 37: Val Loss 1275.91626
Epoch 38: Val Loss 1275.70471
Epoch 39: Val Loss 1275.48010
Epoch 40: Val Loss 1275.25781
Epoch 41: Val Loss 1275.02698
Epoch 42: Val Loss 1274.77710
Epoch 43: Val Loss 1274.52380
Epoch 44: Val Loss 1274.24219
Epoch 45: Val Loss 1273.96143
Epoch 46: Val Loss 1273.68103
Epoch 47: Val Loss 1273.37939
Epoch 48: Val Loss 1273.06421
Epoch 49: Val Loss 1272.75049
Epoch 50: Val Loss 1272.41174
Epoch 51: Val Loss 1272.05286
Epoch 52: Val Loss 1271.68616
Epoch 53: Val Loss 1271.30884
Epoch 54: Val Loss 1270.91565
Epoch 55: Val Loss 1270.51819
Epoch 56: Val Loss 1270.07629
Epoch 57: Val Loss 1269.63342
Epoch 58: Val Loss 1269.16943
Epoch 59: Val Loss 1268.67725
Epoch 60: Val Loss 1268.15295
Epoch 61: Val Loss 1267.61633
Epoch 62: Val Loss 1267.05872
Epoch 63: Val Loss 1266.46899
Epoch 64: Val Loss 1265.84912
Epoch 65: Val Loss 1265.17725
Epoch 66: Val Loss 1264.46631
Epoch 67: Val Loss 1263.70312
Epoch 68: Val Loss 1262.92615
Epoch 69: Val Loss 1262.13501
Epoch 70: Val Loss 1261.29810
Epoch 71: Val Loss 1260.35950
Epoch 72: Val Loss 1259.45093
Epoch 73: Val Loss 1258.45044
Epoch 74: Val Loss 1257.41895
Epoch 75: Val Loss 1256.37524
Epoch 76: Val Loss 1255.31470
Epoch 77: Val Loss 1254.18103
Epoch 78: Val Loss 1253.00842
Epoch 79: Val Loss 1251.78723
Epoch 80: Val Loss 1250.58508
Epoch 81: Val Loss 1249.35242
Epoch 82: Val Loss 1248.09937
Epoch 83: Val Loss 1246.80872
Epoch 84: Val Loss 1245.37683
Epoch 85: Val Loss 1243.90063
Epoch 86: Val Loss 1242.35962
Epoch 87: Val Loss 1240.85046
Epoch 88: Val Loss 1239.29407
Epoch 89: Val Loss 1237.69543
Epoch 90: Val Loss 1235.90137
Epoch 91: Val Loss 1234.16565
Epoch 92: Val Loss 1232.41382
Epoch 93: Val Loss 1230.52563
Epoch 94: Val Loss 1228.58679
Epoch 95: Val Loss 1226.61804
Epoch 96: Val Loss 1224.52100
Epoch 97: Val Loss 1222.44458
Epoch 98: Val Loss 1220.31677
Epoch 99: Val Loss 1218.07727
{'MSE - mean': 1218.0773226442427, 'MSE - std': 0.0, 'R2 - mean': -0.18983246918864372, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1308.35449
Epoch 1: Val Loss 1308.21802
Epoch 2: Val Loss 1308.08960
Epoch 3: Val Loss 1307.97119
Epoch 4: Val Loss 1307.85229
Epoch 5: Val Loss 1307.73267
Epoch 6: Val Loss 1307.61658
Epoch 7: Val Loss 1307.50305
Epoch 8: Val Loss 1307.39270
Epoch 9: Val Loss 1307.28699
Epoch 10: Val Loss 1307.17749
Epoch 11: Val Loss 1307.06873
Epoch 12: Val Loss 1306.95886
Epoch 13: Val Loss 1306.84521
Epoch 14: Val Loss 1306.72974
Epoch 15: Val Loss 1306.61414
Epoch 16: Val Loss 1306.49829
Epoch 17: Val Loss 1306.37036
Epoch 18: Val Loss 1306.24646
Epoch 19: Val Loss 1306.12524
Epoch 20: Val Loss 1306.00085
Epoch 21: Val Loss 1305.88208
Epoch 22: Val Loss 1305.75220
Epoch 23: Val Loss 1305.62280
Epoch 24: Val Loss 1305.48926
Epoch 25: Val Loss 1305.35742
Epoch 26: Val Loss 1305.21899
Epoch 27: Val Loss 1305.07617
Epoch 28: Val Loss 1304.93115
Epoch 29: Val Loss 1304.77466
Epoch 30: Val Loss 1304.62073
Epoch 31: Val Loss 1304.45703
Epoch 32: Val Loss 1304.28296
Epoch 33: Val Loss 1304.11169
Epoch 34: Val Loss 1303.93213
Epoch 35: Val Loss 1303.74219
Epoch 36: Val Loss 1303.54602
Epoch 37: Val Loss 1303.35486
Epoch 38: Val Loss 1303.15051
Epoch 39: Val Loss 1302.94885
Epoch 40: Val Loss 1302.74414
Epoch 41: Val Loss 1302.52856
Epoch 42: Val Loss 1302.30017
Epoch 43: Val Loss 1302.05505
Epoch 44: Val Loss 1301.80432
Epoch 45: Val Loss 1301.53857
Epoch 46: Val Loss 1301.26807
Epoch 47: Val Loss 1300.98792
Epoch 48: Val Loss 1300.69617
Epoch 49: Val Loss 1300.41052
Epoch 50: Val Loss 1300.12402
Epoch 51: Val Loss 1299.82202
Epoch 52: Val Loss 1299.50732
Epoch 53: Val Loss 1299.19177
Epoch 54: Val Loss 1298.86267
Epoch 55: Val Loss 1298.53406
Epoch 56: Val Loss 1298.19348
Epoch 57: Val Loss 1297.83765
Epoch 58: Val Loss 1297.46326
Epoch 59: Val Loss 1297.07483
Epoch 60: Val Loss 1296.66125
Epoch 61: Val Loss 1296.23096
Epoch 62: Val Loss 1295.79639
Epoch 63: Val Loss 1295.34290
Epoch 64: Val Loss 1294.89490
Epoch 65: Val Loss 1294.41638
Epoch 66: Val Loss 1293.91821
Epoch 67: Val Loss 1293.40259
Epoch 68: Val Loss 1292.87170
Epoch 69: Val Loss 1292.28857
Epoch 70: Val Loss 1291.67407
Epoch 71: Val Loss 1291.06360
Epoch 72: Val Loss 1290.41626
Epoch 73: Val Loss 1289.76086
Epoch 74: Val Loss 1289.07544
Epoch 75: Val Loss 1288.35925
Epoch 76: Val Loss 1287.59985
Epoch 77: Val Loss 1286.79602
Epoch 78: Val Loss 1286.02087
Epoch 79: Val Loss 1285.21277
Epoch 80: Val Loss 1284.38635
Epoch 81: Val Loss 1283.52002
Epoch 82: Val Loss 1282.65698
Epoch 83: Val Loss 1281.73181
Epoch 84: Val Loss 1280.76257
Epoch 85: Val Loss 1279.76746
Epoch 86: Val Loss 1278.74976
Epoch 87: Val Loss 1277.67395
Epoch 88: Val Loss 1276.51477
Epoch 89: Val Loss 1275.38770
Epoch 90: Val Loss 1274.21997
Epoch 91: Val Loss 1273.03613
Epoch 92: Val Loss 1271.83447
Epoch 93: Val Loss 1270.63171
Epoch 94: Val Loss 1269.37207
Epoch 95: Val Loss 1268.06104
Epoch 96: Val Loss 1266.68176
Epoch 97: Val Loss 1265.33459
Epoch 98: Val Loss 1263.98511
Epoch 99: Val Loss 1262.60828
{'MSE - mean': 1240.342685764177, 'MSE - std': 22.26536311993425, 'R2 - mean': -0.19611415244087826, 'R2 - std': 0.006281683252234549} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2812.06274
Epoch 1: Val Loss 2811.68286
Epoch 2: Val Loss 2811.29443
Epoch 3: Val Loss 2810.89404
Epoch 4: Val Loss 2810.46704
Epoch 5: Val Loss 2810.05347
Epoch 6: Val Loss 2809.58472
Epoch 7: Val Loss 2809.09839
Epoch 8: Val Loss 2808.60498
Epoch 9: Val Loss 2808.09717
Epoch 10: Val Loss 2807.54346
Epoch 11: Val Loss 2806.99951
Epoch 12: Val Loss 2806.43896
Epoch 13: Val Loss 2805.86426
Epoch 14: Val Loss 2805.28076
Epoch 15: Val Loss 2804.67554
Epoch 16: Val Loss 2803.99731
Epoch 17: Val Loss 2803.31665
Epoch 18: Val Loss 2802.61035
Epoch 19: Val Loss 2801.93213
Epoch 20: Val Loss 2801.22754
Epoch 21: Val Loss 2800.48804
Epoch 22: Val Loss 2799.70898
Epoch 23: Val Loss 2798.94043
Epoch 24: Val Loss 2798.08496
Epoch 25: Val Loss 2797.21021
Epoch 26: Val Loss 2796.30957
Epoch 27: Val Loss 2795.44312
Epoch 28: Val Loss 2794.54321
Epoch 29: Val Loss 2793.57642
Epoch 30: Val Loss 2792.58862
Epoch 31: Val Loss 2791.51709
Epoch 32: Val Loss 2790.46851
Epoch 33: Val Loss 2789.40747
Epoch 34: Val Loss 2788.35742
Epoch 35: Val Loss 2787.27759
Epoch 36: Val Loss 2786.12158
Epoch 37: Val Loss 2784.98950
Epoch 38: Val Loss 2783.74927
Epoch 39: Val Loss 2782.40088
Epoch 40: Val Loss 2781.02417
Epoch 41: Val Loss 2779.61841
Epoch 42: Val Loss 2778.17407
Epoch 43: Val Loss 2776.70483
Epoch 44: Val Loss 2775.22070
Epoch 45: Val Loss 2773.65210
Epoch 46: Val Loss 2772.03076
Epoch 47: Val Loss 2770.27051
Epoch 48: Val Loss 2768.51416
Epoch 49: Val Loss 2766.75146
Epoch 50: Val Loss 2764.87036
Epoch 51: Val Loss 2762.86914
Epoch 52: Val Loss 2760.86694
Epoch 53: Val Loss 2758.78296
Epoch 54: Val Loss 2756.62012
Epoch 55: Val Loss 2754.38013
Epoch 56: Val Loss 2752.17896
Epoch 57: Val Loss 2749.76562
Epoch 58: Val Loss 2747.44409
Epoch 59: Val Loss 2744.96777
Epoch 60: Val Loss 2742.51074
Epoch 61: Val Loss 2739.87231
Epoch 62: Val Loss 2737.10278
Epoch 63: Val Loss 2734.36890
Epoch 64: Val Loss 2731.64551
Epoch 65: Val Loss 2728.91406
Epoch 66: Val Loss 2726.02197
Epoch 67: Val Loss 2723.02417
Epoch 68: Val Loss 2719.98926
Epoch 69: Val Loss 2716.82837
Epoch 70: Val Loss 2713.67725
Epoch 71: Val Loss 2710.42798
Epoch 72: Val Loss 2707.17334
Epoch 73: Val Loss 2703.84155
Epoch 74: Val Loss 2700.24536
Epoch 75: Val Loss 2696.46289
Epoch 76: Val Loss 2692.76587
Epoch 77: Val Loss 2689.03296
Epoch 78: Val Loss 2685.24023
Epoch 79: Val Loss 2681.50269
Epoch 80: Val Loss 2677.35693
Epoch 81: Val Loss 2673.09692
Epoch 82: Val Loss 2668.82104
Epoch 83: Val Loss 2664.52954
Epoch 84: Val Loss 2660.10596
Epoch 85: Val Loss 2655.73218
Epoch 86: Val Loss 2651.16284
Epoch 87: Val Loss 2646.32690
Epoch 88: Val Loss 2641.21973
Epoch 89: Val Loss 2636.21143
Epoch 90: Val Loss 2630.93481
Epoch 91: Val Loss 2625.81519
Epoch 92: Val Loss 2620.45703
Epoch 93: Val Loss 2615.15234
Epoch 94: Val Loss 2609.72559
Epoch 95: Val Loss 2604.20264
Epoch 96: Val Loss 2598.67651
Epoch 97: Val Loss 2592.84692
Epoch 98: Val Loss 2586.84570
Epoch 99: Val Loss 2580.59204
{'MSE - mean': 1687.0923674843798, 'MSE - std': 632.0609573694452, 'R2 - mean': -0.1485183148560818, 'R2 - std': 0.06750580622275575} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2131.40308
Epoch 1: Val Loss 2130.98315
Epoch 2: Val Loss 2130.55420
Epoch 3: Val Loss 2130.11108
Epoch 4: Val Loss 2129.64038
Epoch 5: Val Loss 2129.14673
Epoch 6: Val Loss 2128.65454
Epoch 7: Val Loss 2128.17041
Epoch 8: Val Loss 2127.65112
Epoch 9: Val Loss 2127.09399
Epoch 10: Val Loss 2126.53125
Epoch 11: Val Loss 2125.95728
Epoch 12: Val Loss 2125.37964
Epoch 13: Val Loss 2124.74902
Epoch 14: Val Loss 2124.10400
Epoch 15: Val Loss 2123.44458
Epoch 16: Val Loss 2122.75928
Epoch 17: Val Loss 2122.07520
Epoch 18: Val Loss 2121.36206
Epoch 19: Val Loss 2120.58472
Epoch 20: Val Loss 2119.75439
Epoch 21: Val Loss 2118.93628
Epoch 22: Val Loss 2118.08911
Epoch 23: Val Loss 2117.22754
Epoch 24: Val Loss 2116.34180
Epoch 25: Val Loss 2115.44434
Epoch 26: Val Loss 2114.45288
Epoch 27: Val Loss 2113.41553
Epoch 28: Val Loss 2112.37280
Epoch 29: Val Loss 2111.30322
Epoch 30: Val Loss 2110.10547
Epoch 31: Val Loss 2108.90649
Epoch 32: Val Loss 2107.61060
Epoch 33: Val Loss 2106.33105
Epoch 34: Val Loss 2105.00781
Epoch 35: Val Loss 2103.62402
Epoch 36: Val Loss 2102.17749
Epoch 37: Val Loss 2100.54419
Epoch 38: Val Loss 2098.86719
Epoch 39: Val Loss 2097.05029
Epoch 40: Val Loss 2095.10913
Epoch 41: Val Loss 2093.10059
Epoch 42: Val Loss 2090.91431
Epoch 43: Val Loss 2088.68506
Epoch 44: Val Loss 2086.32910
Epoch 45: Val Loss 2083.98901
Epoch 46: Val Loss 2081.49951
Epoch 47: Val Loss 2078.82764
Epoch 48: Val Loss 2075.98608
Epoch 49: Val Loss 2073.07690
Epoch 50: Val Loss 2070.10791
Epoch 51: Val Loss 2067.05737
Epoch 52: Val Loss 2064.00024
Epoch 53: Val Loss 2060.55957
Epoch 54: Val Loss 2057.19556
Epoch 55: Val Loss 2053.75293
Epoch 56: Val Loss 2050.12402
Epoch 57: Val Loss 2046.49915
Epoch 58: Val Loss 2042.60144
Epoch 59: Val Loss 2038.57556
Epoch 60: Val Loss 2034.74207
Epoch 61: Val Loss 2030.56714
Epoch 62: Val Loss 2026.05457
Epoch 63: Val Loss 2021.76196
Epoch 64: Val Loss 2017.06897
Epoch 65: Val Loss 2012.26880
Epoch 66: Val Loss 2007.28003
Epoch 67: Val Loss 2002.05273
Epoch 68: Val Loss 1996.76953
Epoch 69: Val Loss 1991.34998
Epoch 70: Val Loss 1985.96484
Epoch 71: Val Loss 1980.39014
Epoch 72: Val Loss 1975.01978
Epoch 73: Val Loss 1969.43750
Epoch 74: Val Loss 1963.90369
Epoch 75: Val Loss 1958.16931
Epoch 76: Val Loss 1952.18506
Epoch 77: Val Loss 1945.81653
Epoch 78: Val Loss 1939.66003
Epoch 79: Val Loss 1933.42554
Epoch 80: Val Loss 1926.91931
Epoch 81: Val Loss 1920.50598
Epoch 82: Val Loss 1914.04431
Epoch 83: Val Loss 1906.96533
Epoch 84: Val Loss 1899.75757
Epoch 85: Val Loss 1892.17725
Epoch 86: Val Loss 1884.41858
Epoch 87: Val Loss 1877.17261
Epoch 88: Val Loss 1869.82983
Epoch 89: Val Loss 1862.27209
Epoch 90: Val Loss 1854.11011
Epoch 91: Val Loss 1846.11304
Epoch 92: Val Loss 1837.86792
Epoch 93: Val Loss 1829.62646
Epoch 94: Val Loss 1821.69531
Epoch 95: Val Loss 1813.31482
Epoch 96: Val Loss 1804.75891
Epoch 97: Val Loss 1795.81165
Epoch 98: Val Loss 1787.20227
Epoch 99: Val Loss 1778.94775
{'MSE - mean': 1710.0562290959951, 'MSE - std': 548.8240220630262, 'R2 - mean': -0.11944709772765921, 'R2 - std': 0.07715686878899286} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2517.93726
Epoch 1: Val Loss 2517.53613
Epoch 2: Val Loss 2517.13501
Epoch 3: Val Loss 2516.73438
Epoch 4: Val Loss 2516.32031
Epoch 5: Val Loss 2515.89673
Epoch 6: Val Loss 2515.46655
Epoch 7: Val Loss 2515.03516
Epoch 8: Val Loss 2514.60815
Epoch 9: Val Loss 2514.17676
Epoch 10: Val Loss 2513.73950
Epoch 11: Val Loss 2513.26709
Epoch 12: Val Loss 2512.78687
Epoch 13: Val Loss 2512.29663
Epoch 14: Val Loss 2511.79077
Epoch 15: Val Loss 2511.27759
Epoch 16: Val Loss 2510.74146
Epoch 17: Val Loss 2510.18188
Epoch 18: Val Loss 2509.63208
Epoch 19: Val Loss 2509.03491
Epoch 20: Val Loss 2508.45190
Epoch 21: Val Loss 2507.81348
Epoch 22: Val Loss 2507.14941
Epoch 23: Val Loss 2506.52319
Epoch 24: Val Loss 2505.87012
Epoch 25: Val Loss 2505.18994
Epoch 26: Val Loss 2504.50610
Epoch 27: Val Loss 2503.79980
Epoch 28: Val Loss 2503.06396
Epoch 29: Val Loss 2502.32349
Epoch 30: Val Loss 2501.57739
Epoch 31: Val Loss 2500.80200
Epoch 32: Val Loss 2500.01831
Epoch 33: Val Loss 2499.21826
Epoch 34: Val Loss 2498.39453
Epoch 35: Val Loss 2497.54395
Epoch 36: Val Loss 2496.67261
Epoch 37: Val Loss 2495.76172
Epoch 38: Val Loss 2494.81592
Epoch 39: Val Loss 2493.80566
Epoch 40: Val Loss 2492.79053
Epoch 41: Val Loss 2491.80200
Epoch 42: Val Loss 2490.76904
Epoch 43: Val Loss 2489.69409
Epoch 44: Val Loss 2488.59424
Epoch 45: Val Loss 2487.45825
Epoch 46: Val Loss 2486.25659
Epoch 47: Val Loss 2485.05737
Epoch 48: Val Loss 2483.80908
Epoch 49: Val Loss 2482.56641
Epoch 50: Val Loss 2481.28735
Epoch 51: Val Loss 2479.96582
Epoch 52: Val Loss 2478.54272
Epoch 53: Val Loss 2477.14600
Epoch 54: Val Loss 2475.70288
Epoch 55: Val Loss 2474.22778
Epoch 56: Val Loss 2472.75562
Epoch 57: Val Loss 2471.21143
Epoch 58: Val Loss 2469.63184
Epoch 59: Val Loss 2467.98438
Epoch 60: Val Loss 2466.26245
Epoch 61: Val Loss 2464.51660
Epoch 62: Val Loss 2462.60767
Epoch 63: Val Loss 2460.59961
Epoch 64: Val Loss 2458.65552
Epoch 65: Val Loss 2456.63965
Epoch 66: Val Loss 2454.49609
Epoch 67: Val Loss 2452.29932
Epoch 68: Val Loss 2450.02173
Epoch 69: Val Loss 2447.72803
Epoch 70: Val Loss 2445.29175
Epoch 71: Val Loss 2442.76685
Epoch 72: Val Loss 2440.09277
Epoch 73: Val Loss 2437.39355
Epoch 74: Val Loss 2434.58545
Epoch 75: Val Loss 2431.66333
Epoch 76: Val Loss 2428.75732
Epoch 77: Val Loss 2425.65967
Epoch 78: Val Loss 2422.52002
Epoch 79: Val Loss 2419.40576
Epoch 80: Val Loss 2416.12549
Epoch 81: Val Loss 2412.86499
Epoch 82: Val Loss 2409.45264
Epoch 83: Val Loss 2406.08911
Epoch 84: Val Loss 2402.57080
Epoch 85: Val Loss 2399.00391
Epoch 86: Val Loss 2395.33081
Epoch 87: Val Loss 2391.46167
Epoch 88: Val Loss 2387.45288
Epoch 89: Val Loss 2383.28955
Epoch 90: Val Loss 2379.14990
Epoch 91: Val Loss 2374.64307
Epoch 92: Val Loss 2370.15991
Epoch 93: Val Loss 2365.51294
Epoch 94: Val Loss 2360.73071
Epoch 95: Val Loss 2355.95288
Epoch 96: Val Loss 2351.17651
Epoch 97: Val Loss 2346.27148
Epoch 98: Val Loss 2340.97729
Epoch 99: Val Loss 2335.64722
{'MSE - mean': 1835.1744364491224, 'MSE - std': 550.9850351867918, 'R2 - mean': -0.11393645794869492, 'R2 - std': 0.06988572475228044} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 1835.1744364491224 and parameters: {'dim': 256, 'depth': 2, 'heads': 8, 'weight_decay': -5, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1266.75793
Epoch 1: Val Loss 1262.95203
Epoch 2: Val Loss 1258.36499
Epoch 3: Val Loss 1252.74463
Epoch 4: Val Loss 1245.63232
Epoch 5: Val Loss 1236.61389
Epoch 6: Val Loss 1225.11511
Epoch 7: Val Loss 1210.36426
Epoch 8: Val Loss 1191.94031
Epoch 9: Val Loss 1168.35217
Epoch 10: Val Loss 1139.08569
Epoch 11: Val Loss 1103.21033
Epoch 12: Val Loss 1061.26331
Epoch 13: Val Loss 1013.15698
Epoch 14: Val Loss 960.43024
Epoch 15: Val Loss 902.87177
Epoch 16: Val Loss 846.40289
Epoch 17: Val Loss 790.28326
Epoch 18: Val Loss 738.34106
Epoch 19: Val Loss 699.00671
Epoch 20: Val Loss 666.63782
Epoch 21: Val Loss 638.71185
Epoch 22: Val Loss 612.09790
Epoch 23: Val Loss 585.22272
Epoch 24: Val Loss 555.35590
Epoch 25: Val Loss 526.55164
Epoch 26: Val Loss 499.30457
Epoch 27: Val Loss 468.20782
Epoch 28: Val Loss 439.10950
Epoch 29: Val Loss 414.94376
Epoch 30: Val Loss 393.16199
Epoch 31: Val Loss 373.87781
Epoch 32: Val Loss 357.65787
Epoch 33: Val Loss 344.95920
Epoch 34: Val Loss 331.80563
Epoch 35: Val Loss 320.39804
Epoch 36: Val Loss 310.99057
Epoch 37: Val Loss 302.41528
Epoch 38: Val Loss 293.04291
Epoch 39: Val Loss 285.74023
Epoch 40: Val Loss 276.52109
Epoch 41: Val Loss 269.24783
Epoch 42: Val Loss 264.06763
Epoch 43: Val Loss 255.28468
Epoch 44: Val Loss 247.26596
Epoch 45: Val Loss 242.49454
Epoch 46: Val Loss 241.92821
Epoch 47: Val Loss 237.25874
Epoch 48: Val Loss 235.27650
Epoch 49: Val Loss 229.71339
Epoch 50: Val Loss 229.98393
Epoch 51: Val Loss 225.42989
Epoch 52: Val Loss 226.87285
Epoch 53: Val Loss 219.98215
Epoch 54: Val Loss 217.10635
Epoch 55: Val Loss 217.69994
Epoch 56: Val Loss 215.37373
Epoch 57: Val Loss 214.84587
Epoch 58: Val Loss 212.78186
Epoch 59: Val Loss 210.38553
Epoch 60: Val Loss 209.71303
Epoch 61: Val Loss 206.68275
Epoch 62: Val Loss 205.62749
Epoch 63: Val Loss 205.52843
Epoch 64: Val Loss 204.56044
Epoch 65: Val Loss 204.94872
Epoch 66: Val Loss 206.40555
Epoch 67: Val Loss 203.76123
Epoch 68: Val Loss 202.98212
Epoch 69: Val Loss 203.68423
Epoch 70: Val Loss 202.92363
Epoch 71: Val Loss 207.29073
Epoch 72: Val Loss 204.61322
Epoch 73: Val Loss 200.88121
Epoch 74: Val Loss 199.28180
Epoch 75: Val Loss 198.50635
Epoch 76: Val Loss 199.74213
Epoch 77: Val Loss 200.67670
Epoch 78: Val Loss 197.75563
Epoch 79: Val Loss 196.92824
Epoch 80: Val Loss 195.71657
Epoch 81: Val Loss 194.34163
Epoch 82: Val Loss 193.12560
Epoch 83: Val Loss 192.25511
Epoch 84: Val Loss 195.75629
Epoch 85: Val Loss 195.04788
Epoch 86: Val Loss 193.47935
Epoch 87: Val Loss 192.20306
Epoch 88: Val Loss 190.93860
Epoch 89: Val Loss 189.96225
Epoch 90: Val Loss 191.28314
Epoch 91: Val Loss 190.89146
Epoch 92: Val Loss 190.89789
Epoch 93: Val Loss 190.28781
Epoch 94: Val Loss 188.65880
Epoch 95: Val Loss 190.02721
Epoch 96: Val Loss 191.55122
Epoch 97: Val Loss 191.46053
Epoch 98: Val Loss 190.65427
Epoch 99: Val Loss 188.36328
{'MSE - mean': 188.36329235427962, 'MSE - std': 0.0, 'R2 - mean': 0.8160044874984898, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1319.27161
Epoch 1: Val Loss 1316.76611
Epoch 2: Val Loss 1314.56580
Epoch 3: Val Loss 1312.26978
Epoch 4: Val Loss 1309.50952
Epoch 5: Val Loss 1305.84131
Epoch 6: Val Loss 1300.74377
Epoch 7: Val Loss 1293.95874
Epoch 8: Val Loss 1285.17480
Epoch 9: Val Loss 1274.21887
Epoch 10: Val Loss 1259.93225
Epoch 11: Val Loss 1242.31714
Epoch 12: Val Loss 1220.26978
Epoch 13: Val Loss 1193.17798
Epoch 14: Val Loss 1159.49109
Epoch 15: Val Loss 1121.98645
Epoch 16: Val Loss 1081.11108
Epoch 17: Val Loss 1034.54138
Epoch 18: Val Loss 982.93982
Epoch 19: Val Loss 929.59070
Epoch 20: Val Loss 876.37134
Epoch 21: Val Loss 827.56750
Epoch 22: Val Loss 784.49005
Epoch 23: Val Loss 747.53082
Epoch 24: Val Loss 717.34576
Epoch 25: Val Loss 688.45709
Epoch 26: Val Loss 660.08301
Epoch 27: Val Loss 630.19861
Epoch 28: Val Loss 599.29602
Epoch 29: Val Loss 569.63715
Epoch 30: Val Loss 541.38116
Epoch 31: Val Loss 514.27948
Epoch 32: Val Loss 489.01395
Epoch 33: Val Loss 467.55502
Epoch 34: Val Loss 449.31696
Epoch 35: Val Loss 428.61038
Epoch 36: Val Loss 410.24261
Epoch 37: Val Loss 397.17932
Epoch 38: Val Loss 388.58725
Epoch 39: Val Loss 381.87247
Epoch 40: Val Loss 373.51868
Epoch 41: Val Loss 367.97897
Epoch 42: Val Loss 363.87350
Epoch 43: Val Loss 354.76620
Epoch 44: Val Loss 348.53665
Epoch 45: Val Loss 345.27786
Epoch 46: Val Loss 346.89374
Epoch 47: Val Loss 342.97183
Epoch 48: Val Loss 340.79788
Epoch 49: Val Loss 335.12277
Epoch 50: Val Loss 338.10928
Epoch 51: Val Loss 349.61606
Epoch 52: Val Loss 348.19824
Epoch 53: Val Loss 340.26144
Epoch 54: Val Loss 344.04388
Epoch 55: Val Loss 335.96259
Epoch 56: Val Loss 331.42636
Epoch 57: Val Loss 336.09512
Epoch 58: Val Loss 335.21307
Epoch 59: Val Loss 329.22183
Epoch 60: Val Loss 319.07474
Epoch 61: Val Loss 316.07129
Epoch 62: Val Loss 335.79901
Epoch 63: Val Loss 329.30820
Epoch 64: Val Loss 316.59869
Epoch 65: Val Loss 306.44882
Epoch 66: Val Loss 307.34943
Epoch 67: Val Loss 316.05771
Epoch 68: Val Loss 313.88776
Epoch 69: Val Loss 307.75833
Epoch 70: Val Loss 299.17664
Epoch 71: Val Loss 296.55167
Epoch 72: Val Loss 300.38367
Epoch 73: Val Loss 305.89224
Epoch 74: Val Loss 311.28888
Epoch 75: Val Loss 302.65607
Epoch 76: Val Loss 292.33557
Epoch 77: Val Loss 285.74176
Epoch 78: Val Loss 290.24252
Epoch 79: Val Loss 290.22787
Epoch 80: Val Loss 288.87866
Epoch 81: Val Loss 297.65897
Epoch 82: Val Loss 296.55719
Epoch 83: Val Loss 290.26617
Epoch 84: Val Loss 280.88733
Epoch 85: Val Loss 277.80396
Epoch 86: Val Loss 288.68933
Epoch 87: Val Loss 287.31876
Epoch 88: Val Loss 284.62817
Epoch 89: Val Loss 279.26346
Epoch 90: Val Loss 279.39163
Epoch 91: Val Loss 270.77548
Epoch 92: Val Loss 274.24716
Epoch 93: Val Loss 273.09369
Epoch 94: Val Loss 280.97690
Epoch 95: Val Loss 264.71655
Epoch 96: Val Loss 266.17273
Epoch 97: Val Loss 270.22229
Epoch 98: Val Loss 266.77252
Epoch 99: Val Loss 263.10742
{'MSE - mean': 225.73535187783202, 'MSE - std': 37.372059523552394, 'R2 - mean': 0.7827221712482749, 'R2 - std': 0.03328231625021483} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2836.37988
Epoch 1: Val Loss 2834.25244
Epoch 2: Val Loss 2832.88110
Epoch 3: Val Loss 2831.70288
Epoch 4: Val Loss 2830.14746
Epoch 5: Val Loss 2827.84961
Epoch 6: Val Loss 2824.52393
Epoch 7: Val Loss 2819.65015
Epoch 8: Val Loss 2812.31543
Epoch 9: Val Loss 2801.49146
Epoch 10: Val Loss 2785.84546
Epoch 11: Val Loss 2763.50488
Epoch 12: Val Loss 2734.35742
Epoch 13: Val Loss 2697.81226
Epoch 14: Val Loss 2651.62109
Epoch 15: Val Loss 2592.52393
Epoch 16: Val Loss 2521.11914
Epoch 17: Val Loss 2437.44263
Epoch 18: Val Loss 2345.95508
Epoch 19: Val Loss 2255.86816
Epoch 20: Val Loss 2168.73218
Epoch 21: Val Loss 2090.81396
Epoch 22: Val Loss 2017.13928
Epoch 23: Val Loss 1957.87451
Epoch 24: Val Loss 1900.01514
Epoch 25: Val Loss 1846.55762
Epoch 26: Val Loss 1786.21130
Epoch 27: Val Loss 1727.66321
Epoch 28: Val Loss 1679.39258
Epoch 29: Val Loss 1623.52417
Epoch 30: Val Loss 1567.72913
Epoch 31: Val Loss 1502.97351
Epoch 32: Val Loss 1449.66345
Epoch 33: Val Loss 1401.79736
Epoch 34: Val Loss 1350.53821
Epoch 35: Val Loss 1306.06921
Epoch 36: Val Loss 1249.71057
Epoch 37: Val Loss 1201.11682
Epoch 38: Val Loss 1162.54736
Epoch 39: Val Loss 1122.31226
Epoch 40: Val Loss 1084.24133
Epoch 41: Val Loss 1049.55945
Epoch 42: Val Loss 1014.79510
Epoch 43: Val Loss 990.14459
Epoch 44: Val Loss 966.43225
Epoch 45: Val Loss 939.93250
Epoch 46: Val Loss 905.18396
Epoch 47: Val Loss 875.19440
Epoch 48: Val Loss 853.77631
Epoch 49: Val Loss 826.98669
Epoch 50: Val Loss 814.92896
Epoch 51: Val Loss 796.21606
Epoch 52: Val Loss 783.38513
Epoch 53: Val Loss 767.33423
Epoch 54: Val Loss 755.44330
Epoch 55: Val Loss 737.15918
Epoch 56: Val Loss 725.14111
Epoch 57: Val Loss 708.67950
Epoch 58: Val Loss 703.18359
Epoch 59: Val Loss 694.15771
Epoch 60: Val Loss 689.48834
Epoch 61: Val Loss 682.91180
Epoch 62: Val Loss 671.49768
Epoch 63: Val Loss 662.83588
Epoch 64: Val Loss 655.72473
Epoch 65: Val Loss 645.71887
Epoch 66: Val Loss 628.34705
Epoch 67: Val Loss 624.43878
Epoch 68: Val Loss 623.03070
Epoch 69: Val Loss 624.85864
Epoch 70: Val Loss 618.09442
Epoch 71: Val Loss 610.93805
Epoch 72: Val Loss 600.68640
Epoch 73: Val Loss 597.87927
Epoch 74: Val Loss 598.06641
Epoch 75: Val Loss 593.59497
Epoch 76: Val Loss 589.98822
Epoch 77: Val Loss 584.41937
Epoch 78: Val Loss 577.13831
Epoch 79: Val Loss 572.39539
Epoch 80: Val Loss 561.14703
Epoch 81: Val Loss 555.13757
Epoch 82: Val Loss 548.24353
Epoch 83: Val Loss 550.45172
Epoch 84: Val Loss 541.11298
Epoch 85: Val Loss 543.51410
Epoch 86: Val Loss 534.12018
Epoch 87: Val Loss 528.89911
Epoch 88: Val Loss 522.07172
Epoch 89: Val Loss 523.22772
Epoch 90: Val Loss 511.12073
Epoch 91: Val Loss 509.62784
Epoch 92: Val Loss 507.36182
Epoch 93: Val Loss 503.98749
Epoch 94: Val Loss 498.67746
Epoch 95: Val Loss 498.20380
Epoch 96: Val Loss 497.60199
Epoch 97: Val Loss 497.22165
Epoch 98: Val Loss 488.64539
Epoch 99: Val Loss 481.49622
{'MSE - mean': 310.98896345720095, 'MSE - std': 124.36828560103163, 'R2 - mean': 0.7896369429365476, 'R2 - std': 0.028880844603094348} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2133.93945
Epoch 1: Val Loss 2131.76270
Epoch 2: Val Loss 2129.28906
Epoch 3: Val Loss 2126.12256
Epoch 4: Val Loss 2121.91406
Epoch 5: Val Loss 2115.58057
Epoch 6: Val Loss 2105.68237
Epoch 7: Val Loss 2092.05298
Epoch 8: Val Loss 2074.47388
Epoch 9: Val Loss 2052.11450
Epoch 10: Val Loss 2022.93799
Epoch 11: Val Loss 1988.10339
Epoch 12: Val Loss 1944.95618
Epoch 13: Val Loss 1893.27405
Epoch 14: Val Loss 1833.32898
Epoch 15: Val Loss 1767.13281
Epoch 16: Val Loss 1691.54358
Epoch 17: Val Loss 1611.49182
Epoch 18: Val Loss 1528.16418
Epoch 19: Val Loss 1450.95654
Epoch 20: Val Loss 1378.55469
Epoch 21: Val Loss 1314.28491
Epoch 22: Val Loss 1258.62195
Epoch 23: Val Loss 1210.69592
Epoch 24: Val Loss 1164.77209
Epoch 25: Val Loss 1124.59375
Epoch 26: Val Loss 1086.56018
Epoch 27: Val Loss 1048.43945
Epoch 28: Val Loss 1010.44061
Epoch 29: Val Loss 971.70020
Epoch 30: Val Loss 935.03052
Epoch 31: Val Loss 899.32605
Epoch 32: Val Loss 864.45642
Epoch 33: Val Loss 829.68585
Epoch 34: Val Loss 798.90955
Epoch 35: Val Loss 769.13678
Epoch 36: Val Loss 741.26453
Epoch 37: Val Loss 714.73041
Epoch 38: Val Loss 690.68890
Epoch 39: Val Loss 669.99255
Epoch 40: Val Loss 650.23193
Epoch 41: Val Loss 632.16705
Epoch 42: Val Loss 615.43695
Epoch 43: Val Loss 600.01953
Epoch 44: Val Loss 586.61871
Epoch 45: Val Loss 574.12494
Epoch 46: Val Loss 562.48138
Epoch 47: Val Loss 551.30890
Epoch 48: Val Loss 540.90771
Epoch 49: Val Loss 534.50079
Epoch 50: Val Loss 524.15350
Epoch 51: Val Loss 513.25897
Epoch 52: Val Loss 504.79254
Epoch 53: Val Loss 498.77603
Epoch 54: Val Loss 492.93021
Epoch 55: Val Loss 487.95929
Epoch 56: Val Loss 483.52631
Epoch 57: Val Loss 476.74387
Epoch 58: Val Loss 474.36713
Epoch 59: Val Loss 469.12653
Epoch 60: Val Loss 462.68210
Epoch 61: Val Loss 458.44440
Epoch 62: Val Loss 454.13535
Epoch 63: Val Loss 451.60165
Epoch 64: Val Loss 451.97910
Epoch 65: Val Loss 445.37042
Epoch 66: Val Loss 437.93457
Epoch 67: Val Loss 433.30365
Epoch 68: Val Loss 430.21103
Epoch 69: Val Loss 428.33542
Epoch 70: Val Loss 424.58374
Epoch 71: Val Loss 426.51208
Epoch 72: Val Loss 423.50116
Epoch 73: Val Loss 417.21426
Epoch 74: Val Loss 412.28909
Epoch 75: Val Loss 411.97617
Epoch 76: Val Loss 409.64056
Epoch 77: Val Loss 406.32104
Epoch 78: Val Loss 401.52191
Epoch 79: Val Loss 401.16321
Epoch 80: Val Loss 403.07651
Epoch 81: Val Loss 399.54080
Epoch 82: Val Loss 397.36322
Epoch 83: Val Loss 395.02570
Epoch 84: Val Loss 392.43155
Epoch 85: Val Loss 390.22888
Epoch 86: Val Loss 390.39639
Epoch 87: Val Loss 389.38107
Epoch 88: Val Loss 385.89145
Epoch 89: Val Loss 381.83795
Epoch 90: Val Loss 381.79153
Epoch 91: Val Loss 378.14005
Epoch 92: Val Loss 380.03964
Epoch 93: Val Loss 386.05774
Epoch 94: Val Loss 385.64786
Epoch 95: Val Loss 376.82935
Epoch 96: Val Loss 370.64282
Epoch 97: Val Loss 367.96091
Epoch 98: Val Loss 369.29208
Epoch 99: Val Loss 372.45868
{'MSE - mean': 325.23194776853046, 'MSE - std': 110.49520647423202, 'R2 - mean': 0.7888504319280814, 'R2 - std': 0.025048616477569747} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2518.54297
Epoch 1: Val Loss 2515.99463
Epoch 2: Val Loss 2512.52759
Epoch 3: Val Loss 2507.58936
Epoch 4: Val Loss 2500.72607
Epoch 5: Val Loss 2491.01904
Epoch 6: Val Loss 2477.43164
Epoch 7: Val Loss 2459.07422
Epoch 8: Val Loss 2433.84302
Epoch 9: Val Loss 2399.62500
Epoch 10: Val Loss 2354.45288
Epoch 11: Val Loss 2297.97168
Epoch 12: Val Loss 2229.53784
Epoch 13: Val Loss 2151.25708
Epoch 14: Val Loss 2067.61279
Epoch 15: Val Loss 1978.81226
Epoch 16: Val Loss 1892.51465
Epoch 17: Val Loss 1808.74463
Epoch 18: Val Loss 1728.24622
Epoch 19: Val Loss 1657.78479
Epoch 20: Val Loss 1602.27148
Epoch 21: Val Loss 1547.72034
Epoch 22: Val Loss 1491.76270
Epoch 23: Val Loss 1439.63599
Epoch 24: Val Loss 1393.43750
Epoch 25: Val Loss 1344.80298
Epoch 26: Val Loss 1296.68774
Epoch 27: Val Loss 1251.80127
Epoch 28: Val Loss 1200.46143
Epoch 29: Val Loss 1140.66846
Epoch 30: Val Loss 1088.84521
Epoch 31: Val Loss 1049.25073
Epoch 32: Val Loss 1008.86890
Epoch 33: Val Loss 970.95319
Epoch 34: Val Loss 941.35333
Epoch 35: Val Loss 900.14380
Epoch 36: Val Loss 868.44928
Epoch 37: Val Loss 837.61340
Epoch 38: Val Loss 810.28442
Epoch 39: Val Loss 791.18536
Epoch 40: Val Loss 763.54559
Epoch 41: Val Loss 739.87000
Epoch 42: Val Loss 733.64520
Epoch 43: Val Loss 703.95361
Epoch 44: Val Loss 683.33606
Epoch 45: Val Loss 662.07764
Epoch 46: Val Loss 649.79535
Epoch 47: Val Loss 643.55225
Epoch 48: Val Loss 633.25348
Epoch 49: Val Loss 626.91083
Epoch 50: Val Loss 616.94653
Epoch 51: Val Loss 610.69836
Epoch 52: Val Loss 599.93304
Epoch 53: Val Loss 593.26300
Epoch 54: Val Loss 577.00757
Epoch 55: Val Loss 574.27197
Epoch 56: Val Loss 569.97736
Epoch 57: Val Loss 566.47748
Epoch 58: Val Loss 553.78461
Epoch 59: Val Loss 544.64142
Epoch 60: Val Loss 539.37994
Epoch 61: Val Loss 534.47528
Epoch 62: Val Loss 532.42059
Epoch 63: Val Loss 525.15137
Epoch 64: Val Loss 512.78589
Epoch 65: Val Loss 508.85388
Epoch 66: Val Loss 506.16214
Epoch 67: Val Loss 504.17944
Epoch 68: Val Loss 494.82251
Epoch 69: Val Loss 491.51379
Epoch 70: Val Loss 489.37430
Epoch 71: Val Loss 488.26233
Epoch 72: Val Loss 484.69028
Epoch 73: Val Loss 477.31839
Epoch 74: Val Loss 474.69974
Epoch 75: Val Loss 472.15604
Epoch 76: Val Loss 469.94821
Epoch 77: Val Loss 467.14157
Epoch 78: Val Loss 462.84369
Epoch 79: Val Loss 459.93201
Epoch 80: Val Loss 458.43915
Epoch 81: Val Loss 450.36151
Epoch 82: Val Loss 444.41888
Epoch 83: Val Loss 441.90897
Epoch 84: Val Loss 440.30093
Epoch 85: Val Loss 435.95874
Epoch 86: Val Loss 432.41504
Epoch 87: Val Loss 431.36908
Epoch 88: Val Loss 428.92026
Epoch 89: Val Loss 426.43781
Epoch 90: Val Loss 417.85205
Epoch 91: Val Loss 414.37534
Epoch 92: Val Loss 412.29654
Epoch 93: Val Loss 411.86060
Epoch 94: Val Loss 406.68875
Epoch 95: Val Loss 403.05563
Epoch 96: Val Loss 405.04871
Epoch 97: Val Loss 403.21964
Epoch 98: Val Loss 395.19055
Epoch 99: Val Loss 389.93384
{'MSE - mean': 338.1723314559726, 'MSE - std': 102.16245221674843, 'R2 - mean': 0.7946222368475906, 'R2 - std': 0.025203203726268355} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 338.1723314559726 and parameters: {'dim': 32, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1281.88440
Epoch 1: Val Loss 1279.49060
Epoch 2: Val Loss 1276.93604
Epoch 3: Val Loss 1273.92627
Epoch 4: Val Loss 1270.05139
Epoch 5: Val Loss 1264.76721
Epoch 6: Val Loss 1257.48792
Epoch 7: Val Loss 1247.61816
Epoch 8: Val Loss 1234.12231
Epoch 9: Val Loss 1216.44238
Epoch 10: Val Loss 1193.48657
Epoch 11: Val Loss 1164.28918
Epoch 12: Val Loss 1130.18152
Epoch 13: Val Loss 1088.69458
Epoch 14: Val Loss 1041.58411
Epoch 15: Val Loss 990.97180
Epoch 16: Val Loss 937.53949
Epoch 17: Val Loss 880.22424
Epoch 18: Val Loss 824.61932
Epoch 19: Val Loss 774.17334
Epoch 20: Val Loss 732.64954
Epoch 21: Val Loss 701.56750
Epoch 22: Val Loss 677.18866
Epoch 23: Val Loss 653.55688
Epoch 24: Val Loss 631.17474
Epoch 25: Val Loss 604.16650
Epoch 26: Val Loss 575.30127
Epoch 27: Val Loss 546.66321
Epoch 28: Val Loss 517.91852
Epoch 29: Val Loss 491.33029
Epoch 30: Val Loss 466.86435
Epoch 31: Val Loss 441.37857
Epoch 32: Val Loss 417.46750
Epoch 33: Val Loss 395.65536
Epoch 34: Val Loss 376.24881
Epoch 35: Val Loss 357.23611
Epoch 36: Val Loss 341.89197
Epoch 37: Val Loss 330.14359
Epoch 38: Val Loss 319.87531
Epoch 39: Val Loss 309.74570
Epoch 40: Val Loss 301.43945
Epoch 41: Val Loss 295.32452
Epoch 42: Val Loss 284.74011
Epoch 43: Val Loss 278.88321
Epoch 44: Val Loss 269.60220
Epoch 45: Val Loss 262.83511
Epoch 46: Val Loss 256.81815
Epoch 47: Val Loss 252.13699
Epoch 48: Val Loss 245.56963
Epoch 49: Val Loss 239.90028
Epoch 50: Val Loss 239.01610
Epoch 51: Val Loss 235.71564
Epoch 52: Val Loss 232.65157
Epoch 53: Val Loss 231.34883
Epoch 54: Val Loss 225.84392
Epoch 55: Val Loss 221.05135
Epoch 56: Val Loss 216.57230
Epoch 57: Val Loss 214.97710
Epoch 58: Val Loss 214.23764
Epoch 59: Val Loss 211.40295
Epoch 60: Val Loss 208.14297
Epoch 61: Val Loss 210.24442
Epoch 62: Val Loss 208.75014
Epoch 63: Val Loss 204.94452
Epoch 64: Val Loss 203.54814
Epoch 65: Val Loss 201.76547
Epoch 66: Val Loss 203.19254
Epoch 67: Val Loss 201.15268
Epoch 68: Val Loss 199.94051
Epoch 69: Val Loss 200.25525
Epoch 70: Val Loss 199.73767
Epoch 71: Val Loss 199.46222
Epoch 72: Val Loss 198.63722
Epoch 73: Val Loss 196.88873
Epoch 74: Val Loss 194.34737
Epoch 75: Val Loss 194.19066
Epoch 76: Val Loss 194.46387
Epoch 77: Val Loss 194.72354
Epoch 78: Val Loss 192.55734
Epoch 79: Val Loss 190.28658
Epoch 80: Val Loss 189.84601
Epoch 81: Val Loss 188.94321
Epoch 82: Val Loss 187.09604
Epoch 83: Val Loss 185.74408
Epoch 84: Val Loss 186.48134
Epoch 85: Val Loss 184.82925
Epoch 86: Val Loss 184.11230
Epoch 87: Val Loss 182.06236
Epoch 88: Val Loss 182.96532
Epoch 89: Val Loss 180.98615
Epoch 90: Val Loss 180.03601
Epoch 91: Val Loss 181.86943
Epoch 92: Val Loss 182.48184
Epoch 93: Val Loss 182.55400
Epoch 94: Val Loss 180.91194
Epoch 95: Val Loss 179.39824
Epoch 96: Val Loss 177.21114
Epoch 97: Val Loss 177.85233
Epoch 98: Val Loss 178.09637
Epoch 99: Val Loss 177.79942
{'MSE - mean': 177.21115437572283, 'MSE - std': 0.0, 'R2 - mean': 0.8268980290012192, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1299.14795
Epoch 1: Val Loss 1296.23108
Epoch 2: Val Loss 1294.04541
Epoch 3: Val Loss 1291.83093
Epoch 4: Val Loss 1289.06519
Epoch 5: Val Loss 1285.44324
Epoch 6: Val Loss 1280.62622
Epoch 7: Val Loss 1273.87085
Epoch 8: Val Loss 1265.07446
Epoch 9: Val Loss 1253.73010
Epoch 10: Val Loss 1239.02356
Epoch 11: Val Loss 1219.92664
Epoch 12: Val Loss 1196.21619
Epoch 13: Val Loss 1167.77148
Epoch 14: Val Loss 1135.25293
Epoch 15: Val Loss 1096.83264
Epoch 16: Val Loss 1053.38843
Epoch 17: Val Loss 1004.65991
Epoch 18: Val Loss 954.57428
Epoch 19: Val Loss 904.12708
Epoch 20: Val Loss 854.87299
Epoch 21: Val Loss 810.95374
Epoch 22: Val Loss 772.96307
Epoch 23: Val Loss 741.82574
Epoch 24: Val Loss 716.42944
Epoch 25: Val Loss 693.47089
Epoch 26: Val Loss 671.64795
Epoch 27: Val Loss 648.23743
Epoch 28: Val Loss 624.18152
Epoch 29: Val Loss 600.12439
Epoch 30: Val Loss 575.98547
Epoch 31: Val Loss 555.43213
Epoch 32: Val Loss 528.24188
Epoch 33: Val Loss 504.69745
Epoch 34: Val Loss 479.72971
Epoch 35: Val Loss 457.19772
Epoch 36: Val Loss 440.92532
Epoch 37: Val Loss 426.43893
Epoch 38: Val Loss 416.20407
Epoch 39: Val Loss 409.18130
Epoch 40: Val Loss 399.29898
Epoch 41: Val Loss 391.54944
Epoch 42: Val Loss 386.51364
Epoch 43: Val Loss 386.27020
Epoch 44: Val Loss 379.08521
Epoch 45: Val Loss 370.43460
Epoch 46: Val Loss 367.62411
Epoch 47: Val Loss 366.62692
Epoch 48: Val Loss 360.45486
Epoch 49: Val Loss 356.29993
Epoch 50: Val Loss 346.14413
Epoch 51: Val Loss 336.12476
Epoch 52: Val Loss 335.89645
Epoch 53: Val Loss 329.54926
Epoch 54: Val Loss 323.11557
Epoch 55: Val Loss 318.18259
Epoch 56: Val Loss 312.52762
Epoch 57: Val Loss 320.00662
Epoch 58: Val Loss 327.60492
Epoch 59: Val Loss 314.79257
Epoch 60: Val Loss 303.10233
Epoch 61: Val Loss 303.51288
Epoch 62: Val Loss 304.95819
Epoch 63: Val Loss 312.25146
Epoch 64: Val Loss 310.72070
Epoch 65: Val Loss 308.22052
Epoch 66: Val Loss 313.83887
Epoch 67: Val Loss 307.29272
Epoch 68: Val Loss 300.37952
Epoch 69: Val Loss 295.28830
Epoch 70: Val Loss 296.33771
Epoch 71: Val Loss 296.23947
Epoch 72: Val Loss 299.42395
Epoch 73: Val Loss 295.68222
Epoch 74: Val Loss 289.19556
Epoch 75: Val Loss 291.58426
Epoch 76: Val Loss 290.86569
Epoch 77: Val Loss 296.49506
Epoch 78: Val Loss 289.49966
Epoch 79: Val Loss 292.65771
Epoch 80: Val Loss 294.70401
Epoch 81: Val Loss 282.46603
Epoch 82: Val Loss 279.36832
Epoch 83: Val Loss 280.64697
Epoch 84: Val Loss 277.04999
Epoch 85: Val Loss 276.60260
Epoch 86: Val Loss 282.49173
Epoch 87: Val Loss 276.06784
Epoch 88: Val Loss 276.08084
Epoch 89: Val Loss 278.62186
Epoch 90: Val Loss 272.09848
Epoch 91: Val Loss 264.89404
Epoch 92: Val Loss 265.60385
Epoch 93: Val Loss 265.21268
Epoch 94: Val Loss 263.87833
Epoch 95: Val Loss 259.68753
Epoch 96: Val Loss 251.92206
Epoch 97: Val Loss 252.70987
Epoch 98: Val Loss 256.60779
Epoch 99: Val Loss 261.43631
{'MSE - mean': 214.56661128948872, 'MSE - std': 37.35545691376589, 'R2 - mean': 0.793494906020745, 'R2 - std': 0.03340312298047421} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2813.38477
Epoch 1: Val Loss 2811.96289
Epoch 2: Val Loss 2810.47510
Epoch 3: Val Loss 2808.68823
Epoch 4: Val Loss 2806.50513
Epoch 5: Val Loss 2803.72241
Epoch 6: Val Loss 2799.77856
Epoch 7: Val Loss 2794.49951
Epoch 8: Val Loss 2786.65234
Epoch 9: Val Loss 2775.50171
Epoch 10: Val Loss 2760.19238
Epoch 11: Val Loss 2739.22168
Epoch 12: Val Loss 2711.03174
Epoch 13: Val Loss 2674.43579
Epoch 14: Val Loss 2629.04248
Epoch 15: Val Loss 2572.95679
Epoch 16: Val Loss 2507.90991
Epoch 17: Val Loss 2431.78955
Epoch 18: Val Loss 2345.35303
Epoch 19: Val Loss 2258.82471
Epoch 20: Val Loss 2172.23047
Epoch 21: Val Loss 2084.12231
Epoch 22: Val Loss 2006.28210
Epoch 23: Val Loss 1937.54626
Epoch 24: Val Loss 1881.92615
Epoch 25: Val Loss 1833.44226
Epoch 26: Val Loss 1788.53125
Epoch 27: Val Loss 1745.31958
Epoch 28: Val Loss 1702.85303
Epoch 29: Val Loss 1655.41382
Epoch 30: Val Loss 1608.89551
Epoch 31: Val Loss 1563.08179
Epoch 32: Val Loss 1508.48889
Epoch 33: Val Loss 1463.72852
Epoch 34: Val Loss 1410.39270
Epoch 35: Val Loss 1368.65833
Epoch 36: Val Loss 1329.54089
Epoch 37: Val Loss 1291.98914
Epoch 38: Val Loss 1253.36719
Epoch 39: Val Loss 1203.87720
Epoch 40: Val Loss 1171.47083
Epoch 41: Val Loss 1145.58752
Epoch 42: Val Loss 1117.01196
Epoch 43: Val Loss 1094.77905
Epoch 44: Val Loss 1054.59021
Epoch 45: Val Loss 1028.51208
Epoch 46: Val Loss 1005.51514
Epoch 47: Val Loss 983.74524
Epoch 48: Val Loss 971.56000
Epoch 49: Val Loss 956.51440
Epoch 50: Val Loss 933.79901
Epoch 51: Val Loss 919.30493
Epoch 52: Val Loss 906.43872
Epoch 53: Val Loss 887.23785
Epoch 54: Val Loss 875.45490
Epoch 55: Val Loss 858.28876
Epoch 56: Val Loss 841.23431
Epoch 57: Val Loss 829.55817
Epoch 58: Val Loss 815.09576
Epoch 59: Val Loss 798.27264
Epoch 60: Val Loss 769.15039
Epoch 61: Val Loss 759.33337
Epoch 62: Val Loss 749.48297
Epoch 63: Val Loss 743.43732
Epoch 64: Val Loss 732.12524
Epoch 65: Val Loss 719.74194
Epoch 66: Val Loss 717.10834
Epoch 67: Val Loss 704.06647
Epoch 68: Val Loss 695.18616
Epoch 69: Val Loss 690.79358
Epoch 70: Val Loss 683.93988
Epoch 71: Val Loss 676.96143
Epoch 72: Val Loss 671.90619
Epoch 73: Val Loss 665.93420
Epoch 74: Val Loss 659.74591
Epoch 75: Val Loss 649.32526
Epoch 76: Val Loss 638.46277
Epoch 77: Val Loss 630.04211
Epoch 78: Val Loss 621.23224
Epoch 79: Val Loss 614.52112
Epoch 80: Val Loss 610.53680
Epoch 81: Val Loss 606.04822
Epoch 82: Val Loss 599.06036
Epoch 83: Val Loss 592.46930
Epoch 84: Val Loss 581.33978
Epoch 85: Val Loss 570.97235
Epoch 86: Val Loss 560.53839
Epoch 87: Val Loss 559.44629
Epoch 88: Val Loss 556.40411
Epoch 89: Val Loss 551.39941
Epoch 90: Val Loss 539.64496
Epoch 91: Val Loss 523.38702
Epoch 92: Val Loss 524.03741
Epoch 93: Val Loss 520.70819
Epoch 94: Val Loss 518.52863
Epoch 95: Val Loss 519.79028
Epoch 96: Val Loss 514.60980
Epoch 97: Val Loss 507.09131
Epoch 98: Val Loss 503.46277
Epoch 99: Val Loss 497.87778
{'MSE - mean': 309.00365588607457, 'MSE - std': 136.99269162416132, 'R2 - mean': 0.7945899320976602, 'R2 - std': 0.02731746536773445} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2130.87500
Epoch 1: Val Loss 2127.48853
Epoch 2: Val Loss 2123.29346
Epoch 3: Val Loss 2117.44189
Epoch 4: Val Loss 2109.12378
Epoch 5: Val Loss 2097.52710
Epoch 6: Val Loss 2081.29541
Epoch 7: Val Loss 2059.69238
Epoch 8: Val Loss 2031.13025
Epoch 9: Val Loss 1995.00024
Epoch 10: Val Loss 1949.60962
Epoch 11: Val Loss 1893.44910
Epoch 12: Val Loss 1826.19324
Epoch 13: Val Loss 1747.94397
Epoch 14: Val Loss 1665.66394
Epoch 15: Val Loss 1576.16345
Epoch 16: Val Loss 1487.29712
Epoch 17: Val Loss 1402.43372
Epoch 18: Val Loss 1328.62268
Epoch 19: Val Loss 1263.26489
Epoch 20: Val Loss 1207.05164
Epoch 21: Val Loss 1163.02478
Epoch 22: Val Loss 1120.37378
Epoch 23: Val Loss 1077.07751
Epoch 24: Val Loss 1037.30505
Epoch 25: Val Loss 997.06897
Epoch 26: Val Loss 956.84937
Epoch 27: Val Loss 919.41986
Epoch 28: Val Loss 882.25732
Epoch 29: Val Loss 843.54401
Epoch 30: Val Loss 807.72388
Epoch 31: Val Loss 773.54022
Epoch 32: Val Loss 742.00934
Epoch 33: Val Loss 712.53650
Epoch 34: Val Loss 686.44763
Epoch 35: Val Loss 661.16052
Epoch 36: Val Loss 639.27368
Epoch 37: Val Loss 619.22546
Epoch 38: Val Loss 599.36426
Epoch 39: Val Loss 583.91760
Epoch 40: Val Loss 567.52216
Epoch 41: Val Loss 553.39648
Epoch 42: Val Loss 540.92682
Epoch 43: Val Loss 530.80743
Epoch 44: Val Loss 517.64978
Epoch 45: Val Loss 505.99713
Epoch 46: Val Loss 497.65482
Epoch 47: Val Loss 488.71744
Epoch 48: Val Loss 483.87659
Epoch 49: Val Loss 479.54572
Epoch 50: Val Loss 470.79382
Epoch 51: Val Loss 465.53098
Epoch 52: Val Loss 460.64111
Epoch 53: Val Loss 453.85294
Epoch 54: Val Loss 450.01944
Epoch 55: Val Loss 447.29877
Epoch 56: Val Loss 441.41159
Epoch 57: Val Loss 439.01413
Epoch 58: Val Loss 435.15964
Epoch 59: Val Loss 427.40732
Epoch 60: Val Loss 419.94839
Epoch 61: Val Loss 416.03018
Epoch 62: Val Loss 414.52646
Epoch 63: Val Loss 412.91528
Epoch 64: Val Loss 409.05823
Epoch 65: Val Loss 409.78375
Epoch 66: Val Loss 407.54666
Epoch 67: Val Loss 403.36221
Epoch 68: Val Loss 398.97849
Epoch 69: Val Loss 395.18463
Epoch 70: Val Loss 393.08029
Epoch 71: Val Loss 392.86493
Epoch 72: Val Loss 388.25723
Epoch 73: Val Loss 387.74319
Epoch 74: Val Loss 381.96295
Epoch 75: Val Loss 380.51126
Epoch 76: Val Loss 378.28287
Epoch 77: Val Loss 383.30594
Epoch 78: Val Loss 382.50290
Epoch 79: Val Loss 380.84085
Epoch 80: Val Loss 376.78082
Epoch 81: Val Loss 368.89786
Epoch 82: Val Loss 364.60416
Epoch 83: Val Loss 357.08197
Epoch 84: Val Loss 354.36649
Epoch 85: Val Loss 352.33768
Epoch 86: Val Loss 354.51782
Epoch 87: Val Loss 360.05966
Epoch 88: Val Loss 363.56198
Epoch 89: Val Loss 355.75726
Epoch 90: Val Loss 354.84369
Epoch 91: Val Loss 350.80121
Epoch 92: Val Loss 350.97260
Epoch 93: Val Loss 348.76205
Epoch 94: Val Loss 353.37250
Epoch 95: Val Loss 350.52597
Epoch 96: Val Loss 344.40106
Epoch 97: Val Loss 337.99445
Epoch 98: Val Loss 338.35699
Epoch 99: Val Loss 334.28775
{'MSE - mean': 315.3246844129774, 'MSE - std': 119.14325148214724, 'R2 - mean': 0.7974498774828187, 'R2 - std': 0.024170659867001943} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2516.56177
Epoch 1: Val Loss 2512.10278
Epoch 2: Val Loss 2506.65112
Epoch 3: Val Loss 2499.31055
Epoch 4: Val Loss 2489.53149
Epoch 5: Val Loss 2475.87061
Epoch 6: Val Loss 2457.38721
Epoch 7: Val Loss 2432.70703
Epoch 8: Val Loss 2400.64233
Epoch 9: Val Loss 2360.00854
Epoch 10: Val Loss 2309.82227
Epoch 11: Val Loss 2247.48975
Epoch 12: Val Loss 2177.58398
Epoch 13: Val Loss 2097.95801
Epoch 14: Val Loss 2011.19458
Epoch 15: Val Loss 1920.56055
Epoch 16: Val Loss 1827.07629
Epoch 17: Val Loss 1732.84412
Epoch 18: Val Loss 1646.16235
Epoch 19: Val Loss 1570.94983
Epoch 20: Val Loss 1507.60413
Epoch 21: Val Loss 1447.57764
Epoch 22: Val Loss 1396.75354
Epoch 23: Val Loss 1340.33545
Epoch 24: Val Loss 1292.87842
Epoch 25: Val Loss 1243.06140
Epoch 26: Val Loss 1198.11682
Epoch 27: Val Loss 1149.58484
Epoch 28: Val Loss 1104.94226
Epoch 29: Val Loss 1063.08289
Epoch 30: Val Loss 1018.19507
Epoch 31: Val Loss 988.04828
Epoch 32: Val Loss 957.39062
Epoch 33: Val Loss 927.63580
Epoch 34: Val Loss 901.34991
Epoch 35: Val Loss 875.40698
Epoch 36: Val Loss 839.45398
Epoch 37: Val Loss 817.83514
Epoch 38: Val Loss 798.41528
Epoch 39: Val Loss 784.74664
Epoch 40: Val Loss 766.81903
Epoch 41: Val Loss 736.33051
Epoch 42: Val Loss 714.73059
Epoch 43: Val Loss 701.99664
Epoch 44: Val Loss 688.21008
Epoch 45: Val Loss 677.04602
Epoch 46: Val Loss 670.28253
Epoch 47: Val Loss 657.16144
Epoch 48: Val Loss 648.13641
Epoch 49: Val Loss 637.49109
Epoch 50: Val Loss 621.70795
Epoch 51: Val Loss 610.05981
Epoch 52: Val Loss 599.49847
Epoch 53: Val Loss 597.40387
Epoch 54: Val Loss 589.98853
Epoch 55: Val Loss 583.06464
Epoch 56: Val Loss 576.93152
Epoch 57: Val Loss 561.69696
Epoch 58: Val Loss 551.30542
Epoch 59: Val Loss 547.67401
Epoch 60: Val Loss 544.87872
Epoch 61: Val Loss 541.54651
Epoch 62: Val Loss 539.05676
Epoch 63: Val Loss 531.46393
Epoch 64: Val Loss 517.39331
Epoch 65: Val Loss 511.19479
Epoch 66: Val Loss 510.73898
Epoch 67: Val Loss 507.57693
Epoch 68: Val Loss 504.12332
Epoch 69: Val Loss 498.85870
Epoch 70: Val Loss 497.66409
Epoch 71: Val Loss 487.34937
Epoch 72: Val Loss 480.31805
Epoch 73: Val Loss 481.76389
Epoch 74: Val Loss 478.52496
Epoch 75: Val Loss 478.02997
Epoch 76: Val Loss 472.28604
Epoch 77: Val Loss 466.37656
Epoch 78: Val Loss 458.75031
Epoch 79: Val Loss 455.49396
Epoch 80: Val Loss 450.84714
Epoch 81: Val Loss 451.18848
Epoch 82: Val Loss 449.31689
Epoch 83: Val Loss 445.85208
Epoch 84: Val Loss 437.95670
Epoch 85: Val Loss 434.14212
Epoch 86: Val Loss 433.21552
Epoch 87: Val Loss 433.13187
Epoch 88: Val Loss 431.30679
Epoch 89: Val Loss 426.58368
Epoch 90: Val Loss 424.52701
Epoch 91: Val Loss 422.59674
Epoch 92: Val Loss 421.93842
Epoch 93: Val Loss 417.35315
Epoch 94: Val Loss 418.52087
Epoch 95: Val Loss 414.17270
Epoch 96: Val Loss 409.25687
Epoch 97: Val Loss 407.95840
Epoch 98: Val Loss 406.04248
Epoch 99: Val Loss 403.41885
{'MSE - mean': 332.9435143118155, 'MSE - std': 112.23985106461659, 'R2 - mean': 0.8002409732887749, 'R2 - std': 0.02232795337502175} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 332.9435143118155 and parameters: {'dim': 64, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 286.3899802356503.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1274.89038
Epoch 1: Val Loss 1274.65405
Epoch 2: Val Loss 1274.41223
Epoch 3: Val Loss 1274.16724
Epoch 4: Val Loss 1273.91760
Epoch 5: Val Loss 1273.66284
Epoch 6: Val Loss 1273.41028
Epoch 7: Val Loss 1273.15491
Epoch 8: Val Loss 1272.88989
Epoch 9: Val Loss 1272.61414
Epoch 10: Val Loss 1272.31873
Epoch 11: Val Loss 1272.02075
Epoch 12: Val Loss 1271.71802
Epoch 13: Val Loss 1271.40149
Epoch 14: Val Loss 1271.08118
Epoch 15: Val Loss 1270.75037
Epoch 16: Val Loss 1270.39795
Epoch 17: Val Loss 1270.04211
Epoch 18: Val Loss 1269.68738
Epoch 19: Val Loss 1269.30237
Epoch 20: Val Loss 1268.90796
Epoch 21: Val Loss 1268.46838
Epoch 22: Val Loss 1268.01880
Epoch 23: Val Loss 1267.56421
Epoch 24: Val Loss 1267.07849
Epoch 25: Val Loss 1266.59216
Epoch 26: Val Loss 1266.05676
Epoch 27: Val Loss 1265.49597
Epoch 28: Val Loss 1264.92493
Epoch 29: Val Loss 1264.33081
Epoch 30: Val Loss 1263.74878
Epoch 31: Val Loss 1263.14600
Epoch 32: Val Loss 1262.50696
Epoch 33: Val Loss 1261.84595
Epoch 34: Val Loss 1261.12341
Epoch 35: Val Loss 1260.40051
Epoch 36: Val Loss 1259.64844
Epoch 37: Val Loss 1258.85828
Epoch 38: Val Loss 1258.01562
Epoch 39: Val Loss 1257.17603
Epoch 40: Val Loss 1256.27905
Epoch 41: Val Loss 1255.36548
Epoch 42: Val Loss 1254.38538
Epoch 43: Val Loss 1253.38245
Epoch 44: Val Loss 1252.35754
Epoch 45: Val Loss 1251.29358
Epoch 46: Val Loss 1250.19568
Epoch 47: Val Loss 1249.03967
Epoch 48: Val Loss 1247.88928
Epoch 49: Val Loss 1246.64111
Epoch 50: Val Loss 1245.40234
Epoch 51: Val Loss 1244.08557
Epoch 52: Val Loss 1242.79846
Epoch 53: Val Loss 1241.41052
Epoch 54: Val Loss 1239.95667
Epoch 55: Val Loss 1238.48523
Epoch 56: Val Loss 1236.94373
Epoch 57: Val Loss 1235.34155
Epoch 58: Val Loss 1233.71204
Epoch 59: Val Loss 1232.05249
Epoch 60: Val Loss 1230.34277
Epoch 61: Val Loss 1228.60559
Epoch 62: Val Loss 1226.77234
Epoch 63: Val Loss 1224.93115
Epoch 64: Val Loss 1223.02795
Epoch 65: Val Loss 1220.89343
Epoch 66: Val Loss 1218.66284
Epoch 67: Val Loss 1216.55090
Epoch 68: Val Loss 1214.44299
Epoch 69: Val Loss 1212.17883
Epoch 70: Val Loss 1209.96667
Epoch 71: Val Loss 1207.69666
Epoch 72: Val Loss 1205.31995
Epoch 73: Val Loss 1202.82007
Epoch 74: Val Loss 1200.23840
Epoch 75: Val Loss 1197.61292
Epoch 76: Val Loss 1194.94189
Epoch 77: Val Loss 1192.31421
Epoch 78: Val Loss 1189.67737
Epoch 79: Val Loss 1186.84277
Epoch 80: Val Loss 1183.83167
Epoch 81: Val Loss 1180.76184
Epoch 82: Val Loss 1177.61475
Epoch 83: Val Loss 1174.49573
Epoch 84: Val Loss 1171.42639
Epoch 85: Val Loss 1168.15063
Epoch 86: Val Loss 1164.94165
Epoch 87: Val Loss 1161.69934
Epoch 88: Val Loss 1158.26343
Epoch 89: Val Loss 1154.91797
Epoch 90: Val Loss 1151.36877
Epoch 91: Val Loss 1147.85071
Epoch 92: Val Loss 1144.40015
Epoch 93: Val Loss 1140.64587
Epoch 94: Val Loss 1136.84460
Epoch 95: Val Loss 1133.02881
Epoch 96: Val Loss 1129.24207
Epoch 97: Val Loss 1125.30518
Epoch 98: Val Loss 1121.32825
Epoch 99: Val Loss 1117.16125
{'MSE - mean': 1117.1613482470646, 'MSE - std': 0.0, 'R2 - mean': -0.09125654074354794, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1301.42383
Epoch 1: Val Loss 1301.06824
Epoch 2: Val Loss 1300.70605
Epoch 3: Val Loss 1300.34412
Epoch 4: Val Loss 1299.99084
Epoch 5: Val Loss 1299.62195
Epoch 6: Val Loss 1299.27271
Epoch 7: Val Loss 1298.90527
Epoch 8: Val Loss 1298.53894
Epoch 9: Val Loss 1298.18591
Epoch 10: Val Loss 1297.82251
Epoch 11: Val Loss 1297.46130
Epoch 12: Val Loss 1297.08093
Epoch 13: Val Loss 1296.69824
Epoch 14: Val Loss 1296.30249
Epoch 15: Val Loss 1295.88855
Epoch 16: Val Loss 1295.47156
Epoch 17: Val Loss 1295.07251
Epoch 18: Val Loss 1294.65015
Epoch 19: Val Loss 1294.21912
Epoch 20: Val Loss 1293.78406
Epoch 21: Val Loss 1293.33118
Epoch 22: Val Loss 1292.86731
Epoch 23: Val Loss 1292.35632
Epoch 24: Val Loss 1291.84204
Epoch 25: Val Loss 1291.30957
Epoch 26: Val Loss 1290.77710
Epoch 27: Val Loss 1290.22644
Epoch 28: Val Loss 1289.65894
Epoch 29: Val Loss 1289.07202
Epoch 30: Val Loss 1288.44043
Epoch 31: Val Loss 1287.77576
Epoch 32: Val Loss 1287.12146
Epoch 33: Val Loss 1286.42346
Epoch 34: Val Loss 1285.74597
Epoch 35: Val Loss 1285.05823
Epoch 36: Val Loss 1284.34326
Epoch 37: Val Loss 1283.60547
Epoch 38: Val Loss 1282.83643
Epoch 39: Val Loss 1282.04590
Epoch 40: Val Loss 1281.26709
Epoch 41: Val Loss 1280.43457
Epoch 42: Val Loss 1279.60071
Epoch 43: Val Loss 1278.71094
Epoch 44: Val Loss 1277.83423
Epoch 45: Val Loss 1276.88452
Epoch 46: Val Loss 1275.94678
Epoch 47: Val Loss 1274.93445
Epoch 48: Val Loss 1273.83557
Epoch 49: Val Loss 1272.75537
Epoch 50: Val Loss 1271.66479
Epoch 51: Val Loss 1270.51428
Epoch 52: Val Loss 1269.36548
Epoch 53: Val Loss 1268.14636
Epoch 54: Val Loss 1266.95325
Epoch 55: Val Loss 1265.65698
Epoch 56: Val Loss 1264.24084
Epoch 57: Val Loss 1262.79382
Epoch 58: Val Loss 1261.38281
Epoch 59: Val Loss 1259.95642
Epoch 60: Val Loss 1258.47021
Epoch 61: Val Loss 1256.94995
Epoch 62: Val Loss 1255.40979
Epoch 63: Val Loss 1253.76416
Epoch 64: Val Loss 1252.13550
Epoch 65: Val Loss 1250.43042
Epoch 66: Val Loss 1248.69287
Epoch 67: Val Loss 1246.97217
Epoch 68: Val Loss 1245.16882
Epoch 69: Val Loss 1243.24219
Epoch 70: Val Loss 1241.17285
Epoch 71: Val Loss 1239.06396
Epoch 72: Val Loss 1237.05493
Epoch 73: Val Loss 1235.02161
Epoch 74: Val Loss 1232.85474
Epoch 75: Val Loss 1230.67017
Epoch 76: Val Loss 1228.33032
Epoch 77: Val Loss 1226.00293
Epoch 78: Val Loss 1223.58716
Epoch 79: Val Loss 1220.98755
Epoch 80: Val Loss 1218.43457
Epoch 81: Val Loss 1215.85181
Epoch 82: Val Loss 1213.31348
Epoch 83: Val Loss 1210.75671
Epoch 84: Val Loss 1208.08521
Epoch 85: Val Loss 1205.31897
Epoch 86: Val Loss 1202.51355
Epoch 87: Val Loss 1199.60974
Epoch 88: Val Loss 1196.76489
Epoch 89: Val Loss 1193.89966
Epoch 90: Val Loss 1190.85608
Epoch 91: Val Loss 1187.73413
Epoch 92: Val Loss 1184.49829
Epoch 93: Val Loss 1180.93372
Epoch 94: Val Loss 1177.43665
Epoch 95: Val Loss 1173.96460
Epoch 96: Val Loss 1170.52759
Epoch 97: Val Loss 1167.02002
Epoch 98: Val Loss 1163.29675
Epoch 99: Val Loss 1159.56921
{'MSE - mean': 1138.3653053305102, 'MSE - std': 21.20395708344563, 'R2 - mean': -0.09776369100231663, 'R2 - std': 0.00650715025876869} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2828.54712
Epoch 1: Val Loss 2828.22998
Epoch 2: Val Loss 2827.92017
Epoch 3: Val Loss 2827.63184
Epoch 4: Val Loss 2827.35498
Epoch 5: Val Loss 2827.08716
Epoch 6: Val Loss 2826.82642
Epoch 7: Val Loss 2826.56592
Epoch 8: Val Loss 2826.33057
Epoch 9: Val Loss 2826.08936
Epoch 10: Val Loss 2825.84766
Epoch 11: Val Loss 2825.61719
Epoch 12: Val Loss 2825.39111
Epoch 13: Val Loss 2825.17749
Epoch 14: Val Loss 2824.95654
Epoch 15: Val Loss 2824.74243
Epoch 16: Val Loss 2824.53418
Epoch 17: Val Loss 2824.32935
Epoch 18: Val Loss 2824.12451
Epoch 19: Val Loss 2823.93140
Epoch 20: Val Loss 2823.73950
Epoch 21: Val Loss 2823.55640
Epoch 22: Val Loss 2823.38037
Epoch 23: Val Loss 2823.19922
Epoch 24: Val Loss 2823.02393
Epoch 25: Val Loss 2822.85718
Epoch 26: Val Loss 2822.69263
Epoch 27: Val Loss 2822.52173
Epoch 28: Val Loss 2822.35522
Epoch 29: Val Loss 2822.15649
Epoch 30: Val Loss 2821.95825
Epoch 31: Val Loss 2821.74902
Epoch 32: Val Loss 2821.52710
Epoch 33: Val Loss 2821.28955
Epoch 34: Val Loss 2821.05664
Epoch 35: Val Loss 2820.80688
Epoch 36: Val Loss 2820.54980
Epoch 37: Val Loss 2820.26782
Epoch 38: Val Loss 2819.97949
Epoch 39: Val Loss 2819.67432
Epoch 40: Val Loss 2819.32837
Epoch 41: Val Loss 2818.94971
Epoch 42: Val Loss 2818.52100
Epoch 43: Val Loss 2818.06055
Epoch 44: Val Loss 2817.58740
Epoch 45: Val Loss 2817.10278
Epoch 46: Val Loss 2816.64062
Epoch 47: Val Loss 2816.17285
Epoch 48: Val Loss 2815.69360
Epoch 49: Val Loss 2815.21094
Epoch 50: Val Loss 2814.71313
Epoch 51: Val Loss 2814.21313
Epoch 52: Val Loss 2813.71094
Epoch 53: Val Loss 2813.16943
Epoch 54: Val Loss 2812.58398
Epoch 55: Val Loss 2812.02148
Epoch 56: Val Loss 2811.46851
Epoch 57: Val Loss 2810.87524
Epoch 58: Val Loss 2810.27295
Epoch 59: Val Loss 2809.61768
Epoch 60: Val Loss 2808.96973
Epoch 61: Val Loss 2808.30688
Epoch 62: Val Loss 2807.60913
Epoch 63: Val Loss 2806.85474
Epoch 64: Val Loss 2806.06079
Epoch 65: Val Loss 2805.24292
Epoch 66: Val Loss 2804.39209
Epoch 67: Val Loss 2803.50806
Epoch 68: Val Loss 2802.58203
Epoch 69: Val Loss 2801.59082
Epoch 70: Val Loss 2800.55005
Epoch 71: Val Loss 2799.46924
Epoch 72: Val Loss 2798.28760
Epoch 73: Val Loss 2797.06299
Epoch 74: Val Loss 2795.75659
Epoch 75: Val Loss 2794.36694
Epoch 76: Val Loss 2792.92310
Epoch 77: Val Loss 2791.43604
Epoch 78: Val Loss 2789.87354
Epoch 79: Val Loss 2788.18677
Epoch 80: Val Loss 2786.38062
Epoch 81: Val Loss 2784.50586
Epoch 82: Val Loss 2782.45557
Epoch 83: Val Loss 2780.45020
Epoch 84: Val Loss 2778.37451
Epoch 85: Val Loss 2776.27222
Epoch 86: Val Loss 2774.09253
Epoch 87: Val Loss 2771.80908
Epoch 88: Val Loss 2769.41187
Epoch 89: Val Loss 2766.89233
Epoch 90: Val Loss 2764.24658
Epoch 91: Val Loss 2761.52222
Epoch 92: Val Loss 2758.70728
Epoch 93: Val Loss 2755.69580
Epoch 94: Val Loss 2752.59424
Epoch 95: Val Loss 2749.43774
Epoch 96: Val Loss 2746.22021
Epoch 97: Val Loss 2742.88257
Epoch 98: Val Loss 2739.19849
Epoch 99: Val Loss 2735.43506
{'MSE - mean': 1670.7218211480376, 'MSE - std': 753.0648434214804, 'R2 - mean': -0.10401890984391622, 'R2 - std': 0.010319117945480323} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2121.89771
Epoch 1: Val Loss 2121.58984
Epoch 2: Val Loss 2121.27783
Epoch 3: Val Loss 2120.94751
Epoch 4: Val Loss 2120.60474
Epoch 5: Val Loss 2120.26099
Epoch 6: Val Loss 2119.89404
Epoch 7: Val Loss 2119.48804
Epoch 8: Val Loss 2119.08276
Epoch 9: Val Loss 2118.65918
Epoch 10: Val Loss 2118.18237
Epoch 11: Val Loss 2117.66553
Epoch 12: Val Loss 2117.11060
Epoch 13: Val Loss 2116.48340
Epoch 14: Val Loss 2115.84521
Epoch 15: Val Loss 2115.14160
Epoch 16: Val Loss 2114.37231
Epoch 17: Val Loss 2113.53662
Epoch 18: Val Loss 2112.71216
Epoch 19: Val Loss 2111.87793
Epoch 20: Val Loss 2111.01343
Epoch 21: Val Loss 2110.10034
Epoch 22: Val Loss 2109.16772
Epoch 23: Val Loss 2108.19067
Epoch 24: Val Loss 2107.14844
Epoch 25: Val Loss 2106.11963
Epoch 26: Val Loss 2105.12109
Epoch 27: Val Loss 2104.05713
Epoch 28: Val Loss 2102.98071
Epoch 29: Val Loss 2101.89331
Epoch 30: Val Loss 2100.77759
Epoch 31: Val Loss 2099.63770
Epoch 32: Val Loss 2098.48145
Epoch 33: Val Loss 2097.29736
Epoch 34: Val Loss 2096.04956
Epoch 35: Val Loss 2094.73584
Epoch 36: Val Loss 2093.37549
Epoch 37: Val Loss 2092.03027
Epoch 38: Val Loss 2090.70264
Epoch 39: Val Loss 2089.36035
Epoch 40: Val Loss 2087.91504
Epoch 41: Val Loss 2086.44434
Epoch 42: Val Loss 2084.89209
Epoch 43: Val Loss 2083.35986
Epoch 44: Val Loss 2081.79224
Epoch 45: Val Loss 2080.06323
Epoch 46: Val Loss 2078.33276
Epoch 47: Val Loss 2076.57300
Epoch 48: Val Loss 2074.86108
Epoch 49: Val Loss 2073.11108
Epoch 50: Val Loss 2071.36572
Epoch 51: Val Loss 2069.50928
Epoch 52: Val Loss 2067.44800
Epoch 53: Val Loss 2065.40186
Epoch 54: Val Loss 2063.31934
Epoch 55: Val Loss 2061.15039
Epoch 56: Val Loss 2058.87573
Epoch 57: Val Loss 2056.60352
Epoch 58: Val Loss 2054.31470
Epoch 59: Val Loss 2051.94580
Epoch 60: Val Loss 2049.38013
Epoch 61: Val Loss 2046.74414
Epoch 62: Val Loss 2043.99133
Epoch 63: Val Loss 2041.12280
Epoch 64: Val Loss 2038.43262
Epoch 65: Val Loss 2035.63147
Epoch 66: Val Loss 2032.57874
Epoch 67: Val Loss 2029.41541
Epoch 68: Val Loss 2026.28345
Epoch 69: Val Loss 2023.08032
Epoch 70: Val Loss 2019.89746
Epoch 71: Val Loss 2016.59033
Epoch 72: Val Loss 2012.89087
Epoch 73: Val Loss 2009.18103
Epoch 74: Val Loss 2005.38159
Epoch 75: Val Loss 2001.60071
Epoch 76: Val Loss 1997.74133
Epoch 77: Val Loss 1993.51953
Epoch 78: Val Loss 1989.49280
Epoch 79: Val Loss 1985.33203
Epoch 80: Val Loss 1981.19775
Epoch 81: Val Loss 1976.90308
Epoch 82: Val Loss 1972.09961
Epoch 83: Val Loss 1967.40076
Epoch 84: Val Loss 1962.81628
Epoch 85: Val Loss 1957.85364
Epoch 86: Val Loss 1953.02917
Epoch 87: Val Loss 1948.05127
Epoch 88: Val Loss 1943.04712
Epoch 89: Val Loss 1937.71472
Epoch 90: Val Loss 1932.54553
Epoch 91: Val Loss 1927.11609
Epoch 92: Val Loss 1921.54785
Epoch 93: Val Loss 1916.06409
Epoch 94: Val Loss 1910.62268
Epoch 95: Val Loss 1904.92969
Epoch 96: Val Loss 1898.95959
Epoch 97: Val Loss 1893.09448
Epoch 98: Val Loss 1887.30420
Epoch 99: Val Loss 1881.63000
{'MSE - mean': 1723.4488782837402, 'MSE - std': 658.5365756437824, 'R2 - mean': -0.10096787103584359, 'R2 - std': 0.010382179915829159} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2506.73999
Epoch 1: Val Loss 2506.49414
Epoch 2: Val Loss 2506.23999
Epoch 3: Val Loss 2505.99609
Epoch 4: Val Loss 2505.74292
Epoch 5: Val Loss 2505.48511
Epoch 6: Val Loss 2505.22192
Epoch 7: Val Loss 2504.94995
Epoch 8: Val Loss 2504.67896
Epoch 9: Val Loss 2504.40356
Epoch 10: Val Loss 2504.12012
Epoch 11: Val Loss 2503.82397
Epoch 12: Val Loss 2503.51074
Epoch 13: Val Loss 2503.19482
Epoch 14: Val Loss 2502.86987
Epoch 15: Val Loss 2502.53271
Epoch 16: Val Loss 2502.19263
Epoch 17: Val Loss 2501.82446
Epoch 18: Val Loss 2501.44312
Epoch 19: Val Loss 2501.06543
Epoch 20: Val Loss 2500.68286
Epoch 21: Val Loss 2500.28784
Epoch 22: Val Loss 2499.86768
Epoch 23: Val Loss 2499.42285
Epoch 24: Val Loss 2498.96558
Epoch 25: Val Loss 2498.47949
Epoch 26: Val Loss 2497.94971
Epoch 27: Val Loss 2497.39966
Epoch 28: Val Loss 2496.83276
Epoch 29: Val Loss 2496.24829
Epoch 30: Val Loss 2495.63208
Epoch 31: Val Loss 2494.98950
Epoch 32: Val Loss 2494.32520
Epoch 33: Val Loss 2493.64453
Epoch 34: Val Loss 2492.94971
Epoch 35: Val Loss 2492.20752
Epoch 36: Val Loss 2491.44751
Epoch 37: Val Loss 2490.64062
Epoch 38: Val Loss 2489.81299
Epoch 39: Val Loss 2488.96802
Epoch 40: Val Loss 2488.00317
Epoch 41: Val Loss 2487.04932
Epoch 42: Val Loss 2486.03198
Epoch 43: Val Loss 2484.95605
Epoch 44: Val Loss 2483.87280
Epoch 45: Val Loss 2482.78247
Epoch 46: Val Loss 2481.67603
Epoch 47: Val Loss 2480.52759
Epoch 48: Val Loss 2479.33936
Epoch 49: Val Loss 2478.06812
Epoch 50: Val Loss 2476.71265
Epoch 51: Val Loss 2475.35303
Epoch 52: Val Loss 2473.95679
Epoch 53: Val Loss 2472.49463
Epoch 54: Val Loss 2470.97998
Epoch 55: Val Loss 2469.47852
Epoch 56: Val Loss 2467.91479
Epoch 57: Val Loss 2466.30249
Epoch 58: Val Loss 2464.59912
Epoch 59: Val Loss 2462.93433
Epoch 60: Val Loss 2461.22070
Epoch 61: Val Loss 2459.40845
Epoch 62: Val Loss 2457.52051
Epoch 63: Val Loss 2455.65210
Epoch 64: Val Loss 2453.73340
Epoch 65: Val Loss 2451.56567
Epoch 66: Val Loss 2449.41455
Epoch 67: Val Loss 2447.21509
Epoch 68: Val Loss 2444.99463
Epoch 69: Val Loss 2442.67236
Epoch 70: Val Loss 2440.30298
Epoch 71: Val Loss 2437.96362
Epoch 72: Val Loss 2435.53418
Epoch 73: Val Loss 2432.93921
Epoch 74: Val Loss 2430.35596
Epoch 75: Val Loss 2427.74731
Epoch 76: Val Loss 2424.99146
Epoch 77: Val Loss 2422.08789
Epoch 78: Val Loss 2419.20483
Epoch 79: Val Loss 2416.25562
Epoch 80: Val Loss 2413.31567
Epoch 81: Val Loss 2410.24609
Epoch 82: Val Loss 2407.08862
Epoch 83: Val Loss 2403.90332
Epoch 84: Val Loss 2400.61475
Epoch 85: Val Loss 2397.27295
Epoch 86: Val Loss 2393.54663
Epoch 87: Val Loss 2389.88330
Epoch 88: Val Loss 2385.98779
Epoch 89: Val Loss 2382.02002
Epoch 90: Val Loss 2378.06055
Epoch 91: Val Loss 2374.04858
Epoch 92: Val Loss 2370.04980
Epoch 93: Val Loss 2365.88794
Epoch 94: Val Loss 2361.87939
Epoch 95: Val Loss 2357.82910
Epoch 96: Val Loss 2353.28052
Epoch 97: Val Loss 2348.63013
Epoch 98: Val Loss 2343.92847
Epoch 99: Val Loss 2339.40820
{'MSE - mean': 1846.6407526737917, 'MSE - std': 638.467923102992, 'R2 - mean': -0.09950472180724126, 'R2 - std': 0.009736269845498096} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 1846.6407526737917 and parameters: {'dim': 256, 'depth': 3, 'heads': 8, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 24 with value: 286.3899802356503.
Best parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.5}
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1294.26880
Epoch 1: Val Loss 1292.06946
Epoch 2: Val Loss 1289.41907
Epoch 3: Val Loss 1286.10791
Epoch 4: Val Loss 1281.91235
Epoch 5: Val Loss 1276.43884
Epoch 6: Val Loss 1269.52161
Epoch 7: Val Loss 1260.56531
Epoch 8: Val Loss 1248.80249
Epoch 9: Val Loss 1233.75159
Epoch 10: Val Loss 1215.15137
Epoch 11: Val Loss 1192.33752
Epoch 12: Val Loss 1165.51428
Epoch 13: Val Loss 1132.94690
Epoch 14: Val Loss 1094.64221
Epoch 15: Val Loss 1051.40930
Epoch 16: Val Loss 1003.34161
Epoch 17: Val Loss 952.80780
Epoch 18: Val Loss 898.60986
Epoch 19: Val Loss 845.46948
Epoch 20: Val Loss 796.04388
Epoch 21: Val Loss 754.48529
Epoch 22: Val Loss 722.32880
Epoch 23: Val Loss 694.23596
Epoch 24: Val Loss 670.67023
Epoch 25: Val Loss 648.01837
Epoch 26: Val Loss 623.79407
Epoch 27: Val Loss 597.34503
Epoch 28: Val Loss 571.58453
Epoch 29: Val Loss 546.53442
Epoch 30: Val Loss 520.16138
Epoch 31: Val Loss 494.50479
Epoch 32: Val Loss 469.11887
Epoch 33: Val Loss 445.72293
Epoch 34: Val Loss 422.87640
Epoch 35: Val Loss 402.16205
Epoch 36: Val Loss 382.81958
Epoch 37: Val Loss 364.22437
Epoch 38: Val Loss 346.61136
Epoch 39: Val Loss 331.93753
Epoch 40: Val Loss 318.86591
Epoch 41: Val Loss 308.22433
Epoch 42: Val Loss 298.80203
Epoch 43: Val Loss 289.06903
Epoch 44: Val Loss 279.29926
Epoch 45: Val Loss 275.02396
Epoch 46: Val Loss 269.22241
Epoch 47: Val Loss 264.74094
Epoch 48: Val Loss 256.11444
Epoch 49: Val Loss 249.64163
Epoch 50: Val Loss 242.65738
Epoch 51: Val Loss 235.29988
Epoch 52: Val Loss 236.30692
Epoch 53: Val Loss 229.35754
Epoch 54: Val Loss 221.45522
Epoch 55: Val Loss 215.43745
Epoch 56: Val Loss 211.95557
Epoch 57: Val Loss 209.52188
Epoch 58: Val Loss 209.48367
Epoch 59: Val Loss 212.67632
Epoch 60: Val Loss 208.15608
Epoch 61: Val Loss 202.37068
Epoch 62: Val Loss 199.58876
Epoch 63: Val Loss 198.27794
Epoch 64: Val Loss 196.33347
Epoch 65: Val Loss 196.44037
Epoch 66: Val Loss 195.31680
Epoch 67: Val Loss 195.99297
Epoch 68: Val Loss 196.26619
Epoch 69: Val Loss 197.24875
Epoch 70: Val Loss 195.18471
Epoch 71: Val Loss 192.71423
Epoch 72: Val Loss 189.93251
Epoch 73: Val Loss 188.61813
Epoch 74: Val Loss 187.52885
Epoch 75: Val Loss 187.85008
Epoch 76: Val Loss 192.21124
Epoch 77: Val Loss 191.53337
Epoch 78: Val Loss 190.47484
Epoch 79: Val Loss 189.97777
Epoch 80: Val Loss 187.86989
Epoch 81: Val Loss 187.14032
Epoch 82: Val Loss 187.20480
Epoch 83: Val Loss 186.15022
Epoch 84: Val Loss 185.00272
Epoch 85: Val Loss 184.43370
Epoch 86: Val Loss 184.54266
Epoch 87: Val Loss 183.23167
Epoch 88: Val Loss 180.86160
Epoch 89: Val Loss 179.39964
Epoch 90: Val Loss 179.13155
Epoch 91: Val Loss 177.90709
Epoch 92: Val Loss 177.98860
Epoch 93: Val Loss 177.79459
Epoch 94: Val Loss 178.08064
Epoch 95: Val Loss 177.53645
Epoch 96: Val Loss 176.39255
Epoch 97: Val Loss 176.81348
Epoch 98: Val Loss 173.66599
Epoch 99: Val Loss 172.05122
Saved Losses
{'MSE - mean': 172.05122054267764, 'MSE - std': 0.0, 'R2 - mean': 0.8319383139644878, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1306.99536
Epoch 1: Val Loss 1303.35828
Epoch 2: Val Loss 1298.80591
Epoch 3: Val Loss 1293.19617
Epoch 4: Val Loss 1286.22375
Epoch 5: Val Loss 1278.13135
Epoch 6: Val Loss 1268.06738
Epoch 7: Val Loss 1255.11255
Epoch 8: Val Loss 1238.55530
Epoch 9: Val Loss 1218.10950
Epoch 10: Val Loss 1192.59143
Epoch 11: Val Loss 1162.58008
Epoch 12: Val Loss 1126.37085
Epoch 13: Val Loss 1083.25134
Epoch 14: Val Loss 1033.99548
Epoch 15: Val Loss 983.89746
Epoch 16: Val Loss 930.59619
Epoch 17: Val Loss 877.99390
Epoch 18: Val Loss 829.03461
Epoch 19: Val Loss 787.42651
Epoch 20: Val Loss 752.55298
Epoch 21: Val Loss 723.09320
Epoch 22: Val Loss 699.82672
Epoch 23: Val Loss 678.52002
Epoch 24: Val Loss 655.47144
Epoch 25: Val Loss 630.40271
Epoch 26: Val Loss 605.73169
Epoch 27: Val Loss 582.89661
Epoch 28: Val Loss 556.92480
Epoch 29: Val Loss 533.10986
Epoch 30: Val Loss 511.91501
Epoch 31: Val Loss 492.39053
Epoch 32: Val Loss 472.21194
Epoch 33: Val Loss 453.70813
Epoch 34: Val Loss 442.75919
Epoch 35: Val Loss 432.48816
Epoch 36: Val Loss 416.54678
Epoch 37: Val Loss 409.29010
Epoch 38: Val Loss 396.62756
Epoch 39: Val Loss 386.78586
Epoch 40: Val Loss 374.86520
Epoch 41: Val Loss 362.71695
Epoch 42: Val Loss 351.79764
Epoch 43: Val Loss 348.43848
Epoch 44: Val Loss 343.50696
Epoch 45: Val Loss 355.67664
Epoch 46: Val Loss 352.48834
Epoch 47: Val Loss 355.05585
Epoch 48: Val Loss 348.23099
Epoch 49: Val Loss 337.02243
Epoch 50: Val Loss 333.24945
Epoch 51: Val Loss 331.08594
Epoch 52: Val Loss 329.51123
Epoch 53: Val Loss 329.03677
Epoch 54: Val Loss 326.18320
Epoch 55: Val Loss 323.98407
Epoch 56: Val Loss 320.52441
Epoch 57: Val Loss 319.02740
Epoch 58: Val Loss 321.32358
Epoch 59: Val Loss 316.31470
Epoch 60: Val Loss 317.25592
Epoch 61: Val Loss 325.18213
Epoch 62: Val Loss 324.10626
Epoch 63: Val Loss 320.55838
Epoch 64: Val Loss 314.32895
Epoch 65: Val Loss 315.71207
Epoch 66: Val Loss 307.03870
Epoch 67: Val Loss 318.97174
Epoch 68: Val Loss 327.25418
Epoch 69: Val Loss 312.26263
Epoch 70: Val Loss 308.25323
Epoch 71: Val Loss 317.99182
Epoch 72: Val Loss 312.57730
Epoch 73: Val Loss 317.75790
Epoch 74: Val Loss 307.78525
Epoch 75: Val Loss 302.74936
Epoch 76: Val Loss 299.15897
Epoch 77: Val Loss 313.98849
Epoch 78: Val Loss 322.53351
Epoch 79: Val Loss 318.32291
Epoch 80: Val Loss 307.55450
Epoch 81: Val Loss 305.04630
Epoch 82: Val Loss 297.73251
Epoch 83: Val Loss 292.72437
Epoch 84: Val Loss 295.94501
Epoch 85: Val Loss 307.90051
Epoch 86: Val Loss 314.64774
Epoch 87: Val Loss 306.64746
Epoch 88: Val Loss 304.20056
Epoch 89: Val Loss 292.08340
Epoch 90: Val Loss 282.94266
Epoch 91: Val Loss 281.77588
Epoch 92: Val Loss 284.76285
Epoch 93: Val Loss 282.65475
Epoch 94: Val Loss 283.43393
Epoch 95: Val Loss 276.48764
Epoch 96: Val Loss 267.53085
Epoch 97: Val Loss 271.64755
Epoch 98: Val Loss 275.94302
Epoch 99: Val Loss 281.56653
Saved Losses
{'MSE - mean': 219.79102787776247, 'MSE - std': 47.73980733508483, 'R2 - mean': 0.7885828463755097, 'R2 - std': 0.043355467588978125} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2827.51758
Epoch 1: Val Loss 2823.34009
Epoch 2: Val Loss 2818.96362
Epoch 3: Val Loss 2813.29663
Epoch 4: Val Loss 2804.87549
Epoch 5: Val Loss 2793.11182
Epoch 6: Val Loss 2776.85107
Epoch 7: Val Loss 2755.60718
Epoch 8: Val Loss 2727.19409
Epoch 9: Val Loss 2691.14111
Epoch 10: Val Loss 2646.47656
Epoch 11: Val Loss 2588.48999
Epoch 12: Val Loss 2522.25269
Epoch 13: Val Loss 2443.87158
Epoch 14: Val Loss 2352.92700
Epoch 15: Val Loss 2258.54663
Epoch 16: Val Loss 2157.65430
Epoch 17: Val Loss 2067.54077
Epoch 18: Val Loss 1979.96875
Epoch 19: Val Loss 1892.97131
Epoch 20: Val Loss 1821.59094
Epoch 21: Val Loss 1763.80493
Epoch 22: Val Loss 1712.52527
Epoch 23: Val Loss 1653.34802
Epoch 24: Val Loss 1599.00024
Epoch 25: Val Loss 1550.77930
Epoch 26: Val Loss 1499.49475
Epoch 27: Val Loss 1450.48608
Epoch 28: Val Loss 1394.19641
Epoch 29: Val Loss 1345.05151
Epoch 30: Val Loss 1306.71704
Epoch 31: Val Loss 1265.06885
Epoch 32: Val Loss 1222.34644
Epoch 33: Val Loss 1186.86914
Epoch 34: Val Loss 1151.78760
Epoch 35: Val Loss 1112.18530
Epoch 36: Val Loss 1075.95093
Epoch 37: Val Loss 1044.95447
Epoch 38: Val Loss 1016.93481
Epoch 39: Val Loss 988.22351
Epoch 40: Val Loss 947.44550
Epoch 41: Val Loss 922.10223
Epoch 42: Val Loss 909.62598
Epoch 43: Val Loss 892.50049
Epoch 44: Val Loss 877.02167
Epoch 45: Val Loss 857.75311
Epoch 46: Val Loss 831.97418
Epoch 47: Val Loss 812.06390
Epoch 48: Val Loss 795.17426
Epoch 49: Val Loss 781.57257
Epoch 50: Val Loss 765.54767
Epoch 51: Val Loss 753.15735
Epoch 52: Val Loss 740.89319
Epoch 53: Val Loss 724.63257
Epoch 54: Val Loss 699.11798
Epoch 55: Val Loss 690.87384
Epoch 56: Val Loss 685.85956
Epoch 57: Val Loss 686.10889
Epoch 58: Val Loss 683.11865
Epoch 59: Val Loss 667.83807
Epoch 60: Val Loss 652.11646
Epoch 61: Val Loss 644.43951
Epoch 62: Val Loss 632.65704
Epoch 63: Val Loss 629.07562
Epoch 64: Val Loss 611.84949
Epoch 65: Val Loss 602.43384
Epoch 66: Val Loss 594.98108
Epoch 67: Val Loss 595.37213
Epoch 68: Val Loss 587.54926
Epoch 69: Val Loss 575.60901
Epoch 70: Val Loss 569.66412
Epoch 71: Val Loss 558.12488
Epoch 72: Val Loss 559.28571
Epoch 73: Val Loss 558.71637
Epoch 74: Val Loss 563.26038
Epoch 75: Val Loss 556.73840
Epoch 76: Val Loss 547.89008
Epoch 77: Val Loss 542.10852
Epoch 78: Val Loss 530.92352
Epoch 79: Val Loss 530.82153
Epoch 80: Val Loss 529.63672
Epoch 81: Val Loss 529.62854
Epoch 82: Val Loss 532.00830
Epoch 83: Val Loss 520.56750
Epoch 84: Val Loss 515.26947
Epoch 85: Val Loss 508.26447
Epoch 86: Val Loss 498.65219
Epoch 87: Val Loss 495.39490
Epoch 88: Val Loss 481.92249
Epoch 89: Val Loss 480.73138
Epoch 90: Val Loss 483.39905
Epoch 91: Val Loss 465.17148
Epoch 92: Val Loss 452.87332
Epoch 93: Val Loss 453.08957
Epoch 94: Val Loss 463.60406
Epoch 95: Val Loss 474.90915
Epoch 96: Val Loss 470.90894
Epoch 97: Val Loss 457.07095
Epoch 98: Val Loss 453.22504
Epoch 99: Val Loss 438.37628
Saved Losses
{'MSE - mean': 292.6527659819091, 'MSE - std': 110.16831923035701, 'R2 - mean': 0.7994108518687018, 'R2 - std': 0.038569709001933294} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2129.95776
Epoch 1: Val Loss 2126.92090
Epoch 2: Val Loss 2123.28052
Epoch 3: Val Loss 2118.80786
Epoch 4: Val Loss 2113.41553
Epoch 5: Val Loss 2106.27686
Epoch 6: Val Loss 2096.80859
Epoch 7: Val Loss 2084.49609
Epoch 8: Val Loss 2068.95850
Epoch 9: Val Loss 2048.85303
Epoch 10: Val Loss 2023.47034
Epoch 11: Val Loss 1990.68018
Epoch 12: Val Loss 1949.08655
Epoch 13: Val Loss 1900.34167
Epoch 14: Val Loss 1841.66797
Epoch 15: Val Loss 1769.37585
Epoch 16: Val Loss 1690.09839
Epoch 17: Val Loss 1606.18701
Epoch 18: Val Loss 1521.37122
Epoch 19: Val Loss 1441.30542
Epoch 20: Val Loss 1365.84998
Epoch 21: Val Loss 1298.66992
Epoch 22: Val Loss 1239.32324
Epoch 23: Val Loss 1185.33862
Epoch 24: Val Loss 1132.00391
Epoch 25: Val Loss 1080.71350
Epoch 26: Val Loss 1030.85681
Epoch 27: Val Loss 982.24561
Epoch 28: Val Loss 931.03009
Epoch 29: Val Loss 883.73944
Epoch 30: Val Loss 840.61945
Epoch 31: Val Loss 802.56573
Epoch 32: Val Loss 769.15924
Epoch 33: Val Loss 738.32562
Epoch 34: Val Loss 710.65448
Epoch 35: Val Loss 682.52100
Epoch 36: Val Loss 657.78149
Epoch 37: Val Loss 636.39514
Epoch 38: Val Loss 616.33276
Epoch 39: Val Loss 598.21320
Epoch 40: Val Loss 584.13312
Epoch 41: Val Loss 571.25269
Epoch 42: Val Loss 557.40479
Epoch 43: Val Loss 544.11877
Epoch 44: Val Loss 534.29688
Epoch 45: Val Loss 526.32428
Epoch 46: Val Loss 513.23413
Epoch 47: Val Loss 502.89539
Epoch 48: Val Loss 495.34076
Epoch 49: Val Loss 482.54803
Epoch 50: Val Loss 472.07120
Epoch 51: Val Loss 467.42838
Epoch 52: Val Loss 464.34674
Epoch 53: Val Loss 458.60321
Epoch 54: Val Loss 450.65421
Epoch 55: Val Loss 442.64719
Epoch 56: Val Loss 434.86719
Epoch 57: Val Loss 428.82318
Epoch 58: Val Loss 423.92587
Epoch 59: Val Loss 418.36884
Epoch 60: Val Loss 413.97321
Epoch 61: Val Loss 410.30887
Epoch 62: Val Loss 410.90790
Epoch 63: Val Loss 407.17978
Epoch 64: Val Loss 401.48135
Epoch 65: Val Loss 400.32977
Epoch 66: Val Loss 393.23029
Epoch 67: Val Loss 389.85571
Epoch 68: Val Loss 381.40903
Epoch 69: Val Loss 379.19550
Epoch 70: Val Loss 378.33932
Epoch 71: Val Loss 375.62744
Epoch 72: Val Loss 375.54965
Epoch 73: Val Loss 375.75708
Epoch 74: Val Loss 366.65158
Epoch 75: Val Loss 362.27698
Epoch 76: Val Loss 361.00897
Epoch 77: Val Loss 357.76559
Epoch 78: Val Loss 356.35443
Epoch 79: Val Loss 354.74081
Epoch 80: Val Loss 352.35931
Epoch 81: Val Loss 352.94211
Epoch 82: Val Loss 346.15176
Epoch 83: Val Loss 341.48694
Epoch 84: Val Loss 338.98309
Epoch 85: Val Loss 341.92154
Epoch 86: Val Loss 337.71408
Epoch 87: Val Loss 334.65799
Epoch 88: Val Loss 339.65613
Epoch 89: Val Loss 342.22870
Epoch 90: Val Loss 340.31857
Epoch 91: Val Loss 332.05243
Epoch 92: Val Loss 324.94583
Epoch 93: Val Loss 324.22415
Epoch 94: Val Loss 324.08743
Epoch 95: Val Loss 322.84888
Epoch 96: Val Loss 322.46887
Epoch 97: Val Loss 320.12201
Epoch 98: Val Loss 316.70151
Epoch 99: Val Loss 322.26651
Saved Losses
{'MSE - mean': 298.66495399745355, 'MSE - std': 95.97516937069794, 'R2 - mean': 0.8036166704570191, 'R2 - std': 0.03418747679141983} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 2507.03003
Epoch 1: Val Loss 2505.46094
Epoch 2: Val Loss 2503.50806
Epoch 3: Val Loss 2500.94800
Epoch 4: Val Loss 2497.15576
Epoch 5: Val Loss 2491.62231
Epoch 6: Val Loss 2483.37891
Epoch 7: Val Loss 2470.18408
Epoch 8: Val Loss 2449.67993
Epoch 9: Val Loss 2421.15674
Epoch 10: Val Loss 2382.22241
Epoch 11: Val Loss 2333.09253
Epoch 12: Val Loss 2271.30078
Epoch 13: Val Loss 2196.82690
Epoch 14: Val Loss 2114.93237
Epoch 15: Val Loss 2028.45667
Epoch 16: Val Loss 1938.70483
Epoch 17: Val Loss 1848.12402
Epoch 18: Val Loss 1761.96460
Epoch 19: Val Loss 1692.94458
Epoch 20: Val Loss 1637.04675
Epoch 21: Val Loss 1583.85876
Epoch 22: Val Loss 1525.49915
Epoch 23: Val Loss 1475.05334
Epoch 24: Val Loss 1432.21289
Epoch 25: Val Loss 1394.71838
Epoch 26: Val Loss 1350.38013
Epoch 27: Val Loss 1308.69983
Epoch 28: Val Loss 1261.97681
Epoch 29: Val Loss 1216.43542
Epoch 30: Val Loss 1178.07153
Epoch 31: Val Loss 1142.60388
Epoch 32: Val Loss 1105.60303
Epoch 33: Val Loss 1069.90540
Epoch 34: Val Loss 1041.25354
Epoch 35: Val Loss 1004.72021
Epoch 36: Val Loss 977.65027
Epoch 37: Val Loss 937.13043
Epoch 38: Val Loss 911.71442
Epoch 39: Val Loss 890.39581
Epoch 40: Val Loss 860.58917
Epoch 41: Val Loss 843.09772
Epoch 42: Val Loss 820.55603
Epoch 43: Val Loss 794.02936
Epoch 44: Val Loss 771.57318
Epoch 45: Val Loss 758.43127
Epoch 46: Val Loss 746.24976
Epoch 47: Val Loss 726.38354
Epoch 48: Val Loss 709.39203
Epoch 49: Val Loss 696.30792
Epoch 50: Val Loss 688.60901
Epoch 51: Val Loss 679.66992
Epoch 52: Val Loss 661.64557
Epoch 53: Val Loss 647.64691
Epoch 54: Val Loss 640.32233
Epoch 55: Val Loss 629.31543
Epoch 56: Val Loss 612.29260
Epoch 57: Val Loss 608.36517
Epoch 58: Val Loss 603.80621
Epoch 59: Val Loss 601.76996
Epoch 60: Val Loss 593.96521
Epoch 61: Val Loss 579.99109
Epoch 62: Val Loss 578.08173
Epoch 63: Val Loss 573.72888
Epoch 64: Val Loss 566.52930
Epoch 65: Val Loss 559.75476
Epoch 66: Val Loss 553.45807
Epoch 67: Val Loss 536.15216
Epoch 68: Val Loss 527.75610
Epoch 69: Val Loss 526.60645
Epoch 70: Val Loss 531.11584
Epoch 71: Val Loss 528.72321
Epoch 72: Val Loss 522.61957
Epoch 73: Val Loss 522.72540
Epoch 74: Val Loss 519.75513
Epoch 75: Val Loss 504.75534
Epoch 76: Val Loss 497.23166
Epoch 77: Val Loss 495.19980
Epoch 78: Val Loss 493.68716
Epoch 79: Val Loss 496.33481
Epoch 80: Val Loss 495.53241
Epoch 81: Val Loss 484.95023
Epoch 82: Val Loss 477.00607
Epoch 83: Val Loss 477.28226
Epoch 84: Val Loss 470.69992
Epoch 85: Val Loss 464.86737
Epoch 86: Val Loss 463.06299
Epoch 87: Val Loss 457.30722
Epoch 88: Val Loss 457.50052
Epoch 89: Val Loss 455.53122
Epoch 90: Val Loss 455.80032
Epoch 91: Val Loss 450.69357
Epoch 92: Val Loss 446.35376
Epoch 93: Val Loss 440.89563
Epoch 94: Val Loss 437.94370
Epoch 95: Val Loss 434.16873
Epoch 96: Val Loss 433.01779
Epoch 97: Val Loss 427.29031
Epoch 98: Val Loss 424.75262
Epoch 99: Val Loss 424.07690
Saved Losses
{'MSE - mean': 323.7473406374253, 'MSE - std': 99.42580644287203, 'R2 - mean': 0.8032429171833382, 'R2 - std': 0.030587344139847195} 
 

Saving model.....
Results After CV: {'MSE - mean': 323.7473406374253, 'MSE - std': 99.42580644287203, 'R2 - mean': 0.8032429171833382, 'R2 - std': 0.030587344139847195}
Train time: 45.85031166440003
Inference time: 0.09435575780007639
Finished cross validation


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/sensory.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/sensory.yml', data_parallel=False, dataset='Sensory', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], num_classes=1, num_features=11, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=False, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Sensory...
Dataset loaded! 

X b4 encoding : [1 1 1 1 1 1 3 3 1 2 1] 

(576, 11)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Cat Idx Part II: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 
ENDE 
 

X after Nominal Encoding: [1 1 1 1 1 1 3 3 1 2 1] 
 

X after Scaling: [1 1 1 1 1 1 3 3 1 2 1] 
 

One Hot Encoding...
X after One Hot Encoding: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.] 
 

args.num_features: 36
args.cat_idx: None
Cat Dims: []
New Shape: (576, 36)
False 
 

Using an existing study with name 'TabTransformer_Sensory' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 228.77576
Epoch 1: Val Loss 228.77518
Epoch 2: Val Loss 228.77458
Epoch 3: Val Loss 228.77399
Epoch 4: Val Loss 228.77341
Epoch 5: Val Loss 228.77281
Epoch 6: Val Loss 228.77222
Epoch 7: Val Loss 228.77162
Epoch 8: Val Loss 228.77104
Epoch 9: Val Loss 228.77045
Epoch 10: Val Loss 228.76987
Epoch 11: Val Loss 228.76927
Epoch 12: Val Loss 228.76869
Epoch 13: Val Loss 228.76808
Epoch 14: Val Loss 228.76752
Epoch 15: Val Loss 228.76691
Epoch 16: Val Loss 228.76633
Epoch 17: Val Loss 228.76573
Epoch 18: Val Loss 228.76514
Epoch 19: Val Loss 228.76454
Epoch 20: Val Loss 228.76398
Epoch 21: Val Loss 228.76337
Epoch 22: Val Loss 228.76277
Epoch 23: Val Loss 228.76219
Epoch 24: Val Loss 228.76160
Epoch 25: Val Loss 228.76102
Epoch 26: Val Loss 228.76042
Epoch 27: Val Loss 228.75983
Epoch 28: Val Loss 228.75926
Epoch 29: Val Loss 228.75865
Epoch 30: Val Loss 228.75809
Epoch 31: Val Loss 228.75748
Epoch 32: Val Loss 228.75690
Epoch 33: Val Loss 228.75630
Epoch 34: Val Loss 228.75572
Epoch 35: Val Loss 228.75513
Epoch 36: Val Loss 228.75455
Epoch 37: Val Loss 228.75394
Epoch 38: Val Loss 228.75337
Epoch 39: Val Loss 228.75276
Epoch 40: Val Loss 228.75218
Epoch 41: Val Loss 228.75160
Epoch 42: Val Loss 228.75101
Epoch 43: Val Loss 228.75040
Epoch 44: Val Loss 228.74983
Epoch 45: Val Loss 228.74924
Epoch 46: Val Loss 228.74866
Epoch 47: Val Loss 228.74806
Epoch 48: Val Loss 228.74747
Epoch 49: Val Loss 228.74689
Epoch 50: Val Loss 228.74629
Epoch 51: Val Loss 228.74568
Epoch 52: Val Loss 228.74512
Epoch 53: Val Loss 228.74452
Epoch 54: Val Loss 228.74394
Epoch 55: Val Loss 228.74336
Epoch 56: Val Loss 228.74275
Epoch 57: Val Loss 228.74217
Epoch 58: Val Loss 228.74158
Epoch 59: Val Loss 228.74100
Epoch 60: Val Loss 228.74040
Epoch 61: Val Loss 228.73982
Epoch 62: Val Loss 228.73924
Epoch 63: Val Loss 228.73865
Epoch 64: Val Loss 228.73807
Epoch 65: Val Loss 228.73747
Epoch 66: Val Loss 228.73688
Epoch 67: Val Loss 228.73630
Epoch 68: Val Loss 228.73570
Epoch 69: Val Loss 228.73511
Epoch 70: Val Loss 228.73454
Epoch 71: Val Loss 228.73393
Epoch 72: Val Loss 228.73337
Epoch 73: Val Loss 228.73279
Epoch 74: Val Loss 228.73218
Epoch 75: Val Loss 228.73161
Epoch 76: Val Loss 228.73100
Epoch 77: Val Loss 228.73044
Epoch 78: Val Loss 228.72983
Epoch 79: Val Loss 228.72925
Epoch 80: Val Loss 228.72868
Epoch 81: Val Loss 228.72807
Epoch 82: Val Loss 228.72749
Epoch 83: Val Loss 228.72691
Epoch 84: Val Loss 228.72632
Epoch 85: Val Loss 228.72575
Epoch 86: Val Loss 228.72514
Epoch 87: Val Loss 228.72458
Epoch 88: Val Loss 228.72398
Epoch 89: Val Loss 228.72339
Epoch 90: Val Loss 228.72281
Epoch 91: Val Loss 228.72221
Epoch 92: Val Loss 228.72165
Epoch 93: Val Loss 228.72105
Epoch 94: Val Loss 228.72046
Epoch 95: Val Loss 228.71986
Epoch 96: Val Loss 228.71928
Epoch 97: Val Loss 228.71870
Epoch 98: Val Loss 228.71811
Epoch 99: Val Loss 228.71753
{'MSE - mean': 228.71752926806465, 'MSE - std': 0.0, 'R2 - mean': -415.19028010833057, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.00319
Epoch 1: Val Loss 229.00220
Epoch 2: Val Loss 229.00121
Epoch 3: Val Loss 229.00024
Epoch 4: Val Loss 228.99928
Epoch 5: Val Loss 228.99829
Epoch 6: Val Loss 228.99730
Epoch 7: Val Loss 228.99635
Epoch 8: Val Loss 228.99535
Epoch 9: Val Loss 228.99438
Epoch 10: Val Loss 228.99342
Epoch 11: Val Loss 228.99242
Epoch 12: Val Loss 228.99147
Epoch 13: Val Loss 228.99048
Epoch 14: Val Loss 228.98952
Epoch 15: Val Loss 228.98854
Epoch 16: Val Loss 228.98756
Epoch 17: Val Loss 228.98657
Epoch 18: Val Loss 228.98561
Epoch 19: Val Loss 228.98462
Epoch 20: Val Loss 228.98366
Epoch 21: Val Loss 228.98270
Epoch 22: Val Loss 228.98170
Epoch 23: Val Loss 228.98073
Epoch 24: Val Loss 228.97975
Epoch 25: Val Loss 228.97879
Epoch 26: Val Loss 228.97781
Epoch 27: Val Loss 228.97682
Epoch 28: Val Loss 228.97586
Epoch 29: Val Loss 228.97488
Epoch 30: Val Loss 228.97392
Epoch 31: Val Loss 228.97292
Epoch 32: Val Loss 228.97197
Epoch 33: Val Loss 228.97101
Epoch 34: Val Loss 228.97003
Epoch 35: Val Loss 228.96904
Epoch 36: Val Loss 228.96808
Epoch 37: Val Loss 228.96713
Epoch 38: Val Loss 228.96614
Epoch 39: Val Loss 228.96518
Epoch 40: Val Loss 228.96420
Epoch 41: Val Loss 228.96324
Epoch 42: Val Loss 228.96225
Epoch 43: Val Loss 228.96129
Epoch 44: Val Loss 228.96031
Epoch 45: Val Loss 228.95937
Epoch 46: Val Loss 228.95837
Epoch 47: Val Loss 228.95743
Epoch 48: Val Loss 228.95644
Epoch 49: Val Loss 228.95547
Epoch 50: Val Loss 228.95451
Epoch 51: Val Loss 228.95355
Epoch 52: Val Loss 228.95258
Epoch 53: Val Loss 228.95160
Epoch 54: Val Loss 228.95064
Epoch 55: Val Loss 228.94966
Epoch 56: Val Loss 228.94870
Epoch 57: Val Loss 228.94775
Epoch 58: Val Loss 228.94678
Epoch 59: Val Loss 228.94582
Epoch 60: Val Loss 228.94482
Epoch 61: Val Loss 228.94388
Epoch 62: Val Loss 228.94292
Epoch 63: Val Loss 228.94194
Epoch 64: Val Loss 228.94099
Epoch 65: Val Loss 228.94002
Epoch 66: Val Loss 228.93906
Epoch 67: Val Loss 228.93810
Epoch 68: Val Loss 228.93712
Epoch 69: Val Loss 228.93616
Epoch 70: Val Loss 228.93521
Epoch 71: Val Loss 228.93425
Epoch 72: Val Loss 228.93327
Epoch 73: Val Loss 228.93231
Epoch 74: Val Loss 228.93137
Epoch 75: Val Loss 228.93039
Epoch 76: Val Loss 228.92944
Epoch 77: Val Loss 228.92848
Epoch 78: Val Loss 228.92751
Epoch 79: Val Loss 228.92656
Epoch 80: Val Loss 228.92558
Epoch 81: Val Loss 228.92462
Epoch 82: Val Loss 228.92365
Epoch 83: Val Loss 228.92268
Epoch 84: Val Loss 228.92172
Epoch 85: Val Loss 228.92075
Epoch 86: Val Loss 228.91980
Epoch 87: Val Loss 228.91881
Epoch 88: Val Loss 228.91786
Epoch 89: Val Loss 228.91687
Epoch 90: Val Loss 228.91592
Epoch 91: Val Loss 228.91496
Epoch 92: Val Loss 228.91399
Epoch 93: Val Loss 228.91304
Epoch 94: Val Loss 228.91206
Epoch 95: Val Loss 228.91110
Epoch 96: Val Loss 228.91013
Epoch 97: Val Loss 228.90918
Epoch 98: Val Loss 228.90817
Epoch 99: Val Loss 228.90723
{'MSE - mean': 228.81238241929344, 'MSE - std': 0.09485315122880422, 'R2 - mean': -362.08685572179564, 'R2 - std': 53.103424386534925} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.61548
Epoch 1: Val Loss 229.61372
Epoch 2: Val Loss 229.61198
Epoch 3: Val Loss 229.61024
Epoch 4: Val Loss 229.60851
Epoch 5: Val Loss 229.60675
Epoch 6: Val Loss 229.60501
Epoch 7: Val Loss 229.60329
Epoch 8: Val Loss 229.60153
Epoch 9: Val Loss 229.59979
Epoch 10: Val Loss 229.59805
Epoch 11: Val Loss 229.59631
Epoch 12: Val Loss 229.59457
Epoch 13: Val Loss 229.59282
Epoch 14: Val Loss 229.59109
Epoch 15: Val Loss 229.58936
Epoch 16: Val Loss 229.58763
Epoch 17: Val Loss 229.58589
Epoch 18: Val Loss 229.58417
Epoch 19: Val Loss 229.58244
Epoch 20: Val Loss 229.58069
Epoch 21: Val Loss 229.57898
Epoch 22: Val Loss 229.57727
Epoch 23: Val Loss 229.57552
Epoch 24: Val Loss 229.57381
Epoch 25: Val Loss 229.57207
Epoch 26: Val Loss 229.57031
Epoch 27: Val Loss 229.56859
Epoch 28: Val Loss 229.56683
Epoch 29: Val Loss 229.56511
Epoch 30: Val Loss 229.56339
Epoch 31: Val Loss 229.56166
Epoch 32: Val Loss 229.55991
Epoch 33: Val Loss 229.55818
Epoch 34: Val Loss 229.55644
Epoch 35: Val Loss 229.55469
Epoch 36: Val Loss 229.55295
Epoch 37: Val Loss 229.55122
Epoch 38: Val Loss 229.54948
Epoch 39: Val Loss 229.54774
Epoch 40: Val Loss 229.54602
Epoch 41: Val Loss 229.54428
Epoch 42: Val Loss 229.54253
Epoch 43: Val Loss 229.54079
Epoch 44: Val Loss 229.53906
Epoch 45: Val Loss 229.53734
Epoch 46: Val Loss 229.53558
Epoch 47: Val Loss 229.53386
Epoch 48: Val Loss 229.53212
Epoch 49: Val Loss 229.53040
Epoch 50: Val Loss 229.52866
Epoch 51: Val Loss 229.52693
Epoch 52: Val Loss 229.52519
Epoch 53: Val Loss 229.52347
Epoch 54: Val Loss 229.52171
Epoch 55: Val Loss 229.51997
Epoch 56: Val Loss 229.51823
Epoch 57: Val Loss 229.51649
Epoch 58: Val Loss 229.51477
Epoch 59: Val Loss 229.51303
Epoch 60: Val Loss 229.51131
Epoch 61: Val Loss 229.50957
Epoch 62: Val Loss 229.50781
Epoch 63: Val Loss 229.50609
Epoch 64: Val Loss 229.50436
Epoch 65: Val Loss 229.50262
Epoch 66: Val Loss 229.50089
Epoch 67: Val Loss 229.49916
Epoch 68: Val Loss 229.49744
Epoch 69: Val Loss 229.49570
Epoch 70: Val Loss 229.49394
Epoch 71: Val Loss 229.49219
Epoch 72: Val Loss 229.49046
Epoch 73: Val Loss 229.48874
Epoch 74: Val Loss 229.48701
Epoch 75: Val Loss 229.48528
Epoch 76: Val Loss 229.48357
Epoch 77: Val Loss 229.48181
Epoch 78: Val Loss 229.48012
Epoch 79: Val Loss 229.47839
Epoch 80: Val Loss 229.47665
Epoch 81: Val Loss 229.47491
Epoch 82: Val Loss 229.47321
Epoch 83: Val Loss 229.47148
Epoch 84: Val Loss 229.46974
Epoch 85: Val Loss 229.46803
Epoch 86: Val Loss 229.46629
Epoch 87: Val Loss 229.46457
Epoch 88: Val Loss 229.46286
Epoch 89: Val Loss 229.46114
Epoch 90: Val Loss 229.45941
Epoch 91: Val Loss 229.45767
Epoch 92: Val Loss 229.45595
Epoch 93: Val Loss 229.45424
Epoch 94: Val Loss 229.45250
Epoch 95: Val Loss 229.45079
Epoch 96: Val Loss 229.44907
Epoch 97: Val Loss 229.44734
Epoch 98: Val Loss 229.44565
Epoch 99: Val Loss 229.44391
{'MSE - mean': 229.02289445161492, 'MSE - std': 0.30761780134274563, 'R2 - mean': -352.0676469002833, 'R2 - std': 45.615255619172295} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.11658
Epoch 1: Val Loss 230.11522
Epoch 2: Val Loss 230.11385
Epoch 3: Val Loss 230.11249
Epoch 4: Val Loss 230.11111
Epoch 5: Val Loss 230.10976
Epoch 6: Val Loss 230.10840
Epoch 7: Val Loss 230.10704
Epoch 8: Val Loss 230.10568
Epoch 9: Val Loss 230.10432
Epoch 10: Val Loss 230.10297
Epoch 11: Val Loss 230.10162
Epoch 12: Val Loss 230.10025
Epoch 13: Val Loss 230.09889
Epoch 14: Val Loss 230.09753
Epoch 15: Val Loss 230.09616
Epoch 16: Val Loss 230.09480
Epoch 17: Val Loss 230.09343
Epoch 18: Val Loss 230.09207
Epoch 19: Val Loss 230.09071
Epoch 20: Val Loss 230.08936
Epoch 21: Val Loss 230.08800
Epoch 22: Val Loss 230.08664
Epoch 23: Val Loss 230.08527
Epoch 24: Val Loss 230.08391
Epoch 25: Val Loss 230.08257
Epoch 26: Val Loss 230.08119
Epoch 27: Val Loss 230.07982
Epoch 28: Val Loss 230.07846
Epoch 29: Val Loss 230.07710
Epoch 30: Val Loss 230.07573
Epoch 31: Val Loss 230.07437
Epoch 32: Val Loss 230.07301
Epoch 33: Val Loss 230.07166
Epoch 34: Val Loss 230.07030
Epoch 35: Val Loss 230.06894
Epoch 36: Val Loss 230.06755
Epoch 37: Val Loss 230.06619
Epoch 38: Val Loss 230.06483
Epoch 39: Val Loss 230.06348
Epoch 40: Val Loss 230.06210
Epoch 41: Val Loss 230.06073
Epoch 42: Val Loss 230.05937
Epoch 43: Val Loss 230.05800
Epoch 44: Val Loss 230.05661
Epoch 45: Val Loss 230.05525
Epoch 46: Val Loss 230.05388
Epoch 47: Val Loss 230.05251
Epoch 48: Val Loss 230.05116
Epoch 49: Val Loss 230.04979
Epoch 50: Val Loss 230.04842
Epoch 51: Val Loss 230.04703
Epoch 52: Val Loss 230.04567
Epoch 53: Val Loss 230.04431
Epoch 54: Val Loss 230.04294
Epoch 55: Val Loss 230.04156
Epoch 56: Val Loss 230.04018
Epoch 57: Val Loss 230.03882
Epoch 58: Val Loss 230.03741
Epoch 59: Val Loss 230.03606
Epoch 60: Val Loss 230.03468
Epoch 61: Val Loss 230.03333
Epoch 62: Val Loss 230.03195
Epoch 63: Val Loss 230.03058
Epoch 64: Val Loss 230.02922
Epoch 65: Val Loss 230.02785
Epoch 66: Val Loss 230.02649
Epoch 67: Val Loss 230.02509
Epoch 68: Val Loss 230.02373
Epoch 69: Val Loss 230.02235
Epoch 70: Val Loss 230.02098
Epoch 71: Val Loss 230.01961
Epoch 72: Val Loss 230.01823
Epoch 73: Val Loss 230.01688
Epoch 74: Val Loss 230.01547
Epoch 75: Val Loss 230.01411
Epoch 76: Val Loss 230.01276
Epoch 77: Val Loss 230.01137
Epoch 78: Val Loss 230.01001
Epoch 79: Val Loss 230.00862
Epoch 80: Val Loss 230.00726
Epoch 81: Val Loss 230.00589
Epoch 82: Val Loss 230.00450
Epoch 83: Val Loss 230.00311
Epoch 84: Val Loss 230.00174
Epoch 85: Val Loss 230.00037
Epoch 86: Val Loss 229.99899
Epoch 87: Val Loss 229.99760
Epoch 88: Val Loss 229.99622
Epoch 89: Val Loss 229.99486
Epoch 90: Val Loss 229.99348
Epoch 91: Val Loss 229.99211
Epoch 92: Val Loss 229.99074
Epoch 93: Val Loss 229.98936
Epoch 94: Val Loss 229.98796
Epoch 95: Val Loss 229.98659
Epoch 96: Val Loss 229.98523
Epoch 97: Val Loss 229.98383
Epoch 98: Val Loss 229.98248
Epoch 99: Val Loss 229.98111
{'MSE - mean': 229.2624484078773, 'MSE - std': 0.49308196850022307, 'R2 - mean': -337.13230483981096, 'R2 - std': 47.220302691657615} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.99306
Epoch 1: Val Loss 220.99231
Epoch 2: Val Loss 220.99156
Epoch 3: Val Loss 220.99080
Epoch 4: Val Loss 220.99005
Epoch 5: Val Loss 220.98930
Epoch 6: Val Loss 220.98856
Epoch 7: Val Loss 220.98782
Epoch 8: Val Loss 220.98706
Epoch 9: Val Loss 220.98633
Epoch 10: Val Loss 220.98558
Epoch 11: Val Loss 220.98485
Epoch 12: Val Loss 220.98410
Epoch 13: Val Loss 220.98335
Epoch 14: Val Loss 220.98260
Epoch 15: Val Loss 220.98186
Epoch 16: Val Loss 220.98111
Epoch 17: Val Loss 220.98036
Epoch 18: Val Loss 220.97961
Epoch 19: Val Loss 220.97887
Epoch 20: Val Loss 220.97812
Epoch 21: Val Loss 220.97737
Epoch 22: Val Loss 220.97662
Epoch 23: Val Loss 220.97588
Epoch 24: Val Loss 220.97511
Epoch 25: Val Loss 220.97437
Epoch 26: Val Loss 220.97360
Epoch 27: Val Loss 220.97287
Epoch 28: Val Loss 220.97211
Epoch 29: Val Loss 220.97136
Epoch 30: Val Loss 220.97061
Epoch 31: Val Loss 220.96986
Epoch 32: Val Loss 220.96912
Epoch 33: Val Loss 220.96837
Epoch 34: Val Loss 220.96762
Epoch 35: Val Loss 220.96687
Epoch 36: Val Loss 220.96611
Epoch 37: Val Loss 220.96536
Epoch 38: Val Loss 220.96460
Epoch 39: Val Loss 220.96385
Epoch 40: Val Loss 220.96310
Epoch 41: Val Loss 220.96236
Epoch 42: Val Loss 220.96158
Epoch 43: Val Loss 220.96083
Epoch 44: Val Loss 220.96008
Epoch 45: Val Loss 220.95934
Epoch 46: Val Loss 220.95859
Epoch 47: Val Loss 220.95782
Epoch 48: Val Loss 220.95708
Epoch 49: Val Loss 220.95633
Epoch 50: Val Loss 220.95557
Epoch 51: Val Loss 220.95482
Epoch 52: Val Loss 220.95406
Epoch 53: Val Loss 220.95331
Epoch 54: Val Loss 220.95253
Epoch 55: Val Loss 220.95180
Epoch 56: Val Loss 220.95102
Epoch 57: Val Loss 220.95027
Epoch 58: Val Loss 220.94951
Epoch 59: Val Loss 220.94876
Epoch 60: Val Loss 220.94801
Epoch 61: Val Loss 220.94725
Epoch 62: Val Loss 220.94649
Epoch 63: Val Loss 220.94574
Epoch 64: Val Loss 220.94496
Epoch 65: Val Loss 220.94421
Epoch 66: Val Loss 220.94347
Epoch 67: Val Loss 220.94270
Epoch 68: Val Loss 220.94194
Epoch 69: Val Loss 220.94119
Epoch 70: Val Loss 220.94043
Epoch 71: Val Loss 220.93968
Epoch 72: Val Loss 220.93892
Epoch 73: Val Loss 220.93816
Epoch 74: Val Loss 220.93741
Epoch 75: Val Loss 220.93665
Epoch 76: Val Loss 220.93588
Epoch 77: Val Loss 220.93515
Epoch 78: Val Loss 220.93439
Epoch 79: Val Loss 220.93362
Epoch 80: Val Loss 220.93285
Epoch 81: Val Loss 220.93208
Epoch 82: Val Loss 220.93134
Epoch 83: Val Loss 220.93057
Epoch 84: Val Loss 220.92981
Epoch 85: Val Loss 220.92906
Epoch 86: Val Loss 220.92828
Epoch 87: Val Loss 220.92754
Epoch 88: Val Loss 220.92679
Epoch 89: Val Loss 220.92604
Epoch 90: Val Loss 220.92526
Epoch 91: Val Loss 220.92451
Epoch 92: Val Loss 220.92374
Epoch 93: Val Loss 220.92299
Epoch 94: Val Loss 220.92223
Epoch 95: Val Loss 220.92146
Epoch 96: Val Loss 220.92070
Epoch 97: Val Loss 220.91995
Epoch 98: Val Loss 220.91917
Epoch 99: Val Loss 220.91840
{'MSE - mean': 227.59364411721214, 'MSE - std': 3.3666206950439204, 'R2 - mean': -342.8311342840446, 'R2 - std': 43.745996584502116} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 227.59364411721214 and parameters: {'dim': 128, 'depth': 3, 'heads': 8, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 2 with value: 227.59364411721214.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.09874
Epoch 1: Val Loss 227.09778
Epoch 2: Val Loss 227.09682
Epoch 3: Val Loss 227.09584
Epoch 4: Val Loss 227.09486
Epoch 5: Val Loss 227.09389
Epoch 6: Val Loss 227.09291
Epoch 7: Val Loss 227.09193
Epoch 8: Val Loss 227.09096
Epoch 9: Val Loss 227.09001
Epoch 10: Val Loss 227.08904
Epoch 11: Val Loss 227.08806
Epoch 12: Val Loss 227.08710
Epoch 13: Val Loss 227.08614
Epoch 14: Val Loss 227.08516
Epoch 15: Val Loss 227.08420
Epoch 16: Val Loss 227.08324
Epoch 17: Val Loss 227.08226
Epoch 18: Val Loss 227.08130
Epoch 19: Val Loss 227.08032
Epoch 20: Val Loss 227.07938
Epoch 21: Val Loss 227.07840
Epoch 22: Val Loss 227.07742
Epoch 23: Val Loss 227.07646
Epoch 24: Val Loss 227.07550
Epoch 25: Val Loss 227.07452
Epoch 26: Val Loss 227.07355
Epoch 27: Val Loss 227.07259
Epoch 28: Val Loss 227.07162
Epoch 29: Val Loss 227.07066
Epoch 30: Val Loss 227.06969
Epoch 31: Val Loss 227.06873
Epoch 32: Val Loss 227.06775
Epoch 33: Val Loss 227.06680
Epoch 34: Val Loss 227.06583
Epoch 35: Val Loss 227.06488
Epoch 36: Val Loss 227.06392
Epoch 37: Val Loss 227.06294
Epoch 38: Val Loss 227.06198
Epoch 39: Val Loss 227.06102
Epoch 40: Val Loss 227.06007
Epoch 41: Val Loss 227.05911
Epoch 42: Val Loss 227.05812
Epoch 43: Val Loss 227.05717
Epoch 44: Val Loss 227.05623
Epoch 45: Val Loss 227.05525
Epoch 46: Val Loss 227.05431
Epoch 47: Val Loss 227.05334
Epoch 48: Val Loss 227.05237
Epoch 49: Val Loss 227.05142
Epoch 50: Val Loss 227.05046
Epoch 51: Val Loss 227.04950
Epoch 52: Val Loss 227.04854
Epoch 53: Val Loss 227.04758
Epoch 54: Val Loss 227.04663
Epoch 55: Val Loss 227.04565
Epoch 56: Val Loss 227.04472
Epoch 57: Val Loss 227.04375
Epoch 58: Val Loss 227.04279
Epoch 59: Val Loss 227.04182
Epoch 60: Val Loss 227.04086
Epoch 61: Val Loss 227.03990
Epoch 62: Val Loss 227.03894
Epoch 63: Val Loss 227.03798
Epoch 64: Val Loss 227.03702
Epoch 65: Val Loss 227.03607
Epoch 66: Val Loss 227.03513
Epoch 67: Val Loss 227.03415
Epoch 68: Val Loss 227.03320
Epoch 69: Val Loss 227.03226
Epoch 70: Val Loss 227.03128
Epoch 71: Val Loss 227.03032
Epoch 72: Val Loss 227.02936
Epoch 73: Val Loss 227.02838
Epoch 74: Val Loss 227.02745
Epoch 75: Val Loss 227.02647
Epoch 76: Val Loss 227.02553
Epoch 77: Val Loss 227.02455
Epoch 78: Val Loss 227.02359
Epoch 79: Val Loss 227.02263
Epoch 80: Val Loss 227.02168
Epoch 81: Val Loss 227.02071
Epoch 82: Val Loss 227.01973
Epoch 83: Val Loss 227.01878
Epoch 84: Val Loss 227.01781
Epoch 85: Val Loss 227.01688
Epoch 86: Val Loss 227.01590
Epoch 87: Val Loss 227.01495
Epoch 88: Val Loss 227.01399
Epoch 89: Val Loss 227.01303
Epoch 90: Val Loss 227.01207
Epoch 91: Val Loss 227.01111
Epoch 92: Val Loss 227.01016
Epoch 93: Val Loss 227.00922
Epoch 94: Val Loss 227.00826
Epoch 95: Val Loss 227.00729
Epoch 96: Val Loss 227.00633
Epoch 97: Val Loss 227.00539
Epoch 98: Val Loss 227.00444
Epoch 99: Val Loss 227.00348
{'MSE - mean': 227.00348594197305, 'MSE - std': 0.0, 'R2 - mean': -412.07128798609676, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.71124
Epoch 1: Val Loss 222.70978
Epoch 2: Val Loss 222.70828
Epoch 3: Val Loss 222.70679
Epoch 4: Val Loss 222.70531
Epoch 5: Val Loss 222.70383
Epoch 6: Val Loss 222.70235
Epoch 7: Val Loss 222.70087
Epoch 8: Val Loss 222.69937
Epoch 9: Val Loss 222.69789
Epoch 10: Val Loss 222.69643
Epoch 11: Val Loss 222.69495
Epoch 12: Val Loss 222.69347
Epoch 13: Val Loss 222.69200
Epoch 14: Val Loss 222.69051
Epoch 15: Val Loss 222.68906
Epoch 16: Val Loss 222.68758
Epoch 17: Val Loss 222.68610
Epoch 18: Val Loss 222.68462
Epoch 19: Val Loss 222.68314
Epoch 20: Val Loss 222.68169
Epoch 21: Val Loss 222.68021
Epoch 22: Val Loss 222.67873
Epoch 23: Val Loss 222.67725
Epoch 24: Val Loss 222.67577
Epoch 25: Val Loss 222.67429
Epoch 26: Val Loss 222.67284
Epoch 27: Val Loss 222.67136
Epoch 28: Val Loss 222.66989
Epoch 29: Val Loss 222.66840
Epoch 30: Val Loss 222.66695
Epoch 31: Val Loss 222.66545
Epoch 32: Val Loss 222.66397
Epoch 33: Val Loss 222.66249
Epoch 34: Val Loss 222.66101
Epoch 35: Val Loss 222.65953
Epoch 36: Val Loss 222.65805
Epoch 37: Val Loss 222.65660
Epoch 38: Val Loss 222.65512
Epoch 39: Val Loss 222.65364
Epoch 40: Val Loss 222.65216
Epoch 41: Val Loss 222.65068
Epoch 42: Val Loss 222.64922
Epoch 43: Val Loss 222.64775
Epoch 44: Val Loss 222.64627
Epoch 45: Val Loss 222.64479
Epoch 46: Val Loss 222.64331
Epoch 47: Val Loss 222.64183
Epoch 48: Val Loss 222.64038
Epoch 49: Val Loss 222.63889
Epoch 50: Val Loss 222.63742
Epoch 51: Val Loss 222.63593
Epoch 52: Val Loss 222.63446
Epoch 53: Val Loss 222.63300
Epoch 54: Val Loss 222.63152
Epoch 55: Val Loss 222.63004
Epoch 56: Val Loss 222.62856
Epoch 57: Val Loss 222.62709
Epoch 58: Val Loss 222.62560
Epoch 59: Val Loss 222.62413
Epoch 60: Val Loss 222.62267
Epoch 61: Val Loss 222.62119
Epoch 62: Val Loss 222.61972
Epoch 63: Val Loss 222.61823
Epoch 64: Val Loss 222.61678
Epoch 65: Val Loss 222.61531
Epoch 66: Val Loss 222.61383
Epoch 67: Val Loss 222.61235
Epoch 68: Val Loss 222.61090
Epoch 69: Val Loss 222.60942
Epoch 70: Val Loss 222.60794
Epoch 71: Val Loss 222.60646
Epoch 72: Val Loss 222.60498
Epoch 73: Val Loss 222.60352
Epoch 74: Val Loss 222.60205
Epoch 75: Val Loss 222.60057
Epoch 76: Val Loss 222.59911
Epoch 77: Val Loss 222.59764
Epoch 78: Val Loss 222.59616
Epoch 79: Val Loss 222.59470
Epoch 80: Val Loss 222.59323
Epoch 81: Val Loss 222.59174
Epoch 82: Val Loss 222.59027
Epoch 83: Val Loss 222.58879
Epoch 84: Val Loss 222.58733
Epoch 85: Val Loss 222.58585
Epoch 86: Val Loss 222.58437
Epoch 87: Val Loss 222.58289
Epoch 88: Val Loss 222.58141
Epoch 89: Val Loss 222.57996
Epoch 90: Val Loss 222.57848
Epoch 91: Val Loss 222.57700
Epoch 92: Val Loss 222.57553
Epoch 93: Val Loss 222.57405
Epoch 94: Val Loss 222.57257
Epoch 95: Val Loss 222.57112
Epoch 96: Val Loss 222.56963
Epoch 97: Val Loss 222.56816
Epoch 98: Val Loss 222.56668
Epoch 99: Val Loss 222.56520
{'MSE - mean': 224.78435639844278, 'MSE - std': 2.219129543530272, 'R2 - mean': -356.23322361398925, 'R2 - std': 55.83806437210751} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.93471
Epoch 1: Val Loss 219.93373
Epoch 2: Val Loss 219.93277
Epoch 3: Val Loss 219.93178
Epoch 4: Val Loss 219.93079
Epoch 5: Val Loss 219.92979
Epoch 6: Val Loss 219.92883
Epoch 7: Val Loss 219.92784
Epoch 8: Val Loss 219.92683
Epoch 9: Val Loss 219.92587
Epoch 10: Val Loss 219.92487
Epoch 11: Val Loss 219.92390
Epoch 12: Val Loss 219.92291
Epoch 13: Val Loss 219.92194
Epoch 14: Val Loss 219.92093
Epoch 15: Val Loss 219.91995
Epoch 16: Val Loss 219.91898
Epoch 17: Val Loss 219.91800
Epoch 18: Val Loss 219.91701
Epoch 19: Val Loss 219.91602
Epoch 20: Val Loss 219.91502
Epoch 21: Val Loss 219.91402
Epoch 22: Val Loss 219.91304
Epoch 23: Val Loss 219.91205
Epoch 24: Val Loss 219.91106
Epoch 25: Val Loss 219.91008
Epoch 26: Val Loss 219.90909
Epoch 27: Val Loss 219.90811
Epoch 28: Val Loss 219.90712
Epoch 29: Val Loss 219.90614
Epoch 30: Val Loss 219.90514
Epoch 31: Val Loss 219.90413
Epoch 32: Val Loss 219.90315
Epoch 33: Val Loss 219.90216
Epoch 34: Val Loss 219.90117
Epoch 35: Val Loss 219.90016
Epoch 36: Val Loss 219.89917
Epoch 37: Val Loss 219.89819
Epoch 38: Val Loss 219.89720
Epoch 39: Val Loss 219.89622
Epoch 40: Val Loss 219.89523
Epoch 41: Val Loss 219.89421
Epoch 42: Val Loss 219.89323
Epoch 43: Val Loss 219.89224
Epoch 44: Val Loss 219.89127
Epoch 45: Val Loss 219.89026
Epoch 46: Val Loss 219.88930
Epoch 47: Val Loss 219.88828
Epoch 48: Val Loss 219.88728
Epoch 49: Val Loss 219.88632
Epoch 50: Val Loss 219.88531
Epoch 51: Val Loss 219.88434
Epoch 52: Val Loss 219.88335
Epoch 53: Val Loss 219.88234
Epoch 54: Val Loss 219.88135
Epoch 55: Val Loss 219.88036
Epoch 56: Val Loss 219.87938
Epoch 57: Val Loss 219.87839
Epoch 58: Val Loss 219.87740
Epoch 59: Val Loss 219.87640
Epoch 60: Val Loss 219.87541
Epoch 61: Val Loss 219.87440
Epoch 62: Val Loss 219.87343
Epoch 63: Val Loss 219.87241
Epoch 64: Val Loss 219.87143
Epoch 65: Val Loss 219.87044
Epoch 66: Val Loss 219.86945
Epoch 67: Val Loss 219.86845
Epoch 68: Val Loss 219.86745
Epoch 69: Val Loss 219.86647
Epoch 70: Val Loss 219.86546
Epoch 71: Val Loss 219.86446
Epoch 72: Val Loss 219.86348
Epoch 73: Val Loss 219.86249
Epoch 74: Val Loss 219.86147
Epoch 75: Val Loss 219.86050
Epoch 76: Val Loss 219.85950
Epoch 77: Val Loss 219.85852
Epoch 78: Val Loss 219.85751
Epoch 79: Val Loss 219.85651
Epoch 80: Val Loss 219.85553
Epoch 81: Val Loss 219.85454
Epoch 82: Val Loss 219.85356
Epoch 83: Val Loss 219.85254
Epoch 84: Val Loss 219.85155
Epoch 85: Val Loss 219.85057
Epoch 86: Val Loss 219.84956
Epoch 87: Val Loss 219.84856
Epoch 88: Val Loss 219.84758
Epoch 89: Val Loss 219.84657
Epoch 90: Val Loss 219.84558
Epoch 91: Val Loss 219.84459
Epoch 92: Val Loss 219.84358
Epoch 93: Val Loss 219.84259
Epoch 94: Val Loss 219.84158
Epoch 95: Val Loss 219.84058
Epoch 96: Val Loss 219.83960
Epoch 97: Val Loss 219.83859
Epoch 98: Val Loss 219.83759
Epoch 99: Val Loss 219.83658
{'MSE - mean': 223.13510563758572, 'MSE - std': 2.953486092459973, 'R2 - mean': -343.5170068356885, 'R2 - std': 49.0101754079826} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.26733
Epoch 1: Val Loss 229.26643
Epoch 2: Val Loss 229.26552
Epoch 3: Val Loss 229.26460
Epoch 4: Val Loss 229.26370
Epoch 5: Val Loss 229.26280
Epoch 6: Val Loss 229.26190
Epoch 7: Val Loss 229.26100
Epoch 8: Val Loss 229.26007
Epoch 9: Val Loss 229.25917
Epoch 10: Val Loss 229.25827
Epoch 11: Val Loss 229.25735
Epoch 12: Val Loss 229.25645
Epoch 13: Val Loss 229.25555
Epoch 14: Val Loss 229.25464
Epoch 15: Val Loss 229.25374
Epoch 16: Val Loss 229.25284
Epoch 17: Val Loss 229.25192
Epoch 18: Val Loss 229.25101
Epoch 19: Val Loss 229.25011
Epoch 20: Val Loss 229.24921
Epoch 21: Val Loss 229.24829
Epoch 22: Val Loss 229.24738
Epoch 23: Val Loss 229.24649
Epoch 24: Val Loss 229.24557
Epoch 25: Val Loss 229.24466
Epoch 26: Val Loss 229.24377
Epoch 27: Val Loss 229.24286
Epoch 28: Val Loss 229.24197
Epoch 29: Val Loss 229.24106
Epoch 30: Val Loss 229.24014
Epoch 31: Val Loss 229.23924
Epoch 32: Val Loss 229.23834
Epoch 33: Val Loss 229.23744
Epoch 34: Val Loss 229.23654
Epoch 35: Val Loss 229.23563
Epoch 36: Val Loss 229.23473
Epoch 37: Val Loss 229.23383
Epoch 38: Val Loss 229.23291
Epoch 39: Val Loss 229.23201
Epoch 40: Val Loss 229.23111
Epoch 41: Val Loss 229.23021
Epoch 42: Val Loss 229.22931
Epoch 43: Val Loss 229.22841
Epoch 44: Val Loss 229.22751
Epoch 45: Val Loss 229.22659
Epoch 46: Val Loss 229.22566
Epoch 47: Val Loss 229.22479
Epoch 48: Val Loss 229.22386
Epoch 49: Val Loss 229.22296
Epoch 50: Val Loss 229.22206
Epoch 51: Val Loss 229.22116
Epoch 52: Val Loss 229.22026
Epoch 53: Val Loss 229.21935
Epoch 54: Val Loss 229.21844
Epoch 55: Val Loss 229.21754
Epoch 56: Val Loss 229.21663
Epoch 57: Val Loss 229.21573
Epoch 58: Val Loss 229.21481
Epoch 59: Val Loss 229.21391
Epoch 60: Val Loss 229.21301
Epoch 61: Val Loss 229.21210
Epoch 62: Val Loss 229.21121
Epoch 63: Val Loss 229.21031
Epoch 64: Val Loss 229.20940
Epoch 65: Val Loss 229.20851
Epoch 66: Val Loss 229.20760
Epoch 67: Val Loss 229.20670
Epoch 68: Val Loss 229.20580
Epoch 69: Val Loss 229.20491
Epoch 70: Val Loss 229.20399
Epoch 71: Val Loss 229.20308
Epoch 72: Val Loss 229.20219
Epoch 73: Val Loss 229.20129
Epoch 74: Val Loss 229.20039
Epoch 75: Val Loss 229.19948
Epoch 76: Val Loss 229.19859
Epoch 77: Val Loss 229.19769
Epoch 78: Val Loss 229.19678
Epoch 79: Val Loss 229.19588
Epoch 80: Val Loss 229.19498
Epoch 81: Val Loss 229.19408
Epoch 82: Val Loss 229.19318
Epoch 83: Val Loss 229.19228
Epoch 84: Val Loss 229.19138
Epoch 85: Val Loss 229.19048
Epoch 86: Val Loss 229.18958
Epoch 87: Val Loss 229.18867
Epoch 88: Val Loss 229.18776
Epoch 89: Val Loss 229.18686
Epoch 90: Val Loss 229.18596
Epoch 91: Val Loss 229.18507
Epoch 92: Val Loss 229.18416
Epoch 93: Val Loss 229.18326
Epoch 94: Val Loss 229.18236
Epoch 95: Val Loss 229.18147
Epoch 96: Val Loss 229.18057
Epoch 97: Val Loss 229.17967
Epoch 98: Val Loss 229.17876
Epoch 99: Val Loss 229.17787
{'MSE - mean': 224.64579834848828, 'MSE - std': 3.659082873356154, 'R2 - mean': -330.46320638686416, 'R2 - std': 48.090571722678604} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 239.20142
Epoch 1: Val Loss 239.20045
Epoch 2: Val Loss 239.19946
Epoch 3: Val Loss 239.19850
Epoch 4: Val Loss 239.19751
Epoch 5: Val Loss 239.19656
Epoch 6: Val Loss 239.19559
Epoch 7: Val Loss 239.19463
Epoch 8: Val Loss 239.19363
Epoch 9: Val Loss 239.19269
Epoch 10: Val Loss 239.19170
Epoch 11: Val Loss 239.19072
Epoch 12: Val Loss 239.18976
Epoch 13: Val Loss 239.18878
Epoch 14: Val Loss 239.18784
Epoch 15: Val Loss 239.18684
Epoch 16: Val Loss 239.18590
Epoch 17: Val Loss 239.18494
Epoch 18: Val Loss 239.18396
Epoch 19: Val Loss 239.18300
Epoch 20: Val Loss 239.18202
Epoch 21: Val Loss 239.18108
Epoch 22: Val Loss 239.18010
Epoch 23: Val Loss 239.17914
Epoch 24: Val Loss 239.17816
Epoch 25: Val Loss 239.17720
Epoch 26: Val Loss 239.17624
Epoch 27: Val Loss 239.17526
Epoch 28: Val Loss 239.17432
Epoch 29: Val Loss 239.17334
Epoch 30: Val Loss 239.17238
Epoch 31: Val Loss 239.17140
Epoch 32: Val Loss 239.17044
Epoch 33: Val Loss 239.16946
Epoch 34: Val Loss 239.16852
Epoch 35: Val Loss 239.16756
Epoch 36: Val Loss 239.16658
Epoch 37: Val Loss 239.16562
Epoch 38: Val Loss 239.16466
Epoch 39: Val Loss 239.16368
Epoch 40: Val Loss 239.16272
Epoch 41: Val Loss 239.16176
Epoch 42: Val Loss 239.16080
Epoch 43: Val Loss 239.15982
Epoch 44: Val Loss 239.15886
Epoch 45: Val Loss 239.15790
Epoch 46: Val Loss 239.15694
Epoch 47: Val Loss 239.15596
Epoch 48: Val Loss 239.15500
Epoch 49: Val Loss 239.15404
Epoch 50: Val Loss 239.15308
Epoch 51: Val Loss 239.15210
Epoch 52: Val Loss 239.15114
Epoch 53: Val Loss 239.15019
Epoch 54: Val Loss 239.14920
Epoch 55: Val Loss 239.14825
Epoch 56: Val Loss 239.14728
Epoch 57: Val Loss 239.14629
Epoch 58: Val Loss 239.14534
Epoch 59: Val Loss 239.14438
Epoch 60: Val Loss 239.14340
Epoch 61: Val Loss 239.14244
Epoch 62: Val Loss 239.14149
Epoch 63: Val Loss 239.14052
Epoch 64: Val Loss 239.13956
Epoch 65: Val Loss 239.13860
Epoch 66: Val Loss 239.13760
Epoch 67: Val Loss 239.13664
Epoch 68: Val Loss 239.13570
Epoch 69: Val Loss 239.13472
Epoch 70: Val Loss 239.13376
Epoch 71: Val Loss 239.13280
Epoch 72: Val Loss 239.13184
Epoch 73: Val Loss 239.13086
Epoch 74: Val Loss 239.12991
Epoch 75: Val Loss 239.12894
Epoch 76: Val Loss 239.12798
Epoch 77: Val Loss 239.12700
Epoch 78: Val Loss 239.12604
Epoch 79: Val Loss 239.12509
Epoch 80: Val Loss 239.12410
Epoch 81: Val Loss 239.12315
Epoch 82: Val Loss 239.12221
Epoch 83: Val Loss 239.12122
Epoch 84: Val Loss 239.12027
Epoch 85: Val Loss 239.11929
Epoch 86: Val Loss 239.11833
Epoch 87: Val Loss 239.11737
Epoch 88: Val Loss 239.11639
Epoch 89: Val Loss 239.11543
Epoch 90: Val Loss 239.11446
Epoch 91: Val Loss 239.11351
Epoch 92: Val Loss 239.11252
Epoch 93: Val Loss 239.11156
Epoch 94: Val Loss 239.11061
Epoch 95: Val Loss 239.10963
Epoch 96: Val Loss 239.10866
Epoch 97: Val Loss 239.10770
Epoch 98: Val Loss 239.10672
Epoch 99: Val Loss 239.10576
{'MSE - mean': 227.53779150813006, 'MSE - std': 6.645721008357913, 'R2 - mean': -343.53243562875525, 'R2 - std': 50.332707882645} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 227.53779150813006 and parameters: {'dim': 32, 'depth': 1, 'heads': 8, 'weight_decay': -1, 'learning_rate': -6, 'dropout': 0.1}. Best is trial 3 with value: 227.53779150813006.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 235.71675
Epoch 1: Val Loss 235.58990
Epoch 2: Val Loss 235.46376
Epoch 3: Val Loss 235.33826
Epoch 4: Val Loss 235.21356
Epoch 5: Val Loss 235.08943
Epoch 6: Val Loss 234.96690
Epoch 7: Val Loss 234.84494
Epoch 8: Val Loss 234.72343
Epoch 9: Val Loss 234.60260
Epoch 10: Val Loss 234.48253
Epoch 11: Val Loss 234.36237
Epoch 12: Val Loss 234.24249
Epoch 13: Val Loss 234.12344
Epoch 14: Val Loss 234.00533
Epoch 15: Val Loss 233.88757
Epoch 16: Val Loss 233.77023
Epoch 17: Val Loss 233.65305
Epoch 18: Val Loss 233.53629
Epoch 19: Val Loss 233.41989
Epoch 20: Val Loss 233.30362
Epoch 21: Val Loss 233.18779
Epoch 22: Val Loss 233.07237
Epoch 23: Val Loss 232.95679
Epoch 24: Val Loss 232.84210
Epoch 25: Val Loss 232.72765
Epoch 26: Val Loss 232.61359
Epoch 27: Val Loss 232.49942
Epoch 28: Val Loss 232.38577
Epoch 29: Val Loss 232.27165
Epoch 30: Val Loss 232.15747
Epoch 31: Val Loss 232.04367
Epoch 32: Val Loss 231.92924
Epoch 33: Val Loss 231.81448
Epoch 34: Val Loss 231.69952
Epoch 35: Val Loss 231.58434
Epoch 36: Val Loss 231.46878
Epoch 37: Val Loss 231.35318
Epoch 38: Val Loss 231.23697
Epoch 39: Val Loss 231.12032
Epoch 40: Val Loss 231.00299
Epoch 41: Val Loss 230.88542
Epoch 42: Val Loss 230.76733
Epoch 43: Val Loss 230.64873
Epoch 44: Val Loss 230.52979
Epoch 45: Val Loss 230.40958
Epoch 46: Val Loss 230.28854
Epoch 47: Val Loss 230.16682
Epoch 48: Val Loss 230.04424
Epoch 49: Val Loss 229.92120
Epoch 50: Val Loss 229.79747
Epoch 51: Val Loss 229.67201
Epoch 52: Val Loss 229.54512
Epoch 53: Val Loss 229.41736
Epoch 54: Val Loss 229.28853
Epoch 55: Val Loss 229.15802
Epoch 56: Val Loss 229.02657
Epoch 57: Val Loss 228.89400
Epoch 58: Val Loss 228.76036
Epoch 59: Val Loss 228.62581
Epoch 60: Val Loss 228.48991
Epoch 61: Val Loss 228.35243
Epoch 62: Val Loss 228.21416
Epoch 63: Val Loss 228.07416
Epoch 64: Val Loss 227.93228
Epoch 65: Val Loss 227.78870
Epoch 66: Val Loss 227.64278
Epoch 67: Val Loss 227.49452
Epoch 68: Val Loss 227.34431
Epoch 69: Val Loss 227.19197
Epoch 70: Val Loss 227.03809
Epoch 71: Val Loss 226.88177
Epoch 72: Val Loss 226.72346
Epoch 73: Val Loss 226.56294
Epoch 74: Val Loss 226.39896
Epoch 75: Val Loss 226.23262
Epoch 76: Val Loss 226.06310
Epoch 77: Val Loss 225.89011
Epoch 78: Val Loss 225.71487
Epoch 79: Val Loss 225.53664
Epoch 80: Val Loss 225.35544
Epoch 81: Val Loss 225.17113
Epoch 82: Val Loss 224.98413
Epoch 83: Val Loss 224.79364
Epoch 84: Val Loss 224.60028
Epoch 85: Val Loss 224.40248
Epoch 86: Val Loss 224.20108
Epoch 87: Val Loss 223.99510
Epoch 88: Val Loss 223.78516
Epoch 89: Val Loss 223.57173
Epoch 90: Val Loss 223.35405
Epoch 91: Val Loss 223.13200
Epoch 92: Val Loss 222.90675
Epoch 93: Val Loss 222.67647
Epoch 94: Val Loss 222.44269
Epoch 95: Val Loss 222.20448
Epoch 96: Val Loss 221.96214
Epoch 97: Val Loss 221.71504
Epoch 98: Val Loss 221.46179
Epoch 99: Val Loss 221.20430
{'MSE - mean': 221.20429795013183, 'MSE - std': 0.0, 'R2 - mean': -401.51868328435353, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.29556
Epoch 1: Val Loss 234.18423
Epoch 2: Val Loss 234.07343
Epoch 3: Val Loss 233.96368
Epoch 4: Val Loss 233.85449
Epoch 5: Val Loss 233.74597
Epoch 6: Val Loss 233.63800
Epoch 7: Val Loss 233.53081
Epoch 8: Val Loss 233.42487
Epoch 9: Val Loss 233.31987
Epoch 10: Val Loss 233.21487
Epoch 11: Val Loss 233.11086
Epoch 12: Val Loss 233.00780
Epoch 13: Val Loss 232.90587
Epoch 14: Val Loss 232.80377
Epoch 15: Val Loss 232.70204
Epoch 16: Val Loss 232.60036
Epoch 17: Val Loss 232.50012
Epoch 18: Val Loss 232.40132
Epoch 19: Val Loss 232.30312
Epoch 20: Val Loss 232.20569
Epoch 21: Val Loss 232.10861
Epoch 22: Val Loss 232.01089
Epoch 23: Val Loss 231.91435
Epoch 24: Val Loss 231.81731
Epoch 25: Val Loss 231.72101
Epoch 26: Val Loss 231.62509
Epoch 27: Val Loss 231.52942
Epoch 28: Val Loss 231.43324
Epoch 29: Val Loss 231.33707
Epoch 30: Val Loss 231.24173
Epoch 31: Val Loss 231.14510
Epoch 32: Val Loss 231.04861
Epoch 33: Val Loss 230.95424
Epoch 34: Val Loss 230.86015
Epoch 35: Val Loss 230.76648
Epoch 36: Val Loss 230.67371
Epoch 37: Val Loss 230.58087
Epoch 38: Val Loss 230.48778
Epoch 39: Val Loss 230.39537
Epoch 40: Val Loss 230.30342
Epoch 41: Val Loss 230.21060
Epoch 42: Val Loss 230.11707
Epoch 43: Val Loss 230.02364
Epoch 44: Val Loss 229.92967
Epoch 45: Val Loss 229.83548
Epoch 46: Val Loss 229.74110
Epoch 47: Val Loss 229.64764
Epoch 48: Val Loss 229.55461
Epoch 49: Val Loss 229.46089
Epoch 50: Val Loss 229.36734
Epoch 51: Val Loss 229.27303
Epoch 52: Val Loss 229.17891
Epoch 53: Val Loss 229.08395
Epoch 54: Val Loss 228.98773
Epoch 55: Val Loss 228.89082
Epoch 56: Val Loss 228.79309
Epoch 57: Val Loss 228.69472
Epoch 58: Val Loss 228.59578
Epoch 59: Val Loss 228.49654
Epoch 60: Val Loss 228.39531
Epoch 61: Val Loss 228.29228
Epoch 62: Val Loss 228.18753
Epoch 63: Val Loss 228.08215
Epoch 64: Val Loss 227.97668
Epoch 65: Val Loss 227.86945
Epoch 66: Val Loss 227.76089
Epoch 67: Val Loss 227.65005
Epoch 68: Val Loss 227.53838
Epoch 69: Val Loss 227.42432
Epoch 70: Val Loss 227.30908
Epoch 71: Val Loss 227.19231
Epoch 72: Val Loss 227.07338
Epoch 73: Val Loss 226.95288
Epoch 74: Val Loss 226.83118
Epoch 75: Val Loss 226.70799
Epoch 76: Val Loss 226.58318
Epoch 77: Val Loss 226.45677
Epoch 78: Val Loss 226.32921
Epoch 79: Val Loss 226.19812
Epoch 80: Val Loss 226.06569
Epoch 81: Val Loss 225.93094
Epoch 82: Val Loss 225.79332
Epoch 83: Val Loss 225.65352
Epoch 84: Val Loss 225.51118
Epoch 85: Val Loss 225.36617
Epoch 86: Val Loss 225.21964
Epoch 87: Val Loss 225.07034
Epoch 88: Val Loss 224.91843
Epoch 89: Val Loss 224.76402
Epoch 90: Val Loss 224.60883
Epoch 91: Val Loss 224.45078
Epoch 92: Val Loss 224.29004
Epoch 93: Val Loss 224.12720
Epoch 94: Val Loss 223.96114
Epoch 95: Val Loss 223.79312
Epoch 96: Val Loss 223.62227
Epoch 97: Val Loss 223.44876
Epoch 98: Val Loss 223.27251
Epoch 99: Val Loss 223.09262
{'MSE - mean': 222.14845902693634, 'MSE - std': 0.9441610768045052, 'R2 - mean': -351.3140160673238, 'R2 - std': 50.20466721702974} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.41238
Epoch 1: Val Loss 234.35194
Epoch 2: Val Loss 234.29169
Epoch 3: Val Loss 234.23114
Epoch 4: Val Loss 234.17059
Epoch 5: Val Loss 234.10976
Epoch 6: Val Loss 234.04860
Epoch 7: Val Loss 233.98705
Epoch 8: Val Loss 233.92529
Epoch 9: Val Loss 233.86313
Epoch 10: Val Loss 233.80072
Epoch 11: Val Loss 233.73784
Epoch 12: Val Loss 233.67488
Epoch 13: Val Loss 233.61066
Epoch 14: Val Loss 233.54550
Epoch 15: Val Loss 233.47934
Epoch 16: Val Loss 233.41208
Epoch 17: Val Loss 233.34428
Epoch 18: Val Loss 233.27568
Epoch 19: Val Loss 233.20631
Epoch 20: Val Loss 233.13535
Epoch 21: Val Loss 233.06302
Epoch 22: Val Loss 232.98988
Epoch 23: Val Loss 232.91570
Epoch 24: Val Loss 232.83952
Epoch 25: Val Loss 232.76196
Epoch 26: Val Loss 232.68349
Epoch 27: Val Loss 232.60403
Epoch 28: Val Loss 232.52324
Epoch 29: Val Loss 232.44150
Epoch 30: Val Loss 232.35843
Epoch 31: Val Loss 232.27403
Epoch 32: Val Loss 232.18959
Epoch 33: Val Loss 232.10349
Epoch 34: Val Loss 232.01630
Epoch 35: Val Loss 231.92809
Epoch 36: Val Loss 231.83844
Epoch 37: Val Loss 231.74724
Epoch 38: Val Loss 231.65434
Epoch 39: Val Loss 231.55984
Epoch 40: Val Loss 231.46460
Epoch 41: Val Loss 231.36800
Epoch 42: Val Loss 231.26962
Epoch 43: Val Loss 231.16922
Epoch 44: Val Loss 231.06734
Epoch 45: Val Loss 230.96301
Epoch 46: Val Loss 230.85753
Epoch 47: Val Loss 230.75066
Epoch 48: Val Loss 230.64265
Epoch 49: Val Loss 230.53304
Epoch 50: Val Loss 230.42180
Epoch 51: Val Loss 230.30844
Epoch 52: Val Loss 230.19347
Epoch 53: Val Loss 230.07689
Epoch 54: Val Loss 229.95793
Epoch 55: Val Loss 229.83647
Epoch 56: Val Loss 229.71277
Epoch 57: Val Loss 229.58679
Epoch 58: Val Loss 229.45808
Epoch 59: Val Loss 229.32645
Epoch 60: Val Loss 229.19220
Epoch 61: Val Loss 229.05627
Epoch 62: Val Loss 228.91800
Epoch 63: Val Loss 228.77785
Epoch 64: Val Loss 228.63531
Epoch 65: Val Loss 228.49081
Epoch 66: Val Loss 228.34421
Epoch 67: Val Loss 228.19604
Epoch 68: Val Loss 228.04559
Epoch 69: Val Loss 227.89240
Epoch 70: Val Loss 227.73706
Epoch 71: Val Loss 227.58002
Epoch 72: Val Loss 227.42062
Epoch 73: Val Loss 227.25984
Epoch 74: Val Loss 227.09526
Epoch 75: Val Loss 226.92853
Epoch 76: Val Loss 226.76018
Epoch 77: Val Loss 226.58977
Epoch 78: Val Loss 226.41676
Epoch 79: Val Loss 226.24097
Epoch 80: Val Loss 226.06242
Epoch 81: Val Loss 225.87999
Epoch 82: Val Loss 225.69382
Epoch 83: Val Loss 225.50464
Epoch 84: Val Loss 225.31340
Epoch 85: Val Loss 225.12000
Epoch 86: Val Loss 224.92305
Epoch 87: Val Loss 224.72258
Epoch 88: Val Loss 224.51976
Epoch 89: Val Loss 224.31422
Epoch 90: Val Loss 224.10570
Epoch 91: Val Loss 223.89345
Epoch 92: Val Loss 223.67818
Epoch 93: Val Loss 223.45943
Epoch 94: Val Loss 223.23804
Epoch 95: Val Loss 223.01381
Epoch 96: Val Loss 222.78674
Epoch 97: Val Loss 222.55765
Epoch 98: Val Loss 222.32581
Epoch 99: Val Loss 222.09114
{'MSE - mean': 222.129357947461, 'MSE - std': 0.7713774228240531, 'R2 - mean': -341.32833416005013, 'R2 - std': 43.35627705290558} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 221.71239
Epoch 1: Val Loss 221.59058
Epoch 2: Val Loss 221.46764
Epoch 3: Val Loss 221.34349
Epoch 4: Val Loss 221.21841
Epoch 5: Val Loss 221.09154
Epoch 6: Val Loss 220.96376
Epoch 7: Val Loss 220.83495
Epoch 8: Val Loss 220.70421
Epoch 9: Val Loss 220.57166
Epoch 10: Val Loss 220.43802
Epoch 11: Val Loss 220.30235
Epoch 12: Val Loss 220.16505
Epoch 13: Val Loss 220.02592
Epoch 14: Val Loss 219.88512
Epoch 15: Val Loss 219.74138
Epoch 16: Val Loss 219.59528
Epoch 17: Val Loss 219.44646
Epoch 18: Val Loss 219.29588
Epoch 19: Val Loss 219.14320
Epoch 20: Val Loss 218.98956
Epoch 21: Val Loss 218.83226
Epoch 22: Val Loss 218.67340
Epoch 23: Val Loss 218.51283
Epoch 24: Val Loss 218.35030
Epoch 25: Val Loss 218.18619
Epoch 26: Val Loss 218.01932
Epoch 27: Val Loss 217.84935
Epoch 28: Val Loss 217.67754
Epoch 29: Val Loss 217.50433
Epoch 30: Val Loss 217.32915
Epoch 31: Val Loss 217.15143
Epoch 32: Val Loss 216.97037
Epoch 33: Val Loss 216.78642
Epoch 34: Val Loss 216.60092
Epoch 35: Val Loss 216.41180
Epoch 36: Val Loss 216.21996
Epoch 37: Val Loss 216.02621
Epoch 38: Val Loss 215.82909
Epoch 39: Val Loss 215.62894
Epoch 40: Val Loss 215.42543
Epoch 41: Val Loss 215.21776
Epoch 42: Val Loss 215.00862
Epoch 43: Val Loss 214.79610
Epoch 44: Val Loss 214.58151
Epoch 45: Val Loss 214.36365
Epoch 46: Val Loss 214.14151
Epoch 47: Val Loss 213.91612
Epoch 48: Val Loss 213.68709
Epoch 49: Val Loss 213.45457
Epoch 50: Val Loss 213.21826
Epoch 51: Val Loss 212.97968
Epoch 52: Val Loss 212.73621
Epoch 53: Val Loss 212.48921
Epoch 54: Val Loss 212.23885
Epoch 55: Val Loss 211.98503
Epoch 56: Val Loss 211.72743
Epoch 57: Val Loss 211.46483
Epoch 58: Val Loss 211.19795
Epoch 59: Val Loss 210.92694
Epoch 60: Val Loss 210.65118
Epoch 61: Val Loss 210.37051
Epoch 62: Val Loss 210.08562
Epoch 63: Val Loss 209.79692
Epoch 64: Val Loss 209.50270
Epoch 65: Val Loss 209.20383
Epoch 66: Val Loss 208.90074
Epoch 67: Val Loss 208.59383
Epoch 68: Val Loss 208.28281
Epoch 69: Val Loss 207.96643
Epoch 70: Val Loss 207.64503
Epoch 71: Val Loss 207.31966
Epoch 72: Val Loss 206.99008
Epoch 73: Val Loss 206.65393
Epoch 74: Val Loss 206.31419
Epoch 75: Val Loss 205.96895
Epoch 76: Val Loss 205.61853
Epoch 77: Val Loss 205.26286
Epoch 78: Val Loss 204.90036
Epoch 79: Val Loss 204.53130
Epoch 80: Val Loss 204.15683
Epoch 81: Val Loss 203.77606
Epoch 82: Val Loss 203.39102
Epoch 83: Val Loss 202.99931
Epoch 84: Val Loss 202.60071
Epoch 85: Val Loss 202.19806
Epoch 86: Val Loss 201.78766
Epoch 87: Val Loss 201.37202
Epoch 88: Val Loss 200.95056
Epoch 89: Val Loss 200.52159
Epoch 90: Val Loss 200.08588
Epoch 91: Val Loss 199.64369
Epoch 92: Val Loss 199.19374
Epoch 93: Val Loss 198.73712
Epoch 94: Val Loss 198.27425
Epoch 95: Val Loss 197.80241
Epoch 96: Val Loss 197.32352
Epoch 97: Val Loss 196.83685
Epoch 98: Val Loss 196.34514
Epoch 99: Val Loss 195.84361
{'MSE - mean': 215.5579233267153, 'MSE - std': 11.401645770262169, 'R2 - mean': -318.19277071401297, 'R2 - std': 54.91436931999728} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 240.51642
Epoch 1: Val Loss 240.40451
Epoch 2: Val Loss 240.29335
Epoch 3: Val Loss 240.18314
Epoch 4: Val Loss 240.07460
Epoch 5: Val Loss 239.96796
Epoch 6: Val Loss 239.86287
Epoch 7: Val Loss 239.75853
Epoch 8: Val Loss 239.65517
Epoch 9: Val Loss 239.55328
Epoch 10: Val Loss 239.45222
Epoch 11: Val Loss 239.35301
Epoch 12: Val Loss 239.25523
Epoch 13: Val Loss 239.15846
Epoch 14: Val Loss 239.06279
Epoch 15: Val Loss 238.96779
Epoch 16: Val Loss 238.87392
Epoch 17: Val Loss 238.78192
Epoch 18: Val Loss 238.69064
Epoch 19: Val Loss 238.60005
Epoch 20: Val Loss 238.51151
Epoch 21: Val Loss 238.42302
Epoch 22: Val Loss 238.33556
Epoch 23: Val Loss 238.24823
Epoch 24: Val Loss 238.16158
Epoch 25: Val Loss 238.07573
Epoch 26: Val Loss 237.99059
Epoch 27: Val Loss 237.90674
Epoch 28: Val Loss 237.82497
Epoch 29: Val Loss 237.74420
Epoch 30: Val Loss 237.66467
Epoch 31: Val Loss 237.58603
Epoch 32: Val Loss 237.50696
Epoch 33: Val Loss 237.42831
Epoch 34: Val Loss 237.34991
Epoch 35: Val Loss 237.27094
Epoch 36: Val Loss 237.19189
Epoch 37: Val Loss 237.11290
Epoch 38: Val Loss 237.03493
Epoch 39: Val Loss 236.95668
Epoch 40: Val Loss 236.87747
Epoch 41: Val Loss 236.79836
Epoch 42: Val Loss 236.71898
Epoch 43: Val Loss 236.63902
Epoch 44: Val Loss 236.55774
Epoch 45: Val Loss 236.47517
Epoch 46: Val Loss 236.39265
Epoch 47: Val Loss 236.30934
Epoch 48: Val Loss 236.22459
Epoch 49: Val Loss 236.13857
Epoch 50: Val Loss 236.05209
Epoch 51: Val Loss 235.96494
Epoch 52: Val Loss 235.87643
Epoch 53: Val Loss 235.78596
Epoch 54: Val Loss 235.69394
Epoch 55: Val Loss 235.59886
Epoch 56: Val Loss 235.50127
Epoch 57: Val Loss 235.40254
Epoch 58: Val Loss 235.30190
Epoch 59: Val Loss 235.19920
Epoch 60: Val Loss 235.09415
Epoch 61: Val Loss 234.98698
Epoch 62: Val Loss 234.87819
Epoch 63: Val Loss 234.76901
Epoch 64: Val Loss 234.65742
Epoch 65: Val Loss 234.54253
Epoch 66: Val Loss 234.42413
Epoch 67: Val Loss 234.30406
Epoch 68: Val Loss 234.18207
Epoch 69: Val Loss 234.05649
Epoch 70: Val Loss 233.92642
Epoch 71: Val Loss 233.79268
Epoch 72: Val Loss 233.65587
Epoch 73: Val Loss 233.51608
Epoch 74: Val Loss 233.37190
Epoch 75: Val Loss 233.22383
Epoch 76: Val Loss 233.07338
Epoch 77: Val Loss 232.91910
Epoch 78: Val Loss 232.76198
Epoch 79: Val Loss 232.59924
Epoch 80: Val Loss 232.43253
Epoch 81: Val Loss 232.26270
Epoch 82: Val Loss 232.08849
Epoch 83: Val Loss 231.90926
Epoch 84: Val Loss 231.72452
Epoch 85: Val Loss 231.53392
Epoch 86: Val Loss 231.33897
Epoch 87: Val Loss 231.13866
Epoch 88: Val Loss 230.93250
Epoch 89: Val Loss 230.72343
Epoch 90: Val Loss 230.51019
Epoch 91: Val Loss 230.29242
Epoch 92: Val Loss 230.06905
Epoch 93: Val Loss 229.84007
Epoch 94: Val Loss 229.60451
Epoch 95: Val Loss 229.36452
Epoch 96: Val Loss 229.11839
Epoch 97: Val Loss 228.86664
Epoch 98: Val Loss 228.60773
Epoch 99: Val Loss 228.34404
{'MSE - mean': 218.1151509115306, 'MSE - std': 11.40857890793553, 'R2 - mean': -330.1441578446882, 'R2 - std': 54.62428931930852} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 218.1151509115306 and parameters: {'dim': 256, 'depth': 2, 'heads': 4, 'weight_decay': -5, 'learning_rate': -4, 'dropout': 0.4}. Best is trial 4 with value: 218.1151509115306.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.73387
Epoch 1: Val Loss 233.58664
Epoch 2: Val Loss 233.44006
Epoch 3: Val Loss 233.29468
Epoch 4: Val Loss 233.15024
Epoch 5: Val Loss 233.00677
Epoch 6: Val Loss 232.86423
Epoch 7: Val Loss 232.72244
Epoch 8: Val Loss 232.58200
Epoch 9: Val Loss 232.44235
Epoch 10: Val Loss 232.30409
Epoch 11: Val Loss 232.16736
Epoch 12: Val Loss 232.03195
Epoch 13: Val Loss 231.89810
Epoch 14: Val Loss 231.76482
Epoch 15: Val Loss 231.63217
Epoch 16: Val Loss 231.49997
Epoch 17: Val Loss 231.36887
Epoch 18: Val Loss 231.23875
Epoch 19: Val Loss 231.10884
Epoch 20: Val Loss 230.97894
Epoch 21: Val Loss 230.84967
Epoch 22: Val Loss 230.72104
Epoch 23: Val Loss 230.59283
Epoch 24: Val Loss 230.46431
Epoch 25: Val Loss 230.33627
Epoch 26: Val Loss 230.20903
Epoch 27: Val Loss 230.08173
Epoch 28: Val Loss 229.95421
Epoch 29: Val Loss 229.82738
Epoch 30: Val Loss 229.70087
Epoch 31: Val Loss 229.57426
Epoch 32: Val Loss 229.44791
Epoch 33: Val Loss 229.32202
Epoch 34: Val Loss 229.19600
Epoch 35: Val Loss 229.06989
Epoch 36: Val Loss 228.94450
Epoch 37: Val Loss 228.81848
Epoch 38: Val Loss 228.69145
Epoch 39: Val Loss 228.56544
Epoch 40: Val Loss 228.43851
Epoch 41: Val Loss 228.31102
Epoch 42: Val Loss 228.18333
Epoch 43: Val Loss 228.05568
Epoch 44: Val Loss 227.92761
Epoch 45: Val Loss 227.79939
Epoch 46: Val Loss 227.67088
Epoch 47: Val Loss 227.54196
Epoch 48: Val Loss 227.41287
Epoch 49: Val Loss 227.28258
Epoch 50: Val Loss 227.15201
Epoch 51: Val Loss 227.02145
Epoch 52: Val Loss 226.89111
Epoch 53: Val Loss 226.75990
Epoch 54: Val Loss 226.62848
Epoch 55: Val Loss 226.49747
Epoch 56: Val Loss 226.36580
Epoch 57: Val Loss 226.23428
Epoch 58: Val Loss 226.10240
Epoch 59: Val Loss 225.96956
Epoch 60: Val Loss 225.83557
Epoch 61: Val Loss 225.70020
Epoch 62: Val Loss 225.56433
Epoch 63: Val Loss 225.42780
Epoch 64: Val Loss 225.29105
Epoch 65: Val Loss 225.15381
Epoch 66: Val Loss 225.01566
Epoch 67: Val Loss 224.87738
Epoch 68: Val Loss 224.73752
Epoch 69: Val Loss 224.59674
Epoch 70: Val Loss 224.45462
Epoch 71: Val Loss 224.31152
Epoch 72: Val Loss 224.16728
Epoch 73: Val Loss 224.02240
Epoch 74: Val Loss 223.87651
Epoch 75: Val Loss 223.72978
Epoch 76: Val Loss 223.58165
Epoch 77: Val Loss 223.43245
Epoch 78: Val Loss 223.28217
Epoch 79: Val Loss 223.12956
Epoch 80: Val Loss 222.97636
Epoch 81: Val Loss 222.82246
Epoch 82: Val Loss 222.66544
Epoch 83: Val Loss 222.50757
Epoch 84: Val Loss 222.34718
Epoch 85: Val Loss 222.18608
Epoch 86: Val Loss 222.02435
Epoch 87: Val Loss 221.86078
Epoch 88: Val Loss 221.69548
Epoch 89: Val Loss 221.52785
Epoch 90: Val Loss 221.35811
Epoch 91: Val Loss 221.18607
Epoch 92: Val Loss 221.01212
Epoch 93: Val Loss 220.83467
Epoch 94: Val Loss 220.65524
Epoch 95: Val Loss 220.47369
Epoch 96: Val Loss 220.29027
Epoch 97: Val Loss 220.10516
Epoch 98: Val Loss 219.91750
Epoch 99: Val Loss 219.72807
{'MSE - mean': 219.72807353863035, 'MSE - std': 0.0, 'R2 - mean': -398.8324429542323, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.01103
Epoch 1: Val Loss 223.93745
Epoch 2: Val Loss 223.86308
Epoch 3: Val Loss 223.78711
Epoch 4: Val Loss 223.71024
Epoch 5: Val Loss 223.63258
Epoch 6: Val Loss 223.55432
Epoch 7: Val Loss 223.47510
Epoch 8: Val Loss 223.39552
Epoch 9: Val Loss 223.31479
Epoch 10: Val Loss 223.23331
Epoch 11: Val Loss 223.15042
Epoch 12: Val Loss 223.06558
Epoch 13: Val Loss 222.97859
Epoch 14: Val Loss 222.89020
Epoch 15: Val Loss 222.80023
Epoch 16: Val Loss 222.70798
Epoch 17: Val Loss 222.61473
Epoch 18: Val Loss 222.52072
Epoch 19: Val Loss 222.42487
Epoch 20: Val Loss 222.32657
Epoch 21: Val Loss 222.22672
Epoch 22: Val Loss 222.12379
Epoch 23: Val Loss 222.01758
Epoch 24: Val Loss 221.91005
Epoch 25: Val Loss 221.80106
Epoch 26: Val Loss 221.69072
Epoch 27: Val Loss 221.57892
Epoch 28: Val Loss 221.46571
Epoch 29: Val Loss 221.34927
Epoch 30: Val Loss 221.23151
Epoch 31: Val Loss 221.11205
Epoch 32: Val Loss 220.99107
Epoch 33: Val Loss 220.86905
Epoch 34: Val Loss 220.74564
Epoch 35: Val Loss 220.62091
Epoch 36: Val Loss 220.49452
Epoch 37: Val Loss 220.36592
Epoch 38: Val Loss 220.23546
Epoch 39: Val Loss 220.10312
Epoch 40: Val Loss 219.96873
Epoch 41: Val Loss 219.83284
Epoch 42: Val Loss 219.69484
Epoch 43: Val Loss 219.55467
Epoch 44: Val Loss 219.41296
Epoch 45: Val Loss 219.26929
Epoch 46: Val Loss 219.12329
Epoch 47: Val Loss 218.97566
Epoch 48: Val Loss 218.82666
Epoch 49: Val Loss 218.67621
Epoch 50: Val Loss 218.52373
Epoch 51: Val Loss 218.36911
Epoch 52: Val Loss 218.21207
Epoch 53: Val Loss 218.05325
Epoch 54: Val Loss 217.89145
Epoch 55: Val Loss 217.72665
Epoch 56: Val Loss 217.56007
Epoch 57: Val Loss 217.38980
Epoch 58: Val Loss 217.21793
Epoch 59: Val Loss 217.04393
Epoch 60: Val Loss 216.86732
Epoch 61: Val Loss 216.68864
Epoch 62: Val Loss 216.50569
Epoch 63: Val Loss 216.31955
Epoch 64: Val Loss 216.13158
Epoch 65: Val Loss 215.93983
Epoch 66: Val Loss 215.74493
Epoch 67: Val Loss 215.54724
Epoch 68: Val Loss 215.34734
Epoch 69: Val Loss 215.14410
Epoch 70: Val Loss 214.93748
Epoch 71: Val Loss 214.72829
Epoch 72: Val Loss 214.51544
Epoch 73: Val Loss 214.29945
Epoch 74: Val Loss 214.07982
Epoch 75: Val Loss 213.85640
Epoch 76: Val Loss 213.62944
Epoch 77: Val Loss 213.39813
Epoch 78: Val Loss 213.16315
Epoch 79: Val Loss 212.92406
Epoch 80: Val Loss 212.68153
Epoch 81: Val Loss 212.43610
Epoch 82: Val Loss 212.18701
Epoch 83: Val Loss 211.93388
Epoch 84: Val Loss 211.67743
Epoch 85: Val Loss 211.41728
Epoch 86: Val Loss 211.15260
Epoch 87: Val Loss 210.88377
Epoch 88: Val Loss 210.61050
Epoch 89: Val Loss 210.33337
Epoch 90: Val Loss 210.05191
Epoch 91: Val Loss 209.76514
Epoch 92: Val Loss 209.47292
Epoch 93: Val Loss 209.17465
Epoch 94: Val Loss 208.87163
Epoch 95: Val Loss 208.56270
Epoch 96: Val Loss 208.24797
Epoch 97: Val Loss 207.92856
Epoch 98: Val Loss 207.60384
Epoch 99: Val Loss 207.27333
{'MSE - mean': 213.50070910696218, 'MSE - std': 6.227364431668164, 'R2 - mean': -339.2597594317938, 'R2 - std': 59.57268352243855} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.56866
Epoch 1: Val Loss 227.44968
Epoch 2: Val Loss 227.33064
Epoch 3: Val Loss 227.21191
Epoch 4: Val Loss 227.09279
Epoch 5: Val Loss 226.97318
Epoch 6: Val Loss 226.85336
Epoch 7: Val Loss 226.73340
Epoch 8: Val Loss 226.61317
Epoch 9: Val Loss 226.49225
Epoch 10: Val Loss 226.37035
Epoch 11: Val Loss 226.24738
Epoch 12: Val Loss 226.12282
Epoch 13: Val Loss 225.99785
Epoch 14: Val Loss 225.87277
Epoch 15: Val Loss 225.74622
Epoch 16: Val Loss 225.61848
Epoch 17: Val Loss 225.48975
Epoch 18: Val Loss 225.36015
Epoch 19: Val Loss 225.22906
Epoch 20: Val Loss 225.09663
Epoch 21: Val Loss 224.96422
Epoch 22: Val Loss 224.83080
Epoch 23: Val Loss 224.69656
Epoch 24: Val Loss 224.56201
Epoch 25: Val Loss 224.42496
Epoch 26: Val Loss 224.28590
Epoch 27: Val Loss 224.14543
Epoch 28: Val Loss 224.00403
Epoch 29: Val Loss 223.86195
Epoch 30: Val Loss 223.71878
Epoch 31: Val Loss 223.57321
Epoch 32: Val Loss 223.42679
Epoch 33: Val Loss 223.27846
Epoch 34: Val Loss 223.12886
Epoch 35: Val Loss 222.97740
Epoch 36: Val Loss 222.82343
Epoch 37: Val Loss 222.66847
Epoch 38: Val Loss 222.51161
Epoch 39: Val Loss 222.35303
Epoch 40: Val Loss 222.19272
Epoch 41: Val Loss 222.03137
Epoch 42: Val Loss 221.86717
Epoch 43: Val Loss 221.70105
Epoch 44: Val Loss 221.53200
Epoch 45: Val Loss 221.36130
Epoch 46: Val Loss 221.18761
Epoch 47: Val Loss 221.01045
Epoch 48: Val Loss 220.83087
Epoch 49: Val Loss 220.64745
Epoch 50: Val Loss 220.46034
Epoch 51: Val Loss 220.27191
Epoch 52: Val Loss 220.08139
Epoch 53: Val Loss 219.88684
Epoch 54: Val Loss 219.68996
Epoch 55: Val Loss 219.49117
Epoch 56: Val Loss 219.29007
Epoch 57: Val Loss 219.08597
Epoch 58: Val Loss 218.87784
Epoch 59: Val Loss 218.66638
Epoch 60: Val Loss 218.45180
Epoch 61: Val Loss 218.23347
Epoch 62: Val Loss 218.01089
Epoch 63: Val Loss 217.78549
Epoch 64: Val Loss 217.55586
Epoch 65: Val Loss 217.31915
Epoch 66: Val Loss 217.07930
Epoch 67: Val Loss 216.83501
Epoch 68: Val Loss 216.58632
Epoch 69: Val Loss 216.33426
Epoch 70: Val Loss 216.07852
Epoch 71: Val Loss 215.82013
Epoch 72: Val Loss 215.55696
Epoch 73: Val Loss 215.29095
Epoch 74: Val Loss 215.01907
Epoch 75: Val Loss 214.74051
Epoch 76: Val Loss 214.45837
Epoch 77: Val Loss 214.17149
Epoch 78: Val Loss 213.88083
Epoch 79: Val Loss 213.58623
Epoch 80: Val Loss 213.28580
Epoch 81: Val Loss 212.98079
Epoch 82: Val Loss 212.67172
Epoch 83: Val Loss 212.35721
Epoch 84: Val Loss 212.03969
Epoch 85: Val Loss 211.71738
Epoch 86: Val Loss 211.38911
Epoch 87: Val Loss 211.05640
Epoch 88: Val Loss 210.71892
Epoch 89: Val Loss 210.37614
Epoch 90: Val Loss 210.02971
Epoch 91: Val Loss 209.67661
Epoch 92: Val Loss 209.31898
Epoch 93: Val Loss 208.95801
Epoch 94: Val Loss 208.59163
Epoch 95: Val Loss 208.22159
Epoch 96: Val Loss 207.84445
Epoch 97: Val Loss 207.46211
Epoch 98: Val Loss 207.07104
Epoch 99: Val Loss 206.67467
{'MSE - mean': 211.22536596506893, 'MSE - std': 6.017287705984458, 'R2 - mean': -325.8333510732056, 'R2 - std': 52.21564226870027} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.24812
Epoch 1: Val Loss 222.14839
Epoch 2: Val Loss 222.04897
Epoch 3: Val Loss 221.94901
Epoch 4: Val Loss 221.84833
Epoch 5: Val Loss 221.74654
Epoch 6: Val Loss 221.64357
Epoch 7: Val Loss 221.53954
Epoch 8: Val Loss 221.43440
Epoch 9: Val Loss 221.32880
Epoch 10: Val Loss 221.22238
Epoch 11: Val Loss 221.11481
Epoch 12: Val Loss 221.00635
Epoch 13: Val Loss 220.89734
Epoch 14: Val Loss 220.78722
Epoch 15: Val Loss 220.67661
Epoch 16: Val Loss 220.56471
Epoch 17: Val Loss 220.45174
Epoch 18: Val Loss 220.33742
Epoch 19: Val Loss 220.22128
Epoch 20: Val Loss 220.10408
Epoch 21: Val Loss 219.98610
Epoch 22: Val Loss 219.86676
Epoch 23: Val Loss 219.74562
Epoch 24: Val Loss 219.62370
Epoch 25: Val Loss 219.50023
Epoch 26: Val Loss 219.37494
Epoch 27: Val Loss 219.24834
Epoch 28: Val Loss 219.12000
Epoch 29: Val Loss 218.99034
Epoch 30: Val Loss 218.85910
Epoch 31: Val Loss 218.72621
Epoch 32: Val Loss 218.59099
Epoch 33: Val Loss 218.45349
Epoch 34: Val Loss 218.31348
Epoch 35: Val Loss 218.17094
Epoch 36: Val Loss 218.02679
Epoch 37: Val Loss 217.88005
Epoch 38: Val Loss 217.73058
Epoch 39: Val Loss 217.57918
Epoch 40: Val Loss 217.42609
Epoch 41: Val Loss 217.26994
Epoch 42: Val Loss 217.11168
Epoch 43: Val Loss 216.95151
Epoch 44: Val Loss 216.78830
Epoch 45: Val Loss 216.62219
Epoch 46: Val Loss 216.45351
Epoch 47: Val Loss 216.28264
Epoch 48: Val Loss 216.10973
Epoch 49: Val Loss 215.93396
Epoch 50: Val Loss 215.75549
Epoch 51: Val Loss 215.57442
Epoch 52: Val Loss 215.38953
Epoch 53: Val Loss 215.20247
Epoch 54: Val Loss 215.01320
Epoch 55: Val Loss 214.82193
Epoch 56: Val Loss 214.62665
Epoch 57: Val Loss 214.42812
Epoch 58: Val Loss 214.22710
Epoch 59: Val Loss 214.02228
Epoch 60: Val Loss 213.81409
Epoch 61: Val Loss 213.60254
Epoch 62: Val Loss 213.38660
Epoch 63: Val Loss 213.16748
Epoch 64: Val Loss 212.94550
Epoch 65: Val Loss 212.72054
Epoch 66: Val Loss 212.49136
Epoch 67: Val Loss 212.25925
Epoch 68: Val Loss 212.02426
Epoch 69: Val Loss 211.78531
Epoch 70: Val Loss 211.54323
Epoch 71: Val Loss 211.29735
Epoch 72: Val Loss 211.04637
Epoch 73: Val Loss 210.79195
Epoch 74: Val Loss 210.53282
Epoch 75: Val Loss 210.26952
Epoch 76: Val Loss 210.00111
Epoch 77: Val Loss 209.72859
Epoch 78: Val Loss 209.45271
Epoch 79: Val Loss 209.17183
Epoch 80: Val Loss 208.88477
Epoch 81: Val Loss 208.59131
Epoch 82: Val Loss 208.29378
Epoch 83: Val Loss 207.99139
Epoch 84: Val Loss 207.68416
Epoch 85: Val Loss 207.37267
Epoch 86: Val Loss 207.05617
Epoch 87: Val Loss 206.73560
Epoch 88: Val Loss 206.40932
Epoch 89: Val Loss 206.07849
Epoch 90: Val Loss 205.74309
Epoch 91: Val Loss 205.40224
Epoch 92: Val Loss 205.05641
Epoch 93: Val Loss 204.70438
Epoch 94: Val Loss 204.34785
Epoch 95: Val Loss 203.98451
Epoch 96: Val Loss 203.61543
Epoch 97: Val Loss 203.24112
Epoch 98: Val Loss 202.86099
Epoch 99: Val Loss 202.47372
{'MSE - mean': 209.037454514859, 'MSE - std': 6.44334408861755, 'R2 - mean': -308.68560133586027, 'R2 - std': 54.10167221365293} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.37434
Epoch 1: Val Loss 227.27054
Epoch 2: Val Loss 227.16621
Epoch 3: Val Loss 227.06076
Epoch 4: Val Loss 226.95486
Epoch 5: Val Loss 226.84859
Epoch 6: Val Loss 226.74208
Epoch 7: Val Loss 226.63470
Epoch 8: Val Loss 226.52667
Epoch 9: Val Loss 226.41743
Epoch 10: Val Loss 226.30699
Epoch 11: Val Loss 226.19594
Epoch 12: Val Loss 226.08371
Epoch 13: Val Loss 225.97061
Epoch 14: Val Loss 225.85637
Epoch 15: Val Loss 225.74127
Epoch 16: Val Loss 225.62495
Epoch 17: Val Loss 225.50754
Epoch 18: Val Loss 225.38901
Epoch 19: Val Loss 225.26965
Epoch 20: Val Loss 225.14880
Epoch 21: Val Loss 225.02655
Epoch 22: Val Loss 224.90306
Epoch 23: Val Loss 224.77881
Epoch 24: Val Loss 224.65382
Epoch 25: Val Loss 224.52774
Epoch 26: Val Loss 224.40036
Epoch 27: Val Loss 224.27112
Epoch 28: Val Loss 224.14053
Epoch 29: Val Loss 224.00912
Epoch 30: Val Loss 223.87717
Epoch 31: Val Loss 223.74377
Epoch 32: Val Loss 223.60876
Epoch 33: Val Loss 223.47218
Epoch 34: Val Loss 223.33311
Epoch 35: Val Loss 223.19165
Epoch 36: Val Loss 223.04907
Epoch 37: Val Loss 222.90523
Epoch 38: Val Loss 222.76028
Epoch 39: Val Loss 222.61385
Epoch 40: Val Loss 222.46484
Epoch 41: Val Loss 222.31380
Epoch 42: Val Loss 222.16110
Epoch 43: Val Loss 222.00652
Epoch 44: Val Loss 221.85020
Epoch 45: Val Loss 221.69235
Epoch 46: Val Loss 221.53239
Epoch 47: Val Loss 221.37108
Epoch 48: Val Loss 221.20837
Epoch 49: Val Loss 221.04408
Epoch 50: Val Loss 220.87874
Epoch 51: Val Loss 220.71169
Epoch 52: Val Loss 220.54337
Epoch 53: Val Loss 220.37302
Epoch 54: Val Loss 220.20172
Epoch 55: Val Loss 220.02875
Epoch 56: Val Loss 219.85385
Epoch 57: Val Loss 219.67842
Epoch 58: Val Loss 219.50162
Epoch 59: Val Loss 219.32341
Epoch 60: Val Loss 219.14330
Epoch 61: Val Loss 218.96222
Epoch 62: Val Loss 218.77873
Epoch 63: Val Loss 218.59337
Epoch 64: Val Loss 218.40582
Epoch 65: Val Loss 218.21602
Epoch 66: Val Loss 218.02425
Epoch 67: Val Loss 217.83098
Epoch 68: Val Loss 217.63533
Epoch 69: Val Loss 217.43790
Epoch 70: Val Loss 217.23862
Epoch 71: Val Loss 217.03612
Epoch 72: Val Loss 216.83185
Epoch 73: Val Loss 216.62550
Epoch 74: Val Loss 216.41660
Epoch 75: Val Loss 216.20581
Epoch 76: Val Loss 215.99246
Epoch 77: Val Loss 215.77679
Epoch 78: Val Loss 215.55948
Epoch 79: Val Loss 215.33980
Epoch 80: Val Loss 215.11748
Epoch 81: Val Loss 214.89323
Epoch 82: Val Loss 214.66745
Epoch 83: Val Loss 214.43907
Epoch 84: Val Loss 214.20833
Epoch 85: Val Loss 213.97473
Epoch 86: Val Loss 213.73926
Epoch 87: Val Loss 213.50041
Epoch 88: Val Loss 213.25871
Epoch 89: Val Loss 213.01337
Epoch 90: Val Loss 212.76633
Epoch 91: Val Loss 212.51704
Epoch 92: Val Loss 212.26585
Epoch 93: Val Loss 212.01205
Epoch 94: Val Loss 211.75523
Epoch 95: Val Loss 211.49451
Epoch 96: Val Loss 211.23155
Epoch 97: Val Loss 210.96478
Epoch 98: Val Loss 210.69601
Epoch 99: Val Loss 210.42407
{'MSE - mean': 209.31478207128575, 'MSE - std': 5.789731317571889, 'R2 - mean': -316.59058473609264, 'R2 - std': 50.90724702125067} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 209.31478207128575 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 5 with value: 209.31478207128575.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 218.15932
Epoch 1: Val Loss 218.14520
Epoch 2: Val Loss 218.13110
Epoch 3: Val Loss 218.11700
Epoch 4: Val Loss 218.10292
Epoch 5: Val Loss 218.08884
Epoch 6: Val Loss 218.07481
Epoch 7: Val Loss 218.06075
Epoch 8: Val Loss 218.04668
Epoch 9: Val Loss 218.03259
Epoch 10: Val Loss 218.01852
Epoch 11: Val Loss 218.00447
Epoch 12: Val Loss 217.99040
Epoch 13: Val Loss 217.97629
Epoch 14: Val Loss 217.96225
Epoch 15: Val Loss 217.94810
Epoch 16: Val Loss 217.93396
Epoch 17: Val Loss 217.91982
Epoch 18: Val Loss 217.90567
Epoch 19: Val Loss 217.89157
Epoch 20: Val Loss 217.87740
Epoch 21: Val Loss 217.86324
Epoch 22: Val Loss 217.84915
Epoch 23: Val Loss 217.83502
Epoch 24: Val Loss 217.82094
Epoch 25: Val Loss 217.80681
Epoch 26: Val Loss 217.79266
Epoch 27: Val Loss 217.77853
Epoch 28: Val Loss 217.76443
Epoch 29: Val Loss 217.75031
Epoch 30: Val Loss 217.73619
Epoch 31: Val Loss 217.72211
Epoch 32: Val Loss 217.70796
Epoch 33: Val Loss 217.69380
Epoch 34: Val Loss 217.67958
Epoch 35: Val Loss 217.66528
Epoch 36: Val Loss 217.65092
Epoch 37: Val Loss 217.63661
Epoch 38: Val Loss 217.62230
Epoch 39: Val Loss 217.60802
Epoch 40: Val Loss 217.59375
Epoch 41: Val Loss 217.57948
Epoch 42: Val Loss 217.56516
Epoch 43: Val Loss 217.55084
Epoch 44: Val Loss 217.53642
Epoch 45: Val Loss 217.52203
Epoch 46: Val Loss 217.50769
Epoch 47: Val Loss 217.49332
Epoch 48: Val Loss 217.47891
Epoch 49: Val Loss 217.46451
Epoch 50: Val Loss 217.45010
Epoch 51: Val Loss 217.43576
Epoch 52: Val Loss 217.42134
Epoch 53: Val Loss 217.40689
Epoch 54: Val Loss 217.39241
Epoch 55: Val Loss 217.37798
Epoch 56: Val Loss 217.36357
Epoch 57: Val Loss 217.34912
Epoch 58: Val Loss 217.33464
Epoch 59: Val Loss 217.32011
Epoch 60: Val Loss 217.30563
Epoch 61: Val Loss 217.29112
Epoch 62: Val Loss 217.27657
Epoch 63: Val Loss 217.26202
Epoch 64: Val Loss 217.24747
Epoch 65: Val Loss 217.23289
Epoch 66: Val Loss 217.21844
Epoch 67: Val Loss 217.20386
Epoch 68: Val Loss 217.18932
Epoch 69: Val Loss 217.17479
Epoch 70: Val Loss 217.16022
Epoch 71: Val Loss 217.14569
Epoch 72: Val Loss 217.13107
Epoch 73: Val Loss 217.11652
Epoch 74: Val Loss 217.10191
Epoch 75: Val Loss 217.08725
Epoch 76: Val Loss 217.07266
Epoch 77: Val Loss 217.05811
Epoch 78: Val Loss 217.04356
Epoch 79: Val Loss 217.02899
Epoch 80: Val Loss 217.01434
Epoch 81: Val Loss 216.99973
Epoch 82: Val Loss 216.98515
Epoch 83: Val Loss 216.97057
Epoch 84: Val Loss 216.95596
Epoch 85: Val Loss 216.94138
Epoch 86: Val Loss 216.92676
Epoch 87: Val Loss 216.91216
Epoch 88: Val Loss 216.89752
Epoch 89: Val Loss 216.88292
Epoch 90: Val Loss 216.86830
Epoch 91: Val Loss 216.85367
Epoch 92: Val Loss 216.83904
Epoch 93: Val Loss 216.82439
Epoch 94: Val Loss 216.80974
Epoch 95: Val Loss 216.79515
Epoch 96: Val Loss 216.78055
Epoch 97: Val Loss 216.76585
Epoch 98: Val Loss 216.75117
Epoch 99: Val Loss 216.73647
{'MSE - mean': 216.73646683637887, 'MSE - std': 0.0, 'R2 - mean': -393.388707900918, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.16418
Epoch 1: Val Loss 223.15048
Epoch 2: Val Loss 223.13681
Epoch 3: Val Loss 223.12309
Epoch 4: Val Loss 223.10938
Epoch 5: Val Loss 223.09567
Epoch 6: Val Loss 223.08194
Epoch 7: Val Loss 223.06824
Epoch 8: Val Loss 223.05449
Epoch 9: Val Loss 223.04077
Epoch 10: Val Loss 223.02701
Epoch 11: Val Loss 223.01324
Epoch 12: Val Loss 222.99945
Epoch 13: Val Loss 222.98564
Epoch 14: Val Loss 222.97191
Epoch 15: Val Loss 222.95808
Epoch 16: Val Loss 222.94429
Epoch 17: Val Loss 222.93047
Epoch 18: Val Loss 222.91666
Epoch 19: Val Loss 222.90283
Epoch 20: Val Loss 222.88902
Epoch 21: Val Loss 222.87518
Epoch 22: Val Loss 222.86137
Epoch 23: Val Loss 222.84758
Epoch 24: Val Loss 222.83377
Epoch 25: Val Loss 222.81995
Epoch 26: Val Loss 222.80611
Epoch 27: Val Loss 222.79227
Epoch 28: Val Loss 222.77838
Epoch 29: Val Loss 222.76453
Epoch 30: Val Loss 222.75066
Epoch 31: Val Loss 222.73676
Epoch 32: Val Loss 222.72292
Epoch 33: Val Loss 222.70898
Epoch 34: Val Loss 222.69510
Epoch 35: Val Loss 222.68121
Epoch 36: Val Loss 222.66728
Epoch 37: Val Loss 222.65337
Epoch 38: Val Loss 222.63942
Epoch 39: Val Loss 222.62546
Epoch 40: Val Loss 222.61151
Epoch 41: Val Loss 222.59749
Epoch 42: Val Loss 222.58350
Epoch 43: Val Loss 222.56946
Epoch 44: Val Loss 222.55542
Epoch 45: Val Loss 222.54140
Epoch 46: Val Loss 222.52737
Epoch 47: Val Loss 222.51331
Epoch 48: Val Loss 222.49928
Epoch 49: Val Loss 222.48521
Epoch 50: Val Loss 222.47112
Epoch 51: Val Loss 222.45703
Epoch 52: Val Loss 222.44298
Epoch 53: Val Loss 222.42883
Epoch 54: Val Loss 222.41470
Epoch 55: Val Loss 222.40050
Epoch 56: Val Loss 222.38632
Epoch 57: Val Loss 222.37221
Epoch 58: Val Loss 222.35805
Epoch 59: Val Loss 222.34384
Epoch 60: Val Loss 222.32961
Epoch 61: Val Loss 222.31541
Epoch 62: Val Loss 222.30116
Epoch 63: Val Loss 222.28691
Epoch 64: Val Loss 222.27261
Epoch 65: Val Loss 222.25832
Epoch 66: Val Loss 222.24391
Epoch 67: Val Loss 222.22955
Epoch 68: Val Loss 222.21524
Epoch 69: Val Loss 222.20087
Epoch 70: Val Loss 222.18651
Epoch 71: Val Loss 222.17213
Epoch 72: Val Loss 222.15770
Epoch 73: Val Loss 222.14330
Epoch 74: Val Loss 222.12886
Epoch 75: Val Loss 222.11443
Epoch 76: Val Loss 222.09999
Epoch 77: Val Loss 222.08548
Epoch 78: Val Loss 222.07098
Epoch 79: Val Loss 222.05644
Epoch 80: Val Loss 222.04187
Epoch 81: Val Loss 222.02734
Epoch 82: Val Loss 222.01273
Epoch 83: Val Loss 221.99815
Epoch 84: Val Loss 221.98354
Epoch 85: Val Loss 221.96887
Epoch 86: Val Loss 221.95427
Epoch 87: Val Loss 221.93959
Epoch 88: Val Loss 221.92488
Epoch 89: Val Loss 221.91025
Epoch 90: Val Loss 221.89555
Epoch 91: Val Loss 221.88091
Epoch 92: Val Loss 221.86623
Epoch 93: Val Loss 221.85153
Epoch 94: Val Loss 221.83679
Epoch 95: Val Loss 221.82207
Epoch 96: Val Loss 221.80737
Epoch 97: Val Loss 221.79263
Epoch 98: Val Loss 221.77788
Epoch 99: Val Loss 221.76314
{'MSE - mean': 219.24980151089852, 'MSE - std': 2.5133346745196405, 'R2 - mean': -346.3488427919578, 'R2 - std': 47.039865108960214} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.90518
Epoch 1: Val Loss 217.89011
Epoch 2: Val Loss 217.87498
Epoch 3: Val Loss 217.85988
Epoch 4: Val Loss 217.84470
Epoch 5: Val Loss 217.82951
Epoch 6: Val Loss 217.81435
Epoch 7: Val Loss 217.79918
Epoch 8: Val Loss 217.78400
Epoch 9: Val Loss 217.76877
Epoch 10: Val Loss 217.75359
Epoch 11: Val Loss 217.73837
Epoch 12: Val Loss 217.72316
Epoch 13: Val Loss 217.70793
Epoch 14: Val Loss 217.69266
Epoch 15: Val Loss 217.67740
Epoch 16: Val Loss 217.66211
Epoch 17: Val Loss 217.64679
Epoch 18: Val Loss 217.63153
Epoch 19: Val Loss 217.61623
Epoch 20: Val Loss 217.60091
Epoch 21: Val Loss 217.58556
Epoch 22: Val Loss 217.57024
Epoch 23: Val Loss 217.55487
Epoch 24: Val Loss 217.53946
Epoch 25: Val Loss 217.52408
Epoch 26: Val Loss 217.50864
Epoch 27: Val Loss 217.49318
Epoch 28: Val Loss 217.47771
Epoch 29: Val Loss 217.46225
Epoch 30: Val Loss 217.44676
Epoch 31: Val Loss 217.43132
Epoch 32: Val Loss 217.41585
Epoch 33: Val Loss 217.40036
Epoch 34: Val Loss 217.38484
Epoch 35: Val Loss 217.36932
Epoch 36: Val Loss 217.35379
Epoch 37: Val Loss 217.33827
Epoch 38: Val Loss 217.32281
Epoch 39: Val Loss 217.30734
Epoch 40: Val Loss 217.29184
Epoch 41: Val Loss 217.27632
Epoch 42: Val Loss 217.26079
Epoch 43: Val Loss 217.24524
Epoch 44: Val Loss 217.22971
Epoch 45: Val Loss 217.21416
Epoch 46: Val Loss 217.19859
Epoch 47: Val Loss 217.18294
Epoch 48: Val Loss 217.16730
Epoch 49: Val Loss 217.15166
Epoch 50: Val Loss 217.13596
Epoch 51: Val Loss 217.12032
Epoch 52: Val Loss 217.10471
Epoch 53: Val Loss 217.08905
Epoch 54: Val Loss 217.07335
Epoch 55: Val Loss 217.05763
Epoch 56: Val Loss 217.04185
Epoch 57: Val Loss 217.02611
Epoch 58: Val Loss 217.01035
Epoch 59: Val Loss 216.99455
Epoch 60: Val Loss 216.97879
Epoch 61: Val Loss 216.96298
Epoch 62: Val Loss 216.94716
Epoch 63: Val Loss 216.93134
Epoch 64: Val Loss 216.91554
Epoch 65: Val Loss 216.89969
Epoch 66: Val Loss 216.88385
Epoch 67: Val Loss 216.86801
Epoch 68: Val Loss 216.85216
Epoch 69: Val Loss 216.83627
Epoch 70: Val Loss 216.82040
Epoch 71: Val Loss 216.80447
Epoch 72: Val Loss 216.78854
Epoch 73: Val Loss 216.77260
Epoch 74: Val Loss 216.75658
Epoch 75: Val Loss 216.74062
Epoch 76: Val Loss 216.72466
Epoch 77: Val Loss 216.70862
Epoch 78: Val Loss 216.69267
Epoch 79: Val Loss 216.67662
Epoch 80: Val Loss 216.66057
Epoch 81: Val Loss 216.64456
Epoch 82: Val Loss 216.62848
Epoch 83: Val Loss 216.61232
Epoch 84: Val Loss 216.59613
Epoch 85: Val Loss 216.57996
Epoch 86: Val Loss 216.56378
Epoch 87: Val Loss 216.54759
Epoch 88: Val Loss 216.53136
Epoch 89: Val Loss 216.51512
Epoch 90: Val Loss 216.49890
Epoch 91: Val Loss 216.48257
Epoch 92: Val Loss 216.46629
Epoch 93: Val Loss 216.45004
Epoch 94: Val Loss 216.43372
Epoch 95: Val Loss 216.41742
Epoch 96: Val Loss 216.40108
Epoch 97: Val Loss 216.38467
Epoch 98: Val Loss 216.36830
Epoch 99: Val Loss 216.35190
{'MSE - mean': 218.2838341234409, 'MSE - std': 2.465242404178251, 'R2 - mean': -335.24144692904935, 'R2 - std': 41.495956735046214} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.85101
Epoch 1: Val Loss 232.84189
Epoch 2: Val Loss 232.83273
Epoch 3: Val Loss 232.82356
Epoch 4: Val Loss 232.81442
Epoch 5: Val Loss 232.80528
Epoch 6: Val Loss 232.79616
Epoch 7: Val Loss 232.78691
Epoch 8: Val Loss 232.77771
Epoch 9: Val Loss 232.76848
Epoch 10: Val Loss 232.75922
Epoch 11: Val Loss 232.74991
Epoch 12: Val Loss 232.74059
Epoch 13: Val Loss 232.73128
Epoch 14: Val Loss 232.72195
Epoch 15: Val Loss 232.71259
Epoch 16: Val Loss 232.70323
Epoch 17: Val Loss 232.69389
Epoch 18: Val Loss 232.68449
Epoch 19: Val Loss 232.67509
Epoch 20: Val Loss 232.66570
Epoch 21: Val Loss 232.65628
Epoch 22: Val Loss 232.64687
Epoch 23: Val Loss 232.63744
Epoch 24: Val Loss 232.62791
Epoch 25: Val Loss 232.61842
Epoch 26: Val Loss 232.60886
Epoch 27: Val Loss 232.59929
Epoch 28: Val Loss 232.58974
Epoch 29: Val Loss 232.58014
Epoch 30: Val Loss 232.57051
Epoch 31: Val Loss 232.56084
Epoch 32: Val Loss 232.55115
Epoch 33: Val Loss 232.54143
Epoch 34: Val Loss 232.53171
Epoch 35: Val Loss 232.52194
Epoch 36: Val Loss 232.51225
Epoch 37: Val Loss 232.50250
Epoch 38: Val Loss 232.49278
Epoch 39: Val Loss 232.48306
Epoch 40: Val Loss 232.47339
Epoch 41: Val Loss 232.46368
Epoch 42: Val Loss 232.45390
Epoch 43: Val Loss 232.44405
Epoch 44: Val Loss 232.43419
Epoch 45: Val Loss 232.42438
Epoch 46: Val Loss 232.41452
Epoch 47: Val Loss 232.40471
Epoch 48: Val Loss 232.39488
Epoch 49: Val Loss 232.38504
Epoch 50: Val Loss 232.37520
Epoch 51: Val Loss 232.36531
Epoch 52: Val Loss 232.35536
Epoch 53: Val Loss 232.34541
Epoch 54: Val Loss 232.33548
Epoch 55: Val Loss 232.32556
Epoch 56: Val Loss 232.31569
Epoch 57: Val Loss 232.30568
Epoch 58: Val Loss 232.29567
Epoch 59: Val Loss 232.28561
Epoch 60: Val Loss 232.27557
Epoch 61: Val Loss 232.26553
Epoch 62: Val Loss 232.25542
Epoch 63: Val Loss 232.24544
Epoch 64: Val Loss 232.23532
Epoch 65: Val Loss 232.22525
Epoch 66: Val Loss 232.21521
Epoch 67: Val Loss 232.20512
Epoch 68: Val Loss 232.19499
Epoch 69: Val Loss 232.18484
Epoch 70: Val Loss 232.17465
Epoch 71: Val Loss 232.16441
Epoch 72: Val Loss 232.15417
Epoch 73: Val Loss 232.14397
Epoch 74: Val Loss 232.13374
Epoch 75: Val Loss 232.12350
Epoch 76: Val Loss 232.11327
Epoch 77: Val Loss 232.10303
Epoch 78: Val Loss 232.09273
Epoch 79: Val Loss 232.08243
Epoch 80: Val Loss 232.07210
Epoch 81: Val Loss 232.06175
Epoch 82: Val Loss 232.05138
Epoch 83: Val Loss 232.04099
Epoch 84: Val Loss 232.03056
Epoch 85: Val Loss 232.02013
Epoch 86: Val Loss 232.00972
Epoch 87: Val Loss 231.99930
Epoch 88: Val Loss 231.98889
Epoch 89: Val Loss 231.97839
Epoch 90: Val Loss 231.96794
Epoch 91: Val Loss 231.95749
Epoch 92: Val Loss 231.94701
Epoch 93: Val Loss 231.93654
Epoch 94: Val Loss 231.92609
Epoch 95: Val Loss 231.91565
Epoch 96: Val Loss 231.90517
Epoch 97: Val Loss 231.89468
Epoch 98: Val Loss 231.88423
Epoch 99: Val Loss 231.87373
{'MSE - mean': 221.68131184617974, 'MSE - std': 6.259922500561746, 'R2 - mean': -325.1161386690454, 'R2 - std': 39.987515815538366} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.41037
Epoch 1: Val Loss 227.39986
Epoch 2: Val Loss 227.38931
Epoch 3: Val Loss 227.37880
Epoch 4: Val Loss 227.36830
Epoch 5: Val Loss 227.35780
Epoch 6: Val Loss 227.34731
Epoch 7: Val Loss 227.33688
Epoch 8: Val Loss 227.32645
Epoch 9: Val Loss 227.31602
Epoch 10: Val Loss 227.30557
Epoch 11: Val Loss 227.29514
Epoch 12: Val Loss 227.28470
Epoch 13: Val Loss 227.27434
Epoch 14: Val Loss 227.26392
Epoch 15: Val Loss 227.25356
Epoch 16: Val Loss 227.24319
Epoch 17: Val Loss 227.23280
Epoch 18: Val Loss 227.22243
Epoch 19: Val Loss 227.21208
Epoch 20: Val Loss 227.20172
Epoch 21: Val Loss 227.19136
Epoch 22: Val Loss 227.18103
Epoch 23: Val Loss 227.17067
Epoch 24: Val Loss 227.16035
Epoch 25: Val Loss 227.15005
Epoch 26: Val Loss 227.13972
Epoch 27: Val Loss 227.12944
Epoch 28: Val Loss 227.11909
Epoch 29: Val Loss 227.10875
Epoch 30: Val Loss 227.09846
Epoch 31: Val Loss 227.08813
Epoch 32: Val Loss 227.07780
Epoch 33: Val Loss 227.06749
Epoch 34: Val Loss 227.05717
Epoch 35: Val Loss 227.04686
Epoch 36: Val Loss 227.03656
Epoch 37: Val Loss 227.02626
Epoch 38: Val Loss 227.01599
Epoch 39: Val Loss 227.00571
Epoch 40: Val Loss 226.99544
Epoch 41: Val Loss 226.98517
Epoch 42: Val Loss 226.97488
Epoch 43: Val Loss 226.96457
Epoch 44: Val Loss 226.95418
Epoch 45: Val Loss 226.94388
Epoch 46: Val Loss 226.93355
Epoch 47: Val Loss 226.92319
Epoch 48: Val Loss 226.91283
Epoch 49: Val Loss 226.90253
Epoch 50: Val Loss 226.89221
Epoch 51: Val Loss 226.88188
Epoch 52: Val Loss 226.87152
Epoch 53: Val Loss 226.86121
Epoch 54: Val Loss 226.85088
Epoch 55: Val Loss 226.84058
Epoch 56: Val Loss 226.83023
Epoch 57: Val Loss 226.81996
Epoch 58: Val Loss 226.80965
Epoch 59: Val Loss 226.79935
Epoch 60: Val Loss 226.78903
Epoch 61: Val Loss 226.77870
Epoch 62: Val Loss 226.76840
Epoch 63: Val Loss 226.75812
Epoch 64: Val Loss 226.74785
Epoch 65: Val Loss 226.73759
Epoch 66: Val Loss 226.72734
Epoch 67: Val Loss 226.71715
Epoch 68: Val Loss 226.70692
Epoch 69: Val Loss 226.69666
Epoch 70: Val Loss 226.68645
Epoch 71: Val Loss 226.67621
Epoch 72: Val Loss 226.66595
Epoch 73: Val Loss 226.65573
Epoch 74: Val Loss 226.64555
Epoch 75: Val Loss 226.63541
Epoch 76: Val Loss 226.62526
Epoch 77: Val Loss 226.61507
Epoch 78: Val Loss 226.60487
Epoch 79: Val Loss 226.59473
Epoch 80: Val Loss 226.58461
Epoch 81: Val Loss 226.57446
Epoch 82: Val Loss 226.56429
Epoch 83: Val Loss 226.55414
Epoch 84: Val Loss 226.54398
Epoch 85: Val Loss 226.53383
Epoch 86: Val Loss 226.52365
Epoch 87: Val Loss 226.51350
Epoch 88: Val Loss 226.50331
Epoch 89: Val Loss 226.49319
Epoch 90: Val Loss 226.48305
Epoch 91: Val Loss 226.47290
Epoch 92: Val Loss 226.46277
Epoch 93: Val Loss 226.45261
Epoch 94: Val Loss 226.44244
Epoch 95: Val Loss 226.43234
Epoch 96: Val Loss 226.42224
Epoch 97: Val Loss 226.41211
Epoch 98: Val Loss 226.40198
Epoch 99: Val Loss 226.39191
{'MSE - mean': 222.62343419800783, 'MSE - std': 5.907595269962872, 'R2 - mean': -335.0349159466646, 'R2 - std': 40.89901841866627} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 222.62343419800783 and parameters: {'dim': 64, 'depth': 12, 'heads': 4, 'weight_decay': -4, 'learning_rate': -5, 'dropout': 0.5}. Best is trial 5 with value: 209.31478207128575.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.77522
Epoch 1: Val Loss 222.77390
Epoch 2: Val Loss 222.77257
Epoch 3: Val Loss 222.77121
Epoch 4: Val Loss 222.76988
Epoch 5: Val Loss 222.76855
Epoch 6: Val Loss 222.76721
Epoch 7: Val Loss 222.76590
Epoch 8: Val Loss 222.76454
Epoch 9: Val Loss 222.76323
Epoch 10: Val Loss 222.76190
Epoch 11: Val Loss 222.76059
Epoch 12: Val Loss 222.75926
Epoch 13: Val Loss 222.75793
Epoch 14: Val Loss 222.75662
Epoch 15: Val Loss 222.75528
Epoch 16: Val Loss 222.75395
Epoch 17: Val Loss 222.75262
Epoch 18: Val Loss 222.75130
Epoch 19: Val Loss 222.74998
Epoch 20: Val Loss 222.74866
Epoch 21: Val Loss 222.74734
Epoch 22: Val Loss 222.74600
Epoch 23: Val Loss 222.74467
Epoch 24: Val Loss 222.74336
Epoch 25: Val Loss 222.74203
Epoch 26: Val Loss 222.74071
Epoch 27: Val Loss 222.73938
Epoch 28: Val Loss 222.73804
Epoch 29: Val Loss 222.73672
Epoch 30: Val Loss 222.73538
Epoch 31: Val Loss 222.73407
Epoch 32: Val Loss 222.73276
Epoch 33: Val Loss 222.73141
Epoch 34: Val Loss 222.73010
Epoch 35: Val Loss 222.72879
Epoch 36: Val Loss 222.72743
Epoch 37: Val Loss 222.72612
Epoch 38: Val Loss 222.72479
Epoch 39: Val Loss 222.72346
Epoch 40: Val Loss 222.72215
Epoch 41: Val Loss 222.72084
Epoch 42: Val Loss 222.71948
Epoch 43: Val Loss 222.71817
Epoch 44: Val Loss 222.71686
Epoch 45: Val Loss 222.71553
Epoch 46: Val Loss 222.71422
Epoch 47: Val Loss 222.71289
Epoch 48: Val Loss 222.71158
Epoch 49: Val Loss 222.71024
Epoch 50: Val Loss 222.70892
Epoch 51: Val Loss 222.70760
Epoch 52: Val Loss 222.70627
Epoch 53: Val Loss 222.70494
Epoch 54: Val Loss 222.70363
Epoch 55: Val Loss 222.70230
Epoch 56: Val Loss 222.70097
Epoch 57: Val Loss 222.69966
Epoch 58: Val Loss 222.69835
Epoch 59: Val Loss 222.69702
Epoch 60: Val Loss 222.69569
Epoch 61: Val Loss 222.69437
Epoch 62: Val Loss 222.69307
Epoch 63: Val Loss 222.69173
Epoch 64: Val Loss 222.69043
Epoch 65: Val Loss 222.68910
Epoch 66: Val Loss 222.68779
Epoch 67: Val Loss 222.68646
Epoch 68: Val Loss 222.68513
Epoch 69: Val Loss 222.68379
Epoch 70: Val Loss 222.68248
Epoch 71: Val Loss 222.68115
Epoch 72: Val Loss 222.67982
Epoch 73: Val Loss 222.67851
Epoch 74: Val Loss 222.67719
Epoch 75: Val Loss 222.67586
Epoch 76: Val Loss 222.67455
Epoch 77: Val Loss 222.67322
Epoch 78: Val Loss 222.67191
Epoch 79: Val Loss 222.67058
Epoch 80: Val Loss 222.66925
Epoch 81: Val Loss 222.66794
Epoch 82: Val Loss 222.66664
Epoch 83: Val Loss 222.66531
Epoch 84: Val Loss 222.66400
Epoch 85: Val Loss 222.66266
Epoch 86: Val Loss 222.66133
Epoch 87: Val Loss 222.66002
Epoch 88: Val Loss 222.65869
Epoch 89: Val Loss 222.65736
Epoch 90: Val Loss 222.65605
Epoch 91: Val Loss 222.65474
Epoch 92: Val Loss 222.65343
Epoch 93: Val Loss 222.65208
Epoch 94: Val Loss 222.65076
Epoch 95: Val Loss 222.64944
Epoch 96: Val Loss 222.64813
Epoch 97: Val Loss 222.64682
Epoch 98: Val Loss 222.64549
Epoch 99: Val Loss 222.64418
{'MSE - mean': 222.64417395278443, 'MSE - std': 0.0, 'R2 - mean': -404.138781528607, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.09409
Epoch 1: Val Loss 229.09279
Epoch 2: Val Loss 229.09149
Epoch 3: Val Loss 229.09018
Epoch 4: Val Loss 229.08888
Epoch 5: Val Loss 229.08759
Epoch 6: Val Loss 229.08629
Epoch 7: Val Loss 229.08499
Epoch 8: Val Loss 229.08368
Epoch 9: Val Loss 229.08238
Epoch 10: Val Loss 229.08107
Epoch 11: Val Loss 229.07977
Epoch 12: Val Loss 229.07848
Epoch 13: Val Loss 229.07716
Epoch 14: Val Loss 229.07585
Epoch 15: Val Loss 229.07455
Epoch 16: Val Loss 229.07326
Epoch 17: Val Loss 229.07196
Epoch 18: Val Loss 229.07065
Epoch 19: Val Loss 229.06935
Epoch 20: Val Loss 229.06805
Epoch 21: Val Loss 229.06674
Epoch 22: Val Loss 229.06544
Epoch 23: Val Loss 229.06413
Epoch 24: Val Loss 229.06284
Epoch 25: Val Loss 229.06154
Epoch 26: Val Loss 229.06023
Epoch 27: Val Loss 229.05893
Epoch 28: Val Loss 229.05762
Epoch 29: Val Loss 229.05630
Epoch 30: Val Loss 229.05502
Epoch 31: Val Loss 229.05371
Epoch 32: Val Loss 229.05240
Epoch 33: Val Loss 229.05112
Epoch 34: Val Loss 229.04979
Epoch 35: Val Loss 229.04849
Epoch 36: Val Loss 229.04718
Epoch 37: Val Loss 229.04588
Epoch 38: Val Loss 229.04457
Epoch 39: Val Loss 229.04326
Epoch 40: Val Loss 229.04195
Epoch 41: Val Loss 229.04065
Epoch 42: Val Loss 229.03932
Epoch 43: Val Loss 229.03801
Epoch 44: Val Loss 229.03671
Epoch 45: Val Loss 229.03540
Epoch 46: Val Loss 229.03409
Epoch 47: Val Loss 229.03279
Epoch 48: Val Loss 229.03148
Epoch 49: Val Loss 229.03015
Epoch 50: Val Loss 229.02884
Epoch 51: Val Loss 229.02754
Epoch 52: Val Loss 229.02623
Epoch 53: Val Loss 229.02492
Epoch 54: Val Loss 229.02362
Epoch 55: Val Loss 229.02231
Epoch 56: Val Loss 229.02100
Epoch 57: Val Loss 229.01967
Epoch 58: Val Loss 229.01837
Epoch 59: Val Loss 229.01704
Epoch 60: Val Loss 229.01573
Epoch 61: Val Loss 229.01442
Epoch 62: Val Loss 229.01311
Epoch 63: Val Loss 229.01181
Epoch 64: Val Loss 229.01048
Epoch 65: Val Loss 229.00917
Epoch 66: Val Loss 229.00787
Epoch 67: Val Loss 229.00656
Epoch 68: Val Loss 229.00523
Epoch 69: Val Loss 229.00394
Epoch 70: Val Loss 229.00261
Epoch 71: Val Loss 229.00128
Epoch 72: Val Loss 228.99998
Epoch 73: Val Loss 228.99866
Epoch 74: Val Loss 228.99738
Epoch 75: Val Loss 228.99605
Epoch 76: Val Loss 228.99472
Epoch 77: Val Loss 228.99342
Epoch 78: Val Loss 228.99211
Epoch 79: Val Loss 228.99078
Epoch 80: Val Loss 228.98947
Epoch 81: Val Loss 228.98817
Epoch 82: Val Loss 228.98685
Epoch 83: Val Loss 228.98553
Epoch 84: Val Loss 228.98421
Epoch 85: Val Loss 228.98291
Epoch 86: Val Loss 228.98158
Epoch 87: Val Loss 228.98027
Epoch 88: Val Loss 228.97896
Epoch 89: Val Loss 228.97766
Epoch 90: Val Loss 228.97633
Epoch 91: Val Loss 228.97502
Epoch 92: Val Loss 228.97369
Epoch 93: Val Loss 228.97240
Epoch 94: Val Loss 228.97107
Epoch 95: Val Loss 228.96976
Epoch 96: Val Loss 228.96843
Epoch 97: Val Loss 228.96713
Epoch 98: Val Loss 228.96581
Epoch 99: Val Loss 228.96449
{'MSE - mean': 225.8043355108058, 'MSE - std': 3.1601615580213718, 'R2 - mean': -356.5998778488437, 'R2 - std': 47.53890367976334} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.72289
Epoch 1: Val Loss 229.72218
Epoch 2: Val Loss 229.72150
Epoch 3: Val Loss 229.72078
Epoch 4: Val Loss 229.72008
Epoch 5: Val Loss 229.71939
Epoch 6: Val Loss 229.71867
Epoch 7: Val Loss 229.71797
Epoch 8: Val Loss 229.71729
Epoch 9: Val Loss 229.71658
Epoch 10: Val Loss 229.71588
Epoch 11: Val Loss 229.71518
Epoch 12: Val Loss 229.71449
Epoch 13: Val Loss 229.71378
Epoch 14: Val Loss 229.71307
Epoch 15: Val Loss 229.71237
Epoch 16: Val Loss 229.71167
Epoch 17: Val Loss 229.71097
Epoch 18: Val Loss 229.71027
Epoch 19: Val Loss 229.70956
Epoch 20: Val Loss 229.70888
Epoch 21: Val Loss 229.70818
Epoch 22: Val Loss 229.70747
Epoch 23: Val Loss 229.70677
Epoch 24: Val Loss 229.70607
Epoch 25: Val Loss 229.70535
Epoch 26: Val Loss 229.70468
Epoch 27: Val Loss 229.70396
Epoch 28: Val Loss 229.70328
Epoch 29: Val Loss 229.70258
Epoch 30: Val Loss 229.70186
Epoch 31: Val Loss 229.70117
Epoch 32: Val Loss 229.70047
Epoch 33: Val Loss 229.69978
Epoch 34: Val Loss 229.69910
Epoch 35: Val Loss 229.69838
Epoch 36: Val Loss 229.69768
Epoch 37: Val Loss 229.69701
Epoch 38: Val Loss 229.69629
Epoch 39: Val Loss 229.69559
Epoch 40: Val Loss 229.69490
Epoch 41: Val Loss 229.69418
Epoch 42: Val Loss 229.69350
Epoch 43: Val Loss 229.69281
Epoch 44: Val Loss 229.69209
Epoch 45: Val Loss 229.69141
Epoch 46: Val Loss 229.69070
Epoch 47: Val Loss 229.69000
Epoch 48: Val Loss 229.68933
Epoch 49: Val Loss 229.68861
Epoch 50: Val Loss 229.68791
Epoch 51: Val Loss 229.68723
Epoch 52: Val Loss 229.68651
Epoch 53: Val Loss 229.68582
Epoch 54: Val Loss 229.68510
Epoch 55: Val Loss 229.68443
Epoch 56: Val Loss 229.68372
Epoch 57: Val Loss 229.68304
Epoch 58: Val Loss 229.68233
Epoch 59: Val Loss 229.68163
Epoch 60: Val Loss 229.68094
Epoch 61: Val Loss 229.68025
Epoch 62: Val Loss 229.67953
Epoch 63: Val Loss 229.67886
Epoch 64: Val Loss 229.67815
Epoch 65: Val Loss 229.67746
Epoch 66: Val Loss 229.67676
Epoch 67: Val Loss 229.67607
Epoch 68: Val Loss 229.67537
Epoch 69: Val Loss 229.67468
Epoch 70: Val Loss 229.67397
Epoch 71: Val Loss 229.67329
Epoch 72: Val Loss 229.67258
Epoch 73: Val Loss 229.67186
Epoch 74: Val Loss 229.67119
Epoch 75: Val Loss 229.67047
Epoch 76: Val Loss 229.66980
Epoch 77: Val Loss 229.66910
Epoch 78: Val Loss 229.66840
Epoch 79: Val Loss 229.66769
Epoch 80: Val Loss 229.66699
Epoch 81: Val Loss 229.66631
Epoch 82: Val Loss 229.66562
Epoch 83: Val Loss 229.66492
Epoch 84: Val Loss 229.66420
Epoch 85: Val Loss 229.66353
Epoch 86: Val Loss 229.66283
Epoch 87: Val Loss 229.66216
Epoch 88: Val Loss 229.66144
Epoch 89: Val Loss 229.66074
Epoch 90: Val Loss 229.66005
Epoch 91: Val Loss 229.65935
Epoch 92: Val Loss 229.65865
Epoch 93: Val Loss 229.65796
Epoch 94: Val Loss 229.65726
Epoch 95: Val Loss 229.65654
Epoch 96: Val Loss 229.65587
Epoch 97: Val Loss 229.65517
Epoch 98: Val Loss 229.65448
Epoch 99: Val Loss 229.65378
{'MSE - mean': 227.08748662151535, 'MSE - std': 3.154473161669689, 'R2 - mean': -348.51120128362646, 'R2 - std': 40.46584921308254} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.07555
Epoch 1: Val Loss 233.07471
Epoch 2: Val Loss 233.07387
Epoch 3: Val Loss 233.07304
Epoch 4: Val Loss 233.07220
Epoch 5: Val Loss 233.07137
Epoch 6: Val Loss 233.07054
Epoch 7: Val Loss 233.06970
Epoch 8: Val Loss 233.06888
Epoch 9: Val Loss 233.06804
Epoch 10: Val Loss 233.06721
Epoch 11: Val Loss 233.06638
Epoch 12: Val Loss 233.06555
Epoch 13: Val Loss 233.06471
Epoch 14: Val Loss 233.06389
Epoch 15: Val Loss 233.06303
Epoch 16: Val Loss 233.06221
Epoch 17: Val Loss 233.06137
Epoch 18: Val Loss 233.06052
Epoch 19: Val Loss 233.05971
Epoch 20: Val Loss 233.05887
Epoch 21: Val Loss 233.05804
Epoch 22: Val Loss 233.05721
Epoch 23: Val Loss 233.05638
Epoch 24: Val Loss 233.05554
Epoch 25: Val Loss 233.05472
Epoch 26: Val Loss 233.05388
Epoch 27: Val Loss 233.05302
Epoch 28: Val Loss 233.05220
Epoch 29: Val Loss 233.05136
Epoch 30: Val Loss 233.05054
Epoch 31: Val Loss 233.04968
Epoch 32: Val Loss 233.04886
Epoch 33: Val Loss 233.04802
Epoch 34: Val Loss 233.04716
Epoch 35: Val Loss 233.04636
Epoch 36: Val Loss 233.04550
Epoch 37: Val Loss 233.04466
Epoch 38: Val Loss 233.04382
Epoch 39: Val Loss 233.04300
Epoch 40: Val Loss 233.04216
Epoch 41: Val Loss 233.04131
Epoch 42: Val Loss 233.04048
Epoch 43: Val Loss 233.03966
Epoch 44: Val Loss 233.03882
Epoch 45: Val Loss 233.03798
Epoch 46: Val Loss 233.03716
Epoch 47: Val Loss 233.03632
Epoch 48: Val Loss 233.03549
Epoch 49: Val Loss 233.03464
Epoch 50: Val Loss 233.03380
Epoch 51: Val Loss 233.03297
Epoch 52: Val Loss 233.03214
Epoch 53: Val Loss 233.03131
Epoch 54: Val Loss 233.03047
Epoch 55: Val Loss 233.02965
Epoch 56: Val Loss 233.02879
Epoch 57: Val Loss 233.02795
Epoch 58: Val Loss 233.02713
Epoch 59: Val Loss 233.02628
Epoch 60: Val Loss 233.02547
Epoch 61: Val Loss 233.02461
Epoch 62: Val Loss 233.02377
Epoch 63: Val Loss 233.02293
Epoch 64: Val Loss 233.02209
Epoch 65: Val Loss 233.02127
Epoch 66: Val Loss 233.02043
Epoch 67: Val Loss 233.01959
Epoch 68: Val Loss 233.01874
Epoch 69: Val Loss 233.01791
Epoch 70: Val Loss 233.01707
Epoch 71: Val Loss 233.01622
Epoch 72: Val Loss 233.01541
Epoch 73: Val Loss 233.01456
Epoch 74: Val Loss 233.01372
Epoch 75: Val Loss 233.01288
Epoch 76: Val Loss 233.01205
Epoch 77: Val Loss 233.01120
Epoch 78: Val Loss 233.01036
Epoch 79: Val Loss 233.00952
Epoch 80: Val Loss 233.00868
Epoch 81: Val Loss 233.00786
Epoch 82: Val Loss 233.00700
Epoch 83: Val Loss 233.00615
Epoch 84: Val Loss 233.00533
Epoch 85: Val Loss 233.00449
Epoch 86: Val Loss 233.00366
Epoch 87: Val Loss 233.00282
Epoch 88: Val Loss 233.00200
Epoch 89: Val Loss 233.00114
Epoch 90: Val Loss 233.00031
Epoch 91: Val Loss 232.99948
Epoch 92: Val Loss 232.99863
Epoch 93: Val Loss 232.99780
Epoch 94: Val Loss 232.99696
Epoch 95: Val Loss 232.99612
Epoch 96: Val Loss 232.99529
Epoch 97: Val Loss 232.99445
Epoch 98: Val Loss 232.99362
Epoch 99: Val Loss 232.99277
{'MSE - mean': 228.56381203665842, 'MSE - std': 3.7418759853799184, 'R2 - mean': -335.42527156332284, 'R2 - std': 41.73533735740167} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.56325
Epoch 1: Val Loss 236.56247
Epoch 2: Val Loss 236.56169
Epoch 3: Val Loss 236.56090
Epoch 4: Val Loss 236.56010
Epoch 5: Val Loss 236.55931
Epoch 6: Val Loss 236.55853
Epoch 7: Val Loss 236.55775
Epoch 8: Val Loss 236.55695
Epoch 9: Val Loss 236.55617
Epoch 10: Val Loss 236.55539
Epoch 11: Val Loss 236.55460
Epoch 12: Val Loss 236.55382
Epoch 13: Val Loss 236.55302
Epoch 14: Val Loss 236.55223
Epoch 15: Val Loss 236.55145
Epoch 16: Val Loss 236.55067
Epoch 17: Val Loss 236.54985
Epoch 18: Val Loss 236.54907
Epoch 19: Val Loss 236.54829
Epoch 20: Val Loss 236.54752
Epoch 21: Val Loss 236.54669
Epoch 22: Val Loss 236.54591
Epoch 23: Val Loss 236.54514
Epoch 24: Val Loss 236.54436
Epoch 25: Val Loss 236.54355
Epoch 26: Val Loss 236.54276
Epoch 27: Val Loss 236.54198
Epoch 28: Val Loss 236.54120
Epoch 29: Val Loss 236.54041
Epoch 30: Val Loss 236.53961
Epoch 31: Val Loss 236.53883
Epoch 32: Val Loss 236.53804
Epoch 33: Val Loss 236.53725
Epoch 34: Val Loss 236.53647
Epoch 35: Val Loss 236.53566
Epoch 36: Val Loss 236.53487
Epoch 37: Val Loss 236.53409
Epoch 38: Val Loss 236.53331
Epoch 39: Val Loss 236.53252
Epoch 40: Val Loss 236.53171
Epoch 41: Val Loss 236.53091
Epoch 42: Val Loss 236.53014
Epoch 43: Val Loss 236.52934
Epoch 44: Val Loss 236.52855
Epoch 45: Val Loss 236.52776
Epoch 46: Val Loss 236.52696
Epoch 47: Val Loss 236.52618
Epoch 48: Val Loss 236.52536
Epoch 49: Val Loss 236.52460
Epoch 50: Val Loss 236.52380
Epoch 51: Val Loss 236.52299
Epoch 52: Val Loss 236.52222
Epoch 53: Val Loss 236.52142
Epoch 54: Val Loss 236.52065
Epoch 55: Val Loss 236.51984
Epoch 56: Val Loss 236.51906
Epoch 57: Val Loss 236.51826
Epoch 58: Val Loss 236.51749
Epoch 59: Val Loss 236.51671
Epoch 60: Val Loss 236.51590
Epoch 61: Val Loss 236.51509
Epoch 62: Val Loss 236.51431
Epoch 63: Val Loss 236.51350
Epoch 64: Val Loss 236.51273
Epoch 65: Val Loss 236.51193
Epoch 66: Val Loss 236.51112
Epoch 67: Val Loss 236.51035
Epoch 68: Val Loss 236.50955
Epoch 69: Val Loss 236.50877
Epoch 70: Val Loss 236.50797
Epoch 71: Val Loss 236.50719
Epoch 72: Val Loss 236.50639
Epoch 73: Val Loss 236.50560
Epoch 74: Val Loss 236.50479
Epoch 75: Val Loss 236.50401
Epoch 76: Val Loss 236.50322
Epoch 77: Val Loss 236.50244
Epoch 78: Val Loss 236.50162
Epoch 79: Val Loss 236.50084
Epoch 80: Val Loss 236.50006
Epoch 81: Val Loss 236.49924
Epoch 82: Val Loss 236.49846
Epoch 83: Val Loss 236.49768
Epoch 84: Val Loss 236.49686
Epoch 85: Val Loss 236.49608
Epoch 86: Val Loss 236.49529
Epoch 87: Val Loss 236.49449
Epoch 88: Val Loss 236.49370
Epoch 89: Val Loss 236.49292
Epoch 90: Val Loss 236.49211
Epoch 91: Val Loss 236.49132
Epoch 92: Val Loss 236.49051
Epoch 93: Val Loss 236.48973
Epoch 94: Val Loss 236.48894
Epoch 95: Val Loss 236.48813
Epoch 96: Val Loss 236.48735
Epoch 97: Val Loss 236.48657
Epoch 98: Val Loss 236.48576
Epoch 99: Val Loss 236.48497
{'MSE - mean': 230.14804617977217, 'MSE - std': 4.608741693000315, 'R2 - mean': -346.632221188372, 'R2 - std': 43.54140083945922} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 230.14804617977217 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -5, 'learning_rate': -6, 'dropout': 0.3}. Best is trial 5 with value: 209.31478207128575.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.41211
Epoch 1: Val Loss 226.03761
Epoch 2: Val Loss 224.59875
Epoch 3: Val Loss 223.06847
Epoch 4: Val Loss 221.43324
Epoch 5: Val Loss 219.64510
Epoch 6: Val Loss 217.68005
Epoch 7: Val Loss 215.48244
Epoch 8: Val Loss 213.01268
Epoch 9: Val Loss 210.22408
Epoch 10: Val Loss 207.05914
Epoch 11: Val Loss 203.45383
Epoch 12: Val Loss 199.35640
Epoch 13: Val Loss 194.70212
Epoch 14: Val Loss 189.40558
Epoch 15: Val Loss 183.36699
Epoch 16: Val Loss 176.52998
Epoch 17: Val Loss 168.78543
Epoch 18: Val Loss 160.07375
Epoch 19: Val Loss 150.35745
Epoch 20: Val Loss 139.62146
Epoch 21: Val Loss 127.90684
Epoch 22: Val Loss 115.26911
Epoch 23: Val Loss 101.83136
Epoch 24: Val Loss 87.82507
Epoch 25: Val Loss 73.49886
Epoch 26: Val Loss 59.29937
Epoch 27: Val Loss 45.69868
Epoch 28: Val Loss 33.25628
Epoch 29: Val Loss 22.52390
Epoch 30: Val Loss 13.97326
Epoch 31: Val Loss 7.86587
Epoch 32: Val Loss 4.13632
Epoch 33: Val Loss 2.35777
Epoch 34: Val Loss 1.84857
Epoch 35: Val Loss 1.89526
Epoch 36: Val Loss 1.97310
Epoch 37: Val Loss 1.88475
Epoch 38: Val Loss 1.63196
Epoch 39: Val Loss 1.34609
Epoch 40: Val Loss 1.10458
Epoch 41: Val Loss 0.94339
Epoch 42: Val Loss 0.84931
Epoch 43: Val Loss 0.79111
Epoch 44: Val Loss 0.74776
Epoch 45: Val Loss 0.70836
Epoch 46: Val Loss 0.67288
Epoch 47: Val Loss 0.64180
Epoch 48: Val Loss 0.61675
Epoch 49: Val Loss 0.59755
Epoch 50: Val Loss 0.58372
Epoch 51: Val Loss 0.57325
Epoch 52: Val Loss 0.56466
Epoch 53: Val Loss 0.55665
Epoch 54: Val Loss 0.54902
Epoch 55: Val Loss 0.54169
Epoch 56: Val Loss 0.53440
Epoch 57: Val Loss 0.52875
Epoch 58: Val Loss 0.52451
Epoch 59: Val Loss 0.51969
Epoch 60: Val Loss 0.51589
Epoch 61: Val Loss 0.51269
Epoch 62: Val Loss 0.51022
Epoch 63: Val Loss 0.50854
Epoch 64: Val Loss 0.50746
Epoch 65: Val Loss 0.50638
Epoch 66: Val Loss 0.50521
Epoch 67: Val Loss 0.50430
Epoch 68: Val Loss 0.50304
Epoch 69: Val Loss 0.50145
Epoch 70: Val Loss 0.50033
Epoch 71: Val Loss 0.49873
Epoch 72: Val Loss 0.49767
Epoch 73: Val Loss 0.49613
Epoch 74: Val Loss 0.49508
Epoch 75: Val Loss 0.49489
Epoch 76: Val Loss 0.49537
Epoch 77: Val Loss 0.49463
Epoch 78: Val Loss 0.49389
Epoch 79: Val Loss 0.49459
Epoch 80: Val Loss 0.49403
Epoch 81: Val Loss 0.49382
Epoch 82: Val Loss 0.49493
Epoch 83: Val Loss 0.49288
Epoch 84: Val Loss 0.49174
Epoch 85: Val Loss 0.49140
Epoch 86: Val Loss 0.49099
Epoch 87: Val Loss 0.49114
Epoch 88: Val Loss 0.49214
Epoch 89: Val Loss 0.49310
Epoch 90: Val Loss 0.49372
Epoch 91: Val Loss 0.49410
Epoch 92: Val Loss 0.49384
Epoch 93: Val Loss 0.49457
Epoch 94: Val Loss 0.49356
Epoch 95: Val Loss 0.49384
Epoch 96: Val Loss 0.49445
Epoch 97: Val Loss 0.49386
Epoch 98: Val Loss 0.49267
Epoch 99: Val Loss 0.49333
{'MSE - mean': 0.4909851044361591, 'MSE - std': 0.0, 'R2 - mean': 0.10656944923182587, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.54161
Epoch 1: Val Loss 229.42397
Epoch 2: Val Loss 228.23668
Epoch 3: Val Loss 226.96172
Epoch 4: Val Loss 225.57597
Epoch 5: Val Loss 224.05206
Epoch 6: Val Loss 222.36884
Epoch 7: Val Loss 220.49831
Epoch 8: Val Loss 218.39690
Epoch 9: Val Loss 216.03061
Epoch 10: Val Loss 213.33536
Epoch 11: Val Loss 210.27548
Epoch 12: Val Loss 206.82173
Epoch 13: Val Loss 202.89088
Epoch 14: Val Loss 198.44159
Epoch 15: Val Loss 193.34836
Epoch 16: Val Loss 187.57024
Epoch 17: Val Loss 180.99995
Epoch 18: Val Loss 173.61246
Epoch 19: Val Loss 165.34663
Epoch 20: Val Loss 156.15288
Epoch 21: Val Loss 146.05318
Epoch 22: Val Loss 135.05354
Epoch 23: Val Loss 123.23471
Epoch 24: Val Loss 110.68760
Epoch 25: Val Loss 97.56990
Epoch 26: Val Loss 84.13943
Epoch 27: Val Loss 70.70921
Epoch 28: Val Loss 57.61147
Epoch 29: Val Loss 45.35116
Epoch 30: Val Loss 34.33208
Epoch 31: Val Loss 24.92012
Epoch 32: Val Loss 17.41917
Epoch 33: Val Loss 11.92170
Epoch 34: Val Loss 8.29572
Epoch 35: Val Loss 6.21349
Epoch 36: Val Loss 5.20743
Epoch 37: Val Loss 4.78764
Epoch 38: Val Loss 4.58010
Epoch 39: Val Loss 4.36539
Epoch 40: Val Loss 4.09403
Epoch 41: Val Loss 3.78502
Epoch 42: Val Loss 3.48644
Epoch 43: Val Loss 3.22896
Epoch 44: Val Loss 3.01868
Epoch 45: Val Loss 2.84196
Epoch 46: Val Loss 2.68839
Epoch 47: Val Loss 2.54591
Epoch 48: Val Loss 2.40936
Epoch 49: Val Loss 2.27525
Epoch 50: Val Loss 2.14299
Epoch 51: Val Loss 2.01682
Epoch 52: Val Loss 1.90020
Epoch 53: Val Loss 1.79498
Epoch 54: Val Loss 1.70038
Epoch 55: Val Loss 1.61259
Epoch 56: Val Loss 1.53241
Epoch 57: Val Loss 1.45737
Epoch 58: Val Loss 1.38752
Epoch 59: Val Loss 1.32491
Epoch 60: Val Loss 1.26588
Epoch 61: Val Loss 1.21289
Epoch 62: Val Loss 1.16362
Epoch 63: Val Loss 1.11946
Epoch 64: Val Loss 1.07733
Epoch 65: Val Loss 1.03970
Epoch 66: Val Loss 1.00333
Epoch 67: Val Loss 0.97058
Epoch 68: Val Loss 0.93994
Epoch 69: Val Loss 0.91256
Epoch 70: Val Loss 0.88673
Epoch 71: Val Loss 0.86352
Epoch 72: Val Loss 0.84146
Epoch 73: Val Loss 0.82025
Epoch 74: Val Loss 0.80068
Epoch 75: Val Loss 0.78469
Epoch 76: Val Loss 0.76976
Epoch 77: Val Loss 0.75505
Epoch 78: Val Loss 0.74226
Epoch 79: Val Loss 0.73004
Epoch 80: Val Loss 0.71907
Epoch 81: Val Loss 0.70858
Epoch 82: Val Loss 0.69969
Epoch 83: Val Loss 0.69130
Epoch 84: Val Loss 0.68283
Epoch 85: Val Loss 0.67596
Epoch 86: Val Loss 0.66966
Epoch 87: Val Loss 0.66253
Epoch 88: Val Loss 0.65601
Epoch 89: Val Loss 0.65025
Epoch 90: Val Loss 0.64432
Epoch 91: Val Loss 0.63949
Epoch 92: Val Loss 0.63665
Epoch 93: Val Loss 0.63238
Epoch 94: Val Loss 0.62866
Epoch 95: Val Loss 0.62659
Epoch 96: Val Loss 0.62387
Epoch 97: Val Loss 0.62068
Epoch 98: Val Loss 0.61728
Epoch 99: Val Loss 0.61425
{'MSE - mean': 0.5526197044680355, 'MSE - std': 0.06163460003187646, 'R2 - mean': 0.13737682081643848, 'R2 - std': 0.03080737158461261} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 216.56517
Epoch 1: Val Loss 215.55048
Epoch 2: Val Loss 214.54416
Epoch 3: Val Loss 213.52715
Epoch 4: Val Loss 212.46162
Epoch 5: Val Loss 211.30954
Epoch 6: Val Loss 210.02281
Epoch 7: Val Loss 208.58473
Epoch 8: Val Loss 206.95552
Epoch 9: Val Loss 205.08871
Epoch 10: Val Loss 202.94386
Epoch 11: Val Loss 200.46642
Epoch 12: Val Loss 197.59575
Epoch 13: Val Loss 194.28111
Epoch 14: Val Loss 190.47009
Epoch 15: Val Loss 186.09711
Epoch 16: Val Loss 181.10065
Epoch 17: Val Loss 175.41975
Epoch 18: Val Loss 168.98314
Epoch 19: Val Loss 161.73073
Epoch 20: Val Loss 153.60196
Epoch 21: Val Loss 144.59109
Epoch 22: Val Loss 134.69780
Epoch 23: Val Loss 123.95952
Epoch 24: Val Loss 112.44000
Epoch 25: Val Loss 100.28342
Epoch 26: Val Loss 87.65935
Epoch 27: Val Loss 74.79164
Epoch 28: Val Loss 62.03106
Epoch 29: Val Loss 49.75606
Epoch 30: Val Loss 38.39472
Epoch 31: Val Loss 28.34397
Epoch 32: Val Loss 19.98971
Epoch 33: Val Loss 13.56597
Epoch 34: Val Loss 9.02820
Epoch 35: Val Loss 6.20631
Epoch 36: Val Loss 4.70621
Epoch 37: Val Loss 4.04987
Epoch 38: Val Loss 3.77922
Epoch 39: Val Loss 3.58706
Epoch 40: Val Loss 3.35947
Epoch 41: Val Loss 3.09146
Epoch 42: Val Loss 2.82440
Epoch 43: Val Loss 2.59066
Epoch 44: Val Loss 2.40397
Epoch 45: Val Loss 2.25477
Epoch 46: Val Loss 2.12846
Epoch 47: Val Loss 2.01447
Epoch 48: Val Loss 1.90503
Epoch 49: Val Loss 1.80348
Epoch 50: Val Loss 1.70099
Epoch 51: Val Loss 1.60649
Epoch 52: Val Loss 1.52035
Epoch 53: Val Loss 1.44365
Epoch 54: Val Loss 1.37571
Epoch 55: Val Loss 1.31346
Epoch 56: Val Loss 1.25801
Epoch 57: Val Loss 1.20637
Epoch 58: Val Loss 1.15986
Epoch 59: Val Loss 1.11691
Epoch 60: Val Loss 1.07864
Epoch 61: Val Loss 1.04299
Epoch 62: Val Loss 1.01045
Epoch 63: Val Loss 0.98071
Epoch 64: Val Loss 0.95379
Epoch 65: Val Loss 0.92948
Epoch 66: Val Loss 0.90738
Epoch 67: Val Loss 0.88715
Epoch 68: Val Loss 0.86846
Epoch 69: Val Loss 0.85219
Epoch 70: Val Loss 0.83677
Epoch 71: Val Loss 0.82299
Epoch 72: Val Loss 0.81044
Epoch 73: Val Loss 0.79946
Epoch 74: Val Loss 0.78917
Epoch 75: Val Loss 0.78003
Epoch 76: Val Loss 0.77134
Epoch 77: Val Loss 0.76294
Epoch 78: Val Loss 0.75560
Epoch 79: Val Loss 0.74916
Epoch 80: Val Loss 0.74312
Epoch 81: Val Loss 0.73737
Epoch 82: Val Loss 0.73185
Epoch 83: Val Loss 0.72691
Epoch 84: Val Loss 0.72246
Epoch 85: Val Loss 0.71856
Epoch 86: Val Loss 0.71478
Epoch 87: Val Loss 0.71142
Epoch 88: Val Loss 0.70784
Epoch 89: Val Loss 0.70457
Epoch 90: Val Loss 0.70188
Epoch 91: Val Loss 0.69953
Epoch 92: Val Loss 0.69697
Epoch 93: Val Loss 0.69507
Epoch 94: Val Loss 0.69306
Epoch 95: Val Loss 0.69085
Epoch 96: Val Loss 0.68858
Epoch 97: Val Loss 0.68626
Epoch 98: Val Loss 0.68440
Epoch 99: Val Loss 0.68326
{'MSE - mean': 0.5961655898886766, 'MSE - std': 0.0795301047079749, 'R2 - mean': 0.09434374911135208, 'R2 - std': 0.06585149922402535} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.94380
Epoch 1: Val Loss 235.94740
Epoch 2: Val Loss 234.96918
Epoch 3: Val Loss 234.01151
Epoch 4: Val Loss 233.04947
Epoch 5: Val Loss 232.08237
Epoch 6: Val Loss 231.05879
Epoch 7: Val Loss 229.94862
Epoch 8: Val Loss 228.71819
Epoch 9: Val Loss 227.35745
Epoch 10: Val Loss 225.82846
Epoch 11: Val Loss 224.11987
Epoch 12: Val Loss 222.22197
Epoch 13: Val Loss 220.12859
Epoch 14: Val Loss 217.81229
Epoch 15: Val Loss 215.25438
Epoch 16: Val Loss 212.43762
Epoch 17: Val Loss 209.33121
Epoch 18: Val Loss 205.91565
Epoch 19: Val Loss 202.13173
Epoch 20: Val Loss 197.94992
Epoch 21: Val Loss 193.32851
Epoch 22: Val Loss 188.24890
Epoch 23: Val Loss 182.67471
Epoch 24: Val Loss 176.59084
Epoch 25: Val Loss 169.94679
Epoch 26: Val Loss 162.73218
Epoch 27: Val Loss 154.93372
Epoch 28: Val Loss 146.53592
Epoch 29: Val Loss 137.55855
Epoch 30: Val Loss 128.03096
Epoch 31: Val Loss 118.01214
Epoch 32: Val Loss 107.58886
Epoch 33: Val Loss 96.87735
Epoch 34: Val Loss 86.02775
Epoch 35: Val Loss 75.23227
Epoch 36: Val Loss 64.70262
Epoch 37: Val Loss 54.68218
Epoch 38: Val Loss 45.40387
Epoch 39: Val Loss 37.11785
Epoch 40: Val Loss 30.03673
Epoch 41: Val Loss 24.23890
Epoch 42: Val Loss 19.72731
Epoch 43: Val Loss 16.37045
Epoch 44: Val Loss 14.00552
Epoch 45: Val Loss 12.41709
Epoch 46: Val Loss 11.32587
Epoch 47: Val Loss 10.51903
Epoch 48: Val Loss 9.85277
Epoch 49: Val Loss 9.24807
Epoch 50: Val Loss 8.68535
Epoch 51: Val Loss 8.16228
Epoch 52: Val Loss 7.69283
Epoch 53: Val Loss 7.26360
Epoch 54: Val Loss 6.86524
Epoch 55: Val Loss 6.50697
Epoch 56: Val Loss 6.16055
Epoch 57: Val Loss 5.83010
Epoch 58: Val Loss 5.51693
Epoch 59: Val Loss 5.22159
Epoch 60: Val Loss 4.93517
Epoch 61: Val Loss 4.67165
Epoch 62: Val Loss 4.41541
Epoch 63: Val Loss 4.17749
Epoch 64: Val Loss 3.94995
Epoch 65: Val Loss 3.73578
Epoch 66: Val Loss 3.53543
Epoch 67: Val Loss 3.34582
Epoch 68: Val Loss 3.16554
Epoch 69: Val Loss 2.99858
Epoch 70: Val Loss 2.84072
Epoch 71: Val Loss 2.69422
Epoch 72: Val Loss 2.55490
Epoch 73: Val Loss 2.42457
Epoch 74: Val Loss 2.30169
Epoch 75: Val Loss 2.18599
Epoch 76: Val Loss 2.07757
Epoch 77: Val Loss 1.97291
Epoch 78: Val Loss 1.87541
Epoch 79: Val Loss 1.78479
Epoch 80: Val Loss 1.70047
Epoch 81: Val Loss 1.62209
Epoch 82: Val Loss 1.54803
Epoch 83: Val Loss 1.48041
Epoch 84: Val Loss 1.41663
Epoch 85: Val Loss 1.35563
Epoch 86: Val Loss 1.30157
Epoch 87: Val Loss 1.24871
Epoch 88: Val Loss 1.20103
Epoch 89: Val Loss 1.15654
Epoch 90: Val Loss 1.11420
Epoch 91: Val Loss 1.07521
Epoch 92: Val Loss 1.03920
Epoch 93: Val Loss 1.00644
Epoch 94: Val Loss 0.97615
Epoch 95: Val Loss 0.94694
Epoch 96: Val Loss 0.92014
Epoch 97: Val Loss 0.89588
Epoch 98: Val Loss 0.87461
Epoch 99: Val Loss 0.85491
{'MSE - mean': 0.6608515504161334, 'MSE - std': 0.13151653369149136, 'R2 - mean': 0.04816216041630386, 'R2 - std': 0.09823712315481697} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.42570
Epoch 1: Val Loss 233.30450
Epoch 2: Val Loss 232.20589
Epoch 3: Val Loss 231.14709
Epoch 4: Val Loss 230.11400
Epoch 5: Val Loss 229.07452
Epoch 6: Val Loss 228.00427
Epoch 7: Val Loss 226.88152
Epoch 8: Val Loss 225.66388
Epoch 9: Val Loss 224.31682
Epoch 10: Val Loss 222.81201
Epoch 11: Val Loss 221.11664
Epoch 12: Val Loss 219.18568
Epoch 13: Val Loss 216.97475
Epoch 14: Val Loss 214.43929
Epoch 15: Val Loss 211.50148
Epoch 16: Val Loss 208.10384
Epoch 17: Val Loss 204.17809
Epoch 18: Val Loss 199.65022
Epoch 19: Val Loss 194.45053
Epoch 20: Val Loss 188.51230
Epoch 21: Val Loss 181.78534
Epoch 22: Val Loss 174.17226
Epoch 23: Val Loss 165.61235
Epoch 24: Val Loss 156.10565
Epoch 25: Val Loss 145.59128
Epoch 26: Val Loss 134.16760
Epoch 27: Val Loss 121.87723
Epoch 28: Val Loss 108.82835
Epoch 29: Val Loss 95.25618
Epoch 30: Val Loss 81.45974
Epoch 31: Val Loss 67.71412
Epoch 32: Val Loss 54.43310
Epoch 33: Val Loss 42.10526
Epoch 34: Val Loss 31.15220
Epoch 35: Val Loss 21.94839
Epoch 36: Val Loss 14.75632
Epoch 37: Val Loss 9.66582
Epoch 38: Val Loss 6.46895
Epoch 39: Val Loss 4.78312
Epoch 40: Val Loss 4.07656
Epoch 41: Val Loss 3.84902
Epoch 42: Val Loss 3.73096
Epoch 43: Val Loss 3.54196
Epoch 44: Val Loss 3.25924
Epoch 45: Val Loss 2.94074
Epoch 46: Val Loss 2.63926
Epoch 47: Val Loss 2.38827
Epoch 48: Val Loss 2.19153
Epoch 49: Val Loss 2.03805
Epoch 50: Val Loss 1.91015
Epoch 51: Val Loss 1.79700
Epoch 52: Val Loss 1.69580
Epoch 53: Val Loss 1.60148
Epoch 54: Val Loss 1.51396
Epoch 55: Val Loss 1.43238
Epoch 56: Val Loss 1.35782
Epoch 57: Val Loss 1.29051
Epoch 58: Val Loss 1.23039
Epoch 59: Val Loss 1.17550
Epoch 60: Val Loss 1.12525
Epoch 61: Val Loss 1.08198
Epoch 62: Val Loss 1.04206
Epoch 63: Val Loss 1.00589
Epoch 64: Val Loss 0.97495
Epoch 65: Val Loss 0.94631
Epoch 66: Val Loss 0.91976
Epoch 67: Val Loss 0.89504
Epoch 68: Val Loss 0.87291
Epoch 69: Val Loss 0.85343
Epoch 70: Val Loss 0.83528
Epoch 71: Val Loss 0.81850
Epoch 72: Val Loss 0.80303
Epoch 73: Val Loss 0.78909
Epoch 74: Val Loss 0.77659
Epoch 75: Val Loss 0.76509
Epoch 76: Val Loss 0.75455
Epoch 77: Val Loss 0.74590
Epoch 78: Val Loss 0.73768
Epoch 79: Val Loss 0.72943
Epoch 80: Val Loss 0.72229
Epoch 81: Val Loss 0.71663
Epoch 82: Val Loss 0.71061
Epoch 83: Val Loss 0.70409
Epoch 84: Val Loss 0.69807
Epoch 85: Val Loss 0.69238
Epoch 86: Val Loss 0.68806
Epoch 87: Val Loss 0.68345
Epoch 88: Val Loss 0.67875
Epoch 89: Val Loss 0.67404
Epoch 90: Val Loss 0.66973
Epoch 91: Val Loss 0.66586
Epoch 92: Val Loss 0.66333
Epoch 93: Val Loss 0.66017
Epoch 94: Val Loss 0.65770
Epoch 95: Val Loss 0.65627
Epoch 96: Val Loss 0.65370
Epoch 97: Val Loss 0.65217
Epoch 98: Val Loss 0.64908
Epoch 99: Val Loss 0.64652
{'MSE - mean': 0.6579859061194161, 'MSE - std': 0.11777150154566843, 'R2 - mean': 0.023941423021638174, 'R2 - std': 0.10033445256916075} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 0.6579859061194161 and parameters: {'dim': 64, 'depth': 12, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0}. Best is trial 8 with value: 0.6579859061194161.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.73167
Epoch 1: Val Loss 232.72057
Epoch 2: Val Loss 232.70946
Epoch 3: Val Loss 232.69835
Epoch 4: Val Loss 232.68726
Epoch 5: Val Loss 232.67621
Epoch 6: Val Loss 232.66521
Epoch 7: Val Loss 232.65419
Epoch 8: Val Loss 232.64320
Epoch 9: Val Loss 232.63219
Epoch 10: Val Loss 232.62117
Epoch 11: Val Loss 232.61015
Epoch 12: Val Loss 232.59912
Epoch 13: Val Loss 232.58820
Epoch 14: Val Loss 232.57727
Epoch 15: Val Loss 232.56639
Epoch 16: Val Loss 232.55550
Epoch 17: Val Loss 232.54462
Epoch 18: Val Loss 232.53369
Epoch 19: Val Loss 232.52284
Epoch 20: Val Loss 232.51202
Epoch 21: Val Loss 232.50117
Epoch 22: Val Loss 232.49031
Epoch 23: Val Loss 232.47948
Epoch 24: Val Loss 232.46864
Epoch 25: Val Loss 232.45787
Epoch 26: Val Loss 232.44702
Epoch 27: Val Loss 232.43625
Epoch 28: Val Loss 232.42548
Epoch 29: Val Loss 232.41469
Epoch 30: Val Loss 232.40393
Epoch 31: Val Loss 232.39314
Epoch 32: Val Loss 232.38231
Epoch 33: Val Loss 232.37152
Epoch 34: Val Loss 232.36072
Epoch 35: Val Loss 232.34990
Epoch 36: Val Loss 232.33910
Epoch 37: Val Loss 232.32831
Epoch 38: Val Loss 232.31752
Epoch 39: Val Loss 232.30675
Epoch 40: Val Loss 232.29596
Epoch 41: Val Loss 232.28522
Epoch 42: Val Loss 232.27444
Epoch 43: Val Loss 232.26364
Epoch 44: Val Loss 232.25282
Epoch 45: Val Loss 232.24202
Epoch 46: Val Loss 232.23120
Epoch 47: Val Loss 232.22040
Epoch 48: Val Loss 232.20959
Epoch 49: Val Loss 232.19881
Epoch 50: Val Loss 232.18803
Epoch 51: Val Loss 232.17726
Epoch 52: Val Loss 232.16649
Epoch 53: Val Loss 232.15572
Epoch 54: Val Loss 232.14491
Epoch 55: Val Loss 232.13414
Epoch 56: Val Loss 232.12341
Epoch 57: Val Loss 232.11263
Epoch 58: Val Loss 232.10184
Epoch 59: Val Loss 232.09105
Epoch 60: Val Loss 232.08029
Epoch 61: Val Loss 232.06953
Epoch 62: Val Loss 232.05878
Epoch 63: Val Loss 232.04797
Epoch 64: Val Loss 232.03717
Epoch 65: Val Loss 232.02637
Epoch 66: Val Loss 232.01552
Epoch 67: Val Loss 232.00473
Epoch 68: Val Loss 231.99394
Epoch 69: Val Loss 231.98315
Epoch 70: Val Loss 231.97232
Epoch 71: Val Loss 231.96150
Epoch 72: Val Loss 231.95071
Epoch 73: Val Loss 231.93991
Epoch 74: Val Loss 231.92915
Epoch 75: Val Loss 231.91833
Epoch 76: Val Loss 231.90750
Epoch 77: Val Loss 231.89670
Epoch 78: Val Loss 231.88586
Epoch 79: Val Loss 231.87500
Epoch 80: Val Loss 231.86415
Epoch 81: Val Loss 231.85332
Epoch 82: Val Loss 231.84241
Epoch 83: Val Loss 231.83151
Epoch 84: Val Loss 231.82065
Epoch 85: Val Loss 231.80977
Epoch 86: Val Loss 231.79893
Epoch 87: Val Loss 231.78806
Epoch 88: Val Loss 231.77718
Epoch 89: Val Loss 231.76624
Epoch 90: Val Loss 231.75533
Epoch 91: Val Loss 231.74438
Epoch 92: Val Loss 231.73343
Epoch 93: Val Loss 231.72246
Epoch 94: Val Loss 231.71150
Epoch 95: Val Loss 231.70053
Epoch 96: Val Loss 231.68958
Epoch 97: Val Loss 231.67867
Epoch 98: Val Loss 231.66769
Epoch 99: Val Loss 231.65672
{'MSE - mean': 231.6567421882857, 'MSE - std': 0.0, 'R2 - mean': -420.53867580182856, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.10016
Epoch 1: Val Loss 231.08755
Epoch 2: Val Loss 231.07497
Epoch 3: Val Loss 231.06244
Epoch 4: Val Loss 231.04991
Epoch 5: Val Loss 231.03735
Epoch 6: Val Loss 231.02475
Epoch 7: Val Loss 231.01216
Epoch 8: Val Loss 230.99954
Epoch 9: Val Loss 230.98698
Epoch 10: Val Loss 230.97437
Epoch 11: Val Loss 230.96178
Epoch 12: Val Loss 230.94916
Epoch 13: Val Loss 230.93657
Epoch 14: Val Loss 230.92397
Epoch 15: Val Loss 230.91141
Epoch 16: Val Loss 230.89883
Epoch 17: Val Loss 230.88623
Epoch 18: Val Loss 230.87360
Epoch 19: Val Loss 230.86096
Epoch 20: Val Loss 230.84836
Epoch 21: Val Loss 230.83572
Epoch 22: Val Loss 230.82309
Epoch 23: Val Loss 230.81046
Epoch 24: Val Loss 230.79778
Epoch 25: Val Loss 230.78510
Epoch 26: Val Loss 230.77242
Epoch 27: Val Loss 230.75972
Epoch 28: Val Loss 230.74702
Epoch 29: Val Loss 230.73430
Epoch 30: Val Loss 230.72153
Epoch 31: Val Loss 230.70868
Epoch 32: Val Loss 230.69592
Epoch 33: Val Loss 230.68309
Epoch 34: Val Loss 230.67033
Epoch 35: Val Loss 230.65752
Epoch 36: Val Loss 230.64473
Epoch 37: Val Loss 230.63194
Epoch 38: Val Loss 230.61917
Epoch 39: Val Loss 230.60641
Epoch 40: Val Loss 230.59364
Epoch 41: Val Loss 230.58080
Epoch 42: Val Loss 230.56799
Epoch 43: Val Loss 230.55511
Epoch 44: Val Loss 230.54221
Epoch 45: Val Loss 230.52937
Epoch 46: Val Loss 230.51649
Epoch 47: Val Loss 230.50366
Epoch 48: Val Loss 230.49078
Epoch 49: Val Loss 230.47792
Epoch 50: Val Loss 230.46500
Epoch 51: Val Loss 230.45212
Epoch 52: Val Loss 230.43919
Epoch 53: Val Loss 230.42630
Epoch 54: Val Loss 230.41338
Epoch 55: Val Loss 230.40051
Epoch 56: Val Loss 230.38759
Epoch 57: Val Loss 230.37473
Epoch 58: Val Loss 230.36182
Epoch 59: Val Loss 230.34883
Epoch 60: Val Loss 230.33586
Epoch 61: Val Loss 230.32295
Epoch 62: Val Loss 230.31000
Epoch 63: Val Loss 230.29703
Epoch 64: Val Loss 230.28410
Epoch 65: Val Loss 230.27113
Epoch 66: Val Loss 230.25815
Epoch 67: Val Loss 230.24519
Epoch 68: Val Loss 230.23225
Epoch 69: Val Loss 230.21928
Epoch 70: Val Loss 230.20628
Epoch 71: Val Loss 230.19327
Epoch 72: Val Loss 230.18021
Epoch 73: Val Loss 230.16714
Epoch 74: Val Loss 230.15407
Epoch 75: Val Loss 230.14096
Epoch 76: Val Loss 230.12788
Epoch 77: Val Loss 230.11475
Epoch 78: Val Loss 230.10161
Epoch 79: Val Loss 230.08847
Epoch 80: Val Loss 230.07532
Epoch 81: Val Loss 230.06215
Epoch 82: Val Loss 230.04904
Epoch 83: Val Loss 230.03593
Epoch 84: Val Loss 230.02278
Epoch 85: Val Loss 230.00960
Epoch 86: Val Loss 229.99644
Epoch 87: Val Loss 229.98328
Epoch 88: Val Loss 229.97006
Epoch 89: Val Loss 229.95682
Epoch 90: Val Loss 229.94350
Epoch 91: Val Loss 229.93025
Epoch 92: Val Loss 229.91698
Epoch 93: Val Loss 229.90363
Epoch 94: Val Loss 229.89027
Epoch 95: Val Loss 229.87700
Epoch 96: Val Loss 229.86371
Epoch 97: Val Loss 229.85043
Epoch 98: Val Loss 229.83714
Epoch 99: Val Loss 229.82387
{'MSE - mean': 230.74031121766885, 'MSE - std': 0.9164309706168581, 'R2 - mean': -365.3817081787794, 'R2 - std': 55.156967623049155} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.23660
Epoch 1: Val Loss 223.22852
Epoch 2: Val Loss 223.22044
Epoch 3: Val Loss 223.21239
Epoch 4: Val Loss 223.20432
Epoch 5: Val Loss 223.19626
Epoch 6: Val Loss 223.18820
Epoch 7: Val Loss 223.18015
Epoch 8: Val Loss 223.17213
Epoch 9: Val Loss 223.16409
Epoch 10: Val Loss 223.15604
Epoch 11: Val Loss 223.14798
Epoch 12: Val Loss 223.13994
Epoch 13: Val Loss 223.13187
Epoch 14: Val Loss 223.12376
Epoch 15: Val Loss 223.11568
Epoch 16: Val Loss 223.10756
Epoch 17: Val Loss 223.09949
Epoch 18: Val Loss 223.09142
Epoch 19: Val Loss 223.08334
Epoch 20: Val Loss 223.07523
Epoch 21: Val Loss 223.06712
Epoch 22: Val Loss 223.05904
Epoch 23: Val Loss 223.05093
Epoch 24: Val Loss 223.04285
Epoch 25: Val Loss 223.03481
Epoch 26: Val Loss 223.02679
Epoch 27: Val Loss 223.01872
Epoch 28: Val Loss 223.01073
Epoch 29: Val Loss 223.00266
Epoch 30: Val Loss 222.99463
Epoch 31: Val Loss 222.98657
Epoch 32: Val Loss 222.97856
Epoch 33: Val Loss 222.97052
Epoch 34: Val Loss 222.96246
Epoch 35: Val Loss 222.95444
Epoch 36: Val Loss 222.94640
Epoch 37: Val Loss 222.93837
Epoch 38: Val Loss 222.93036
Epoch 39: Val Loss 222.92230
Epoch 40: Val Loss 222.91417
Epoch 41: Val Loss 222.90610
Epoch 42: Val Loss 222.89806
Epoch 43: Val Loss 222.88997
Epoch 44: Val Loss 222.88185
Epoch 45: Val Loss 222.87375
Epoch 46: Val Loss 222.86565
Epoch 47: Val Loss 222.85757
Epoch 48: Val Loss 222.84952
Epoch 49: Val Loss 222.84142
Epoch 50: Val Loss 222.83331
Epoch 51: Val Loss 222.82524
Epoch 52: Val Loss 222.81718
Epoch 53: Val Loss 222.80905
Epoch 54: Val Loss 222.80098
Epoch 55: Val Loss 222.79292
Epoch 56: Val Loss 222.78484
Epoch 57: Val Loss 222.77678
Epoch 58: Val Loss 222.76877
Epoch 59: Val Loss 222.76074
Epoch 60: Val Loss 222.75267
Epoch 61: Val Loss 222.74466
Epoch 62: Val Loss 222.73660
Epoch 63: Val Loss 222.72856
Epoch 64: Val Loss 222.72050
Epoch 65: Val Loss 222.71246
Epoch 66: Val Loss 222.70444
Epoch 67: Val Loss 222.69637
Epoch 68: Val Loss 222.68834
Epoch 69: Val Loss 222.68034
Epoch 70: Val Loss 222.67227
Epoch 71: Val Loss 222.66420
Epoch 72: Val Loss 222.65617
Epoch 73: Val Loss 222.64810
Epoch 74: Val Loss 222.64012
Epoch 75: Val Loss 222.63213
Epoch 76: Val Loss 222.62415
Epoch 77: Val Loss 222.61612
Epoch 78: Val Loss 222.60812
Epoch 79: Val Loss 222.60007
Epoch 80: Val Loss 222.59204
Epoch 81: Val Loss 222.58403
Epoch 82: Val Loss 222.57597
Epoch 83: Val Loss 222.56793
Epoch 84: Val Loss 222.55995
Epoch 85: Val Loss 222.55191
Epoch 86: Val Loss 222.54384
Epoch 87: Val Loss 222.53575
Epoch 88: Val Loss 222.52765
Epoch 89: Val Loss 222.51950
Epoch 90: Val Loss 222.51134
Epoch 91: Val Loss 222.50322
Epoch 92: Val Loss 222.49510
Epoch 93: Val Loss 222.48695
Epoch 94: Val Loss 222.47879
Epoch 95: Val Loss 222.47067
Epoch 96: Val Loss 222.46252
Epoch 97: Val Loss 222.45438
Epoch 98: Val Loss 222.44620
Epoch 99: Val Loss 222.43810
{'MSE - mean': 227.97290712150485, 'MSE - std': 3.984588813441202, 'R2 - mean': -350.874653879905, 'R2 - std': 49.48841582068519} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.88657
Epoch 1: Val Loss 227.87553
Epoch 2: Val Loss 227.86453
Epoch 3: Val Loss 227.85356
Epoch 4: Val Loss 227.84258
Epoch 5: Val Loss 227.83162
Epoch 6: Val Loss 227.82065
Epoch 7: Val Loss 227.80963
Epoch 8: Val Loss 227.79865
Epoch 9: Val Loss 227.78767
Epoch 10: Val Loss 227.77666
Epoch 11: Val Loss 227.76561
Epoch 12: Val Loss 227.75458
Epoch 13: Val Loss 227.74355
Epoch 14: Val Loss 227.73253
Epoch 15: Val Loss 227.72144
Epoch 16: Val Loss 227.71031
Epoch 17: Val Loss 227.69922
Epoch 18: Val Loss 227.68814
Epoch 19: Val Loss 227.67697
Epoch 20: Val Loss 227.66585
Epoch 21: Val Loss 227.65474
Epoch 22: Val Loss 227.64362
Epoch 23: Val Loss 227.63249
Epoch 24: Val Loss 227.62138
Epoch 25: Val Loss 227.61024
Epoch 26: Val Loss 227.59909
Epoch 27: Val Loss 227.58789
Epoch 28: Val Loss 227.57675
Epoch 29: Val Loss 227.56558
Epoch 30: Val Loss 227.55444
Epoch 31: Val Loss 227.54330
Epoch 32: Val Loss 227.53212
Epoch 33: Val Loss 227.52092
Epoch 34: Val Loss 227.50974
Epoch 35: Val Loss 227.49854
Epoch 36: Val Loss 227.48737
Epoch 37: Val Loss 227.47610
Epoch 38: Val Loss 227.46489
Epoch 39: Val Loss 227.45366
Epoch 40: Val Loss 227.44244
Epoch 41: Val Loss 227.43118
Epoch 42: Val Loss 227.42000
Epoch 43: Val Loss 227.40877
Epoch 44: Val Loss 227.39751
Epoch 45: Val Loss 227.38623
Epoch 46: Val Loss 227.37495
Epoch 47: Val Loss 227.36371
Epoch 48: Val Loss 227.35245
Epoch 49: Val Loss 227.34123
Epoch 50: Val Loss 227.32999
Epoch 51: Val Loss 227.31871
Epoch 52: Val Loss 227.30748
Epoch 53: Val Loss 227.29622
Epoch 54: Val Loss 227.28500
Epoch 55: Val Loss 227.27379
Epoch 56: Val Loss 227.26253
Epoch 57: Val Loss 227.25125
Epoch 58: Val Loss 227.23997
Epoch 59: Val Loss 227.22871
Epoch 60: Val Loss 227.21745
Epoch 61: Val Loss 227.20618
Epoch 62: Val Loss 227.19493
Epoch 63: Val Loss 227.18367
Epoch 64: Val Loss 227.17244
Epoch 65: Val Loss 227.16121
Epoch 66: Val Loss 227.14999
Epoch 67: Val Loss 227.13876
Epoch 68: Val Loss 227.12752
Epoch 69: Val Loss 227.11633
Epoch 70: Val Loss 227.10509
Epoch 71: Val Loss 227.09387
Epoch 72: Val Loss 227.08267
Epoch 73: Val Loss 227.07146
Epoch 74: Val Loss 227.06024
Epoch 75: Val Loss 227.04903
Epoch 76: Val Loss 227.03780
Epoch 77: Val Loss 227.02663
Epoch 78: Val Loss 227.01538
Epoch 79: Val Loss 227.00417
Epoch 80: Val Loss 226.99290
Epoch 81: Val Loss 226.98167
Epoch 82: Val Loss 226.97040
Epoch 83: Val Loss 226.95912
Epoch 84: Val Loss 226.94785
Epoch 85: Val Loss 226.93657
Epoch 86: Val Loss 226.92532
Epoch 87: Val Loss 226.91405
Epoch 88: Val Loss 226.90276
Epoch 89: Val Loss 226.89153
Epoch 90: Val Loss 226.88025
Epoch 91: Val Loss 226.86897
Epoch 92: Val Loss 226.85770
Epoch 93: Val Loss 226.84645
Epoch 94: Val Loss 226.83517
Epoch 95: Val Loss 226.82390
Epoch 96: Val Loss 226.81259
Epoch 97: Val Loss 226.80124
Epoch 98: Val Loss 226.78990
Epoch 99: Val Loss 226.77852
{'MSE - mean': 227.67431456104762, 'MSE - std': 3.4892955679666633, 'R2 - mean': -335.2163904059195, 'R2 - std': 50.71854809034881} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.38411
Epoch 1: Val Loss 227.37529
Epoch 2: Val Loss 227.36647
Epoch 3: Val Loss 227.35764
Epoch 4: Val Loss 227.34877
Epoch 5: Val Loss 227.33992
Epoch 6: Val Loss 227.33105
Epoch 7: Val Loss 227.32219
Epoch 8: Val Loss 227.31331
Epoch 9: Val Loss 227.30444
Epoch 10: Val Loss 227.29558
Epoch 11: Val Loss 227.28671
Epoch 12: Val Loss 227.27777
Epoch 13: Val Loss 227.26891
Epoch 14: Val Loss 227.25998
Epoch 15: Val Loss 227.25108
Epoch 16: Val Loss 227.24225
Epoch 17: Val Loss 227.23338
Epoch 18: Val Loss 227.22453
Epoch 19: Val Loss 227.21570
Epoch 20: Val Loss 227.20682
Epoch 21: Val Loss 227.19792
Epoch 22: Val Loss 227.18895
Epoch 23: Val Loss 227.18002
Epoch 24: Val Loss 227.17105
Epoch 25: Val Loss 227.16206
Epoch 26: Val Loss 227.15306
Epoch 27: Val Loss 227.14404
Epoch 28: Val Loss 227.13504
Epoch 29: Val Loss 227.12601
Epoch 30: Val Loss 227.11694
Epoch 31: Val Loss 227.10788
Epoch 32: Val Loss 227.09880
Epoch 33: Val Loss 227.08966
Epoch 34: Val Loss 227.08052
Epoch 35: Val Loss 227.07146
Epoch 36: Val Loss 227.06233
Epoch 37: Val Loss 227.05324
Epoch 38: Val Loss 227.04411
Epoch 39: Val Loss 227.03491
Epoch 40: Val Loss 227.02577
Epoch 41: Val Loss 227.01660
Epoch 42: Val Loss 227.00746
Epoch 43: Val Loss 226.99834
Epoch 44: Val Loss 226.98918
Epoch 45: Val Loss 226.98001
Epoch 46: Val Loss 226.97086
Epoch 47: Val Loss 226.96175
Epoch 48: Val Loss 226.95262
Epoch 49: Val Loss 226.94350
Epoch 50: Val Loss 226.93446
Epoch 51: Val Loss 226.92546
Epoch 52: Val Loss 226.91643
Epoch 53: Val Loss 226.90736
Epoch 54: Val Loss 226.89822
Epoch 55: Val Loss 226.88910
Epoch 56: Val Loss 226.87996
Epoch 57: Val Loss 226.87088
Epoch 58: Val Loss 226.86174
Epoch 59: Val Loss 226.85260
Epoch 60: Val Loss 226.84344
Epoch 61: Val Loss 226.83424
Epoch 62: Val Loss 226.82501
Epoch 63: Val Loss 226.81578
Epoch 64: Val Loss 226.80652
Epoch 65: Val Loss 226.79733
Epoch 66: Val Loss 226.78816
Epoch 67: Val Loss 226.77898
Epoch 68: Val Loss 226.76979
Epoch 69: Val Loss 226.76057
Epoch 70: Val Loss 226.75134
Epoch 71: Val Loss 226.74208
Epoch 72: Val Loss 226.73280
Epoch 73: Val Loss 226.72350
Epoch 74: Val Loss 226.71422
Epoch 75: Val Loss 226.70499
Epoch 76: Val Loss 226.69560
Epoch 77: Val Loss 226.68633
Epoch 78: Val Loss 226.67703
Epoch 79: Val Loss 226.66762
Epoch 80: Val Loss 226.65825
Epoch 81: Val Loss 226.64883
Epoch 82: Val Loss 226.63939
Epoch 83: Val Loss 226.62991
Epoch 84: Val Loss 226.62045
Epoch 85: Val Loss 226.61099
Epoch 86: Val Loss 226.60149
Epoch 87: Val Loss 226.59201
Epoch 88: Val Loss 226.58255
Epoch 89: Val Loss 226.57300
Epoch 90: Val Loss 226.56348
Epoch 91: Val Loss 226.55397
Epoch 92: Val Loss 226.54442
Epoch 93: Val Loss 226.53484
Epoch 94: Val Loss 226.52527
Epoch 95: Val Loss 226.51566
Epoch 96: Val Loss 226.50601
Epoch 97: Val Loss 226.49638
Epoch 98: Val Loss 226.48674
Epoch 99: Val Loss 226.47716
{'MSE - mean': 227.4348851362318, 'MSE - std': 3.157444005263752, 'R2 - mean': -343.143410714845, 'R2 - std': 48.05463037238855} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 227.4348851362318 and parameters: {'dim': 256, 'depth': 1, 'heads': 4, 'weight_decay': -5, 'learning_rate': -5, 'dropout': 0.3}. Best is trial 8 with value: 0.6579859061194161.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.91197
Epoch 1: Val Loss 230.91087
Epoch 2: Val Loss 230.90979
Epoch 3: Val Loss 230.90871
Epoch 4: Val Loss 230.90762
Epoch 5: Val Loss 230.90656
Epoch 6: Val Loss 230.90544
Epoch 7: Val Loss 230.90436
Epoch 8: Val Loss 230.90329
Epoch 9: Val Loss 230.90221
Epoch 10: Val Loss 230.90112
Epoch 11: Val Loss 230.90004
Epoch 12: Val Loss 230.89896
Epoch 13: Val Loss 230.89786
Epoch 14: Val Loss 230.89679
Epoch 15: Val Loss 230.89572
Epoch 16: Val Loss 230.89465
Epoch 17: Val Loss 230.89357
Epoch 18: Val Loss 230.89247
Epoch 19: Val Loss 230.89140
Epoch 20: Val Loss 230.89032
Epoch 21: Val Loss 230.88924
Epoch 22: Val Loss 230.88817
Epoch 23: Val Loss 230.88708
Epoch 24: Val Loss 230.88602
Epoch 25: Val Loss 230.88492
Epoch 26: Val Loss 230.88383
Epoch 27: Val Loss 230.88277
Epoch 28: Val Loss 230.88167
Epoch 29: Val Loss 230.88057
Epoch 30: Val Loss 230.87947
Epoch 31: Val Loss 230.87840
Epoch 32: Val Loss 230.87732
Epoch 33: Val Loss 230.87622
Epoch 34: Val Loss 230.87514
Epoch 35: Val Loss 230.87405
Epoch 36: Val Loss 230.87299
Epoch 37: Val Loss 230.87189
Epoch 38: Val Loss 230.87079
Epoch 39: Val Loss 230.86971
Epoch 40: Val Loss 230.86862
Epoch 41: Val Loss 230.86754
Epoch 42: Val Loss 230.86649
Epoch 43: Val Loss 230.86537
Epoch 44: Val Loss 230.86429
Epoch 45: Val Loss 230.86322
Epoch 46: Val Loss 230.86212
Epoch 47: Val Loss 230.86104
Epoch 48: Val Loss 230.85994
Epoch 49: Val Loss 230.85887
Epoch 50: Val Loss 230.85779
Epoch 51: Val Loss 230.85672
Epoch 52: Val Loss 230.85564
Epoch 53: Val Loss 230.85454
Epoch 54: Val Loss 230.85345
Epoch 55: Val Loss 230.85237
Epoch 56: Val Loss 230.85127
Epoch 57: Val Loss 230.85020
Epoch 58: Val Loss 230.84912
Epoch 59: Val Loss 230.84802
Epoch 60: Val Loss 230.84695
Epoch 61: Val Loss 230.84587
Epoch 62: Val Loss 230.84477
Epoch 63: Val Loss 230.84369
Epoch 64: Val Loss 230.84261
Epoch 65: Val Loss 230.84151
Epoch 66: Val Loss 230.84044
Epoch 67: Val Loss 230.83936
Epoch 68: Val Loss 230.83826
Epoch 69: Val Loss 230.83719
Epoch 70: Val Loss 230.83609
Epoch 71: Val Loss 230.83501
Epoch 72: Val Loss 230.83392
Epoch 73: Val Loss 230.83284
Epoch 74: Val Loss 230.83176
Epoch 75: Val Loss 230.83067
Epoch 76: Val Loss 230.82957
Epoch 77: Val Loss 230.82849
Epoch 78: Val Loss 230.82742
Epoch 79: Val Loss 230.82631
Epoch 80: Val Loss 230.82523
Epoch 81: Val Loss 230.82417
Epoch 82: Val Loss 230.82307
Epoch 83: Val Loss 230.82198
Epoch 84: Val Loss 230.82091
Epoch 85: Val Loss 230.81981
Epoch 86: Val Loss 230.81873
Epoch 87: Val Loss 230.81763
Epoch 88: Val Loss 230.81656
Epoch 89: Val Loss 230.81546
Epoch 90: Val Loss 230.81439
Epoch 91: Val Loss 230.81331
Epoch 92: Val Loss 230.81221
Epoch 93: Val Loss 230.81114
Epoch 94: Val Loss 230.81004
Epoch 95: Val Loss 230.80896
Epoch 96: Val Loss 230.80786
Epoch 97: Val Loss 230.80679
Epoch 98: Val Loss 230.80569
Epoch 99: Val Loss 230.80463
{'MSE - mean': 230.80462345632384, 'MSE - std': 0.0, 'R2 - mean': -418.9881014541794, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.82458
Epoch 1: Val Loss 233.82390
Epoch 2: Val Loss 233.82323
Epoch 3: Val Loss 233.82254
Epoch 4: Val Loss 233.82187
Epoch 5: Val Loss 233.82118
Epoch 6: Val Loss 233.82053
Epoch 7: Val Loss 233.81982
Epoch 8: Val Loss 233.81915
Epoch 9: Val Loss 233.81847
Epoch 10: Val Loss 233.81781
Epoch 11: Val Loss 233.81714
Epoch 12: Val Loss 233.81647
Epoch 13: Val Loss 233.81578
Epoch 14: Val Loss 233.81514
Epoch 15: Val Loss 233.81445
Epoch 16: Val Loss 233.81378
Epoch 17: Val Loss 233.81310
Epoch 18: Val Loss 233.81242
Epoch 19: Val Loss 233.81174
Epoch 20: Val Loss 233.81107
Epoch 21: Val Loss 233.81039
Epoch 22: Val Loss 233.80974
Epoch 23: Val Loss 233.80908
Epoch 24: Val Loss 233.80841
Epoch 25: Val Loss 233.80775
Epoch 26: Val Loss 233.80708
Epoch 27: Val Loss 233.80641
Epoch 28: Val Loss 233.80574
Epoch 29: Val Loss 233.80508
Epoch 30: Val Loss 233.80440
Epoch 31: Val Loss 233.80376
Epoch 32: Val Loss 233.80309
Epoch 33: Val Loss 233.80243
Epoch 34: Val Loss 233.80176
Epoch 35: Val Loss 233.80107
Epoch 36: Val Loss 233.80043
Epoch 37: Val Loss 233.79977
Epoch 38: Val Loss 233.79910
Epoch 39: Val Loss 233.79843
Epoch 40: Val Loss 233.79774
Epoch 41: Val Loss 233.79709
Epoch 42: Val Loss 233.79642
Epoch 43: Val Loss 233.79573
Epoch 44: Val Loss 233.79506
Epoch 45: Val Loss 233.79439
Epoch 46: Val Loss 233.79375
Epoch 47: Val Loss 233.79306
Epoch 48: Val Loss 233.79237
Epoch 49: Val Loss 233.79173
Epoch 50: Val Loss 233.79106
Epoch 51: Val Loss 233.79037
Epoch 52: Val Loss 233.78973
Epoch 53: Val Loss 233.78909
Epoch 54: Val Loss 233.78841
Epoch 55: Val Loss 233.78772
Epoch 56: Val Loss 233.78705
Epoch 57: Val Loss 233.78641
Epoch 58: Val Loss 233.78572
Epoch 59: Val Loss 233.78506
Epoch 60: Val Loss 233.78439
Epoch 61: Val Loss 233.78374
Epoch 62: Val Loss 233.78308
Epoch 63: Val Loss 233.78239
Epoch 64: Val Loss 233.78175
Epoch 65: Val Loss 233.78107
Epoch 66: Val Loss 233.78043
Epoch 67: Val Loss 233.77974
Epoch 68: Val Loss 233.77910
Epoch 69: Val Loss 233.77843
Epoch 70: Val Loss 233.77776
Epoch 71: Val Loss 233.77710
Epoch 72: Val Loss 233.77643
Epoch 73: Val Loss 233.77577
Epoch 74: Val Loss 233.77510
Epoch 75: Val Loss 233.77444
Epoch 76: Val Loss 233.77379
Epoch 77: Val Loss 233.77313
Epoch 78: Val Loss 233.77248
Epoch 79: Val Loss 233.77179
Epoch 80: Val Loss 233.77115
Epoch 81: Val Loss 233.77048
Epoch 82: Val Loss 233.76982
Epoch 83: Val Loss 233.76915
Epoch 84: Val Loss 233.76851
Epoch 85: Val Loss 233.76782
Epoch 86: Val Loss 233.76718
Epoch 87: Val Loss 233.76653
Epoch 88: Val Loss 233.76587
Epoch 89: Val Loss 233.76521
Epoch 90: Val Loss 233.76456
Epoch 91: Val Loss 233.76390
Epoch 92: Val Loss 233.76328
Epoch 93: Val Loss 233.76259
Epoch 94: Val Loss 233.76195
Epoch 95: Val Loss 233.76131
Epoch 96: Val Loss 233.76062
Epoch 97: Val Loss 233.75998
Epoch 98: Val Loss 233.75931
Epoch 99: Val Loss 233.75865
{'MSE - mean': 232.2816502369223, 'MSE - std': 1.4770267805984645, 'R2 - mean': -367.270648287847, 'R2 - std': 51.717453166332405} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.86081
Epoch 1: Val Loss 227.86024
Epoch 2: Val Loss 227.85966
Epoch 3: Val Loss 227.85912
Epoch 4: Val Loss 227.85855
Epoch 5: Val Loss 227.85799
Epoch 6: Val Loss 227.85742
Epoch 7: Val Loss 227.85687
Epoch 8: Val Loss 227.85631
Epoch 9: Val Loss 227.85574
Epoch 10: Val Loss 227.85519
Epoch 11: Val Loss 227.85461
Epoch 12: Val Loss 227.85405
Epoch 13: Val Loss 227.85349
Epoch 14: Val Loss 227.85292
Epoch 15: Val Loss 227.85237
Epoch 16: Val Loss 227.85182
Epoch 17: Val Loss 227.85124
Epoch 18: Val Loss 227.85066
Epoch 19: Val Loss 227.85011
Epoch 20: Val Loss 227.84955
Epoch 21: Val Loss 227.84900
Epoch 22: Val Loss 227.84843
Epoch 23: Val Loss 227.84785
Epoch 24: Val Loss 227.84731
Epoch 25: Val Loss 227.84674
Epoch 26: Val Loss 227.84616
Epoch 27: Val Loss 227.84561
Epoch 28: Val Loss 227.84505
Epoch 29: Val Loss 227.84448
Epoch 30: Val Loss 227.84390
Epoch 31: Val Loss 227.84337
Epoch 32: Val Loss 227.84280
Epoch 33: Val Loss 227.84222
Epoch 34: Val Loss 227.84167
Epoch 35: Val Loss 227.84109
Epoch 36: Val Loss 227.84055
Epoch 37: Val Loss 227.83997
Epoch 38: Val Loss 227.83943
Epoch 39: Val Loss 227.83887
Epoch 40: Val Loss 227.83830
Epoch 41: Val Loss 227.83772
Epoch 42: Val Loss 227.83719
Epoch 43: Val Loss 227.83661
Epoch 44: Val Loss 227.83604
Epoch 45: Val Loss 227.83548
Epoch 46: Val Loss 227.83493
Epoch 47: Val Loss 227.83437
Epoch 48: Val Loss 227.83382
Epoch 49: Val Loss 227.83328
Epoch 50: Val Loss 227.83270
Epoch 51: Val Loss 227.83214
Epoch 52: Val Loss 227.83157
Epoch 53: Val Loss 227.83102
Epoch 54: Val Loss 227.83046
Epoch 55: Val Loss 227.82991
Epoch 56: Val Loss 227.82936
Epoch 57: Val Loss 227.82880
Epoch 58: Val Loss 227.82823
Epoch 59: Val Loss 227.82767
Epoch 60: Val Loss 227.82712
Epoch 61: Val Loss 227.82657
Epoch 62: Val Loss 227.82600
Epoch 63: Val Loss 227.82542
Epoch 64: Val Loss 227.82487
Epoch 65: Val Loss 227.82431
Epoch 66: Val Loss 227.82376
Epoch 67: Val Loss 227.82320
Epoch 68: Val Loss 227.82265
Epoch 69: Val Loss 227.82210
Epoch 70: Val Loss 227.82152
Epoch 71: Val Loss 227.82098
Epoch 72: Val Loss 227.82043
Epoch 73: Val Loss 227.81985
Epoch 74: Val Loss 227.81932
Epoch 75: Val Loss 227.81877
Epoch 76: Val Loss 227.81819
Epoch 77: Val Loss 227.81766
Epoch 78: Val Loss 227.81709
Epoch 79: Val Loss 227.81653
Epoch 80: Val Loss 227.81599
Epoch 81: Val Loss 227.81543
Epoch 82: Val Loss 227.81488
Epoch 83: Val Loss 227.81433
Epoch 84: Val Loss 227.81377
Epoch 85: Val Loss 227.81320
Epoch 86: Val Loss 227.81265
Epoch 87: Val Loss 227.81212
Epoch 88: Val Loss 227.81154
Epoch 89: Val Loss 227.81099
Epoch 90: Val Loss 227.81046
Epoch 91: Val Loss 227.80989
Epoch 92: Val Loss 227.80933
Epoch 93: Val Loss 227.80879
Epoch 94: Val Loss 227.80823
Epoch 95: Val Loss 227.80766
Epoch 96: Val Loss 227.80713
Epoch 97: Val Loss 227.80656
Epoch 98: Val Loss 227.80602
Epoch 99: Val Loss 227.80547
{'MSE - mean': 230.78959199189305, 'MSE - std': 2.4304075831410916, 'R2 - mean': -354.7307957804678, 'R2 - std': 45.799844721223366} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.76622
Epoch 1: Val Loss 217.76527
Epoch 2: Val Loss 217.76436
Epoch 3: Val Loss 217.76343
Epoch 4: Val Loss 217.76250
Epoch 5: Val Loss 217.76157
Epoch 6: Val Loss 217.76062
Epoch 7: Val Loss 217.75970
Epoch 8: Val Loss 217.75877
Epoch 9: Val Loss 217.75784
Epoch 10: Val Loss 217.75690
Epoch 11: Val Loss 217.75598
Epoch 12: Val Loss 217.75505
Epoch 13: Val Loss 217.75414
Epoch 14: Val Loss 217.75320
Epoch 15: Val Loss 217.75226
Epoch 16: Val Loss 217.75134
Epoch 17: Val Loss 217.75041
Epoch 18: Val Loss 217.74948
Epoch 19: Val Loss 217.74854
Epoch 20: Val Loss 217.74759
Epoch 21: Val Loss 217.74667
Epoch 22: Val Loss 217.74574
Epoch 23: Val Loss 217.74480
Epoch 24: Val Loss 217.74388
Epoch 25: Val Loss 217.74297
Epoch 26: Val Loss 217.74200
Epoch 27: Val Loss 217.74109
Epoch 28: Val Loss 217.74014
Epoch 29: Val Loss 217.73923
Epoch 30: Val Loss 217.73827
Epoch 31: Val Loss 217.73735
Epoch 32: Val Loss 217.73642
Epoch 33: Val Loss 217.73549
Epoch 34: Val Loss 217.73453
Epoch 35: Val Loss 217.73361
Epoch 36: Val Loss 217.73268
Epoch 37: Val Loss 217.73175
Epoch 38: Val Loss 217.73082
Epoch 39: Val Loss 217.72989
Epoch 40: Val Loss 217.72893
Epoch 41: Val Loss 217.72801
Epoch 42: Val Loss 217.72708
Epoch 43: Val Loss 217.72614
Epoch 44: Val Loss 217.72519
Epoch 45: Val Loss 217.72426
Epoch 46: Val Loss 217.72333
Epoch 47: Val Loss 217.72240
Epoch 48: Val Loss 217.72145
Epoch 49: Val Loss 217.72054
Epoch 50: Val Loss 217.71959
Epoch 51: Val Loss 217.71867
Epoch 52: Val Loss 217.71776
Epoch 53: Val Loss 217.71683
Epoch 54: Val Loss 217.71588
Epoch 55: Val Loss 217.71494
Epoch 56: Val Loss 217.71402
Epoch 57: Val Loss 217.71307
Epoch 58: Val Loss 217.71214
Epoch 59: Val Loss 217.71121
Epoch 60: Val Loss 217.71028
Epoch 61: Val Loss 217.70937
Epoch 62: Val Loss 217.70844
Epoch 63: Val Loss 217.70750
Epoch 64: Val Loss 217.70656
Epoch 65: Val Loss 217.70563
Epoch 66: Val Loss 217.70470
Epoch 67: Val Loss 217.70377
Epoch 68: Val Loss 217.70285
Epoch 69: Val Loss 217.70190
Epoch 70: Val Loss 217.70097
Epoch 71: Val Loss 217.70003
Epoch 72: Val Loss 217.69911
Epoch 73: Val Loss 217.69815
Epoch 74: Val Loss 217.69724
Epoch 75: Val Loss 217.69629
Epoch 76: Val Loss 217.69537
Epoch 77: Val Loss 217.69441
Epoch 78: Val Loss 217.69348
Epoch 79: Val Loss 217.69255
Epoch 80: Val Loss 217.69162
Epoch 81: Val Loss 217.69069
Epoch 82: Val Loss 217.68973
Epoch 83: Val Loss 217.68880
Epoch 84: Val Loss 217.68787
Epoch 85: Val Loss 217.68692
Epoch 86: Val Loss 217.68597
Epoch 87: Val Loss 217.68504
Epoch 88: Val Loss 217.68411
Epoch 89: Val Loss 217.68318
Epoch 90: Val Loss 217.68222
Epoch 91: Val Loss 217.68130
Epoch 92: Val Loss 217.68037
Epoch 93: Val Loss 217.67943
Epoch 94: Val Loss 217.67848
Epoch 95: Val Loss 217.67754
Epoch 96: Val Loss 217.67662
Epoch 97: Val Loss 217.67569
Epoch 98: Val Loss 217.67476
Epoch 99: Val Loss 217.67381
{'MSE - mean': 227.5106508411752, 'MSE - std': 6.056775212500026, 'R2 - mean': -335.2053773643269, 'R2 - std': 52.12432469914839} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 237.19676
Epoch 1: Val Loss 237.19582
Epoch 2: Val Loss 237.19485
Epoch 3: Val Loss 237.19391
Epoch 4: Val Loss 237.19296
Epoch 5: Val Loss 237.19200
Epoch 6: Val Loss 237.19106
Epoch 7: Val Loss 237.19011
Epoch 8: Val Loss 237.18916
Epoch 9: Val Loss 237.18820
Epoch 10: Val Loss 237.18727
Epoch 11: Val Loss 237.18631
Epoch 12: Val Loss 237.18536
Epoch 13: Val Loss 237.18442
Epoch 14: Val Loss 237.18349
Epoch 15: Val Loss 237.18253
Epoch 16: Val Loss 237.18158
Epoch 17: Val Loss 237.18063
Epoch 18: Val Loss 237.17967
Epoch 19: Val Loss 237.17873
Epoch 20: Val Loss 237.17778
Epoch 21: Val Loss 237.17682
Epoch 22: Val Loss 237.17587
Epoch 23: Val Loss 237.17493
Epoch 24: Val Loss 237.17397
Epoch 25: Val Loss 237.17302
Epoch 26: Val Loss 237.17207
Epoch 27: Val Loss 237.17111
Epoch 28: Val Loss 237.17017
Epoch 29: Val Loss 237.16922
Epoch 30: Val Loss 237.16826
Epoch 31: Val Loss 237.16731
Epoch 32: Val Loss 237.16637
Epoch 33: Val Loss 237.16541
Epoch 34: Val Loss 237.16446
Epoch 35: Val Loss 237.16351
Epoch 36: Val Loss 237.16255
Epoch 37: Val Loss 237.16162
Epoch 38: Val Loss 237.16068
Epoch 39: Val Loss 237.15974
Epoch 40: Val Loss 237.15878
Epoch 41: Val Loss 237.15784
Epoch 42: Val Loss 237.15689
Epoch 43: Val Loss 237.15593
Epoch 44: Val Loss 237.15498
Epoch 45: Val Loss 237.15404
Epoch 46: Val Loss 237.15309
Epoch 47: Val Loss 237.15213
Epoch 48: Val Loss 237.15118
Epoch 49: Val Loss 237.15022
Epoch 50: Val Loss 237.14929
Epoch 51: Val Loss 237.14835
Epoch 52: Val Loss 237.14738
Epoch 53: Val Loss 237.14645
Epoch 54: Val Loss 237.14548
Epoch 55: Val Loss 237.14453
Epoch 56: Val Loss 237.14359
Epoch 57: Val Loss 237.14262
Epoch 58: Val Loss 237.14166
Epoch 59: Val Loss 237.14073
Epoch 60: Val Loss 237.13977
Epoch 61: Val Loss 237.13881
Epoch 62: Val Loss 237.13786
Epoch 63: Val Loss 237.13692
Epoch 64: Val Loss 237.13596
Epoch 65: Val Loss 237.13501
Epoch 66: Val Loss 237.13406
Epoch 67: Val Loss 237.13309
Epoch 68: Val Loss 237.13214
Epoch 69: Val Loss 237.13121
Epoch 70: Val Loss 237.13025
Epoch 71: Val Loss 237.12930
Epoch 72: Val Loss 237.12836
Epoch 73: Val Loss 237.12740
Epoch 74: Val Loss 237.12643
Epoch 75: Val Loss 237.12550
Epoch 76: Val Loss 237.12454
Epoch 77: Val Loss 237.12360
Epoch 78: Val Loss 237.12265
Epoch 79: Val Loss 237.12169
Epoch 80: Val Loss 237.12074
Epoch 81: Val Loss 237.11980
Epoch 82: Val Loss 237.11884
Epoch 83: Val Loss 237.11789
Epoch 84: Val Loss 237.11694
Epoch 85: Val Loss 237.11598
Epoch 86: Val Loss 237.11504
Epoch 87: Val Loss 237.11407
Epoch 88: Val Loss 237.11313
Epoch 89: Val Loss 237.11218
Epoch 90: Val Loss 237.11122
Epoch 91: Val Loss 237.11028
Epoch 92: Val Loss 237.10933
Epoch 93: Val Loss 237.10837
Epoch 94: Val Loss 237.10741
Epoch 95: Val Loss 237.10645
Epoch 96: Val Loss 237.10550
Epoch 97: Val Loss 237.10454
Epoch 98: Val Loss 237.10362
Epoch 99: Val Loss 237.10266
{'MSE - mean': 229.42905339002695, 'MSE - std': 6.638425576713758, 'R2 - mean': -346.66132098839626, 'R2 - std': 51.94719200828107} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 229.42905339002695 and parameters: {'dim': 64, 'depth': 12, 'heads': 8, 'weight_decay': -2, 'learning_rate': -6, 'dropout': 0}. Best is trial 8 with value: 0.6579859061194161.
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 215.58069
Epoch 1: Val Loss 215.56976
Epoch 2: Val Loss 215.55890
Epoch 3: Val Loss 215.54802
Epoch 4: Val Loss 215.53714
Epoch 5: Val Loss 215.52634
Epoch 6: Val Loss 215.51549
Epoch 7: Val Loss 215.50468
Epoch 8: Val Loss 215.49384
Epoch 9: Val Loss 215.48299
Epoch 10: Val Loss 215.47215
Epoch 11: Val Loss 215.46130
Epoch 12: Val Loss 215.45042
Epoch 13: Val Loss 215.43953
Epoch 14: Val Loss 215.42865
Epoch 15: Val Loss 215.41772
Epoch 16: Val Loss 215.40686
Epoch 17: Val Loss 215.39598
Epoch 18: Val Loss 215.38507
Epoch 19: Val Loss 215.37416
Epoch 20: Val Loss 215.36324
Epoch 21: Val Loss 215.35237
Epoch 22: Val Loss 215.34143
Epoch 23: Val Loss 215.33047
Epoch 24: Val Loss 215.31953
Epoch 25: Val Loss 215.30858
Epoch 26: Val Loss 215.29761
Epoch 27: Val Loss 215.28665
Epoch 28: Val Loss 215.27574
Epoch 29: Val Loss 215.26485
Epoch 30: Val Loss 215.25397
Epoch 31: Val Loss 215.24306
Epoch 32: Val Loss 215.23218
Epoch 33: Val Loss 215.22131
Epoch 34: Val Loss 215.21036
Epoch 35: Val Loss 215.19940
Epoch 36: Val Loss 215.18848
Epoch 37: Val Loss 215.17754
Epoch 38: Val Loss 215.16667
Epoch 39: Val Loss 215.15576
Epoch 40: Val Loss 215.14487
Epoch 41: Val Loss 215.13396
Epoch 42: Val Loss 215.12306
Epoch 43: Val Loss 215.11215
Epoch 44: Val Loss 215.10123
Epoch 45: Val Loss 215.09027
Epoch 46: Val Loss 215.07928
Epoch 47: Val Loss 215.06833
Epoch 48: Val Loss 215.05737
Epoch 49: Val Loss 215.04643
Epoch 50: Val Loss 215.03546
Epoch 51: Val Loss 215.02449
Epoch 52: Val Loss 215.01358
Epoch 53: Val Loss 215.00262
Epoch 54: Val Loss 214.99168
Epoch 55: Val Loss 214.98070
Epoch 56: Val Loss 214.96968
Epoch 57: Val Loss 214.95871
Epoch 58: Val Loss 214.94772
Epoch 59: Val Loss 214.93671
Epoch 60: Val Loss 214.92572
Epoch 61: Val Loss 214.91470
Epoch 62: Val Loss 214.90369
Epoch 63: Val Loss 214.89264
Epoch 64: Val Loss 214.88162
Epoch 65: Val Loss 214.87062
Epoch 66: Val Loss 214.85960
Epoch 67: Val Loss 214.84860
Epoch 68: Val Loss 214.83755
Epoch 69: Val Loss 214.82651
Epoch 70: Val Loss 214.81546
Epoch 71: Val Loss 214.80435
Epoch 72: Val Loss 214.79324
Epoch 73: Val Loss 214.78214
Epoch 74: Val Loss 214.77106
Epoch 75: Val Loss 214.75996
Epoch 76: Val Loss 214.74886
Epoch 77: Val Loss 214.73767
Epoch 78: Val Loss 214.72653
Epoch 79: Val Loss 214.71536
Epoch 80: Val Loss 214.70418
Epoch 81: Val Loss 214.69299
Epoch 82: Val Loss 214.68182
Epoch 83: Val Loss 214.67070
Epoch 84: Val Loss 214.65952
Epoch 85: Val Loss 214.64833
Epoch 86: Val Loss 214.63716
Epoch 87: Val Loss 214.62593
Epoch 88: Val Loss 214.61473
Epoch 89: Val Loss 214.60347
Epoch 90: Val Loss 214.59219
Epoch 91: Val Loss 214.58095
Epoch 92: Val Loss 214.56970
Epoch 93: Val Loss 214.55841
Epoch 94: Val Loss 214.54709
Epoch 95: Val Loss 214.53580
Epoch 96: Val Loss 214.52448
Epoch 97: Val Loss 214.51320
Epoch 98: Val Loss 214.50186
Epoch 99: Val Loss 214.49054
{'MSE - mean': 214.49052812068177, 'MSE - std': 0.0, 'R2 - mean': -389.30184203548373, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 225.85760
Epoch 1: Val Loss 225.84470
Epoch 2: Val Loss 225.83182
Epoch 3: Val Loss 225.81891
Epoch 4: Val Loss 225.80600
Epoch 5: Val Loss 225.79315
Epoch 6: Val Loss 225.78023
Epoch 7: Val Loss 225.76735
Epoch 8: Val Loss 225.75447
Epoch 9: Val Loss 225.74156
Epoch 10: Val Loss 225.72867
Epoch 11: Val Loss 225.71577
Epoch 12: Val Loss 225.70282
Epoch 13: Val Loss 225.68983
Epoch 14: Val Loss 225.67688
Epoch 15: Val Loss 225.66391
Epoch 16: Val Loss 225.65085
Epoch 17: Val Loss 225.63780
Epoch 18: Val Loss 225.62479
Epoch 19: Val Loss 225.61171
Epoch 20: Val Loss 225.59860
Epoch 21: Val Loss 225.58545
Epoch 22: Val Loss 225.57227
Epoch 23: Val Loss 225.55899
Epoch 24: Val Loss 225.54585
Epoch 25: Val Loss 225.53267
Epoch 26: Val Loss 225.51945
Epoch 27: Val Loss 225.50623
Epoch 28: Val Loss 225.49300
Epoch 29: Val Loss 225.47971
Epoch 30: Val Loss 225.46643
Epoch 31: Val Loss 225.45316
Epoch 32: Val Loss 225.43983
Epoch 33: Val Loss 225.42651
Epoch 34: Val Loss 225.41319
Epoch 35: Val Loss 225.39975
Epoch 36: Val Loss 225.38631
Epoch 37: Val Loss 225.37285
Epoch 38: Val Loss 225.35938
Epoch 39: Val Loss 225.34595
Epoch 40: Val Loss 225.33253
Epoch 41: Val Loss 225.31905
Epoch 42: Val Loss 225.30562
Epoch 43: Val Loss 225.29207
Epoch 44: Val Loss 225.27859
Epoch 45: Val Loss 225.26503
Epoch 46: Val Loss 225.25148
Epoch 47: Val Loss 225.23787
Epoch 48: Val Loss 225.22424
Epoch 49: Val Loss 225.21066
Epoch 50: Val Loss 225.19707
Epoch 51: Val Loss 225.18344
Epoch 52: Val Loss 225.16983
Epoch 53: Val Loss 225.15620
Epoch 54: Val Loss 225.14255
Epoch 55: Val Loss 225.12889
Epoch 56: Val Loss 225.11523
Epoch 57: Val Loss 225.10153
Epoch 58: Val Loss 225.08780
Epoch 59: Val Loss 225.07404
Epoch 60: Val Loss 225.06024
Epoch 61: Val Loss 225.04645
Epoch 62: Val Loss 225.03264
Epoch 63: Val Loss 225.01877
Epoch 64: Val Loss 225.00494
Epoch 65: Val Loss 224.99109
Epoch 66: Val Loss 224.97726
Epoch 67: Val Loss 224.96347
Epoch 68: Val Loss 224.94955
Epoch 69: Val Loss 224.93565
Epoch 70: Val Loss 224.92172
Epoch 71: Val Loss 224.90775
Epoch 72: Val Loss 224.89374
Epoch 73: Val Loss 224.87975
Epoch 74: Val Loss 224.86568
Epoch 75: Val Loss 224.85165
Epoch 76: Val Loss 224.83762
Epoch 77: Val Loss 224.82356
Epoch 78: Val Loss 224.80949
Epoch 79: Val Loss 224.79544
Epoch 80: Val Loss 224.78137
Epoch 81: Val Loss 224.76724
Epoch 82: Val Loss 224.75311
Epoch 83: Val Loss 224.73898
Epoch 84: Val Loss 224.72482
Epoch 85: Val Loss 224.71066
Epoch 86: Val Loss 224.69644
Epoch 87: Val Loss 224.68233
Epoch 88: Val Loss 224.66817
Epoch 89: Val Loss 224.65399
Epoch 90: Val Loss 224.63983
Epoch 91: Val Loss 224.62570
Epoch 92: Val Loss 224.61157
Epoch 93: Val Loss 224.59746
Epoch 94: Val Loss 224.58328
Epoch 95: Val Loss 224.56917
Epoch 96: Val Loss 224.55502
Epoch 97: Val Loss 224.54083
Epoch 98: Val Loss 224.52655
Epoch 99: Val Loss 224.51236
{'MSE - mean': 219.50144606014152, 'MSE - std': 5.010917939459759, 'R2 - mean': -346.16689551573114, 'R2 - std': 43.13494651975259} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.06441
Epoch 1: Val Loss 217.05910
Epoch 2: Val Loss 217.05376
Epoch 3: Val Loss 217.04845
Epoch 4: Val Loss 217.04312
Epoch 5: Val Loss 217.03783
Epoch 6: Val Loss 217.03253
Epoch 7: Val Loss 217.02727
Epoch 8: Val Loss 217.02200
Epoch 9: Val Loss 217.01674
Epoch 10: Val Loss 217.01147
Epoch 11: Val Loss 217.00620
Epoch 12: Val Loss 217.00090
Epoch 13: Val Loss 216.99564
Epoch 14: Val Loss 216.99036
Epoch 15: Val Loss 216.98509
Epoch 16: Val Loss 216.97981
Epoch 17: Val Loss 216.97455
Epoch 18: Val Loss 216.96928
Epoch 19: Val Loss 216.96402
Epoch 20: Val Loss 216.95876
Epoch 21: Val Loss 216.95346
Epoch 22: Val Loss 216.94817
Epoch 23: Val Loss 216.94290
Epoch 24: Val Loss 216.93762
Epoch 25: Val Loss 216.93236
Epoch 26: Val Loss 216.92711
Epoch 27: Val Loss 216.92186
Epoch 28: Val Loss 216.91664
Epoch 29: Val Loss 216.91141
Epoch 30: Val Loss 216.90616
Epoch 31: Val Loss 216.90091
Epoch 32: Val Loss 216.89565
Epoch 33: Val Loss 216.89038
Epoch 34: Val Loss 216.88512
Epoch 35: Val Loss 216.87985
Epoch 36: Val Loss 216.87457
Epoch 37: Val Loss 216.86928
Epoch 38: Val Loss 216.86398
Epoch 39: Val Loss 216.85869
Epoch 40: Val Loss 216.85335
Epoch 41: Val Loss 216.84807
Epoch 42: Val Loss 216.84276
Epoch 43: Val Loss 216.83746
Epoch 44: Val Loss 216.83214
Epoch 45: Val Loss 216.82686
Epoch 46: Val Loss 216.82156
Epoch 47: Val Loss 216.81630
Epoch 48: Val Loss 216.81104
Epoch 49: Val Loss 216.80577
Epoch 50: Val Loss 216.80051
Epoch 51: Val Loss 216.79520
Epoch 52: Val Loss 216.78990
Epoch 53: Val Loss 216.78459
Epoch 54: Val Loss 216.77928
Epoch 55: Val Loss 216.77399
Epoch 56: Val Loss 216.76871
Epoch 57: Val Loss 216.76343
Epoch 58: Val Loss 216.75810
Epoch 59: Val Loss 216.75281
Epoch 60: Val Loss 216.74753
Epoch 61: Val Loss 216.74225
Epoch 62: Val Loss 216.73697
Epoch 63: Val Loss 216.73167
Epoch 64: Val Loss 216.72638
Epoch 65: Val Loss 216.72110
Epoch 66: Val Loss 216.71582
Epoch 67: Val Loss 216.71053
Epoch 68: Val Loss 216.70520
Epoch 69: Val Loss 216.69991
Epoch 70: Val Loss 216.69461
Epoch 71: Val Loss 216.68933
Epoch 72: Val Loss 216.68402
Epoch 73: Val Loss 216.67871
Epoch 74: Val Loss 216.67340
Epoch 75: Val Loss 216.66806
Epoch 76: Val Loss 216.66273
Epoch 77: Val Loss 216.65742
Epoch 78: Val Loss 216.65211
Epoch 79: Val Loss 216.64679
Epoch 80: Val Loss 216.64149
Epoch 81: Val Loss 216.63618
Epoch 82: Val Loss 216.63089
Epoch 83: Val Loss 216.62556
Epoch 84: Val Loss 216.62025
Epoch 85: Val Loss 216.61493
Epoch 86: Val Loss 216.60963
Epoch 87: Val Loss 216.60435
Epoch 88: Val Loss 216.59909
Epoch 89: Val Loss 216.59380
Epoch 90: Val Loss 216.58849
Epoch 91: Val Loss 216.58319
Epoch 92: Val Loss 216.57791
Epoch 93: Val Loss 216.57259
Epoch 94: Val Loss 216.56728
Epoch 95: Val Loss 216.56200
Epoch 96: Val Loss 216.55664
Epoch 97: Val Loss 216.55130
Epoch 98: Val Loss 216.54591
Epoch 99: Val Loss 216.54054
{'MSE - mean': 218.51448306504224, 'MSE - std': 4.322930060124006, 'R2 - mean': -335.21142527933483, 'R2 - std': 38.47675138369155} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.96391
Epoch 1: Val Loss 222.95488
Epoch 2: Val Loss 222.94591
Epoch 3: Val Loss 222.93690
Epoch 4: Val Loss 222.92795
Epoch 5: Val Loss 222.91898
Epoch 6: Val Loss 222.91005
Epoch 7: Val Loss 222.90114
Epoch 8: Val Loss 222.89227
Epoch 9: Val Loss 222.88345
Epoch 10: Val Loss 222.87462
Epoch 11: Val Loss 222.86575
Epoch 12: Val Loss 222.85692
Epoch 13: Val Loss 222.84807
Epoch 14: Val Loss 222.83923
Epoch 15: Val Loss 222.83047
Epoch 16: Val Loss 222.82164
Epoch 17: Val Loss 222.81287
Epoch 18: Val Loss 222.80408
Epoch 19: Val Loss 222.79530
Epoch 20: Val Loss 222.78654
Epoch 21: Val Loss 222.77780
Epoch 22: Val Loss 222.76906
Epoch 23: Val Loss 222.76036
Epoch 24: Val Loss 222.75165
Epoch 25: Val Loss 222.74294
Epoch 26: Val Loss 222.73421
Epoch 27: Val Loss 222.72549
Epoch 28: Val Loss 222.71677
Epoch 29: Val Loss 222.70807
Epoch 30: Val Loss 222.69937
Epoch 31: Val Loss 222.69070
Epoch 32: Val Loss 222.68204
Epoch 33: Val Loss 222.67340
Epoch 34: Val Loss 222.66475
Epoch 35: Val Loss 222.65610
Epoch 36: Val Loss 222.64746
Epoch 37: Val Loss 222.63881
Epoch 38: Val Loss 222.63017
Epoch 39: Val Loss 222.62152
Epoch 40: Val Loss 222.61292
Epoch 41: Val Loss 222.60429
Epoch 42: Val Loss 222.59566
Epoch 43: Val Loss 222.58704
Epoch 44: Val Loss 222.57843
Epoch 45: Val Loss 222.56989
Epoch 46: Val Loss 222.56129
Epoch 47: Val Loss 222.55273
Epoch 48: Val Loss 222.54417
Epoch 49: Val Loss 222.53564
Epoch 50: Val Loss 222.52708
Epoch 51: Val Loss 222.51855
Epoch 52: Val Loss 222.50998
Epoch 53: Val Loss 222.50148
Epoch 54: Val Loss 222.49303
Epoch 55: Val Loss 222.48460
Epoch 56: Val Loss 222.47615
Epoch 57: Val Loss 222.46768
Epoch 58: Val Loss 222.45924
Epoch 59: Val Loss 222.45081
Epoch 60: Val Loss 222.44238
Epoch 61: Val Loss 222.43394
Epoch 62: Val Loss 222.42554
Epoch 63: Val Loss 222.41711
Epoch 64: Val Loss 222.40871
Epoch 65: Val Loss 222.40030
Epoch 66: Val Loss 222.39191
Epoch 67: Val Loss 222.38354
Epoch 68: Val Loss 222.37515
Epoch 69: Val Loss 222.36682
Epoch 70: Val Loss 222.35847
Epoch 71: Val Loss 222.35013
Epoch 72: Val Loss 222.34184
Epoch 73: Val Loss 222.33351
Epoch 74: Val Loss 222.32524
Epoch 75: Val Loss 222.31693
Epoch 76: Val Loss 222.30864
Epoch 77: Val Loss 222.30037
Epoch 78: Val Loss 222.29207
Epoch 79: Val Loss 222.28380
Epoch 80: Val Loss 222.27554
Epoch 81: Val Loss 222.26732
Epoch 82: Val Loss 222.25912
Epoch 83: Val Loss 222.25093
Epoch 84: Val Loss 222.24275
Epoch 85: Val Loss 222.23460
Epoch 86: Val Loss 222.22638
Epoch 87: Val Loss 222.21820
Epoch 88: Val Loss 222.21004
Epoch 89: Val Loss 222.20189
Epoch 90: Val Loss 222.19377
Epoch 91: Val Loss 222.18562
Epoch 92: Val Loss 222.17743
Epoch 93: Val Loss 222.16928
Epoch 94: Val Loss 222.16113
Epoch 95: Val Loss 222.15297
Epoch 96: Val Loss 222.14482
Epoch 97: Val Loss 222.13670
Epoch 98: Val Loss 222.12854
Epoch 99: Val Loss 222.12038
{'MSE - mean': 219.4159573453815, 'MSE - std': 4.056323564865402, 'R2 - mean': -321.9836733081637, 'R2 - std': 40.43841693753935} 
 

In get_device
()
On Device: cuda
Using dim 256 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.66956
Epoch 1: Val Loss 220.66196
Epoch 2: Val Loss 220.65439
Epoch 3: Val Loss 220.64684
Epoch 4: Val Loss 220.63922
Epoch 5: Val Loss 220.63161
Epoch 6: Val Loss 220.62398
Epoch 7: Val Loss 220.61638
Epoch 8: Val Loss 220.60870
Epoch 9: Val Loss 220.60101
Epoch 10: Val Loss 220.59334
Epoch 11: Val Loss 220.58566
Epoch 12: Val Loss 220.57803
Epoch 13: Val Loss 220.57037
Epoch 14: Val Loss 220.56273
Epoch 15: Val Loss 220.55508
Epoch 16: Val Loss 220.54744
Epoch 17: Val Loss 220.53976
Epoch 18: Val Loss 220.53212
Epoch 19: Val Loss 220.52446
Epoch 20: Val Loss 220.51680
Epoch 21: Val Loss 220.50914
Epoch 22: Val Loss 220.50148
Epoch 23: Val Loss 220.49379
Epoch 24: Val Loss 220.48608
Epoch 25: Val Loss 220.47833
Epoch 26: Val Loss 220.47054
Epoch 27: Val Loss 220.46280
Epoch 28: Val Loss 220.45506
Epoch 29: Val Loss 220.44727
Epoch 30: Val Loss 220.43950
Epoch 31: Val Loss 220.43172
Epoch 32: Val Loss 220.42398
Epoch 33: Val Loss 220.41617
Epoch 34: Val Loss 220.40843
Epoch 35: Val Loss 220.40063
Epoch 36: Val Loss 220.39285
Epoch 37: Val Loss 220.38506
Epoch 38: Val Loss 220.37730
Epoch 39: Val Loss 220.36949
Epoch 40: Val Loss 220.36171
Epoch 41: Val Loss 220.35390
Epoch 42: Val Loss 220.34601
Epoch 43: Val Loss 220.33817
Epoch 44: Val Loss 220.33023
Epoch 45: Val Loss 220.32224
Epoch 46: Val Loss 220.31418
Epoch 47: Val Loss 220.30611
Epoch 48: Val Loss 220.29808
Epoch 49: Val Loss 220.29004
Epoch 50: Val Loss 220.28203
Epoch 51: Val Loss 220.27400
Epoch 52: Val Loss 220.26598
Epoch 53: Val Loss 220.25793
Epoch 54: Val Loss 220.24991
Epoch 55: Val Loss 220.24187
Epoch 56: Val Loss 220.23383
Epoch 57: Val Loss 220.22571
Epoch 58: Val Loss 220.21761
Epoch 59: Val Loss 220.20952
Epoch 60: Val Loss 220.20143
Epoch 61: Val Loss 220.19333
Epoch 62: Val Loss 220.18532
Epoch 63: Val Loss 220.17723
Epoch 64: Val Loss 220.16922
Epoch 65: Val Loss 220.16119
Epoch 66: Val Loss 220.15315
Epoch 67: Val Loss 220.14510
Epoch 68: Val Loss 220.13701
Epoch 69: Val Loss 220.12897
Epoch 70: Val Loss 220.12090
Epoch 71: Val Loss 220.11282
Epoch 72: Val Loss 220.10475
Epoch 73: Val Loss 220.09666
Epoch 74: Val Loss 220.08861
Epoch 75: Val Loss 220.08057
Epoch 76: Val Loss 220.07246
Epoch 77: Val Loss 220.06430
Epoch 78: Val Loss 220.05611
Epoch 79: Val Loss 220.04791
Epoch 80: Val Loss 220.03978
Epoch 81: Val Loss 220.03162
Epoch 82: Val Loss 220.02350
Epoch 83: Val Loss 220.01530
Epoch 84: Val Loss 220.00710
Epoch 85: Val Loss 219.99893
Epoch 86: Val Loss 219.99072
Epoch 87: Val Loss 219.98250
Epoch 88: Val Loss 219.97426
Epoch 89: Val Loss 219.96599
Epoch 90: Val Loss 219.95767
Epoch 91: Val Loss 219.94934
Epoch 92: Val Loss 219.94101
Epoch 93: Val Loss 219.93275
Epoch 94: Val Loss 219.92445
Epoch 95: Val Loss 219.91617
Epoch 96: Val Loss 219.90788
Epoch 97: Val Loss 219.89960
Epoch 98: Val Loss 219.89133
Epoch 99: Val Loss 219.88310
{'MSE - mean': 219.50939005209358, 'MSE - std': 3.6328951778216854, 'R2 - mean': -330.3685994226385, 'R2 - std': 39.86778643607169} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 219.50939005209358 and parameters: {'dim': 256, 'depth': 1, 'heads': 2, 'weight_decay': -4, 'learning_rate': -5, 'dropout': 0.3}. Best is trial 8 with value: 0.6579859061194161.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 218.19000
Epoch 1: Val Loss 217.36182
Epoch 2: Val Loss 216.51320
Epoch 3: Val Loss 215.62274
Epoch 4: Val Loss 214.68739
Epoch 5: Val Loss 213.67661
Epoch 6: Val Loss 212.55313
Epoch 7: Val Loss 211.30020
Epoch 8: Val Loss 209.87251
Epoch 9: Val Loss 208.24757
Epoch 10: Val Loss 206.37370
Epoch 11: Val Loss 204.20357
Epoch 12: Val Loss 201.70789
Epoch 13: Val Loss 198.80997
Epoch 14: Val Loss 195.41733
Epoch 15: Val Loss 191.50560
Epoch 16: Val Loss 186.96208
Epoch 17: Val Loss 181.73605
Epoch 18: Val Loss 175.74974
Epoch 19: Val Loss 168.92244
Epoch 20: Val Loss 161.15547
Epoch 21: Val Loss 152.41142
Epoch 22: Val Loss 142.61894
Epoch 23: Val Loss 131.77863
Epoch 24: Val Loss 119.86222
Epoch 25: Val Loss 106.90473
Epoch 26: Val Loss 93.20808
Epoch 27: Val Loss 79.13078
Epoch 28: Val Loss 65.12623
Epoch 29: Val Loss 51.76452
Epoch 30: Val Loss 39.54955
Epoch 31: Val Loss 29.09194
Epoch 32: Val Loss 20.76814
Epoch 33: Val Loss 14.76831
Epoch 34: Val Loss 10.87571
Epoch 35: Val Loss 8.62843
Epoch 36: Val Loss 7.34770
Epoch 37: Val Loss 6.51652
Epoch 38: Val Loss 5.80705
Epoch 39: Val Loss 5.13621
Epoch 40: Val Loss 4.52373
Epoch 41: Val Loss 4.01697
Epoch 42: Val Loss 3.60983
Epoch 43: Val Loss 3.27823
Epoch 44: Val Loss 2.99269
Epoch 45: Val Loss 2.72103
Epoch 46: Val Loss 2.45907
Epoch 47: Val Loss 2.21575
Epoch 48: Val Loss 1.98315
Epoch 49: Val Loss 1.77592
Epoch 50: Val Loss 1.59494
Epoch 51: Val Loss 1.44072
Epoch 52: Val Loss 1.31324
Epoch 53: Val Loss 1.20396
Epoch 54: Val Loss 1.11005
Epoch 55: Val Loss 1.02839
Epoch 56: Val Loss 0.96006
Epoch 57: Val Loss 0.90036
Epoch 58: Val Loss 0.84700
Epoch 59: Val Loss 0.80060
Epoch 60: Val Loss 0.75946
Epoch 61: Val Loss 0.72452
Epoch 62: Val Loss 0.69493
Epoch 63: Val Loss 0.66918
Epoch 64: Val Loss 0.64687
Epoch 65: Val Loss 0.62859
Epoch 66: Val Loss 0.61272
Epoch 67: Val Loss 0.59956
Epoch 68: Val Loss 0.58759
Epoch 69: Val Loss 0.57784
Epoch 70: Val Loss 0.56919
Epoch 71: Val Loss 0.56204
Epoch 72: Val Loss 0.55538
Epoch 73: Val Loss 0.54972
Epoch 74: Val Loss 0.54512
Epoch 75: Val Loss 0.54152
Epoch 76: Val Loss 0.53849
Epoch 77: Val Loss 0.53544
Epoch 78: Val Loss 0.53222
Epoch 79: Val Loss 0.52993
Epoch 80: Val Loss 0.52835
Epoch 81: Val Loss 0.52711
Epoch 82: Val Loss 0.52611
Epoch 83: Val Loss 0.52534
Epoch 84: Val Loss 0.52611
Epoch 85: Val Loss 0.52664
Epoch 86: Val Loss 0.52720
Epoch 87: Val Loss 0.52803
Epoch 88: Val Loss 0.52897
Epoch 89: Val Loss 0.52883
Epoch 90: Val Loss 0.52903
Epoch 91: Val Loss 0.52835
Epoch 92: Val Loss 0.52834
Epoch 93: Val Loss 0.52813
Epoch 94: Val Loss 0.52785
Epoch 95: Val Loss 0.52868
Epoch 96: Val Loss 0.52902
Epoch 97: Val Loss 0.53046
Epoch 98: Val Loss 0.53079
Epoch 99: Val Loss 0.53065
{'MSE - mean': 0.5253397543264061, 'MSE - std': 0.0, 'R2 - mean': 0.04405534545236567, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.29408
Epoch 1: Val Loss 226.43843
Epoch 2: Val Loss 225.64761
Epoch 3: Val Loss 224.89241
Epoch 4: Val Loss 224.14352
Epoch 5: Val Loss 223.38345
Epoch 6: Val Loss 222.56773
Epoch 7: Val Loss 221.66284
Epoch 8: Val Loss 220.63086
Epoch 9: Val Loss 219.46181
Epoch 10: Val Loss 218.13986
Epoch 11: Val Loss 216.63475
Epoch 12: Val Loss 214.92967
Epoch 13: Val Loss 212.99977
Epoch 14: Val Loss 210.82539
Epoch 15: Val Loss 208.40260
Epoch 16: Val Loss 205.69466
Epoch 17: Val Loss 202.66055
Epoch 18: Val Loss 199.26825
Epoch 19: Val Loss 195.48582
Epoch 20: Val Loss 191.27702
Epoch 21: Val Loss 186.61201
Epoch 22: Val Loss 181.45390
Epoch 23: Val Loss 175.75232
Epoch 24: Val Loss 169.48006
Epoch 25: Val Loss 162.55974
Epoch 26: Val Loss 155.02800
Epoch 27: Val Loss 146.86632
Epoch 28: Val Loss 138.09012
Epoch 29: Val Loss 128.71040
Epoch 30: Val Loss 118.84647
Epoch 31: Val Loss 108.51531
Epoch 32: Val Loss 97.84570
Epoch 33: Val Loss 86.96005
Epoch 34: Val Loss 76.01826
Epoch 35: Val Loss 65.24309
Epoch 36: Val Loss 54.85952
Epoch 37: Val Loss 45.08140
Epoch 38: Val Loss 36.17553
Epoch 39: Val Loss 28.37989
Epoch 40: Val Loss 21.80295
Epoch 41: Val Loss 16.57526
Epoch 42: Val Loss 12.62162
Epoch 43: Val Loss 9.82860
Epoch 44: Val Loss 7.97248
Epoch 45: Val Loss 6.78005
Epoch 46: Val Loss 6.02379
Epoch 47: Val Loss 5.50371
Epoch 48: Val Loss 5.08056
Epoch 49: Val Loss 4.69066
Epoch 50: Val Loss 4.31538
Epoch 51: Val Loss 3.96556
Epoch 52: Val Loss 3.64968
Epoch 53: Val Loss 3.37451
Epoch 54: Val Loss 3.13179
Epoch 55: Val Loss 2.92328
Epoch 56: Val Loss 2.73740
Epoch 57: Val Loss 2.57252
Epoch 58: Val Loss 2.42046
Epoch 59: Val Loss 2.28385
Epoch 60: Val Loss 2.15730
Epoch 61: Val Loss 2.03976
Epoch 62: Val Loss 1.93029
Epoch 63: Val Loss 1.82979
Epoch 64: Val Loss 1.73952
Epoch 65: Val Loss 1.65694
Epoch 66: Val Loss 1.57990
Epoch 67: Val Loss 1.50783
Epoch 68: Val Loss 1.44308
Epoch 69: Val Loss 1.38490
Epoch 70: Val Loss 1.32974
Epoch 71: Val Loss 1.27847
Epoch 72: Val Loss 1.23133
Epoch 73: Val Loss 1.18848
Epoch 74: Val Loss 1.14918
Epoch 75: Val Loss 1.11260
Epoch 76: Val Loss 1.07858
Epoch 77: Val Loss 1.04661
Epoch 78: Val Loss 1.01542
Epoch 79: Val Loss 0.98698
Epoch 80: Val Loss 0.95983
Epoch 81: Val Loss 0.93393
Epoch 82: Val Loss 0.90903
Epoch 83: Val Loss 0.88685
Epoch 84: Val Loss 0.86662
Epoch 85: Val Loss 0.84751
Epoch 86: Val Loss 0.83029
Epoch 87: Val Loss 0.81401
Epoch 88: Val Loss 0.79828
Epoch 89: Val Loss 0.78296
Epoch 90: Val Loss 0.76874
Epoch 91: Val Loss 0.75539
Epoch 92: Val Loss 0.74183
Epoch 93: Val Loss 0.72981
Epoch 94: Val Loss 0.71934
Epoch 95: Val Loss 0.70988
Epoch 96: Val Loss 0.70070
Epoch 97: Val Loss 0.69109
Epoch 98: Val Loss 0.68288
Epoch 99: Val Loss 0.67587
{'MSE - mean': 0.600604038781469, 'MSE - std': 0.07526428445506284, 'R2 - mean': 0.06440128654949173, 'R2 - std': 0.020345941097126063} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.77394
Epoch 1: Val Loss 226.92810
Epoch 2: Val Loss 226.08008
Epoch 3: Val Loss 225.20267
Epoch 4: Val Loss 224.28128
Epoch 5: Val Loss 223.29286
Epoch 6: Val Loss 222.22835
Epoch 7: Val Loss 221.07553
Epoch 8: Val Loss 219.81235
Epoch 9: Val Loss 218.42149
Epoch 10: Val Loss 216.87869
Epoch 11: Val Loss 215.16939
Epoch 12: Val Loss 213.25253
Epoch 13: Val Loss 211.10678
Epoch 14: Val Loss 208.69614
Epoch 15: Val Loss 205.95377
Epoch 16: Val Loss 202.84729
Epoch 17: Val Loss 199.32600
Epoch 18: Val Loss 195.33107
Epoch 19: Val Loss 190.80919
Epoch 20: Val Loss 185.69373
Epoch 21: Val Loss 179.90791
Epoch 22: Val Loss 173.40858
Epoch 23: Val Loss 166.15797
Epoch 24: Val Loss 158.08191
Epoch 25: Val Loss 149.16647
Epoch 26: Val Loss 139.42630
Epoch 27: Val Loss 128.87482
Epoch 28: Val Loss 117.57989
Epoch 29: Val Loss 105.61642
Epoch 30: Val Loss 93.19530
Epoch 31: Val Loss 80.56278
Epoch 32: Val Loss 68.03168
Epoch 33: Val Loss 55.90205
Epoch 34: Val Loss 44.53656
Epoch 35: Val Loss 34.29631
Epoch 36: Val Loss 25.45692
Epoch 37: Val Loss 18.28438
Epoch 38: Val Loss 12.81258
Epoch 39: Val Loss 9.01358
Epoch 40: Val Loss 6.62755
Epoch 41: Val Loss 5.30539
Epoch 42: Val Loss 4.62872
Epoch 43: Val Loss 4.27270
Epoch 44: Val Loss 4.02827
Epoch 45: Val Loss 3.79749
Epoch 46: Val Loss 3.55460
Epoch 47: Val Loss 3.31821
Epoch 48: Val Loss 3.10091
Epoch 49: Val Loss 2.91099
Epoch 50: Val Loss 2.74866
Epoch 51: Val Loss 2.60811
Epoch 52: Val Loss 2.47919
Epoch 53: Val Loss 2.35647
Epoch 54: Val Loss 2.24032
Epoch 55: Val Loss 2.12784
Epoch 56: Val Loss 2.02033
Epoch 57: Val Loss 1.91762
Epoch 58: Val Loss 1.82452
Epoch 59: Val Loss 1.73737
Epoch 60: Val Loss 1.65852
Epoch 61: Val Loss 1.58484
Epoch 62: Val Loss 1.51777
Epoch 63: Val Loss 1.45623
Epoch 64: Val Loss 1.39877
Epoch 65: Val Loss 1.34555
Epoch 66: Val Loss 1.29593
Epoch 67: Val Loss 1.25049
Epoch 68: Val Loss 1.20858
Epoch 69: Val Loss 1.16974
Epoch 70: Val Loss 1.13335
Epoch 71: Val Loss 1.09911
Epoch 72: Val Loss 1.06832
Epoch 73: Val Loss 1.04011
Epoch 74: Val Loss 1.01418
Epoch 75: Val Loss 0.98957
Epoch 76: Val Loss 0.96641
Epoch 77: Val Loss 0.94549
Epoch 78: Val Loss 0.92630
Epoch 79: Val Loss 0.90811
Epoch 80: Val Loss 0.89186
Epoch 81: Val Loss 0.87628
Epoch 82: Val Loss 0.86156
Epoch 83: Val Loss 0.84826
Epoch 84: Val Loss 0.83609
Epoch 85: Val Loss 0.82434
Epoch 86: Val Loss 0.81340
Epoch 87: Val Loss 0.80358
Epoch 88: Val Loss 0.79415
Epoch 89: Val Loss 0.78544
Epoch 90: Val Loss 0.77716
Epoch 91: Val Loss 0.76937
Epoch 92: Val Loss 0.76192
Epoch 93: Val Loss 0.75511
Epoch 94: Val Loss 0.74937
Epoch 95: Val Loss 0.74398
Epoch 96: Val Loss 0.73898
Epoch 97: Val Loss 0.73410
Epoch 98: Val Loss 0.73042
Epoch 99: Val Loss 0.72646
{'MSE - mean': 0.6425543874239381, 'MSE - std': 0.08541743681554625, 'R2 - mean': 0.02479343501862097, 'R2 - std': 0.05842546834958323} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 228.70496
Epoch 1: Val Loss 227.84579
Epoch 2: Val Loss 227.13264
Epoch 3: Val Loss 226.55115
Epoch 4: Val Loss 226.04721
Epoch 5: Val Loss 225.59836
Epoch 6: Val Loss 225.17056
Epoch 7: Val Loss 224.72449
Epoch 8: Val Loss 224.24585
Epoch 9: Val Loss 223.71327
Epoch 10: Val Loss 223.10498
Epoch 11: Val Loss 222.39822
Epoch 12: Val Loss 221.57256
Epoch 13: Val Loss 220.59622
Epoch 14: Val Loss 219.43803
Epoch 15: Val Loss 218.08350
Epoch 16: Val Loss 216.48352
Epoch 17: Val Loss 214.59958
Epoch 18: Val Loss 212.38857
Epoch 19: Val Loss 209.79503
Epoch 20: Val Loss 206.76474
Epoch 21: Val Loss 203.22488
Epoch 22: Val Loss 199.11993
Epoch 23: Val Loss 194.37733
Epoch 24: Val Loss 188.93985
Epoch 25: Val Loss 182.68712
Epoch 26: Val Loss 175.57285
Epoch 27: Val Loss 167.53114
Epoch 28: Val Loss 158.50122
Epoch 29: Val Loss 148.47646
Epoch 30: Val Loss 137.46454
Epoch 31: Val Loss 125.50441
Epoch 32: Val Loss 112.76533
Epoch 33: Val Loss 99.41969
Epoch 34: Val Loss 85.71267
Epoch 35: Val Loss 72.01070
Epoch 36: Val Loss 58.71675
Epoch 37: Val Loss 46.29831
Epoch 38: Val Loss 35.25101
Epoch 39: Val Loss 25.98429
Epoch 40: Val Loss 18.77747
Epoch 41: Val Loss 13.62389
Epoch 42: Val Loss 10.32260
Epoch 43: Val Loss 8.42961
Epoch 44: Val Loss 7.39038
Epoch 45: Val Loss 6.74569
Epoch 46: Val Loss 6.22757
Epoch 47: Val Loss 5.71874
Epoch 48: Val Loss 5.23370
Epoch 49: Val Loss 4.80289
Epoch 50: Val Loss 4.44735
Epoch 51: Val Loss 4.15469
Epoch 52: Val Loss 3.90927
Epoch 53: Val Loss 3.68952
Epoch 54: Val Loss 3.48178
Epoch 55: Val Loss 3.27117
Epoch 56: Val Loss 3.07730
Epoch 57: Val Loss 2.88947
Epoch 58: Val Loss 2.71161
Epoch 59: Val Loss 2.55434
Epoch 60: Val Loss 2.41074
Epoch 61: Val Loss 2.27644
Epoch 62: Val Loss 2.15794
Epoch 63: Val Loss 2.05001
Epoch 64: Val Loss 1.95037
Epoch 65: Val Loss 1.86121
Epoch 66: Val Loss 1.77646
Epoch 67: Val Loss 1.69810
Epoch 68: Val Loss 1.62457
Epoch 69: Val Loss 1.55565
Epoch 70: Val Loss 1.49337
Epoch 71: Val Loss 1.43405
Epoch 72: Val Loss 1.38005
Epoch 73: Val Loss 1.32796
Epoch 74: Val Loss 1.27762
Epoch 75: Val Loss 1.23218
Epoch 76: Val Loss 1.18806
Epoch 77: Val Loss 1.14756
Epoch 78: Val Loss 1.10961
Epoch 79: Val Loss 1.07461
Epoch 80: Val Loss 1.04361
Epoch 81: Val Loss 1.01390
Epoch 82: Val Loss 0.98534
Epoch 83: Val Loss 0.95863
Epoch 84: Val Loss 0.93413
Epoch 85: Val Loss 0.91270
Epoch 86: Val Loss 0.89077
Epoch 87: Val Loss 0.87110
Epoch 88: Val Loss 0.85301
Epoch 89: Val Loss 0.83674
Epoch 90: Val Loss 0.82042
Epoch 91: Val Loss 0.80639
Epoch 92: Val Loss 0.79293
Epoch 93: Val Loss 0.78109
Epoch 94: Val Loss 0.76837
Epoch 95: Val Loss 0.75748
Epoch 96: Val Loss 0.74701
Epoch 97: Val Loss 0.73673
Epoch 98: Val Loss 0.72715
Epoch 99: Val Loss 0.71909
{'MSE - mean': 0.661687595338554, 'MSE - std': 0.0810576512083538, 'R2 - mean': 0.0393075733137111, 'R2 - std': 0.05649895702796751} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 225.39925
Epoch 1: Val Loss 224.13379
Epoch 2: Val Loss 222.72031
Epoch 3: Val Loss 221.14456
Epoch 4: Val Loss 219.39958
Epoch 5: Val Loss 217.44798
Epoch 6: Val Loss 215.27031
Epoch 7: Val Loss 212.85855
Epoch 8: Val Loss 210.17758
Epoch 9: Val Loss 207.20288
Epoch 10: Val Loss 203.89299
Epoch 11: Val Loss 200.19090
Epoch 12: Val Loss 196.05553
Epoch 13: Val Loss 191.41873
Epoch 14: Val Loss 186.27092
Epoch 15: Val Loss 180.49406
Epoch 16: Val Loss 174.07176
Epoch 17: Val Loss 166.92255
Epoch 18: Val Loss 158.96590
Epoch 19: Val Loss 150.20093
Epoch 20: Val Loss 140.57051
Epoch 21: Val Loss 130.09454
Epoch 22: Val Loss 118.83611
Epoch 23: Val Loss 106.82909
Epoch 24: Val Loss 94.29144
Epoch 25: Val Loss 81.42258
Epoch 26: Val Loss 68.45311
Epoch 27: Val Loss 55.77880
Epoch 28: Val Loss 43.78234
Epoch 29: Val Loss 32.90892
Epoch 30: Val Loss 23.58928
Epoch 31: Val Loss 16.13732
Epoch 32: Val Loss 10.71378
Epoch 33: Val Loss 7.23671
Epoch 34: Val Loss 5.38028
Epoch 35: Val Loss 4.61390
Epoch 36: Val Loss 4.37423
Epoch 37: Val Loss 4.21942
Epoch 38: Val Loss 3.94076
Epoch 39: Val Loss 3.51350
Epoch 40: Val Loss 3.01945
Epoch 41: Val Loss 2.55922
Epoch 42: Val Loss 2.18572
Epoch 43: Val Loss 1.90257
Epoch 44: Val Loss 1.69090
Epoch 45: Val Loss 1.53103
Epoch 46: Val Loss 1.40366
Epoch 47: Val Loss 1.29564
Epoch 48: Val Loss 1.20104
Epoch 49: Val Loss 1.12096
Epoch 50: Val Loss 1.05341
Epoch 51: Val Loss 0.99720
Epoch 52: Val Loss 0.94861
Epoch 53: Val Loss 0.90703
Epoch 54: Val Loss 0.87065
Epoch 55: Val Loss 0.83881
Epoch 56: Val Loss 0.81154
Epoch 57: Val Loss 0.78701
Epoch 58: Val Loss 0.76613
Epoch 59: Val Loss 0.74900
Epoch 60: Val Loss 0.73384
Epoch 61: Val Loss 0.71995
Epoch 62: Val Loss 0.70810
Epoch 63: Val Loss 0.69796
Epoch 64: Val Loss 0.68911
Epoch 65: Val Loss 0.68112
Epoch 66: Val Loss 0.67410
Epoch 67: Val Loss 0.66795
Epoch 68: Val Loss 0.66242
Epoch 69: Val Loss 0.65781
Epoch 70: Val Loss 0.65372
Epoch 71: Val Loss 0.65047
Epoch 72: Val Loss 0.64773
Epoch 73: Val Loss 0.64459
Epoch 74: Val Loss 0.64212
Epoch 75: Val Loss 0.63966
Epoch 76: Val Loss 0.63761
Epoch 77: Val Loss 0.63566
Epoch 78: Val Loss 0.63416
Epoch 79: Val Loss 0.63298
Epoch 80: Val Loss 0.63215
Epoch 81: Val Loss 0.63135
Epoch 82: Val Loss 0.63039
Epoch 83: Val Loss 0.62936
Epoch 84: Val Loss 0.62903
Epoch 85: Val Loss 0.62866
Epoch 86: Val Loss 0.62852
Epoch 87: Val Loss 0.62755
Epoch 88: Val Loss 0.62643
Epoch 89: Val Loss 0.62570
Epoch 90: Val Loss 0.62529
Epoch 91: Val Loss 0.62502
Epoch 92: Val Loss 0.62460
Epoch 93: Val Loss 0.62371
Epoch 94: Val Loss 0.62335
Epoch 95: Val Loss 0.62249
Epoch 96: Val Loss 0.62233
Epoch 97: Val Loss 0.62238
Epoch 98: Val Loss 0.62149
Epoch 99: Val Loss 0.62113
{'MSE - mean': 0.6535769532928556, 'MSE - std': 0.07429269353782997, 'R2 - mean': 0.02528462702640959, 'R2 - std': 0.05779513652985576} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 0.6535769532928556 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0}. Best is trial 12 with value: 0.6535769532928556.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.30112
Epoch 1: Val Loss 221.34409
Epoch 2: Val Loss 220.28651
Epoch 3: Val Loss 219.12146
Epoch 4: Val Loss 217.82257
Epoch 5: Val Loss 216.34990
Epoch 6: Val Loss 214.67479
Epoch 7: Val Loss 212.78307
Epoch 8: Val Loss 210.63902
Epoch 9: Val Loss 208.20100
Epoch 10: Val Loss 205.42313
Epoch 11: Val Loss 202.27078
Epoch 12: Val Loss 198.70601
Epoch 13: Val Loss 194.68370
Epoch 14: Val Loss 190.13181
Epoch 15: Val Loss 185.01183
Epoch 16: Val Loss 179.27771
Epoch 17: Val Loss 172.85745
Epoch 18: Val Loss 165.70706
Epoch 19: Val Loss 157.77907
Epoch 20: Val Loss 149.04451
Epoch 21: Val Loss 139.47774
Epoch 22: Val Loss 129.06783
Epoch 23: Val Loss 117.91360
Epoch 24: Val Loss 106.06550
Epoch 25: Val Loss 93.72511
Epoch 26: Val Loss 81.07893
Epoch 27: Val Loss 68.40479
Epoch 28: Val Loss 56.05643
Epoch 29: Val Loss 44.44748
Epoch 30: Val Loss 33.99254
Epoch 31: Val Loss 25.06545
Epoch 32: Val Loss 17.96255
Epoch 33: Val Loss 12.77228
Epoch 34: Val Loss 9.37373
Epoch 35: Val Loss 7.39801
Epoch 36: Val Loss 6.36374
Epoch 37: Val Loss 5.76014
Epoch 38: Val Loss 5.25553
Epoch 39: Val Loss 4.72906
Epoch 40: Val Loss 4.17580
Epoch 41: Val Loss 3.64742
Epoch 42: Val Loss 3.18353
Epoch 43: Val Loss 2.80475
Epoch 44: Val Loss 2.49919
Epoch 45: Val Loss 2.24608
Epoch 46: Val Loss 2.02626
Epoch 47: Val Loss 1.83174
Epoch 48: Val Loss 1.66012
Epoch 49: Val Loss 1.50622
Epoch 50: Val Loss 1.37049
Epoch 51: Val Loss 1.25121
Epoch 52: Val Loss 1.14963
Epoch 53: Val Loss 1.06219
Epoch 54: Val Loss 0.98695
Epoch 55: Val Loss 0.92153
Epoch 56: Val Loss 0.86282
Epoch 57: Val Loss 0.81183
Epoch 58: Val Loss 0.76743
Epoch 59: Val Loss 0.72773
Epoch 60: Val Loss 0.69377
Epoch 61: Val Loss 0.66399
Epoch 62: Val Loss 0.63914
Epoch 63: Val Loss 0.61784
Epoch 64: Val Loss 0.60038
Epoch 65: Val Loss 0.58549
Epoch 66: Val Loss 0.57203
Epoch 67: Val Loss 0.56009
Epoch 68: Val Loss 0.55048
Epoch 69: Val Loss 0.54134
Epoch 70: Val Loss 0.53461
Epoch 71: Val Loss 0.52742
Epoch 72: Val Loss 0.52259
Epoch 73: Val Loss 0.51751
Epoch 74: Val Loss 0.51401
Epoch 75: Val Loss 0.51185
Epoch 76: Val Loss 0.50943
Epoch 77: Val Loss 0.50821
Epoch 78: Val Loss 0.50599
Epoch 79: Val Loss 0.50431
Epoch 80: Val Loss 0.50193
Epoch 81: Val Loss 0.50084
Epoch 82: Val Loss 0.49949
Epoch 83: Val Loss 0.49937
Epoch 84: Val Loss 0.49914
Epoch 85: Val Loss 0.49895
Epoch 86: Val Loss 0.50012
Epoch 87: Val Loss 0.50138
Epoch 88: Val Loss 0.50277
Epoch 89: Val Loss 0.50485
Epoch 90: Val Loss 0.50595
Epoch 91: Val Loss 0.50552
Epoch 92: Val Loss 0.50463
Epoch 93: Val Loss 0.50368
Epoch 94: Val Loss 0.50340
Epoch 95: Val Loss 0.50450
Epoch 96: Val Loss 0.50380
Epoch 97: Val Loss 0.50537
Epoch 98: Val Loss 0.50504
Epoch 99: Val Loss 0.50465
{'MSE - mean': 0.49894635964929435, 'MSE - std': 0.0, 'R2 - mean': 0.09208259705319266, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.06142
Epoch 1: Val Loss 218.92108
Epoch 2: Val Loss 217.78389
Epoch 3: Val Loss 216.65105
Epoch 4: Val Loss 215.49037
Epoch 5: Val Loss 214.28362
Epoch 6: Val Loss 213.01541
Epoch 7: Val Loss 211.63956
Epoch 8: Val Loss 210.13037
Epoch 9: Val Loss 208.46292
Epoch 10: Val Loss 206.57852
Epoch 11: Val Loss 204.44371
Epoch 12: Val Loss 202.00471
Epoch 13: Val Loss 199.21796
Epoch 14: Val Loss 196.00909
Epoch 15: Val Loss 192.31689
Epoch 16: Val Loss 188.06694
Epoch 17: Val Loss 183.22269
Epoch 18: Val Loss 177.71190
Epoch 19: Val Loss 171.44119
Epoch 20: Val Loss 164.35692
Epoch 21: Val Loss 156.48178
Epoch 22: Val Loss 147.74469
Epoch 23: Val Loss 138.17494
Epoch 24: Val Loss 127.79188
Epoch 25: Val Loss 116.67078
Epoch 26: Val Loss 104.97565
Epoch 27: Val Loss 92.87518
Epoch 28: Val Loss 80.61265
Epoch 29: Val Loss 68.47851
Epoch 30: Val Loss 56.83333
Epoch 31: Val Loss 46.07511
Epoch 32: Val Loss 36.61533
Epoch 33: Val Loss 28.76035
Epoch 34: Val Loss 22.62163
Epoch 35: Val Loss 18.16570
Epoch 36: Val Loss 15.14392
Epoch 37: Val Loss 13.16630
Epoch 38: Val Loss 11.80514
Epoch 39: Val Loss 10.68278
Epoch 40: Val Loss 9.65123
Epoch 41: Val Loss 8.66481
Epoch 42: Val Loss 7.77061
Epoch 43: Val Loss 6.99680
Epoch 44: Val Loss 6.33751
Epoch 45: Val Loss 5.75698
Epoch 46: Val Loss 5.25596
Epoch 47: Val Loss 4.79786
Epoch 48: Val Loss 4.37382
Epoch 49: Val Loss 3.97802
Epoch 50: Val Loss 3.61125
Epoch 51: Val Loss 3.28047
Epoch 52: Val Loss 2.98236
Epoch 53: Val Loss 2.71719
Epoch 54: Val Loss 2.48503
Epoch 55: Val Loss 2.28117
Epoch 56: Val Loss 2.09814
Epoch 57: Val Loss 1.93714
Epoch 58: Val Loss 1.79202
Epoch 59: Val Loss 1.66363
Epoch 60: Val Loss 1.55163
Epoch 61: Val Loss 1.45129
Epoch 62: Val Loss 1.35864
Epoch 63: Val Loss 1.27739
Epoch 64: Val Loss 1.20607
Epoch 65: Val Loss 1.14074
Epoch 66: Val Loss 1.08228
Epoch 67: Val Loss 1.03244
Epoch 68: Val Loss 0.98834
Epoch 69: Val Loss 0.94904
Epoch 70: Val Loss 0.91450
Epoch 71: Val Loss 0.88321
Epoch 72: Val Loss 0.85412
Epoch 73: Val Loss 0.83019
Epoch 74: Val Loss 0.81008
Epoch 75: Val Loss 0.79308
Epoch 76: Val Loss 0.77831
Epoch 77: Val Loss 0.76524
Epoch 78: Val Loss 0.75313
Epoch 79: Val Loss 0.74181
Epoch 80: Val Loss 0.73185
Epoch 81: Val Loss 0.72281
Epoch 82: Val Loss 0.71468
Epoch 83: Val Loss 0.70729
Epoch 84: Val Loss 0.70056
Epoch 85: Val Loss 0.69574
Epoch 86: Val Loss 0.68975
Epoch 87: Val Loss 0.68499
Epoch 88: Val Loss 0.68074
Epoch 89: Val Loss 0.67711
Epoch 90: Val Loss 0.67331
Epoch 91: Val Loss 0.67036
Epoch 92: Val Loss 0.66788
Epoch 93: Val Loss 0.66586
Epoch 94: Val Loss 0.66497
Epoch 95: Val Loss 0.66334
Epoch 96: Val Loss 0.66269
Epoch 97: Val Loss 0.66129
Epoch 98: Val Loss 0.66044
Epoch 99: Val Loss 0.65862
{'MSE - mean': 0.5787855651331368, 'MSE - std': 0.07983920548384249, 'R2 - mean': 0.10009041836022575, 'R2 - std': 0.008007821307033092} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.40778
Epoch 1: Val Loss 221.73317
Epoch 2: Val Loss 221.11115
Epoch 3: Val Loss 220.51978
Epoch 4: Val Loss 219.96214
Epoch 5: Val Loss 219.40213
Epoch 6: Val Loss 218.83786
Epoch 7: Val Loss 218.23871
Epoch 8: Val Loss 217.58423
Epoch 9: Val Loss 216.82268
Epoch 10: Val Loss 215.93027
Epoch 11: Val Loss 214.88887
Epoch 12: Val Loss 213.68997
Epoch 13: Val Loss 212.30638
Epoch 14: Val Loss 210.71841
Epoch 15: Val Loss 208.90936
Epoch 16: Val Loss 206.84462
Epoch 17: Val Loss 204.50403
Epoch 18: Val Loss 201.83661
Epoch 19: Val Loss 198.80644
Epoch 20: Val Loss 195.36990
Epoch 21: Val Loss 191.48100
Epoch 22: Val Loss 187.08627
Epoch 23: Val Loss 182.12791
Epoch 24: Val Loss 176.52798
Epoch 25: Val Loss 170.24889
Epoch 26: Val Loss 163.23335
Epoch 27: Val Loss 155.46916
Epoch 28: Val Loss 146.92131
Epoch 29: Val Loss 137.60666
Epoch 30: Val Loss 127.49426
Epoch 31: Val Loss 116.70211
Epoch 32: Val Loss 105.29974
Epoch 33: Val Loss 93.46206
Epoch 34: Val Loss 81.36534
Epoch 35: Val Loss 69.30321
Epoch 36: Val Loss 57.52457
Epoch 37: Val Loss 46.39268
Epoch 38: Val Loss 36.22765
Epoch 39: Val Loss 27.38273
Epoch 40: Val Loss 20.06645
Epoch 41: Val Loss 14.43993
Epoch 42: Val Loss 10.47668
Epoch 43: Val Loss 7.93611
Epoch 44: Val Loss 6.49847
Epoch 45: Val Loss 5.73339
Epoch 46: Val Loss 5.29919
Epoch 47: Val Loss 4.95642
Epoch 48: Val Loss 4.61162
Epoch 49: Val Loss 4.24602
Epoch 50: Val Loss 3.88919
Epoch 51: Val Loss 3.55218
Epoch 52: Val Loss 3.26522
Epoch 53: Val Loss 3.01912
Epoch 54: Val Loss 2.81021
Epoch 55: Val Loss 2.62677
Epoch 56: Val Loss 2.46418
Epoch 57: Val Loss 2.31413
Epoch 58: Val Loss 2.17439
Epoch 59: Val Loss 2.04854
Epoch 60: Val Loss 1.93243
Epoch 61: Val Loss 1.82670
Epoch 62: Val Loss 1.73204
Epoch 63: Val Loss 1.64597
Epoch 64: Val Loss 1.56613
Epoch 65: Val Loss 1.49269
Epoch 66: Val Loss 1.42421
Epoch 67: Val Loss 1.36128
Epoch 68: Val Loss 1.30317
Epoch 69: Val Loss 1.25060
Epoch 70: Val Loss 1.20186
Epoch 71: Val Loss 1.15701
Epoch 72: Val Loss 1.11553
Epoch 73: Val Loss 1.07704
Epoch 74: Val Loss 1.04237
Epoch 75: Val Loss 1.01078
Epoch 76: Val Loss 0.98160
Epoch 77: Val Loss 0.95372
Epoch 78: Val Loss 0.92870
Epoch 79: Val Loss 0.90650
Epoch 80: Val Loss 0.88503
Epoch 81: Val Loss 0.86550
Epoch 82: Val Loss 0.84754
Epoch 83: Val Loss 0.83148
Epoch 84: Val Loss 0.81644
Epoch 85: Val Loss 0.80243
Epoch 86: Val Loss 0.78970
Epoch 87: Val Loss 0.77780
Epoch 88: Val Loss 0.76694
Epoch 89: Val Loss 0.75703
Epoch 90: Val Loss 0.74779
Epoch 91: Val Loss 0.73955
Epoch 92: Val Loss 0.73171
Epoch 93: Val Loss 0.72447
Epoch 94: Val Loss 0.71782
Epoch 95: Val Loss 0.71182
Epoch 96: Val Loss 0.70664
Epoch 97: Val Loss 0.70196
Epoch 98: Val Loss 0.69808
Epoch 99: Val Loss 0.69461
{'MSE - mean': 0.6173938312783495, 'MSE - std': 0.08503369283803504, 'R2 - mean': 0.06399332841578768, 'R2 - std': 0.05146600761315459} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.42285
Epoch 1: Val Loss 230.63600
Epoch 2: Val Loss 229.78915
Epoch 3: Val Loss 228.87981
Epoch 4: Val Loss 227.87927
Epoch 5: Val Loss 226.78680
Epoch 6: Val Loss 225.56589
Epoch 7: Val Loss 224.17439
Epoch 8: Val Loss 222.60693
Epoch 9: Val Loss 220.83803
Epoch 10: Val Loss 218.84409
Epoch 11: Val Loss 216.60506
Epoch 12: Val Loss 214.04677
Epoch 13: Val Loss 211.13461
Epoch 14: Val Loss 207.81212
Epoch 15: Val Loss 204.02287
Epoch 16: Val Loss 199.69205
Epoch 17: Val Loss 194.74704
Epoch 18: Val Loss 189.07173
Epoch 19: Val Loss 182.60162
Epoch 20: Val Loss 175.24863
Epoch 21: Val Loss 166.93100
Epoch 22: Val Loss 157.63315
Epoch 23: Val Loss 147.35135
Epoch 24: Val Loss 136.12120
Epoch 25: Val Loss 123.95101
Epoch 26: Val Loss 110.97975
Epoch 27: Val Loss 97.39582
Epoch 28: Val Loss 83.42960
Epoch 29: Val Loss 69.47128
Epoch 30: Val Loss 55.87745
Epoch 31: Val Loss 43.16112
Epoch 32: Val Loss 31.79701
Epoch 33: Val Loss 22.27987
Epoch 34: Val Loss 14.83000
Epoch 35: Val Loss 9.61747
Epoch 36: Val Loss 6.40083
Epoch 37: Val Loss 4.75005
Epoch 38: Val Loss 4.08466
Epoch 39: Val Loss 3.83162
Epoch 40: Val Loss 3.62376
Epoch 41: Val Loss 3.33832
Epoch 42: Val Loss 2.98989
Epoch 43: Val Loss 2.64650
Epoch 44: Val Loss 2.34865
Epoch 45: Val Loss 2.12230
Epoch 46: Val Loss 1.95144
Epoch 47: Val Loss 1.81580
Epoch 48: Val Loss 1.69625
Epoch 49: Val Loss 1.58247
Epoch 50: Val Loss 1.47684
Epoch 51: Val Loss 1.37456
Epoch 52: Val Loss 1.28128
Epoch 53: Val Loss 1.19981
Epoch 54: Val Loss 1.12907
Epoch 55: Val Loss 1.06910
Epoch 56: Val Loss 1.01864
Epoch 57: Val Loss 0.97506
Epoch 58: Val Loss 0.93621
Epoch 59: Val Loss 0.90246
Epoch 60: Val Loss 0.87246
Epoch 61: Val Loss 0.84624
Epoch 62: Val Loss 0.82371
Epoch 63: Val Loss 0.80346
Epoch 64: Val Loss 0.78568
Epoch 65: Val Loss 0.76994
Epoch 66: Val Loss 0.75490
Epoch 67: Val Loss 0.74260
Epoch 68: Val Loss 0.73045
Epoch 69: Val Loss 0.71967
Epoch 70: Val Loss 0.71033
Epoch 71: Val Loss 0.70207
Epoch 72: Val Loss 0.69355
Epoch 73: Val Loss 0.68697
Epoch 74: Val Loss 0.68086
Epoch 75: Val Loss 0.67628
Epoch 76: Val Loss 0.67160
Epoch 77: Val Loss 0.66702
Epoch 78: Val Loss 0.66313
Epoch 79: Val Loss 0.65913
Epoch 80: Val Loss 0.65592
Epoch 81: Val Loss 0.65263
Epoch 82: Val Loss 0.65037
Epoch 83: Val Loss 0.64845
Epoch 84: Val Loss 0.64619
Epoch 85: Val Loss 0.64440
Epoch 86: Val Loss 0.64207
Epoch 87: Val Loss 0.64040
Epoch 88: Val Loss 0.63887
Epoch 89: Val Loss 0.63632
Epoch 90: Val Loss 0.63435
Epoch 91: Val Loss 0.63247
Epoch 92: Val Loss 0.63187
Epoch 93: Val Loss 0.63092
Epoch 94: Val Loss 0.63053
Epoch 95: Val Loss 0.62973
Epoch 96: Val Loss 0.62902
Epoch 97: Val Loss 0.62809
Epoch 98: Val Loss 0.62697
Epoch 99: Val Loss 0.62566
{'MSE - mean': 0.6194605932815392, 'MSE - std': 0.07372829310236645, 'R2 - mean': 0.09849733191255175, 'R2 - std': 0.07455294244120808} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 243.28505
Epoch 1: Val Loss 242.37872
Epoch 2: Val Loss 241.55054
Epoch 3: Val Loss 240.79279
Epoch 4: Val Loss 240.07445
Epoch 5: Val Loss 239.38786
Epoch 6: Val Loss 238.71565
Epoch 7: Val Loss 238.02997
Epoch 8: Val Loss 237.30882
Epoch 9: Val Loss 236.50912
Epoch 10: Val Loss 235.60448
Epoch 11: Val Loss 234.58012
Epoch 12: Val Loss 233.38086
Epoch 13: Val Loss 231.95831
Epoch 14: Val Loss 230.29340
Epoch 15: Val Loss 228.36032
Epoch 16: Val Loss 226.09264
Epoch 17: Val Loss 223.43593
Epoch 18: Val Loss 220.34428
Epoch 19: Val Loss 216.74411
Epoch 20: Val Loss 212.56129
Epoch 21: Val Loss 207.70438
Epoch 22: Val Loss 202.11281
Epoch 23: Val Loss 195.68127
Epoch 24: Val Loss 188.34708
Epoch 25: Val Loss 180.04259
Epoch 26: Val Loss 170.72893
Epoch 27: Val Loss 160.35863
Epoch 28: Val Loss 148.98654
Epoch 29: Val Loss 136.63376
Epoch 30: Val Loss 123.40343
Epoch 31: Val Loss 109.47144
Epoch 32: Val Loss 95.09125
Epoch 33: Val Loss 80.53426
Epoch 34: Val Loss 66.20599
Epoch 35: Val Loss 52.57824
Epoch 36: Val Loss 40.12807
Epoch 37: Val Loss 29.33584
Epoch 38: Val Loss 20.57716
Epoch 39: Val Loss 14.05449
Epoch 40: Val Loss 9.75825
Epoch 41: Val Loss 7.28641
Epoch 42: Val Loss 6.09015
Epoch 43: Val Loss 5.56494
Epoch 44: Val Loss 5.24502
Epoch 45: Val Loss 4.89396
Epoch 46: Val Loss 4.47191
Epoch 47: Val Loss 4.02828
Epoch 48: Val Loss 3.61904
Epoch 49: Val Loss 3.28966
Epoch 50: Val Loss 3.02866
Epoch 51: Val Loss 2.81583
Epoch 52: Val Loss 2.63217
Epoch 53: Val Loss 2.46461
Epoch 54: Val Loss 2.30737
Epoch 55: Val Loss 2.15298
Epoch 56: Val Loss 2.00821
Epoch 57: Val Loss 1.87623
Epoch 58: Val Loss 1.75553
Epoch 59: Val Loss 1.64686
Epoch 60: Val Loss 1.55084
Epoch 61: Val Loss 1.46311
Epoch 62: Val Loss 1.38406
Epoch 63: Val Loss 1.31036
Epoch 64: Val Loss 1.24265
Epoch 65: Val Loss 1.18177
Epoch 66: Val Loss 1.12572
Epoch 67: Val Loss 1.07530
Epoch 68: Val Loss 1.02941
Epoch 69: Val Loss 0.98749
Epoch 70: Val Loss 0.94919
Epoch 71: Val Loss 0.91255
Epoch 72: Val Loss 0.87926
Epoch 73: Val Loss 0.84900
Epoch 74: Val Loss 0.82065
Epoch 75: Val Loss 0.79476
Epoch 76: Val Loss 0.77183
Epoch 77: Val Loss 0.75063
Epoch 78: Val Loss 0.73190
Epoch 79: Val Loss 0.71473
Epoch 80: Val Loss 0.69961
Epoch 81: Val Loss 0.68550
Epoch 82: Val Loss 0.67273
Epoch 83: Val Loss 0.66178
Epoch 84: Val Loss 0.65169
Epoch 85: Val Loss 0.64179
Epoch 86: Val Loss 0.63283
Epoch 87: Val Loss 0.62518
Epoch 88: Val Loss 0.61802
Epoch 89: Val Loss 0.61222
Epoch 90: Val Loss 0.60697
Epoch 91: Val Loss 0.60201
Epoch 92: Val Loss 0.59761
Epoch 93: Val Loss 0.59390
Epoch 94: Val Loss 0.59104
Epoch 95: Val Loss 0.58857
Epoch 96: Val Loss 0.58708
Epoch 97: Val Loss 0.58508
Epoch 98: Val Loss 0.58359
Epoch 99: Val Loss 0.58162
{'MSE - mean': 0.6118930330337913, 'MSE - std': 0.06765915189716726, 'R2 - mean': 0.08575077242510931, 'R2 - std': 0.0713891595158628} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 0.6118930330337913 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0}. Best is trial 13 with value: 0.6118930330337913.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 225.33694
Epoch 1: Val Loss 224.34621
Epoch 2: Val Loss 223.28415
Epoch 3: Val Loss 222.12227
Epoch 4: Val Loss 220.83495
Epoch 5: Val Loss 219.41711
Epoch 6: Val Loss 217.81865
Epoch 7: Val Loss 216.04384
Epoch 8: Val Loss 214.06546
Epoch 9: Val Loss 211.83392
Epoch 10: Val Loss 209.32962
Epoch 11: Val Loss 206.49483
Epoch 12: Val Loss 203.28827
Epoch 13: Val Loss 199.66905
Epoch 14: Val Loss 195.59174
Epoch 15: Val Loss 190.98845
Epoch 16: Val Loss 185.80139
Epoch 17: Val Loss 179.99191
Epoch 18: Val Loss 173.46474
Epoch 19: Val Loss 166.20895
Epoch 20: Val Loss 158.19716
Epoch 21: Val Loss 149.41565
Epoch 22: Val Loss 139.85602
Epoch 23: Val Loss 129.57574
Epoch 24: Val Loss 118.65465
Epoch 25: Val Loss 107.14001
Epoch 26: Val Loss 95.18414
Epoch 27: Val Loss 82.99999
Epoch 28: Val Loss 70.80130
Epoch 29: Val Loss 58.84653
Epoch 30: Val Loss 47.45260
Epoch 31: Val Loss 36.96655
Epoch 32: Val Loss 27.68992
Epoch 33: Val Loss 19.90016
Epoch 34: Val Loss 13.79050
Epoch 35: Val Loss 9.36651
Epoch 36: Val Loss 6.48189
Epoch 37: Val Loss 4.82112
Epoch 38: Val Loss 3.98760
Epoch 39: Val Loss 3.59010
Epoch 40: Val Loss 3.35425
Epoch 41: Val Loss 3.13076
Epoch 42: Val Loss 2.87560
Epoch 43: Val Loss 2.61282
Epoch 44: Val Loss 2.35902
Epoch 45: Val Loss 2.13934
Epoch 46: Val Loss 1.95886
Epoch 47: Val Loss 1.81163
Epoch 48: Val Loss 1.68703
Epoch 49: Val Loss 1.57707
Epoch 50: Val Loss 1.48071
Epoch 51: Val Loss 1.39015
Epoch 52: Val Loss 1.30992
Epoch 53: Val Loss 1.23845
Epoch 54: Val Loss 1.17546
Epoch 55: Val Loss 1.11884
Epoch 56: Val Loss 1.06883
Epoch 57: Val Loss 1.02417
Epoch 58: Val Loss 0.98293
Epoch 59: Val Loss 0.94619
Epoch 60: Val Loss 0.91273
Epoch 61: Val Loss 0.88258
Epoch 62: Val Loss 0.85413
Epoch 63: Val Loss 0.82786
Epoch 64: Val Loss 0.80406
Epoch 65: Val Loss 0.78199
Epoch 66: Val Loss 0.76195
Epoch 67: Val Loss 0.74425
Epoch 68: Val Loss 0.72797
Epoch 69: Val Loss 0.71318
Epoch 70: Val Loss 0.69917
Epoch 71: Val Loss 0.68580
Epoch 72: Val Loss 0.67376
Epoch 73: Val Loss 0.66323
Epoch 74: Val Loss 0.65426
Epoch 75: Val Loss 0.64470
Epoch 76: Val Loss 0.63707
Epoch 77: Val Loss 0.62931
Epoch 78: Val Loss 0.62132
Epoch 79: Val Loss 0.61389
Epoch 80: Val Loss 0.60731
Epoch 81: Val Loss 0.60091
Epoch 82: Val Loss 0.59504
Epoch 83: Val Loss 0.58995
Epoch 84: Val Loss 0.58463
Epoch 85: Val Loss 0.57961
Epoch 86: Val Loss 0.57526
Epoch 87: Val Loss 0.57111
Epoch 88: Val Loss 0.56705
Epoch 89: Val Loss 0.56372
Epoch 90: Val Loss 0.55979
Epoch 91: Val Loss 0.55720
Epoch 92: Val Loss 0.55371
Epoch 93: Val Loss 0.55054
Epoch 94: Val Loss 0.54907
Epoch 95: Val Loss 0.54721
Epoch 96: Val Loss 0.54631
Epoch 97: Val Loss 0.54441
Epoch 98: Val Loss 0.54357
Epoch 99: Val Loss 0.54180
{'MSE - mean': 0.5417995064203203, 'MSE - std': 0.0, 'R2 - mean': 0.01410403889356271, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 238.41263
Epoch 1: Val Loss 237.56725
Epoch 2: Val Loss 236.66112
Epoch 3: Val Loss 235.66826
Epoch 4: Val Loss 234.57617
Epoch 5: Val Loss 233.38795
Epoch 6: Val Loss 232.07465
Epoch 7: Val Loss 230.59874
Epoch 8: Val Loss 228.93521
Epoch 9: Val Loss 227.05309
Epoch 10: Val Loss 224.93036
Epoch 11: Val Loss 222.53468
Epoch 12: Val Loss 219.83636
Epoch 13: Val Loss 216.80084
Epoch 14: Val Loss 213.39667
Epoch 15: Val Loss 209.57341
Epoch 16: Val Loss 205.27827
Epoch 17: Val Loss 200.45978
Epoch 18: Val Loss 195.06850
Epoch 19: Val Loss 189.06847
Epoch 20: Val Loss 182.37553
Epoch 21: Val Loss 174.96382
Epoch 22: Val Loss 166.78355
Epoch 23: Val Loss 157.82925
Epoch 24: Val Loss 148.11044
Epoch 25: Val Loss 137.62637
Epoch 26: Val Loss 126.45095
Epoch 27: Val Loss 114.65736
Epoch 28: Val Loss 102.36810
Epoch 29: Val Loss 89.79494
Epoch 30: Val Loss 77.14315
Epoch 31: Val Loss 64.72160
Epoch 32: Val Loss 52.93520
Epoch 33: Val Loss 42.12115
Epoch 34: Val Loss 32.65784
Epoch 35: Val Loss 24.80912
Epoch 36: Val Loss 18.74530
Epoch 37: Val Loss 14.47281
Epoch 38: Val Loss 11.70249
Epoch 39: Val Loss 10.08596
Epoch 40: Val Loss 9.14190
Epoch 41: Val Loss 8.49763
Epoch 42: Val Loss 7.90520
Epoch 43: Val Loss 7.27918
Epoch 44: Val Loss 6.63188
Epoch 45: Val Loss 6.02214
Epoch 46: Val Loss 5.48980
Epoch 47: Val Loss 5.02777
Epoch 48: Val Loss 4.63655
Epoch 49: Val Loss 4.29742
Epoch 50: Val Loss 3.99392
Epoch 51: Val Loss 3.71645
Epoch 52: Val Loss 3.45959
Epoch 53: Val Loss 3.21981
Epoch 54: Val Loss 2.99397
Epoch 55: Val Loss 2.78482
Epoch 56: Val Loss 2.59617
Epoch 57: Val Loss 2.42839
Epoch 58: Val Loss 2.27600
Epoch 59: Val Loss 2.13777
Epoch 60: Val Loss 2.01411
Epoch 61: Val Loss 1.89980
Epoch 62: Val Loss 1.79550
Epoch 63: Val Loss 1.69839
Epoch 64: Val Loss 1.60960
Epoch 65: Val Loss 1.52689
Epoch 66: Val Loss 1.45250
Epoch 67: Val Loss 1.38373
Epoch 68: Val Loss 1.32111
Epoch 69: Val Loss 1.26326
Epoch 70: Val Loss 1.21119
Epoch 71: Val Loss 1.16214
Epoch 72: Val Loss 1.11895
Epoch 73: Val Loss 1.07799
Epoch 74: Val Loss 1.04148
Epoch 75: Val Loss 1.00713
Epoch 76: Val Loss 0.97601
Epoch 77: Val Loss 0.94765
Epoch 78: Val Loss 0.92284
Epoch 79: Val Loss 0.89890
Epoch 80: Val Loss 0.87644
Epoch 81: Val Loss 0.85637
Epoch 82: Val Loss 0.83745
Epoch 83: Val Loss 0.82063
Epoch 84: Val Loss 0.80417
Epoch 85: Val Loss 0.78885
Epoch 86: Val Loss 0.77504
Epoch 87: Val Loss 0.76226
Epoch 88: Val Loss 0.75085
Epoch 89: Val Loss 0.73990
Epoch 90: Val Loss 0.73093
Epoch 91: Val Loss 0.72189
Epoch 92: Val Loss 0.71294
Epoch 93: Val Loss 0.70361
Epoch 94: Val Loss 0.69717
Epoch 95: Val Loss 0.69102
Epoch 96: Val Loss 0.68584
Epoch 97: Val Loss 0.68010
Epoch 98: Val Loss 0.67511
Epoch 99: Val Loss 0.66956
{'MSE - mean': 0.6056818551887281, 'MSE - std': 0.06388234876840782, 'R2 - mean': 0.05369411460687146, 'R2 - std': 0.03959007571330875} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.53691
Epoch 1: Val Loss 219.62027
Epoch 2: Val Loss 218.67500
Epoch 3: Val Loss 217.68713
Epoch 4: Val Loss 216.63707
Epoch 5: Val Loss 215.50322
Epoch 6: Val Loss 214.24744
Epoch 7: Val Loss 212.84561
Epoch 8: Val Loss 211.28146
Epoch 9: Val Loss 209.50848
Epoch 10: Val Loss 207.49438
Epoch 11: Val Loss 205.21011
Epoch 12: Val Loss 202.61267
Epoch 13: Val Loss 199.67554
Epoch 14: Val Loss 196.32001
Epoch 15: Val Loss 192.47812
Epoch 16: Val Loss 188.08015
Epoch 17: Val Loss 183.04453
Epoch 18: Val Loss 177.29280
Epoch 19: Val Loss 170.77174
Epoch 20: Val Loss 163.45880
Epoch 21: Val Loss 155.23134
Epoch 22: Val Loss 146.13708
Epoch 23: Val Loss 136.11768
Epoch 24: Val Loss 125.23564
Epoch 25: Val Loss 113.60088
Epoch 26: Val Loss 101.29890
Epoch 27: Val Loss 88.51826
Epoch 28: Val Loss 75.55275
Epoch 29: Val Loss 62.69118
Epoch 30: Val Loss 50.30958
Epoch 31: Val Loss 38.79980
Epoch 32: Val Loss 28.60143
Epoch 33: Val Loss 20.01104
Epoch 34: Val Loss 13.25208
Epoch 35: Val Loss 8.44384
Epoch 36: Val Loss 5.42889
Epoch 37: Val Loss 3.82975
Epoch 38: Val Loss 3.17203
Epoch 39: Val Loss 2.96443
Epoch 40: Val Loss 2.85190
Epoch 41: Val Loss 2.67062
Epoch 42: Val Loss 2.41373
Epoch 43: Val Loss 2.12490
Epoch 44: Val Loss 1.86617
Epoch 45: Val Loss 1.65710
Epoch 46: Val Loss 1.49700
Epoch 47: Val Loss 1.37291
Epoch 48: Val Loss 1.27328
Epoch 49: Val Loss 1.18751
Epoch 50: Val Loss 1.11174
Epoch 51: Val Loss 1.04351
Epoch 52: Val Loss 0.98409
Epoch 53: Val Loss 0.93263
Epoch 54: Val Loss 0.88832
Epoch 55: Val Loss 0.85129
Epoch 56: Val Loss 0.81907
Epoch 57: Val Loss 0.79188
Epoch 58: Val Loss 0.76863
Epoch 59: Val Loss 0.74820
Epoch 60: Val Loss 0.73097
Epoch 61: Val Loss 0.71642
Epoch 62: Val Loss 0.70368
Epoch 63: Val Loss 0.69330
Epoch 64: Val Loss 0.68491
Epoch 65: Val Loss 0.67761
Epoch 66: Val Loss 0.67199
Epoch 67: Val Loss 0.66759
Epoch 68: Val Loss 0.66469
Epoch 69: Val Loss 0.66229
Epoch 70: Val Loss 0.66042
Epoch 71: Val Loss 0.65901
Epoch 72: Val Loss 0.65794
Epoch 73: Val Loss 0.65673
Epoch 74: Val Loss 0.65643
Epoch 75: Val Loss 0.65536
Epoch 76: Val Loss 0.65549
Epoch 77: Val Loss 0.65661
Epoch 78: Val Loss 0.65750
Epoch 79: Val Loss 0.65816
Epoch 80: Val Loss 0.65812
Epoch 81: Val Loss 0.65924
Epoch 82: Val Loss 0.66023
Epoch 83: Val Loss 0.66082
Epoch 84: Val Loss 0.66140
Epoch 85: Val Loss 0.66221
Epoch 86: Val Loss 0.66328
Epoch 87: Val Loss 0.66367
Epoch 88: Val Loss 0.66410
Epoch 89: Val Loss 0.66488
Epoch 90: Val Loss 0.66575
Epoch 91: Val Loss 0.66596
Epoch 92: Val Loss 0.66592
Epoch 93: Val Loss 0.66547
Epoch 94: Val Loss 0.66580
Epoch 95: Val Loss 0.66606
Epoch 96: Val Loss 0.66609
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 0.6222423970495545, 'MSE - std': 0.05717638862526532, 'R2 - mean': 0.052050917971127965, 'R2 - std': 0.03240858302504318} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 237.11246
Epoch 1: Val Loss 235.95496
Epoch 2: Val Loss 234.76372
Epoch 3: Val Loss 233.49931
Epoch 4: Val Loss 232.16025
Epoch 5: Val Loss 230.74284
Epoch 6: Val Loss 229.20923
Epoch 7: Val Loss 227.55019
Epoch 8: Val Loss 225.72784
Epoch 9: Val Loss 223.70004
Epoch 10: Val Loss 221.46574
Epoch 11: Val Loss 218.98758
Epoch 12: Val Loss 216.23256
Epoch 13: Val Loss 213.17465
Epoch 14: Val Loss 209.72382
Epoch 15: Val Loss 205.83589
Epoch 16: Val Loss 201.47411
Epoch 17: Val Loss 196.57971
Epoch 18: Val Loss 191.08315
Epoch 19: Val Loss 184.91235
Epoch 20: Val Loss 177.94733
Epoch 21: Val Loss 170.13344
Epoch 22: Val Loss 161.46332
Epoch 23: Val Loss 151.90695
Epoch 24: Val Loss 141.47774
Epoch 25: Val Loss 130.20247
Epoch 26: Val Loss 118.11117
Epoch 27: Val Loss 105.38542
Epoch 28: Val Loss 92.23800
Epoch 29: Val Loss 78.88658
Epoch 30: Val Loss 65.61905
Epoch 31: Val Loss 52.88031
Epoch 32: Val Loss 41.17624
Epoch 33: Val Loss 30.94402
Epoch 34: Val Loss 22.54705
Epoch 35: Val Loss 16.18111
Epoch 36: Val Loss 11.80058
Epoch 37: Val Loss 9.06028
Epoch 38: Val Loss 7.49545
Epoch 39: Val Loss 6.58645
Epoch 40: Val Loss 5.93044
Epoch 41: Val Loss 5.33123
Epoch 42: Val Loss 4.73898
Epoch 43: Val Loss 4.18754
Epoch 44: Val Loss 3.71995
Epoch 45: Val Loss 3.33617
Epoch 46: Val Loss 3.02721
Epoch 47: Val Loss 2.77368
Epoch 48: Val Loss 2.55508
Epoch 49: Val Loss 2.35043
Epoch 50: Val Loss 2.16217
Epoch 51: Val Loss 1.98706
Epoch 52: Val Loss 1.83461
Epoch 53: Val Loss 1.70098
Epoch 54: Val Loss 1.58288
Epoch 55: Val Loss 1.48133
Epoch 56: Val Loss 1.39122
Epoch 57: Val Loss 1.31277
Epoch 58: Val Loss 1.24540
Epoch 59: Val Loss 1.18498
Epoch 60: Val Loss 1.13235
Epoch 61: Val Loss 1.08612
Epoch 62: Val Loss 1.04388
Epoch 63: Val Loss 1.00779
Epoch 64: Val Loss 0.97478
Epoch 65: Val Loss 0.94590
Epoch 66: Val Loss 0.91967
Epoch 67: Val Loss 0.89497
Epoch 68: Val Loss 0.87362
Epoch 69: Val Loss 0.85398
Epoch 70: Val Loss 0.83681
Epoch 71: Val Loss 0.82127
Epoch 72: Val Loss 0.80810
Epoch 73: Val Loss 0.79445
Epoch 74: Val Loss 0.78264
Epoch 75: Val Loss 0.77200
Epoch 76: Val Loss 0.76235
Epoch 77: Val Loss 0.75406
Epoch 78: Val Loss 0.74655
Epoch 79: Val Loss 0.73968
Epoch 80: Val Loss 0.73246
Epoch 81: Val Loss 0.72603
Epoch 82: Val Loss 0.71946
Epoch 83: Val Loss 0.71460
Epoch 84: Val Loss 0.71001
Epoch 85: Val Loss 0.70559
Epoch 86: Val Loss 0.70189
Epoch 87: Val Loss 0.69755
Epoch 88: Val Loss 0.69330
Epoch 89: Val Loss 0.68944
Epoch 90: Val Loss 0.68598
Epoch 91: Val Loss 0.68193
Epoch 92: Val Loss 0.67940
Epoch 93: Val Loss 0.67722
Epoch 94: Val Loss 0.67513
Epoch 95: Val Loss 0.67229
Epoch 96: Val Loss 0.67009
Epoch 97: Val Loss 0.66741
Epoch 98: Val Loss 0.66539
Epoch 99: Val Loss 0.66371
{'MSE - mean': 0.6326091994125137, 'MSE - std': 0.0526713046721694, 'R2 - mean': 0.0774083411936296, 'R2 - std': 0.05212229779780947} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 221.70357
Epoch 1: Val Loss 220.94373
Epoch 2: Val Loss 220.09335
Epoch 3: Val Loss 219.12341
Epoch 4: Val Loss 218.01086
Epoch 5: Val Loss 216.75276
Epoch 6: Val Loss 215.31187
Epoch 7: Val Loss 213.65399
Epoch 8: Val Loss 211.74774
Epoch 9: Val Loss 209.54865
Epoch 10: Val Loss 207.03960
Epoch 11: Val Loss 204.13759
Epoch 12: Val Loss 200.78658
Epoch 13: Val Loss 196.92607
Epoch 14: Val Loss 192.48796
Epoch 15: Val Loss 187.43536
Epoch 16: Val Loss 181.68994
Epoch 17: Val Loss 175.17842
Epoch 18: Val Loss 167.75356
Epoch 19: Val Loss 159.40251
Epoch 20: Val Loss 150.12271
Epoch 21: Val Loss 139.91933
Epoch 22: Val Loss 128.86024
Epoch 23: Val Loss 117.02431
Epoch 24: Val Loss 104.54212
Epoch 25: Val Loss 91.59776
Epoch 26: Val Loss 78.39429
Epoch 27: Val Loss 65.30322
Epoch 28: Val Loss 52.69726
Epoch 29: Val Loss 41.04020
Epoch 30: Val Loss 30.76850
Epoch 31: Val Loss 22.28477
Epoch 32: Val Loss 15.83341
Epoch 33: Val Loss 11.36612
Epoch 34: Val Loss 8.64456
Epoch 35: Val Loss 7.13711
Epoch 36: Val Loss 6.30151
Epoch 37: Val Loss 5.70629
Epoch 38: Val Loss 5.13440
Epoch 39: Val Loss 4.53100
Epoch 40: Val Loss 3.94710
Epoch 41: Val Loss 3.43099
Epoch 42: Val Loss 3.01314
Epoch 43: Val Loss 2.68428
Epoch 44: Val Loss 2.41834
Epoch 45: Val Loss 2.19018
Epoch 46: Val Loss 1.98400
Epoch 47: Val Loss 1.80030
Epoch 48: Val Loss 1.63544
Epoch 49: Val Loss 1.49450
Epoch 50: Val Loss 1.37405
Epoch 51: Val Loss 1.27081
Epoch 52: Val Loss 1.18193
Epoch 53: Val Loss 1.10654
Epoch 54: Val Loss 1.04214
Epoch 55: Val Loss 0.98681
Epoch 56: Val Loss 0.93934
Epoch 57: Val Loss 0.89785
Epoch 58: Val Loss 0.86258
Epoch 59: Val Loss 0.83244
Epoch 60: Val Loss 0.80603
Epoch 61: Val Loss 0.78319
Epoch 62: Val Loss 0.76280
Epoch 63: Val Loss 0.74522
Epoch 64: Val Loss 0.72952
Epoch 65: Val Loss 0.71564
Epoch 66: Val Loss 0.70268
Epoch 67: Val Loss 0.69287
Epoch 68: Val Loss 0.68400
Epoch 69: Val Loss 0.67682
Epoch 70: Val Loss 0.67051
Epoch 71: Val Loss 0.66535
Epoch 72: Val Loss 0.65982
Epoch 73: Val Loss 0.65454
Epoch 74: Val Loss 0.65100
Epoch 75: Val Loss 0.64766
Epoch 76: Val Loss 0.64554
Epoch 77: Val Loss 0.64373
Epoch 78: Val Loss 0.64159
Epoch 79: Val Loss 0.64048
Epoch 80: Val Loss 0.63999
Epoch 81: Val Loss 0.63915
Epoch 82: Val Loss 0.63863
Epoch 83: Val Loss 0.63737
Epoch 84: Val Loss 0.63740
Epoch 85: Val Loss 0.63664
Epoch 86: Val Loss 0.63603
Epoch 87: Val Loss 0.63447
Epoch 88: Val Loss 0.63335
Epoch 89: Val Loss 0.63299
Epoch 90: Val Loss 0.63271
Epoch 91: Val Loss 0.63222
Epoch 92: Val Loss 0.63286
Epoch 93: Val Loss 0.63401
Epoch 94: Val Loss 0.63285
Epoch 95: Val Loss 0.63231
Epoch 96: Val Loss 0.63118
Epoch 97: Val Loss 0.63141
Epoch 98: Val Loss 0.63110
Epoch 99: Val Loss 0.63085
{'MSE - mean': 0.6322581501361928, 'MSE - std': 0.04711587854505546, 'R2 - mean': 0.05253920830855474, 'R2 - std': 0.06817097787182515} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 0.6322581501361928 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 13 with value: 0.6118930330337913.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 225.98146
Epoch 1: Val Loss 225.11943
Epoch 2: Val Loss 224.29897
Epoch 3: Val Loss 223.46400
Epoch 4: Val Loss 222.59061
Epoch 5: Val Loss 221.62801
Epoch 6: Val Loss 220.52350
Epoch 7: Val Loss 219.25003
Epoch 8: Val Loss 217.79482
Epoch 9: Val Loss 216.13000
Epoch 10: Val Loss 214.22749
Epoch 11: Val Loss 212.05499
Epoch 12: Val Loss 209.57899
Epoch 13: Val Loss 206.76752
Epoch 14: Val Loss 203.54593
Epoch 15: Val Loss 199.86803
Epoch 16: Val Loss 195.64224
Epoch 17: Val Loss 190.82819
Epoch 18: Val Loss 185.34708
Epoch 19: Val Loss 179.17406
Epoch 20: Val Loss 172.20934
Epoch 21: Val Loss 164.39499
Epoch 22: Val Loss 155.67244
Epoch 23: Val Loss 146.00542
Epoch 24: Val Loss 135.39760
Epoch 25: Val Loss 123.83899
Epoch 26: Val Loss 111.45158
Epoch 27: Val Loss 98.30413
Epoch 28: Val Loss 84.64313
Epoch 29: Val Loss 70.84090
Epoch 30: Val Loss 57.32789
Epoch 31: Val Loss 44.60965
Epoch 32: Val Loss 33.19257
Epoch 33: Val Loss 23.54012
Epoch 34: Val Loss 16.07017
Epoch 35: Val Loss 10.84357
Epoch 36: Val Loss 7.67075
Epoch 37: Val Loss 6.01204
Epoch 38: Val Loss 5.22696
Epoch 39: Val Loss 4.74060
Epoch 40: Val Loss 4.24738
Epoch 41: Val Loss 3.67522
Epoch 42: Val Loss 3.08602
Epoch 43: Val Loss 2.58477
Epoch 44: Val Loss 2.19250
Epoch 45: Val Loss 1.89905
Epoch 46: Val Loss 1.66988
Epoch 47: Val Loss 1.48064
Epoch 48: Val Loss 1.32054
Epoch 49: Val Loss 1.17883
Epoch 50: Val Loss 1.05627
Epoch 51: Val Loss 0.95452
Epoch 52: Val Loss 0.87053
Epoch 53: Val Loss 0.80209
Epoch 54: Val Loss 0.74682
Epoch 55: Val Loss 0.70111
Epoch 56: Val Loss 0.66202
Epoch 57: Val Loss 0.62917
Epoch 58: Val Loss 0.60022
Epoch 59: Val Loss 0.57595
Epoch 60: Val Loss 0.55587
Epoch 61: Val Loss 0.53931
Epoch 62: Val Loss 0.52617
Epoch 63: Val Loss 0.51611
Epoch 64: Val Loss 0.50817
Epoch 65: Val Loss 0.50244
Epoch 66: Val Loss 0.49755
Epoch 67: Val Loss 0.49407
Epoch 68: Val Loss 0.49080
Epoch 69: Val Loss 0.48791
Epoch 70: Val Loss 0.48464
Epoch 71: Val Loss 0.48293
Epoch 72: Val Loss 0.48159
Epoch 73: Val Loss 0.48036
Epoch 74: Val Loss 0.47965
Epoch 75: Val Loss 0.48003
Epoch 76: Val Loss 0.48077
Epoch 77: Val Loss 0.48116
Epoch 78: Val Loss 0.48122
Epoch 79: Val Loss 0.48081
Epoch 80: Val Loss 0.47971
Epoch 81: Val Loss 0.47994
Epoch 82: Val Loss 0.48014
Epoch 83: Val Loss 0.48131
Epoch 84: Val Loss 0.48251
Epoch 85: Val Loss 0.48174
Epoch 86: Val Loss 0.48290
Epoch 87: Val Loss 0.48234
Epoch 88: Val Loss 0.48391
Epoch 89: Val Loss 0.48519
Epoch 90: Val Loss 0.48595
Epoch 91: Val Loss 0.48606
Epoch 92: Val Loss 0.48708
Epoch 93: Val Loss 0.48816
Epoch 94: Val Loss 0.48860
Epoch 95: Val Loss 0.48858
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 0.47964901783597397, 'MSE - std': 0.0, 'R2 - mean': 0.12719737868070402, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.65562
Epoch 1: Val Loss 235.99577
Epoch 2: Val Loss 235.41487
Epoch 3: Val Loss 234.89764
Epoch 4: Val Loss 234.39444
Epoch 5: Val Loss 233.88332
Epoch 6: Val Loss 233.36157
Epoch 7: Val Loss 232.80164
Epoch 8: Val Loss 232.18440
Epoch 9: Val Loss 231.49132
Epoch 10: Val Loss 230.71408
Epoch 11: Val Loss 229.84109
Epoch 12: Val Loss 228.86069
Epoch 13: Val Loss 227.75702
Epoch 14: Val Loss 226.50815
Epoch 15: Val Loss 225.08852
Epoch 16: Val Loss 223.47798
Epoch 17: Val Loss 221.64346
Epoch 18: Val Loss 219.54652
Epoch 19: Val Loss 217.14296
Epoch 20: Val Loss 214.40343
Epoch 21: Val Loss 211.27992
Epoch 22: Val Loss 207.72900
Epoch 23: Val Loss 203.71745
Epoch 24: Val Loss 199.17485
Epoch 25: Val Loss 194.04240
Epoch 26: Val Loss 188.28909
Epoch 27: Val Loss 181.88983
Epoch 28: Val Loss 174.76311
Epoch 29: Val Loss 166.92583
Epoch 30: Val Loss 158.35577
Epoch 31: Val Loss 149.05794
Epoch 32: Val Loss 139.06563
Epoch 33: Val Loss 128.45605
Epoch 34: Val Loss 117.31518
Epoch 35: Val Loss 105.76669
Epoch 36: Val Loss 93.96930
Epoch 37: Val Loss 82.09283
Epoch 38: Val Loss 70.36286
Epoch 39: Val Loss 59.01797
Epoch 40: Val Loss 48.32272
Epoch 41: Val Loss 38.53436
Epoch 42: Val Loss 29.90994
Epoch 43: Val Loss 22.61945
Epoch 44: Val Loss 16.73073
Epoch 45: Val Loss 12.29026
Epoch 46: Val Loss 9.16285
Epoch 47: Val Loss 7.09285
Epoch 48: Val Loss 5.81321
Epoch 49: Val Loss 5.02677
Epoch 50: Val Loss 4.53246
Epoch 51: Val Loss 4.17346
Epoch 52: Val Loss 3.87641
Epoch 53: Val Loss 3.60088
Epoch 54: Val Loss 3.34311
Epoch 55: Val Loss 3.10504
Epoch 56: Val Loss 2.89762
Epoch 57: Val Loss 2.71849
Epoch 58: Val Loss 2.56159
Epoch 59: Val Loss 2.41958
Epoch 60: Val Loss 2.28413
Epoch 61: Val Loss 2.15962
Epoch 62: Val Loss 2.04048
Epoch 63: Val Loss 1.92784
Epoch 64: Val Loss 1.82298
Epoch 65: Val Loss 1.72616
Epoch 66: Val Loss 1.63282
Epoch 67: Val Loss 1.55184
Epoch 68: Val Loss 1.47643
Epoch 69: Val Loss 1.40919
Epoch 70: Val Loss 1.34986
Epoch 71: Val Loss 1.29716
Epoch 72: Val Loss 1.25127
Epoch 73: Val Loss 1.20986
Epoch 74: Val Loss 1.16982
Epoch 75: Val Loss 1.13335
Epoch 76: Val Loss 1.09992
Epoch 77: Val Loss 1.06909
Epoch 78: Val Loss 1.04083
Epoch 79: Val Loss 1.01558
Epoch 80: Val Loss 0.99279
Epoch 81: Val Loss 0.97085
Epoch 82: Val Loss 0.95122
Epoch 83: Val Loss 0.93253
Epoch 84: Val Loss 0.91497
Epoch 85: Val Loss 0.89913
Epoch 86: Val Loss 0.88172
Epoch 87: Val Loss 0.86757
Epoch 88: Val Loss 0.85227
Epoch 89: Val Loss 0.83822
Epoch 90: Val Loss 0.82500
Epoch 91: Val Loss 0.81338
Epoch 92: Val Loss 0.80184
Epoch 93: Val Loss 0.79193
Epoch 94: Val Loss 0.78224
Epoch 95: Val Loss 0.77410
Epoch 96: Val Loss 0.76620
Epoch 97: Val Loss 0.75823
Epoch 98: Val Loss 0.75068
Epoch 99: Val Loss 0.74147
{'MSE - mean': 0.6105572239310989, 'MSE - std': 0.13090820609512496, 'R2 - mean': 0.0615568957658687, 'R2 - std': 0.06564048291483532} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 215.49452
Epoch 1: Val Loss 214.43845
Epoch 2: Val Loss 213.29520
Epoch 3: Val Loss 212.05754
Epoch 4: Val Loss 210.71449
Epoch 5: Val Loss 209.24007
Epoch 6: Val Loss 207.58495
Epoch 7: Val Loss 205.72893
Epoch 8: Val Loss 203.65750
Epoch 9: Val Loss 201.30182
Epoch 10: Val Loss 198.61739
Epoch 11: Val Loss 195.54451
Epoch 12: Val Loss 192.02934
Epoch 13: Val Loss 188.03915
Epoch 14: Val Loss 183.54391
Epoch 15: Val Loss 178.43182
Epoch 16: Val Loss 172.63321
Epoch 17: Val Loss 166.10791
Epoch 18: Val Loss 158.76215
Epoch 19: Val Loss 150.61719
Epoch 20: Val Loss 141.62317
Epoch 21: Val Loss 131.79440
Epoch 22: Val Loss 121.10564
Epoch 23: Val Loss 109.67139
Epoch 24: Val Loss 97.61224
Epoch 25: Val Loss 85.10995
Epoch 26: Val Loss 72.43017
Epoch 27: Val Loss 59.88112
Epoch 28: Val Loss 47.87495
Epoch 29: Val Loss 36.83862
Epoch 30: Val Loss 27.21860
Epoch 31: Val Loss 19.33167
Epoch 32: Val Loss 13.39956
Epoch 33: Val Loss 9.42228
Epoch 34: Val Loss 7.10669
Epoch 35: Val Loss 5.92852
Epoch 36: Val Loss 5.36910
Epoch 37: Val Loss 4.98636
Epoch 38: Val Loss 4.57020
Epoch 39: Val Loss 4.08142
Epoch 40: Val Loss 3.58882
Epoch 41: Val Loss 3.14890
Epoch 42: Val Loss 2.77840
Epoch 43: Val Loss 2.48839
Epoch 44: Val Loss 2.25490
Epoch 45: Val Loss 2.05481
Epoch 46: Val Loss 1.87855
Epoch 47: Val Loss 1.72115
Epoch 48: Val Loss 1.58205
Epoch 49: Val Loss 1.45911
Epoch 50: Val Loss 1.35206
Epoch 51: Val Loss 1.25994
Epoch 52: Val Loss 1.17939
Epoch 53: Val Loss 1.11067
Epoch 54: Val Loss 1.05120
Epoch 55: Val Loss 0.99988
Epoch 56: Val Loss 0.95392
Epoch 57: Val Loss 0.91370
Epoch 58: Val Loss 0.87924
Epoch 59: Val Loss 0.84904
Epoch 60: Val Loss 0.82376
Epoch 61: Val Loss 0.80189
Epoch 62: Val Loss 0.78369
Epoch 63: Val Loss 0.76774
Epoch 64: Val Loss 0.75428
Epoch 65: Val Loss 0.74263
Epoch 66: Val Loss 0.73293
Epoch 67: Val Loss 0.72369
Epoch 68: Val Loss 0.71634
Epoch 69: Val Loss 0.70998
Epoch 70: Val Loss 0.70505
Epoch 71: Val Loss 0.69996
Epoch 72: Val Loss 0.69607
Epoch 73: Val Loss 0.69268
Epoch 74: Val Loss 0.68974
Epoch 75: Val Loss 0.68736
Epoch 76: Val Loss 0.68584
Epoch 77: Val Loss 0.68467
Epoch 78: Val Loss 0.68308
Epoch 79: Val Loss 0.68146
Epoch 80: Val Loss 0.67996
Epoch 81: Val Loss 0.67868
Epoch 82: Val Loss 0.67771
Epoch 83: Val Loss 0.67693
Epoch 84: Val Loss 0.67605
Epoch 85: Val Loss 0.67533
Epoch 86: Val Loss 0.67424
Epoch 87: Val Loss 0.67280
Epoch 88: Val Loss 0.67251
Epoch 89: Val Loss 0.67200
Epoch 90: Val Loss 0.67167
Epoch 91: Val Loss 0.67239
Epoch 92: Val Loss 0.67238
Epoch 93: Val Loss 0.67160
Epoch 94: Val Loss 0.67120
Epoch 95: Val Loss 0.67136
Epoch 96: Val Loss 0.67217
Epoch 97: Val Loss 0.67225
Epoch 98: Val Loss 0.67196
Epoch 99: Val Loss 0.67109
{'MSE - mean': 0.6307343703741356, 'MSE - std': 0.110629441948621, 'R2 - mean': 0.049684601508173255, 'R2 - std': 0.05616361283557598} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 215.27272
Epoch 1: Val Loss 213.68246
Epoch 2: Val Loss 211.90878
Epoch 3: Val Loss 209.92532
Epoch 4: Val Loss 207.68309
Epoch 5: Val Loss 205.18559
Epoch 6: Val Loss 202.38310
Epoch 7: Val Loss 199.21667
Epoch 8: Val Loss 195.62537
Epoch 9: Val Loss 191.54088
Epoch 10: Val Loss 186.90215
Epoch 11: Val Loss 181.62714
Epoch 12: Val Loss 175.66125
Epoch 13: Val Loss 168.93430
Epoch 14: Val Loss 161.36240
Epoch 15: Val Loss 152.91039
Epoch 16: Val Loss 143.52812
Epoch 17: Val Loss 133.23529
Epoch 18: Val Loss 122.06179
Epoch 19: Val Loss 110.06390
Epoch 20: Val Loss 97.37375
Epoch 21: Val Loss 84.20514
Epoch 22: Val Loss 70.82542
Epoch 23: Val Loss 57.62455
Epoch 24: Val Loss 45.07085
Epoch 25: Val Loss 33.65938
Epoch 26: Val Loss 23.84748
Epoch 27: Val Loss 16.06800
Epoch 28: Val Loss 10.47004
Epoch 29: Val Loss 6.94834
Epoch 30: Val Loss 5.07296
Epoch 31: Val Loss 4.23934
Epoch 32: Val Loss 3.85371
Epoch 33: Val Loss 3.53645
Epoch 34: Val Loss 3.16279
Epoch 35: Val Loss 2.75704
Epoch 36: Val Loss 2.39709
Epoch 37: Val Loss 2.11653
Epoch 38: Val Loss 1.91505
Epoch 39: Val Loss 1.76164
Epoch 40: Val Loss 1.63656
Epoch 41: Val Loss 1.51202
Epoch 42: Val Loss 1.38710
Epoch 43: Val Loss 1.27056
Epoch 44: Val Loss 1.16463
Epoch 45: Val Loss 1.07578
Epoch 46: Val Loss 1.00033
Epoch 47: Val Loss 0.93946
Epoch 48: Val Loss 0.88826
Epoch 49: Val Loss 0.84667
Epoch 50: Val Loss 0.81268
Epoch 51: Val Loss 0.78519
Epoch 52: Val Loss 0.76367
Epoch 53: Val Loss 0.74651
Epoch 54: Val Loss 0.73118
Epoch 55: Val Loss 0.71963
Epoch 56: Val Loss 0.70940
Epoch 57: Val Loss 0.70019
Epoch 58: Val Loss 0.69066
Epoch 59: Val Loss 0.68271
Epoch 60: Val Loss 0.67580
Epoch 61: Val Loss 0.66922
Epoch 62: Val Loss 0.66447
Epoch 63: Val Loss 0.66104
Epoch 64: Val Loss 0.65825
Epoch 65: Val Loss 0.65526
Epoch 66: Val Loss 0.65344
Epoch 67: Val Loss 0.65190
Epoch 68: Val Loss 0.64999
Epoch 69: Val Loss 0.64928
Epoch 70: Val Loss 0.64928
Epoch 71: Val Loss 0.64812
Epoch 72: Val Loss 0.64518
Epoch 73: Val Loss 0.64408
Epoch 74: Val Loss 0.64249
Epoch 75: Val Loss 0.64171
Epoch 76: Val Loss 0.64117
Epoch 77: Val Loss 0.64027
Epoch 78: Val Loss 0.64003
Epoch 79: Val Loss 0.63929
Epoch 80: Val Loss 0.63883
Epoch 81: Val Loss 0.63834
Epoch 82: Val Loss 0.63944
Epoch 83: Val Loss 0.63897
Epoch 84: Val Loss 0.63894
Epoch 85: Val Loss 0.63893
Epoch 86: Val Loss 0.63827
Epoch 87: Val Loss 0.63758
Epoch 88: Val Loss 0.63695
Epoch 89: Val Loss 0.63605
Epoch 90: Val Loss 0.63505
Epoch 91: Val Loss 0.63322
Epoch 92: Val Loss 0.63244
Epoch 93: Val Loss 0.63076
Epoch 94: Val Loss 0.63000
Epoch 95: Val Loss 0.63005
Epoch 96: Val Loss 0.62955
Epoch 97: Val Loss 0.63039
Epoch 98: Val Loss 0.62952
Epoch 99: Val Loss 0.62865
{'MSE - mean': 0.6302122937506125, 'MSE - std': 0.09581217439031411, 'R2 - mean': 0.08681393346275373, 'R2 - std': 0.08063203723854835} 
 

In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 235.67604
Epoch 1: Val Loss 234.93906
Epoch 2: Val Loss 234.18919
Epoch 3: Val Loss 233.39571
Epoch 4: Val Loss 232.53891
Epoch 5: Val Loss 231.63036
Epoch 6: Val Loss 230.65100
Epoch 7: Val Loss 229.57431
Epoch 8: Val Loss 228.37187
Epoch 9: Val Loss 227.02176
Epoch 10: Val Loss 225.48587
Epoch 11: Val Loss 223.73837
Epoch 12: Val Loss 221.75267
Epoch 13: Val Loss 219.48375
Epoch 14: Val Loss 216.88661
Epoch 15: Val Loss 213.87811
Epoch 16: Val Loss 210.42764
Epoch 17: Val Loss 206.43263
Epoch 18: Val Loss 201.78151
Epoch 19: Val Loss 196.35587
Epoch 20: Val Loss 190.11815
Epoch 21: Val Loss 183.00755
Epoch 22: Val Loss 174.94527
Epoch 23: Val Loss 165.89841
Epoch 24: Val Loss 155.82727
Epoch 25: Val Loss 144.73624
Epoch 26: Val Loss 132.67174
Epoch 27: Val Loss 119.73451
Epoch 28: Val Loss 106.08420
Epoch 29: Val Loss 91.91660
Epoch 30: Val Loss 77.55199
Epoch 31: Val Loss 63.42869
Epoch 32: Val Loss 49.97865
Epoch 33: Val Loss 37.67151
Epoch 34: Val Loss 26.97958
Epoch 35: Val Loss 18.35367
Epoch 36: Val Loss 11.94680
Epoch 37: Val Loss 7.72383
Epoch 38: Val Loss 5.35481
Epoch 39: Val Loss 4.28883
Epoch 40: Val Loss 3.87113
Epoch 41: Val Loss 3.64016
Epoch 42: Val Loss 3.35737
Epoch 43: Val Loss 3.01518
Epoch 44: Val Loss 2.66456
Epoch 45: Val Loss 2.36539
Epoch 46: Val Loss 2.13531
Epoch 47: Val Loss 1.96546
Epoch 48: Val Loss 1.83259
Epoch 49: Val Loss 1.71213
Epoch 50: Val Loss 1.60121
Epoch 51: Val Loss 1.49391
Epoch 52: Val Loss 1.38983
Epoch 53: Val Loss 1.29513
Epoch 54: Val Loss 1.21059
Epoch 55: Val Loss 1.13809
Epoch 56: Val Loss 1.07530
Epoch 57: Val Loss 1.02273
Epoch 58: Val Loss 0.97799
Epoch 59: Val Loss 0.93725
Epoch 60: Val Loss 0.90172
Epoch 61: Val Loss 0.87104
Epoch 62: Val Loss 0.84378
Epoch 63: Val Loss 0.81886
Epoch 64: Val Loss 0.79627
Epoch 65: Val Loss 0.77531
Epoch 66: Val Loss 0.75696
Epoch 67: Val Loss 0.74037
Epoch 68: Val Loss 0.72528
Epoch 69: Val Loss 0.71183
Epoch 70: Val Loss 0.70009
Epoch 71: Val Loss 0.68924
Epoch 72: Val Loss 0.67927
Epoch 73: Val Loss 0.67100
Epoch 74: Val Loss 0.66368
Epoch 75: Val Loss 0.65655
Epoch 76: Val Loss 0.64990
Epoch 77: Val Loss 0.64390
Epoch 78: Val Loss 0.63850
Epoch 79: Val Loss 0.63377
Epoch 80: Val Loss 0.62954
Epoch 81: Val Loss 0.62628
Epoch 82: Val Loss 0.62372
Epoch 83: Val Loss 0.62048
Epoch 84: Val Loss 0.61754
Epoch 85: Val Loss 0.61492
Epoch 86: Val Loss 0.61236
Epoch 87: Val Loss 0.61007
Epoch 88: Val Loss 0.60807
Epoch 89: Val Loss 0.60609
Epoch 90: Val Loss 0.60460
Epoch 91: Val Loss 0.60348
Epoch 92: Val Loss 0.60279
Epoch 93: Val Loss 0.60172
Epoch 94: Val Loss 0.60097
Epoch 95: Val Loss 0.60041
Epoch 96: Val Loss 0.60081
Epoch 97: Val Loss 0.60049
Epoch 98: Val Loss 0.60011
Epoch 99: Val Loss 0.59928
{'MSE - mean': 0.6240268036547264, 'MSE - std': 0.08658532993541798, 'R2 - mean': 0.07054182182952322, 'R2 - std': 0.07912235342435273} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 0.6240268036547264 and parameters: {'dim': 64, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 13 with value: 0.6118930330337913.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.60939
Epoch 1: Val Loss 230.48950
Epoch 2: Val Loss 230.36967
Epoch 3: Val Loss 230.24930
Epoch 4: Val Loss 230.12880
Epoch 5: Val Loss 230.00853
Epoch 6: Val Loss 229.88840
Epoch 7: Val Loss 229.76762
Epoch 8: Val Loss 229.64729
Epoch 9: Val Loss 229.52701
Epoch 10: Val Loss 229.40645
Epoch 11: Val Loss 229.28539
Epoch 12: Val Loss 229.16451
Epoch 13: Val Loss 229.04353
Epoch 14: Val Loss 228.92258
Epoch 15: Val Loss 228.80093
Epoch 16: Val Loss 228.67952
Epoch 17: Val Loss 228.55745
Epoch 18: Val Loss 228.43501
Epoch 19: Val Loss 228.31247
Epoch 20: Val Loss 228.18900
Epoch 21: Val Loss 228.06544
Epoch 22: Val Loss 227.94017
Epoch 23: Val Loss 227.81432
Epoch 24: Val Loss 227.68806
Epoch 25: Val Loss 227.56094
Epoch 26: Val Loss 227.43295
Epoch 27: Val Loss 227.30399
Epoch 28: Val Loss 227.17313
Epoch 29: Val Loss 227.04141
Epoch 30: Val Loss 226.90881
Epoch 31: Val Loss 226.77498
Epoch 32: Val Loss 226.63937
Epoch 33: Val Loss 226.50273
Epoch 34: Val Loss 226.36388
Epoch 35: Val Loss 226.22253
Epoch 36: Val Loss 226.08051
Epoch 37: Val Loss 225.93674
Epoch 38: Val Loss 225.79146
Epoch 39: Val Loss 225.64517
Epoch 40: Val Loss 225.49777
Epoch 41: Val Loss 225.34819
Epoch 42: Val Loss 225.19775
Epoch 43: Val Loss 225.04483
Epoch 44: Val Loss 224.89085
Epoch 45: Val Loss 224.73586
Epoch 46: Val Loss 224.57957
Epoch 47: Val Loss 224.42152
Epoch 48: Val Loss 224.26166
Epoch 49: Val Loss 224.09930
Epoch 50: Val Loss 223.93509
Epoch 51: Val Loss 223.76892
Epoch 52: Val Loss 223.59946
Epoch 53: Val Loss 223.42769
Epoch 54: Val Loss 223.25293
Epoch 55: Val Loss 223.07483
Epoch 56: Val Loss 222.89465
Epoch 57: Val Loss 222.71175
Epoch 58: Val Loss 222.52715
Epoch 59: Val Loss 222.33891
Epoch 60: Val Loss 222.14807
Epoch 61: Val Loss 221.95586
Epoch 62: Val Loss 221.76222
Epoch 63: Val Loss 221.56644
Epoch 64: Val Loss 221.36845
Epoch 65: Val Loss 221.16794
Epoch 66: Val Loss 220.96596
Epoch 67: Val Loss 220.76105
Epoch 68: Val Loss 220.55414
Epoch 69: Val Loss 220.34560
Epoch 70: Val Loss 220.13448
Epoch 71: Val Loss 219.92198
Epoch 72: Val Loss 219.70491
Epoch 73: Val Loss 219.48508
Epoch 74: Val Loss 219.26366
Epoch 75: Val Loss 219.03923
Epoch 76: Val Loss 218.81250
Epoch 77: Val Loss 218.58292
Epoch 78: Val Loss 218.35011
Epoch 79: Val Loss 218.11465
Epoch 80: Val Loss 217.87686
Epoch 81: Val Loss 217.63745
Epoch 82: Val Loss 217.39339
Epoch 83: Val Loss 217.14751
Epoch 84: Val Loss 216.89914
Epoch 85: Val Loss 216.64760
Epoch 86: Val Loss 216.39380
Epoch 87: Val Loss 216.13823
Epoch 88: Val Loss 215.87950
Epoch 89: Val Loss 215.61742
Epoch 90: Val Loss 215.35081
Epoch 91: Val Loss 215.08104
Epoch 92: Val Loss 214.80815
Epoch 93: Val Loss 214.53122
Epoch 94: Val Loss 214.25266
Epoch 95: Val Loss 213.96941
Epoch 96: Val Loss 213.68370
Epoch 97: Val Loss 213.39540
Epoch 98: Val Loss 213.10385
Epoch 99: Val Loss 212.80870
{'MSE - mean': 212.80869771375794, 'MSE - std': 0.0, 'R2 - mean': -386.24146677525624, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.69002
Epoch 1: Val Loss 227.56602
Epoch 2: Val Loss 227.44130
Epoch 3: Val Loss 227.31595
Epoch 4: Val Loss 227.18979
Epoch 5: Val Loss 227.06345
Epoch 6: Val Loss 226.93649
Epoch 7: Val Loss 226.80902
Epoch 8: Val Loss 226.68094
Epoch 9: Val Loss 226.55144
Epoch 10: Val Loss 226.42078
Epoch 11: Val Loss 226.28990
Epoch 12: Val Loss 226.15779
Epoch 13: Val Loss 226.02499
Epoch 14: Val Loss 225.89047
Epoch 15: Val Loss 225.75635
Epoch 16: Val Loss 225.61972
Epoch 17: Val Loss 225.48161
Epoch 18: Val Loss 225.34187
Epoch 19: Val Loss 225.20110
Epoch 20: Val Loss 225.05928
Epoch 21: Val Loss 224.91615
Epoch 22: Val Loss 224.77313
Epoch 23: Val Loss 224.62889
Epoch 24: Val Loss 224.48308
Epoch 25: Val Loss 224.33572
Epoch 26: Val Loss 224.18634
Epoch 27: Val Loss 224.03705
Epoch 28: Val Loss 223.88640
Epoch 29: Val Loss 223.73354
Epoch 30: Val Loss 223.57947
Epoch 31: Val Loss 223.42404
Epoch 32: Val Loss 223.26738
Epoch 33: Val Loss 223.10846
Epoch 34: Val Loss 222.94751
Epoch 35: Val Loss 222.78456
Epoch 36: Val Loss 222.62119
Epoch 37: Val Loss 222.45532
Epoch 38: Val Loss 222.28847
Epoch 39: Val Loss 222.11998
Epoch 40: Val Loss 221.95045
Epoch 41: Val Loss 221.77965
Epoch 42: Val Loss 221.60852
Epoch 43: Val Loss 221.43555
Epoch 44: Val Loss 221.25897
Epoch 45: Val Loss 221.08057
Epoch 46: Val Loss 220.90097
Epoch 47: Val Loss 220.71994
Epoch 48: Val Loss 220.53766
Epoch 49: Val Loss 220.35440
Epoch 50: Val Loss 220.16946
Epoch 51: Val Loss 219.98225
Epoch 52: Val Loss 219.79356
Epoch 53: Val Loss 219.60390
Epoch 54: Val Loss 219.41324
Epoch 55: Val Loss 219.22092
Epoch 56: Val Loss 219.02660
Epoch 57: Val Loss 218.83148
Epoch 58: Val Loss 218.63454
Epoch 59: Val Loss 218.43610
Epoch 60: Val Loss 218.23547
Epoch 61: Val Loss 218.03310
Epoch 62: Val Loss 217.82776
Epoch 63: Val Loss 217.62082
Epoch 64: Val Loss 217.41174
Epoch 65: Val Loss 217.20200
Epoch 66: Val Loss 216.99026
Epoch 67: Val Loss 216.77519
Epoch 68: Val Loss 216.55904
Epoch 69: Val Loss 216.34093
Epoch 70: Val Loss 216.11807
Epoch 71: Val Loss 215.89468
Epoch 72: Val Loss 215.66800
Epoch 73: Val Loss 215.43848
Epoch 74: Val Loss 215.20824
Epoch 75: Val Loss 214.97565
Epoch 76: Val Loss 214.74033
Epoch 77: Val Loss 214.50255
Epoch 78: Val Loss 214.26361
Epoch 79: Val Loss 214.02316
Epoch 80: Val Loss 213.78072
Epoch 81: Val Loss 213.53650
Epoch 82: Val Loss 213.29137
Epoch 83: Val Loss 213.04480
Epoch 84: Val Loss 212.79547
Epoch 85: Val Loss 212.54269
Epoch 86: Val Loss 212.28587
Epoch 87: Val Loss 212.02603
Epoch 88: Val Loss 211.76250
Epoch 89: Val Loss 211.49757
Epoch 90: Val Loss 211.22874
Epoch 91: Val Loss 210.95699
Epoch 92: Val Loss 210.68343
Epoch 93: Val Loss 210.40530
Epoch 94: Val Loss 210.12422
Epoch 95: Val Loss 209.84032
Epoch 96: Val Loss 209.55367
Epoch 97: Val Loss 209.26486
Epoch 98: Val Loss 208.97227
Epoch 99: Val Loss 208.67712
{'MSE - mean': 210.74291549209408, 'MSE - std': 2.065782221663852, 'R2 - mean': -333.9147681767659, 'R2 - std': 52.32669859849034} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.48286
Epoch 1: Val Loss 232.40681
Epoch 2: Val Loss 232.33014
Epoch 3: Val Loss 232.25232
Epoch 4: Val Loss 232.17313
Epoch 5: Val Loss 232.09245
Epoch 6: Val Loss 232.00951
Epoch 7: Val Loss 231.92598
Epoch 8: Val Loss 231.83960
Epoch 9: Val Loss 231.75078
Epoch 10: Val Loss 231.66000
Epoch 11: Val Loss 231.56772
Epoch 12: Val Loss 231.47414
Epoch 13: Val Loss 231.37866
Epoch 14: Val Loss 231.28081
Epoch 15: Val Loss 231.18114
Epoch 16: Val Loss 231.08026
Epoch 17: Val Loss 230.97588
Epoch 18: Val Loss 230.86902
Epoch 19: Val Loss 230.75970
Epoch 20: Val Loss 230.64827
Epoch 21: Val Loss 230.53561
Epoch 22: Val Loss 230.42003
Epoch 23: Val Loss 230.30280
Epoch 24: Val Loss 230.18365
Epoch 25: Val Loss 230.06093
Epoch 26: Val Loss 229.93579
Epoch 27: Val Loss 229.80807
Epoch 28: Val Loss 229.67873
Epoch 29: Val Loss 229.54663
Epoch 30: Val Loss 229.41286
Epoch 31: Val Loss 229.27664
Epoch 32: Val Loss 229.13818
Epoch 33: Val Loss 228.99725
Epoch 34: Val Loss 228.85274
Epoch 35: Val Loss 228.70482
Epoch 36: Val Loss 228.55316
Epoch 37: Val Loss 228.39900
Epoch 38: Val Loss 228.24081
Epoch 39: Val Loss 228.08041
Epoch 40: Val Loss 227.91800
Epoch 41: Val Loss 227.75192
Epoch 42: Val Loss 227.58292
Epoch 43: Val Loss 227.41110
Epoch 44: Val Loss 227.23457
Epoch 45: Val Loss 227.05472
Epoch 46: Val Loss 226.87238
Epoch 47: Val Loss 226.68494
Epoch 48: Val Loss 226.49272
Epoch 49: Val Loss 226.29774
Epoch 50: Val Loss 226.09969
Epoch 51: Val Loss 225.89651
Epoch 52: Val Loss 225.68991
Epoch 53: Val Loss 225.47940
Epoch 54: Val Loss 225.26479
Epoch 55: Val Loss 225.04517
Epoch 56: Val Loss 224.82019
Epoch 57: Val Loss 224.59164
Epoch 58: Val Loss 224.35872
Epoch 59: Val Loss 224.12192
Epoch 60: Val Loss 223.88080
Epoch 61: Val Loss 223.63518
Epoch 62: Val Loss 223.38348
Epoch 63: Val Loss 223.12866
Epoch 64: Val Loss 222.86891
Epoch 65: Val Loss 222.60435
Epoch 66: Val Loss 222.33441
Epoch 67: Val Loss 222.05984
Epoch 68: Val Loss 221.77937
Epoch 69: Val Loss 221.49336
Epoch 70: Val Loss 221.20427
Epoch 71: Val Loss 220.91039
Epoch 72: Val Loss 220.60896
Epoch 73: Val Loss 220.30206
Epoch 74: Val Loss 219.98662
Epoch 75: Val Loss 219.66571
Epoch 76: Val Loss 219.33960
Epoch 77: Val Loss 219.00859
Epoch 78: Val Loss 218.66969
Epoch 79: Val Loss 218.32652
Epoch 80: Val Loss 217.97781
Epoch 81: Val Loss 217.62248
Epoch 82: Val Loss 217.26192
Epoch 83: Val Loss 216.89447
Epoch 84: Val Loss 216.52040
Epoch 85: Val Loss 216.14117
Epoch 86: Val Loss 215.75375
Epoch 87: Val Loss 215.35919
Epoch 88: Val Loss 214.95518
Epoch 89: Val Loss 214.54500
Epoch 90: Val Loss 214.12640
Epoch 91: Val Loss 213.69881
Epoch 92: Val Loss 213.26480
Epoch 93: Val Loss 212.82256
Epoch 94: Val Loss 212.37221
Epoch 95: Val Loss 211.91211
Epoch 96: Val Loss 211.44556
Epoch 97: Val Loss 210.97023
Epoch 98: Val Loss 210.48427
Epoch 99: Val Loss 209.99258
{'MSE - mean': 210.49281103039255, 'MSE - std': 1.7233906333410487, 'R2 - mean': -323.8753034937585, 'R2 - std': 45.02189052436093} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.85434
Epoch 1: Val Loss 230.77516
Epoch 2: Val Loss 230.69705
Epoch 3: Val Loss 230.61972
Epoch 4: Val Loss 230.54445
Epoch 5: Val Loss 230.47037
Epoch 6: Val Loss 230.39781
Epoch 7: Val Loss 230.32639
Epoch 8: Val Loss 230.25627
Epoch 9: Val Loss 230.18704
Epoch 10: Val Loss 230.11864
Epoch 11: Val Loss 230.05115
Epoch 12: Val Loss 229.98442
Epoch 13: Val Loss 229.91895
Epoch 14: Val Loss 229.85452
Epoch 15: Val Loss 229.79129
Epoch 16: Val Loss 229.72855
Epoch 17: Val Loss 229.66676
Epoch 18: Val Loss 229.60590
Epoch 19: Val Loss 229.54646
Epoch 20: Val Loss 229.48746
Epoch 21: Val Loss 229.42902
Epoch 22: Val Loss 229.37082
Epoch 23: Val Loss 229.31328
Epoch 24: Val Loss 229.25589
Epoch 25: Val Loss 229.19853
Epoch 26: Val Loss 229.14177
Epoch 27: Val Loss 229.08522
Epoch 28: Val Loss 229.02885
Epoch 29: Val Loss 228.97247
Epoch 30: Val Loss 228.91612
Epoch 31: Val Loss 228.86058
Epoch 32: Val Loss 228.80537
Epoch 33: Val Loss 228.75021
Epoch 34: Val Loss 228.69534
Epoch 35: Val Loss 228.64098
Epoch 36: Val Loss 228.58678
Epoch 37: Val Loss 228.53293
Epoch 38: Val Loss 228.47893
Epoch 39: Val Loss 228.42543
Epoch 40: Val Loss 228.37248
Epoch 41: Val Loss 228.31952
Epoch 42: Val Loss 228.26692
Epoch 43: Val Loss 228.21452
Epoch 44: Val Loss 228.16238
Epoch 45: Val Loss 228.10989
Epoch 46: Val Loss 228.05751
Epoch 47: Val Loss 228.00542
Epoch 48: Val Loss 227.95332
Epoch 49: Val Loss 227.90092
Epoch 50: Val Loss 227.84822
Epoch 51: Val Loss 227.79544
Epoch 52: Val Loss 227.74234
Epoch 53: Val Loss 227.68921
Epoch 54: Val Loss 227.63625
Epoch 55: Val Loss 227.58365
Epoch 56: Val Loss 227.53073
Epoch 57: Val Loss 227.47720
Epoch 58: Val Loss 227.42317
Epoch 59: Val Loss 227.36890
Epoch 60: Val Loss 227.31398
Epoch 61: Val Loss 227.25909
Epoch 62: Val Loss 227.20328
Epoch 63: Val Loss 227.14745
Epoch 64: Val Loss 227.09138
Epoch 65: Val Loss 227.03461
Epoch 66: Val Loss 226.97743
Epoch 67: Val Loss 226.91992
Epoch 68: Val Loss 226.86182
Epoch 69: Val Loss 226.80312
Epoch 70: Val Loss 226.74374
Epoch 71: Val Loss 226.68430
Epoch 72: Val Loss 226.62424
Epoch 73: Val Loss 226.56332
Epoch 74: Val Loss 226.50131
Epoch 75: Val Loss 226.43858
Epoch 76: Val Loss 226.37500
Epoch 77: Val Loss 226.31068
Epoch 78: Val Loss 226.24615
Epoch 79: Val Loss 226.18063
Epoch 80: Val Loss 226.11403
Epoch 81: Val Loss 226.04637
Epoch 82: Val Loss 225.97746
Epoch 83: Val Loss 225.90750
Epoch 84: Val Loss 225.83623
Epoch 85: Val Loss 225.76402
Epoch 86: Val Loss 225.69116
Epoch 87: Val Loss 225.61684
Epoch 88: Val Loss 225.54036
Epoch 89: Val Loss 225.46233
Epoch 90: Val Loss 225.38284
Epoch 91: Val Loss 225.30234
Epoch 92: Val Loss 225.21971
Epoch 93: Val Loss 225.13655
Epoch 94: Val Loss 225.05194
Epoch 95: Val Loss 224.96585
Epoch 96: Val Loss 224.87946
Epoch 97: Val Loss 224.79083
Epoch 98: Val Loss 224.69992
Epoch 99: Val Loss 224.60779
{'MSE - mean': 214.02155690937875, 'MSE - std': 6.291557747625135, 'R2 - mean': -314.27471667122325, 'R2 - std': 42.38799089130985} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.08797
Epoch 1: Val Loss 222.97527
Epoch 2: Val Loss 222.86295
Epoch 3: Val Loss 222.75043
Epoch 4: Val Loss 222.63821
Epoch 5: Val Loss 222.52597
Epoch 6: Val Loss 222.41367
Epoch 7: Val Loss 222.30069
Epoch 8: Val Loss 222.18709
Epoch 9: Val Loss 222.07312
Epoch 10: Val Loss 221.95912
Epoch 11: Val Loss 221.84521
Epoch 12: Val Loss 221.73262
Epoch 13: Val Loss 221.61932
Epoch 14: Val Loss 221.50610
Epoch 15: Val Loss 221.39232
Epoch 16: Val Loss 221.27747
Epoch 17: Val Loss 221.16141
Epoch 18: Val Loss 221.04408
Epoch 19: Val Loss 220.92653
Epoch 20: Val Loss 220.80797
Epoch 21: Val Loss 220.68900
Epoch 22: Val Loss 220.56946
Epoch 23: Val Loss 220.44901
Epoch 24: Val Loss 220.32785
Epoch 25: Val Loss 220.20618
Epoch 26: Val Loss 220.08348
Epoch 27: Val Loss 219.96004
Epoch 28: Val Loss 219.83667
Epoch 29: Val Loss 219.71190
Epoch 30: Val Loss 219.58600
Epoch 31: Val Loss 219.45850
Epoch 32: Val Loss 219.32942
Epoch 33: Val Loss 219.19867
Epoch 34: Val Loss 219.06670
Epoch 35: Val Loss 218.93327
Epoch 36: Val Loss 218.79814
Epoch 37: Val Loss 218.66069
Epoch 38: Val Loss 218.52124
Epoch 39: Val Loss 218.38043
Epoch 40: Val Loss 218.23872
Epoch 41: Val Loss 218.09573
Epoch 42: Val Loss 217.95187
Epoch 43: Val Loss 217.80621
Epoch 44: Val Loss 217.65964
Epoch 45: Val Loss 217.51244
Epoch 46: Val Loss 217.36313
Epoch 47: Val Loss 217.21159
Epoch 48: Val Loss 217.05818
Epoch 49: Val Loss 216.90259
Epoch 50: Val Loss 216.74471
Epoch 51: Val Loss 216.58539
Epoch 52: Val Loss 216.42477
Epoch 53: Val Loss 216.26184
Epoch 54: Val Loss 216.09622
Epoch 55: Val Loss 215.92825
Epoch 56: Val Loss 215.75766
Epoch 57: Val Loss 215.58453
Epoch 58: Val Loss 215.40874
Epoch 59: Val Loss 215.22998
Epoch 60: Val Loss 215.04921
Epoch 61: Val Loss 214.86610
Epoch 62: Val Loss 214.67950
Epoch 63: Val Loss 214.49026
Epoch 64: Val Loss 214.29747
Epoch 65: Val Loss 214.10226
Epoch 66: Val Loss 213.90448
Epoch 67: Val Loss 213.70316
Epoch 68: Val Loss 213.49831
Epoch 69: Val Loss 213.29050
Epoch 70: Val Loss 213.07944
Epoch 71: Val Loss 212.86504
Epoch 72: Val Loss 212.64825
Epoch 73: Val Loss 212.42851
Epoch 74: Val Loss 212.20590
Epoch 75: Val Loss 211.97995
Epoch 76: Val Loss 211.75075
Epoch 77: Val Loss 211.51846
Epoch 78: Val Loss 211.28172
Epoch 79: Val Loss 211.04108
Epoch 80: Val Loss 210.79692
Epoch 81: Val Loss 210.54941
Epoch 82: Val Loss 210.29758
Epoch 83: Val Loss 210.04131
Epoch 84: Val Loss 209.78218
Epoch 85: Val Loss 209.51898
Epoch 86: Val Loss 209.25111
Epoch 87: Val Loss 208.97826
Epoch 88: Val Loss 208.70207
Epoch 89: Val Loss 208.42079
Epoch 90: Val Loss 208.13495
Epoch 91: Val Loss 207.84387
Epoch 92: Val Loss 207.54866
Epoch 93: Val Loss 207.24823
Epoch 94: Val Loss 206.94359
Epoch 95: Val Loss 206.63356
Epoch 96: Val Loss 206.31873
Epoch 97: Val Loss 205.99887
Epoch 98: Val Loss 205.67468
Epoch 99: Val Loss 205.34596
{'MSE - mean': 212.28644082412924, 'MSE - std': 6.611313820302873, 'R2 - mean': -319.37639371568133, 'R2 - std': 39.26196444531546} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 212.28644082412924 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 13 with value: 0.6118930330337913.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.50220
Epoch 1: Val Loss 228.48579
Epoch 2: Val Loss 227.38052
Epoch 3: Val Loss 226.17525
Epoch 4: Val Loss 224.81596
Epoch 5: Val Loss 223.28926
Epoch 6: Val Loss 221.55034
Epoch 7: Val Loss 219.58643
Epoch 8: Val Loss 217.32761
Epoch 9: Val Loss 214.77731
Epoch 10: Val Loss 211.86713
Epoch 11: Val Loss 208.56009
Epoch 12: Val Loss 204.79744
Epoch 13: Val Loss 200.55888
Epoch 14: Val Loss 195.76324
Epoch 15: Val Loss 190.38100
Epoch 16: Val Loss 184.30684
Epoch 17: Val Loss 177.48079
Epoch 18: Val Loss 169.81435
Epoch 19: Val Loss 161.30739
Epoch 20: Val Loss 151.88907
Epoch 21: Val Loss 141.55016
Epoch 22: Val Loss 130.33769
Epoch 23: Val Loss 118.31210
Epoch 24: Val Loss 105.62177
Epoch 25: Val Loss 92.48199
Epoch 26: Val Loss 79.11559
Epoch 27: Val Loss 65.85001
Epoch 28: Val Loss 53.07949
Epoch 29: Val Loss 41.20012
Epoch 30: Val Loss 30.64281
Epoch 31: Val Loss 21.79731
Epoch 32: Val Loss 14.88765
Epoch 33: Val Loss 9.95368
Epoch 34: Val Loss 6.85735
Epoch 35: Val Loss 5.22509
Epoch 36: Val Loss 4.48059
Epoch 37: Val Loss 4.14367
Epoch 38: Val Loss 3.86921
Epoch 39: Val Loss 3.52342
Epoch 40: Val Loss 3.10172
Epoch 41: Val Loss 2.69083
Epoch 42: Val Loss 2.34257
Epoch 43: Val Loss 2.07036
Epoch 44: Val Loss 1.85641
Epoch 45: Val Loss 1.68484
Epoch 46: Val Loss 1.53297
Epoch 47: Val Loss 1.39607
Epoch 48: Val Loss 1.27304
Epoch 49: Val Loss 1.16334
Epoch 50: Val Loss 1.06799
Epoch 51: Val Loss 0.98643
Epoch 52: Val Loss 0.91638
Epoch 53: Val Loss 0.85548
Epoch 54: Val Loss 0.80222
Epoch 55: Val Loss 0.75603
Epoch 56: Val Loss 0.71558
Epoch 57: Val Loss 0.68058
Epoch 58: Val Loss 0.65038
Epoch 59: Val Loss 0.62412
Epoch 60: Val Loss 0.60138
Epoch 61: Val Loss 0.58248
Epoch 62: Val Loss 0.56618
Epoch 63: Val Loss 0.55130
Epoch 64: Val Loss 0.53828
Epoch 65: Val Loss 0.52709
Epoch 66: Val Loss 0.51808
Epoch 67: Val Loss 0.51007
Epoch 68: Val Loss 0.50390
Epoch 69: Val Loss 0.49879
Epoch 70: Val Loss 0.49516
Epoch 71: Val Loss 0.49264
Epoch 72: Val Loss 0.49016
Epoch 73: Val Loss 0.48808
Epoch 74: Val Loss 0.48657
Epoch 75: Val Loss 0.48492
Epoch 76: Val Loss 0.48392
Epoch 77: Val Loss 0.48310
Epoch 78: Val Loss 0.48273
Epoch 79: Val Loss 0.48185
Epoch 80: Val Loss 0.48200
Epoch 81: Val Loss 0.48218
Epoch 82: Val Loss 0.48223
Epoch 83: Val Loss 0.48116
Epoch 84: Val Loss 0.48092
Epoch 85: Val Loss 0.48115
Epoch 86: Val Loss 0.48095
Epoch 87: Val Loss 0.48097
Epoch 88: Val Loss 0.48078
Epoch 89: Val Loss 0.48197
Epoch 90: Val Loss 0.48165
Epoch 91: Val Loss 0.48239
Epoch 92: Val Loss 0.48323
Epoch 93: Val Loss 0.48493
Epoch 94: Val Loss 0.48544
Epoch 95: Val Loss 0.48524
Epoch 96: Val Loss 0.48537
Epoch 97: Val Loss 0.48655
Epoch 98: Val Loss 0.48663
Epoch 99: Val Loss 0.48641
{'MSE - mean': 0.48077854845675166, 'MSE - std': 0.0, 'R2 - mean': 0.12514200641887163, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.46490
Epoch 1: Val Loss 230.09172
Epoch 2: Val Loss 228.56047
Epoch 3: Val Loss 226.83725
Epoch 4: Val Loss 224.89577
Epoch 5: Val Loss 222.68584
Epoch 6: Val Loss 220.17609
Epoch 7: Val Loss 217.32477
Epoch 8: Val Loss 214.11734
Epoch 9: Val Loss 210.48596
Epoch 10: Val Loss 206.36514
Epoch 11: Val Loss 201.67073
Epoch 12: Val Loss 196.32545
Epoch 13: Val Loss 190.22646
Epoch 14: Val Loss 183.31473
Epoch 15: Val Loss 175.52582
Epoch 16: Val Loss 166.76964
Epoch 17: Val Loss 156.97495
Epoch 18: Val Loss 146.12399
Epoch 19: Val Loss 134.22084
Epoch 20: Val Loss 121.28739
Epoch 21: Val Loss 107.42156
Epoch 22: Val Loss 92.87916
Epoch 23: Val Loss 77.99157
Epoch 24: Val Loss 63.15471
Epoch 25: Val Loss 48.93719
Epoch 26: Val Loss 35.93462
Epoch 27: Val Loss 24.79164
Epoch 28: Val Loss 15.96486
Epoch 29: Val Loss 9.74307
Epoch 30: Val Loss 5.96846
Epoch 31: Val Loss 4.13532
Epoch 32: Val Loss 3.46993
Epoch 33: Val Loss 3.25711
Epoch 34: Val Loss 3.03553
Epoch 35: Val Loss 2.69711
Epoch 36: Val Loss 2.30602
Epoch 37: Val Loss 1.96725
Epoch 38: Val Loss 1.72792
Epoch 39: Val Loss 1.57341
Epoch 40: Val Loss 1.46105
Epoch 41: Val Loss 1.36294
Epoch 42: Val Loss 1.26012
Epoch 43: Val Loss 1.15849
Epoch 44: Val Loss 1.06218
Epoch 45: Val Loss 0.97954
Epoch 46: Val Loss 0.91051
Epoch 47: Val Loss 0.85309
Epoch 48: Val Loss 0.80920
Epoch 49: Val Loss 0.77343
Epoch 50: Val Loss 0.74579
Epoch 51: Val Loss 0.72304
Epoch 52: Val Loss 0.70400
Epoch 53: Val Loss 0.68677
Epoch 54: Val Loss 0.67340
Epoch 55: Val Loss 0.66236
Epoch 56: Val Loss 0.65303
Epoch 57: Val Loss 0.64499
Epoch 58: Val Loss 0.63768
Epoch 59: Val Loss 0.63193
Epoch 60: Val Loss 0.62612
Epoch 61: Val Loss 0.62149
Epoch 62: Val Loss 0.61709
Epoch 63: Val Loss 0.61323
Epoch 64: Val Loss 0.60948
Epoch 65: Val Loss 0.60692
Epoch 66: Val Loss 0.60449
Epoch 67: Val Loss 0.60286
Epoch 68: Val Loss 0.60133
Epoch 69: Val Loss 0.59915
Epoch 70: Val Loss 0.59733
Epoch 71: Val Loss 0.59592
Epoch 72: Val Loss 0.59398
Epoch 73: Val Loss 0.59295
Epoch 74: Val Loss 0.59217
Epoch 75: Val Loss 0.59152
Epoch 76: Val Loss 0.59168
Epoch 77: Val Loss 0.59089
Epoch 78: Val Loss 0.59045
Epoch 79: Val Loss 0.58928
Epoch 80: Val Loss 0.58842
Epoch 81: Val Loss 0.58778
Epoch 82: Val Loss 0.58684
Epoch 83: Val Loss 0.58650
Epoch 84: Val Loss 0.58658
Epoch 85: Val Loss 0.58638
Epoch 86: Val Loss 0.58655
Epoch 87: Val Loss 0.58663
Epoch 88: Val Loss 0.58750
Epoch 89: Val Loss 0.58733
Epoch 90: Val Loss 0.58815
Epoch 91: Val Loss 0.58868
Epoch 92: Val Loss 0.58814
Epoch 93: Val Loss 0.58835
Epoch 94: Val Loss 0.58861
Epoch 95: Val Loss 0.58787
Epoch 96: Val Loss 0.58792
Epoch 97: Val Loss 0.58872
Epoch 98: Val Loss 0.58769
Epoch 99: Val Loss 0.58789
{'MSE - mean': 0.5335783956834527, 'MSE - std': 0.05279984722670103, 'R2 - mean': 0.16553781344459956, 'R2 - std': 0.040395807025727926} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 214.79805
Epoch 1: Val Loss 214.22208
Epoch 2: Val Loss 213.65108
Epoch 3: Val Loss 213.05943
Epoch 4: Val Loss 212.43172
Epoch 5: Val Loss 211.74846
Epoch 6: Val Loss 210.97382
Epoch 7: Val Loss 210.09683
Epoch 8: Val Loss 209.10159
Epoch 9: Val Loss 207.93718
Epoch 10: Val Loss 206.56868
Epoch 11: Val Loss 204.98456
Epoch 12: Val Loss 203.14099
Epoch 13: Val Loss 200.97319
Epoch 14: Val Loss 198.42441
Epoch 15: Val Loss 195.43507
Epoch 16: Val Loss 191.96111
Epoch 17: Val Loss 187.91107
Epoch 18: Val Loss 183.24788
Epoch 19: Val Loss 177.87111
Epoch 20: Val Loss 171.70190
Epoch 21: Val Loss 164.72456
Epoch 22: Val Loss 156.84462
Epoch 23: Val Loss 147.98956
Epoch 24: Val Loss 138.17485
Epoch 25: Val Loss 127.34278
Epoch 26: Val Loss 115.60147
Epoch 27: Val Loss 103.01898
Epoch 28: Val Loss 89.81797
Epoch 29: Val Loss 76.30601
Epoch 30: Val Loss 62.83632
Epoch 31: Val Loss 49.84910
Epoch 32: Val Loss 37.87099
Epoch 33: Val Loss 27.46014
Epoch 34: Val Loss 19.06646
Epoch 35: Val Loss 12.97919
Epoch 36: Val Loss 9.10799
Epoch 37: Val Loss 7.08250
Epoch 38: Val Loss 6.19902
Epoch 39: Val Loss 5.78713
Epoch 40: Val Loss 5.37350
Epoch 41: Val Loss 4.81255
Epoch 42: Val Loss 4.18535
Epoch 43: Val Loss 3.61296
Epoch 44: Val Loss 3.15096
Epoch 45: Val Loss 2.79661
Epoch 46: Val Loss 2.51939
Epoch 47: Val Loss 2.28543
Epoch 48: Val Loss 2.07693
Epoch 49: Val Loss 1.88652
Epoch 50: Val Loss 1.71528
Epoch 51: Val Loss 1.56785
Epoch 52: Val Loss 1.44032
Epoch 53: Val Loss 1.33375
Epoch 54: Val Loss 1.24252
Epoch 55: Val Loss 1.16348
Epoch 56: Val Loss 1.09597
Epoch 57: Val Loss 1.03699
Epoch 58: Val Loss 0.98540
Epoch 59: Val Loss 0.94092
Epoch 60: Val Loss 0.90400
Epoch 61: Val Loss 0.87350
Epoch 62: Val Loss 0.84718
Epoch 63: Val Loss 0.82487
Epoch 64: Val Loss 0.80672
Epoch 65: Val Loss 0.79119
Epoch 66: Val Loss 0.77852
Epoch 67: Val Loss 0.76818
Epoch 68: Val Loss 0.75905
Epoch 69: Val Loss 0.75135
Epoch 70: Val Loss 0.74503
Epoch 71: Val Loss 0.73875
Epoch 72: Val Loss 0.73365
Epoch 73: Val Loss 0.72963
Epoch 74: Val Loss 0.72625
Epoch 75: Val Loss 0.72355
Epoch 76: Val Loss 0.72169
Epoch 77: Val Loss 0.71953
Epoch 78: Val Loss 0.71719
Epoch 79: Val Loss 0.71450
Epoch 80: Val Loss 0.71244
Epoch 81: Val Loss 0.71036
Epoch 82: Val Loss 0.70909
Epoch 83: Val Loss 0.70692
Epoch 84: Val Loss 0.70556
Epoch 85: Val Loss 0.70406
Epoch 86: Val Loss 0.70262
Epoch 87: Val Loss 0.70046
Epoch 88: Val Loss 0.69955
Epoch 89: Val Loss 0.69804
Epoch 90: Val Loss 0.69651
Epoch 91: Val Loss 0.69583
Epoch 92: Val Loss 0.69518
Epoch 93: Val Loss 0.69346
Epoch 94: Val Loss 0.69180
Epoch 95: Val Loss 0.69105
Epoch 96: Val Loss 0.68992
Epoch 97: Val Loss 0.68938
Epoch 98: Val Loss 0.68809
Epoch 99: Val Loss 0.68812
{'MSE - mean': 0.5850830660419665, 'MSE - std': 0.08464048311684925, 'R2 - mean': 0.11077844829842291, 'R2 - std': 0.08417278039404728} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.09775
Epoch 1: Val Loss 229.38876
Epoch 2: Val Loss 228.72426
Epoch 3: Val Loss 228.08359
Epoch 4: Val Loss 227.44670
Epoch 5: Val Loss 226.80048
Epoch 6: Val Loss 226.12300
Epoch 7: Val Loss 225.39662
Epoch 8: Val Loss 224.59598
Epoch 9: Val Loss 223.70438
Epoch 10: Val Loss 222.69611
Epoch 11: Val Loss 221.54190
Epoch 12: Val Loss 220.20549
Epoch 13: Val Loss 218.65674
Epoch 14: Val Loss 216.86417
Epoch 15: Val Loss 214.77599
Epoch 16: Val Loss 212.34102
Epoch 17: Val Loss 209.51125
Epoch 18: Val Loss 206.22084
Epoch 19: Val Loss 202.40057
Epoch 20: Val Loss 198.00760
Epoch 21: Val Loss 192.96864
Epoch 22: Val Loss 187.21977
Epoch 23: Val Loss 180.70386
Epoch 24: Val Loss 173.33835
Epoch 25: Val Loss 165.08009
Epoch 26: Val Loss 155.89230
Epoch 27: Val Loss 145.77054
Epoch 28: Val Loss 134.74806
Epoch 29: Val Loss 122.89237
Epoch 30: Val Loss 110.32503
Epoch 31: Val Loss 97.20322
Epoch 32: Val Loss 83.77286
Epoch 33: Val Loss 70.38517
Epoch 34: Val Loss 57.41345
Epoch 35: Val Loss 45.27545
Epoch 36: Val Loss 34.38428
Epoch 37: Val Loss 25.13836
Epoch 38: Val Loss 17.84146
Epoch 39: Val Loss 12.56951
Epoch 40: Val Loss 9.12397
Epoch 41: Val Loss 7.12153
Epoch 42: Val Loss 6.04841
Epoch 43: Val Loss 5.45393
Epoch 44: Val Loss 5.01685
Epoch 45: Val Loss 4.61855
Epoch 46: Val Loss 4.24476
Epoch 47: Val Loss 3.92096
Epoch 48: Val Loss 3.66552
Epoch 49: Val Loss 3.45428
Epoch 50: Val Loss 3.26975
Epoch 51: Val Loss 3.09662
Epoch 52: Val Loss 2.91661
Epoch 53: Val Loss 2.73452
Epoch 54: Val Loss 2.55176
Epoch 55: Val Loss 2.37340
Epoch 56: Val Loss 2.20910
Epoch 57: Val Loss 2.05828
Epoch 58: Val Loss 1.92289
Epoch 59: Val Loss 1.79476
Epoch 60: Val Loss 1.68676
Epoch 61: Val Loss 1.58726
Epoch 62: Val Loss 1.49680
Epoch 63: Val Loss 1.41275
Epoch 64: Val Loss 1.33998
Epoch 65: Val Loss 1.27270
Epoch 66: Val Loss 1.20989
Epoch 67: Val Loss 1.15689
Epoch 68: Val Loss 1.10730
Epoch 69: Val Loss 1.06115
Epoch 70: Val Loss 1.01690
Epoch 71: Val Loss 0.97695
Epoch 72: Val Loss 0.94027
Epoch 73: Val Loss 0.90742
Epoch 74: Val Loss 0.87762
Epoch 75: Val Loss 0.84948
Epoch 76: Val Loss 0.82572
Epoch 77: Val Loss 0.80424
Epoch 78: Val Loss 0.78394
Epoch 79: Val Loss 0.76463
Epoch 80: Val Loss 0.74778
Epoch 81: Val Loss 0.73166
Epoch 82: Val Loss 0.71707
Epoch 83: Val Loss 0.70419
Epoch 84: Val Loss 0.69261
Epoch 85: Val Loss 0.68247
Epoch 86: Val Loss 0.67356
Epoch 87: Val Loss 0.66573
Epoch 88: Val Loss 0.65994
Epoch 89: Val Loss 0.65414
Epoch 90: Val Loss 0.64841
Epoch 91: Val Loss 0.64340
Epoch 92: Val Loss 0.63923
Epoch 93: Val Loss 0.63538
Epoch 94: Val Loss 0.63208
Epoch 95: Val Loss 0.62912
Epoch 96: Val Loss 0.62568
Epoch 97: Val Loss 0.62330
Epoch 98: Val Loss 0.62052
Epoch 99: Val Loss 0.61913
{'MSE - mean': 0.5935950009103033, 'MSE - std': 0.07476876113925292, 'R2 - mean': 0.13566834526663657, 'R2 - std': 0.0846895131212434} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.10062
Epoch 1: Val Loss 226.37610
Epoch 2: Val Loss 225.58049
Epoch 3: Val Loss 224.71013
Epoch 4: Val Loss 223.74147
Epoch 5: Val Loss 222.64989
Epoch 6: Val Loss 221.43028
Epoch 7: Val Loss 220.06514
Epoch 8: Val Loss 218.52200
Epoch 9: Val Loss 216.76956
Epoch 10: Val Loss 214.75285
Epoch 11: Val Loss 212.42200
Epoch 12: Val Loss 209.70322
Epoch 13: Val Loss 206.53888
Epoch 14: Val Loss 202.85417
Epoch 15: Val Loss 198.58450
Epoch 16: Val Loss 193.62398
Epoch 17: Val Loss 187.89449
Epoch 18: Val Loss 181.30516
Epoch 19: Val Loss 173.76088
Epoch 20: Val Loss 165.15402
Epoch 21: Val Loss 155.42863
Epoch 22: Val Loss 144.56021
Epoch 23: Val Loss 132.59698
Epoch 24: Val Loss 119.62325
Epoch 25: Val Loss 105.82583
Epoch 26: Val Loss 91.43612
Epoch 27: Val Loss 76.73378
Epoch 28: Val Loss 62.17877
Epoch 29: Val Loss 48.33216
Epoch 30: Val Loss 35.72105
Epoch 31: Val Loss 24.90745
Epoch 32: Val Loss 16.36700
Epoch 33: Val Loss 10.32951
Epoch 34: Val Loss 6.66710
Epoch 35: Val Loss 4.90728
Epoch 36: Val Loss 4.31731
Epoch 37: Val Loss 4.15172
Epoch 38: Val Loss 3.96677
Epoch 39: Val Loss 3.62259
Epoch 40: Val Loss 3.16728
Epoch 41: Val Loss 2.75246
Epoch 42: Val Loss 2.41934
Epoch 43: Val Loss 2.18539
Epoch 44: Val Loss 2.01761
Epoch 45: Val Loss 1.88453
Epoch 46: Val Loss 1.76265
Epoch 47: Val Loss 1.63841
Epoch 48: Val Loss 1.51887
Epoch 49: Val Loss 1.41033
Epoch 50: Val Loss 1.31281
Epoch 51: Val Loss 1.22642
Epoch 52: Val Loss 1.15345
Epoch 53: Val Loss 1.09091
Epoch 54: Val Loss 1.03609
Epoch 55: Val Loss 0.98827
Epoch 56: Val Loss 0.94656
Epoch 57: Val Loss 0.90934
Epoch 58: Val Loss 0.87584
Epoch 59: Val Loss 0.84714
Epoch 60: Val Loss 0.81988
Epoch 61: Val Loss 0.79515
Epoch 62: Val Loss 0.77330
Epoch 63: Val Loss 0.75367
Epoch 64: Val Loss 0.73648
Epoch 65: Val Loss 0.72062
Epoch 66: Val Loss 0.70630
Epoch 67: Val Loss 0.69475
Epoch 68: Val Loss 0.68454
Epoch 69: Val Loss 0.67540
Epoch 70: Val Loss 0.66783
Epoch 71: Val Loss 0.66092
Epoch 72: Val Loss 0.65482
Epoch 73: Val Loss 0.64952
Epoch 74: Val Loss 0.64454
Epoch 75: Val Loss 0.64000
Epoch 76: Val Loss 0.63476
Epoch 77: Val Loss 0.62977
Epoch 78: Val Loss 0.62523
Epoch 79: Val Loss 0.62114
Epoch 80: Val Loss 0.61757
Epoch 81: Val Loss 0.61458
Epoch 82: Val Loss 0.61259
Epoch 83: Val Loss 0.61127
Epoch 84: Val Loss 0.60967
Epoch 85: Val Loss 0.60744
Epoch 86: Val Loss 0.60536
Epoch 87: Val Loss 0.60442
Epoch 88: Val Loss 0.60328
Epoch 89: Val Loss 0.60168
Epoch 90: Val Loss 0.59973
Epoch 91: Val Loss 0.59941
Epoch 92: Val Loss 0.59821
Epoch 93: Val Loss 0.59738
Epoch 94: Val Loss 0.59701
Epoch 95: Val Loss 0.59533
Epoch 96: Val Loss 0.59364
Epoch 97: Val Loss 0.59347
Epoch 98: Val Loss 0.59277
Epoch 99: Val Loss 0.59290
{'MSE - mean': 0.5934298657769321, 'MSE - std': 0.0668760285325618, 'R2 - mean': 0.11178792439138438, 'R2 - std': 0.08954858408086265} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 0.5934298657769321 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 17 with value: 0.5934298657769321.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.98781
Epoch 1: Val Loss 229.92615
Epoch 2: Val Loss 229.86423
Epoch 3: Val Loss 229.80101
Epoch 4: Val Loss 229.73689
Epoch 5: Val Loss 229.67172
Epoch 6: Val Loss 229.60558
Epoch 7: Val Loss 229.53859
Epoch 8: Val Loss 229.47098
Epoch 9: Val Loss 229.40237
Epoch 10: Val Loss 229.33337
Epoch 11: Val Loss 229.26340
Epoch 12: Val Loss 229.19246
Epoch 13: Val Loss 229.12057
Epoch 14: Val Loss 229.04750
Epoch 15: Val Loss 228.97322
Epoch 16: Val Loss 228.89758
Epoch 17: Val Loss 228.82101
Epoch 18: Val Loss 228.74265
Epoch 19: Val Loss 228.66246
Epoch 20: Val Loss 228.58102
Epoch 21: Val Loss 228.49774
Epoch 22: Val Loss 228.41331
Epoch 23: Val Loss 228.32715
Epoch 24: Val Loss 228.23938
Epoch 25: Val Loss 228.15009
Epoch 26: Val Loss 228.05887
Epoch 27: Val Loss 227.96568
Epoch 28: Val Loss 227.87032
Epoch 29: Val Loss 227.77341
Epoch 30: Val Loss 227.67444
Epoch 31: Val Loss 227.57330
Epoch 32: Val Loss 227.47040
Epoch 33: Val Loss 227.36546
Epoch 34: Val Loss 227.25821
Epoch 35: Val Loss 227.14864
Epoch 36: Val Loss 227.03653
Epoch 37: Val Loss 226.92210
Epoch 38: Val Loss 226.80597
Epoch 39: Val Loss 226.68790
Epoch 40: Val Loss 226.56822
Epoch 41: Val Loss 226.44604
Epoch 42: Val Loss 226.32182
Epoch 43: Val Loss 226.19495
Epoch 44: Val Loss 226.06627
Epoch 45: Val Loss 225.93544
Epoch 46: Val Loss 225.80183
Epoch 47: Val Loss 225.66490
Epoch 48: Val Loss 225.52495
Epoch 49: Val Loss 225.38116
Epoch 50: Val Loss 225.23419
Epoch 51: Val Loss 225.08374
Epoch 52: Val Loss 224.93039
Epoch 53: Val Loss 224.77338
Epoch 54: Val Loss 224.61322
Epoch 55: Val Loss 224.44852
Epoch 56: Val Loss 224.28081
Epoch 57: Val Loss 224.10965
Epoch 58: Val Loss 223.93504
Epoch 59: Val Loss 223.75601
Epoch 60: Val Loss 223.57271
Epoch 61: Val Loss 223.38524
Epoch 62: Val Loss 223.19380
Epoch 63: Val Loss 222.99892
Epoch 64: Val Loss 222.80042
Epoch 65: Val Loss 222.59706
Epoch 66: Val Loss 222.38881
Epoch 67: Val Loss 222.17578
Epoch 68: Val Loss 221.95605
Epoch 69: Val Loss 221.73022
Epoch 70: Val Loss 221.49950
Epoch 71: Val Loss 221.26329
Epoch 72: Val Loss 221.02084
Epoch 73: Val Loss 220.77226
Epoch 74: Val Loss 220.51891
Epoch 75: Val Loss 220.25909
Epoch 76: Val Loss 219.99306
Epoch 77: Val Loss 219.72078
Epoch 78: Val Loss 219.44109
Epoch 79: Val Loss 219.15648
Epoch 80: Val Loss 218.86739
Epoch 81: Val Loss 218.57321
Epoch 82: Val Loss 218.27219
Epoch 83: Val Loss 217.96521
Epoch 84: Val Loss 217.65012
Epoch 85: Val Loss 217.32840
Epoch 86: Val Loss 216.99901
Epoch 87: Val Loss 216.66177
Epoch 88: Val Loss 216.31615
Epoch 89: Val Loss 215.96231
Epoch 90: Val Loss 215.60088
Epoch 91: Val Loss 215.22794
Epoch 92: Val Loss 214.84604
Epoch 93: Val Loss 214.45547
Epoch 94: Val Loss 214.05627
Epoch 95: Val Loss 213.64835
Epoch 96: Val Loss 213.23164
Epoch 97: Val Loss 212.80537
Epoch 98: Val Loss 212.36646
Epoch 99: Val Loss 211.91788
{'MSE - mean': 211.91788969048523, 'MSE - std': 0.0, 'R2 - mean': -384.62049070964787, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.33113
Epoch 1: Val Loss 229.23726
Epoch 2: Val Loss 229.14313
Epoch 3: Val Loss 229.04828
Epoch 4: Val Loss 228.95265
Epoch 5: Val Loss 228.85692
Epoch 6: Val Loss 228.76093
Epoch 7: Val Loss 228.66476
Epoch 8: Val Loss 228.56805
Epoch 9: Val Loss 228.47093
Epoch 10: Val Loss 228.37329
Epoch 11: Val Loss 228.27490
Epoch 12: Val Loss 228.17636
Epoch 13: Val Loss 228.07816
Epoch 14: Val Loss 227.97856
Epoch 15: Val Loss 227.87730
Epoch 16: Val Loss 227.77570
Epoch 17: Val Loss 227.67389
Epoch 18: Val Loss 227.57173
Epoch 19: Val Loss 227.46989
Epoch 20: Val Loss 227.36635
Epoch 21: Val Loss 227.26131
Epoch 22: Val Loss 227.15421
Epoch 23: Val Loss 227.04564
Epoch 24: Val Loss 226.93529
Epoch 25: Val Loss 226.82312
Epoch 26: Val Loss 226.70998
Epoch 27: Val Loss 226.59537
Epoch 28: Val Loss 226.47856
Epoch 29: Val Loss 226.36069
Epoch 30: Val Loss 226.24103
Epoch 31: Val Loss 226.12027
Epoch 32: Val Loss 225.99677
Epoch 33: Val Loss 225.86958
Epoch 34: Val Loss 225.74095
Epoch 35: Val Loss 225.60934
Epoch 36: Val Loss 225.47580
Epoch 37: Val Loss 225.33975
Epoch 38: Val Loss 225.20122
Epoch 39: Val Loss 225.06079
Epoch 40: Val Loss 224.91653
Epoch 41: Val Loss 224.76959
Epoch 42: Val Loss 224.61948
Epoch 43: Val Loss 224.46608
Epoch 44: Val Loss 224.31049
Epoch 45: Val Loss 224.15256
Epoch 46: Val Loss 223.99214
Epoch 47: Val Loss 223.82835
Epoch 48: Val Loss 223.66151
Epoch 49: Val Loss 223.49168
Epoch 50: Val Loss 223.31750
Epoch 51: Val Loss 223.14102
Epoch 52: Val Loss 222.96187
Epoch 53: Val Loss 222.77838
Epoch 54: Val Loss 222.59149
Epoch 55: Val Loss 222.40083
Epoch 56: Val Loss 222.20712
Epoch 57: Val Loss 222.01079
Epoch 58: Val Loss 221.81117
Epoch 59: Val Loss 221.60892
Epoch 60: Val Loss 221.40338
Epoch 61: Val Loss 221.19370
Epoch 62: Val Loss 220.98167
Epoch 63: Val Loss 220.76605
Epoch 64: Val Loss 220.54564
Epoch 65: Val Loss 220.32184
Epoch 66: Val Loss 220.09323
Epoch 67: Val Loss 219.86081
Epoch 68: Val Loss 219.62440
Epoch 69: Val Loss 219.38443
Epoch 70: Val Loss 219.14131
Epoch 71: Val Loss 218.89305
Epoch 72: Val Loss 218.64186
Epoch 73: Val Loss 218.38670
Epoch 74: Val Loss 218.12683
Epoch 75: Val Loss 217.86226
Epoch 76: Val Loss 217.59428
Epoch 77: Val Loss 217.32248
Epoch 78: Val Loss 217.04660
Epoch 79: Val Loss 216.76588
Epoch 80: Val Loss 216.47989
Epoch 81: Val Loss 216.18939
Epoch 82: Val Loss 215.89426
Epoch 83: Val Loss 215.59489
Epoch 84: Val Loss 215.29033
Epoch 85: Val Loss 214.98105
Epoch 86: Val Loss 214.66806
Epoch 87: Val Loss 214.35107
Epoch 88: Val Loss 214.02930
Epoch 89: Val Loss 213.70267
Epoch 90: Val Loss 213.37143
Epoch 91: Val Loss 213.03516
Epoch 92: Val Loss 212.69289
Epoch 93: Val Loss 212.34505
Epoch 94: Val Loss 211.99222
Epoch 95: Val Loss 211.63557
Epoch 96: Val Loss 211.27347
Epoch 97: Val Loss 210.90559
Epoch 98: Val Loss 210.53374
Epoch 99: Val Loss 210.15504
{'MSE - mean': 211.0364756292082, 'MSE - std': 0.881414061276999, 'R2 - mean': -334.10497652602453, 'R2 - std': 50.51551418362334} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 237.33955
Epoch 1: Val Loss 237.23361
Epoch 2: Val Loss 237.12901
Epoch 3: Val Loss 237.02530
Epoch 4: Val Loss 236.92227
Epoch 5: Val Loss 236.82068
Epoch 6: Val Loss 236.71980
Epoch 7: Val Loss 236.61949
Epoch 8: Val Loss 236.52025
Epoch 9: Val Loss 236.42230
Epoch 10: Val Loss 236.32520
Epoch 11: Val Loss 236.22842
Epoch 12: Val Loss 236.13219
Epoch 13: Val Loss 236.03722
Epoch 14: Val Loss 235.94301
Epoch 15: Val Loss 235.84935
Epoch 16: Val Loss 235.75659
Epoch 17: Val Loss 235.66460
Epoch 18: Val Loss 235.57326
Epoch 19: Val Loss 235.48219
Epoch 20: Val Loss 235.39180
Epoch 21: Val Loss 235.30128
Epoch 22: Val Loss 235.21086
Epoch 23: Val Loss 235.12103
Epoch 24: Val Loss 235.03128
Epoch 25: Val Loss 234.94185
Epoch 26: Val Loss 234.85284
Epoch 27: Val Loss 234.76364
Epoch 28: Val Loss 234.67444
Epoch 29: Val Loss 234.58525
Epoch 30: Val Loss 234.49599
Epoch 31: Val Loss 234.40675
Epoch 32: Val Loss 234.31714
Epoch 33: Val Loss 234.22720
Epoch 34: Val Loss 234.13753
Epoch 35: Val Loss 234.04742
Epoch 36: Val Loss 233.95738
Epoch 37: Val Loss 233.86699
Epoch 38: Val Loss 233.77635
Epoch 39: Val Loss 233.68532
Epoch 40: Val Loss 233.59396
Epoch 41: Val Loss 233.50201
Epoch 42: Val Loss 233.40956
Epoch 43: Val Loss 233.31664
Epoch 44: Val Loss 233.22336
Epoch 45: Val Loss 233.12958
Epoch 46: Val Loss 233.03497
Epoch 47: Val Loss 232.93987
Epoch 48: Val Loss 232.84421
Epoch 49: Val Loss 232.74788
Epoch 50: Val Loss 232.65091
Epoch 51: Val Loss 232.55312
Epoch 52: Val Loss 232.45421
Epoch 53: Val Loss 232.35463
Epoch 54: Val Loss 232.25427
Epoch 55: Val Loss 232.15331
Epoch 56: Val Loss 232.05183
Epoch 57: Val Loss 231.95003
Epoch 58: Val Loss 231.84799
Epoch 59: Val Loss 231.74454
Epoch 60: Val Loss 231.63963
Epoch 61: Val Loss 231.53386
Epoch 62: Val Loss 231.42668
Epoch 63: Val Loss 231.31789
Epoch 64: Val Loss 231.20767
Epoch 65: Val Loss 231.09543
Epoch 66: Val Loss 230.98186
Epoch 67: Val Loss 230.86659
Epoch 68: Val Loss 230.74982
Epoch 69: Val Loss 230.63225
Epoch 70: Val Loss 230.51328
Epoch 71: Val Loss 230.39311
Epoch 72: Val Loss 230.27147
Epoch 73: Val Loss 230.14775
Epoch 74: Val Loss 230.02220
Epoch 75: Val Loss 229.89502
Epoch 76: Val Loss 229.76567
Epoch 77: Val Loss 229.63396
Epoch 78: Val Loss 229.50037
Epoch 79: Val Loss 229.36467
Epoch 80: Val Loss 229.22679
Epoch 81: Val Loss 229.08563
Epoch 82: Val Loss 228.94107
Epoch 83: Val Loss 228.79366
Epoch 84: Val Loss 228.64333
Epoch 85: Val Loss 228.49060
Epoch 86: Val Loss 228.33554
Epoch 87: Val Loss 228.17853
Epoch 88: Val Loss 228.01863
Epoch 89: Val Loss 227.85576
Epoch 90: Val Loss 227.68976
Epoch 91: Val Loss 227.52145
Epoch 92: Val Loss 227.35027
Epoch 93: Val Loss 227.17635
Epoch 94: Val Loss 226.99925
Epoch 95: Val Loss 226.81834
Epoch 96: Val Loss 226.63402
Epoch 97: Val Loss 226.44598
Epoch 98: Val Loss 226.25525
Epoch 99: Val Loss 226.06123
{'MSE - mean': 216.04473236544777, 'MSE - std': 7.1192133158883975, 'R2 - mean': -331.7764533260286, 'R2 - std': 41.37699227118543} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.43082
Epoch 1: Val Loss 224.30437
Epoch 2: Val Loss 224.17944
Epoch 3: Val Loss 224.05630
Epoch 4: Val Loss 223.93462
Epoch 5: Val Loss 223.81409
Epoch 6: Val Loss 223.69696
Epoch 7: Val Loss 223.58134
Epoch 8: Val Loss 223.46692
Epoch 9: Val Loss 223.35487
Epoch 10: Val Loss 223.24545
Epoch 11: Val Loss 223.13870
Epoch 12: Val Loss 223.03488
Epoch 13: Val Loss 222.93311
Epoch 14: Val Loss 222.83246
Epoch 15: Val Loss 222.73314
Epoch 16: Val Loss 222.63629
Epoch 17: Val Loss 222.54021
Epoch 18: Val Loss 222.44446
Epoch 19: Val Loss 222.34874
Epoch 20: Val Loss 222.25342
Epoch 21: Val Loss 222.15819
Epoch 22: Val Loss 222.06398
Epoch 23: Val Loss 221.97063
Epoch 24: Val Loss 221.87866
Epoch 25: Val Loss 221.78741
Epoch 26: Val Loss 221.69611
Epoch 27: Val Loss 221.60495
Epoch 28: Val Loss 221.51358
Epoch 29: Val Loss 221.42238
Epoch 30: Val Loss 221.33055
Epoch 31: Val Loss 221.23848
Epoch 32: Val Loss 221.14600
Epoch 33: Val Loss 221.05318
Epoch 34: Val Loss 220.96097
Epoch 35: Val Loss 220.86806
Epoch 36: Val Loss 220.77509
Epoch 37: Val Loss 220.68147
Epoch 38: Val Loss 220.58870
Epoch 39: Val Loss 220.49561
Epoch 40: Val Loss 220.40263
Epoch 41: Val Loss 220.30853
Epoch 42: Val Loss 220.21451
Epoch 43: Val Loss 220.12038
Epoch 44: Val Loss 220.02580
Epoch 45: Val Loss 219.92879
Epoch 46: Val Loss 219.83026
Epoch 47: Val Loss 219.73059
Epoch 48: Val Loss 219.62872
Epoch 49: Val Loss 219.52505
Epoch 50: Val Loss 219.41928
Epoch 51: Val Loss 219.31148
Epoch 52: Val Loss 219.20215
Epoch 53: Val Loss 219.09085
Epoch 54: Val Loss 218.97791
Epoch 55: Val Loss 218.86436
Epoch 56: Val Loss 218.74992
Epoch 57: Val Loss 218.63368
Epoch 58: Val Loss 218.51688
Epoch 59: Val Loss 218.39891
Epoch 60: Val Loss 218.27798
Epoch 61: Val Loss 218.15443
Epoch 62: Val Loss 218.02974
Epoch 63: Val Loss 217.90321
Epoch 64: Val Loss 217.77336
Epoch 65: Val Loss 217.64047
Epoch 66: Val Loss 217.50592
Epoch 67: Val Loss 217.36945
Epoch 68: Val Loss 217.23088
Epoch 69: Val Loss 217.09035
Epoch 70: Val Loss 216.94687
Epoch 71: Val Loss 216.80054
Epoch 72: Val Loss 216.65121
Epoch 73: Val Loss 216.49763
Epoch 74: Val Loss 216.34093
Epoch 75: Val Loss 216.18011
Epoch 76: Val Loss 216.01704
Epoch 77: Val Loss 215.85260
Epoch 78: Val Loss 215.68480
Epoch 79: Val Loss 215.51320
Epoch 80: Val Loss 215.33833
Epoch 81: Val Loss 215.16080
Epoch 82: Val Loss 214.98071
Epoch 83: Val Loss 214.79935
Epoch 84: Val Loss 214.61465
Epoch 85: Val Loss 214.42680
Epoch 86: Val Loss 214.23637
Epoch 87: Val Loss 214.04140
Epoch 88: Val Loss 213.84283
Epoch 89: Val Loss 213.64061
Epoch 90: Val Loss 213.43398
Epoch 91: Val Loss 213.22350
Epoch 92: Val Loss 213.01042
Epoch 93: Val Loss 212.79295
Epoch 94: Val Loss 212.57153
Epoch 95: Val Loss 212.34506
Epoch 96: Val Loss 212.11369
Epoch 97: Val Loss 211.87897
Epoch 98: Val Loss 211.64207
Epoch 99: Val Loss 211.40060
{'MSE - mean': 214.88370334236578, 'MSE - std': 6.48508780627975, 'R2 - mean': -315.98935007775054, 'R2 - std': 45.07482117516034} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 226.83949
Epoch 1: Val Loss 226.72806
Epoch 2: Val Loss 226.61604
Epoch 3: Val Loss 226.50380
Epoch 4: Val Loss 226.39044
Epoch 5: Val Loss 226.27652
Epoch 6: Val Loss 226.16171
Epoch 7: Val Loss 226.04588
Epoch 8: Val Loss 225.92879
Epoch 9: Val Loss 225.81018
Epoch 10: Val Loss 225.69032
Epoch 11: Val Loss 225.56885
Epoch 12: Val Loss 225.44597
Epoch 13: Val Loss 225.32214
Epoch 14: Val Loss 225.19670
Epoch 15: Val Loss 225.07043
Epoch 16: Val Loss 224.94266
Epoch 17: Val Loss 224.81340
Epoch 18: Val Loss 224.68288
Epoch 19: Val Loss 224.55051
Epoch 20: Val Loss 224.41681
Epoch 21: Val Loss 224.28098
Epoch 22: Val Loss 224.14430
Epoch 23: Val Loss 224.00624
Epoch 24: Val Loss 223.86650
Epoch 25: Val Loss 223.72456
Epoch 26: Val Loss 223.58061
Epoch 27: Val Loss 223.43477
Epoch 28: Val Loss 223.28735
Epoch 29: Val Loss 223.13817
Epoch 30: Val Loss 222.98706
Epoch 31: Val Loss 222.83348
Epoch 32: Val Loss 222.67778
Epoch 33: Val Loss 222.52010
Epoch 34: Val Loss 222.36049
Epoch 35: Val Loss 222.19872
Epoch 36: Val Loss 222.03481
Epoch 37: Val Loss 221.86781
Epoch 38: Val Loss 221.69946
Epoch 39: Val Loss 221.52808
Epoch 40: Val Loss 221.35417
Epoch 41: Val Loss 221.17769
Epoch 42: Val Loss 220.99861
Epoch 43: Val Loss 220.81625
Epoch 44: Val Loss 220.63063
Epoch 45: Val Loss 220.44249
Epoch 46: Val Loss 220.25160
Epoch 47: Val Loss 220.05719
Epoch 48: Val Loss 219.85931
Epoch 49: Val Loss 219.65817
Epoch 50: Val Loss 219.45355
Epoch 51: Val Loss 219.24615
Epoch 52: Val Loss 219.03546
Epoch 53: Val Loss 218.82043
Epoch 54: Val Loss 218.60396
Epoch 55: Val Loss 218.38324
Epoch 56: Val Loss 218.15831
Epoch 57: Val Loss 217.92969
Epoch 58: Val Loss 217.69807
Epoch 59: Val Loss 217.46277
Epoch 60: Val Loss 217.22429
Epoch 61: Val Loss 216.98175
Epoch 62: Val Loss 216.73582
Epoch 63: Val Loss 216.48552
Epoch 64: Val Loss 216.23134
Epoch 65: Val Loss 215.97217
Epoch 66: Val Loss 215.70930
Epoch 67: Val Loss 215.44302
Epoch 68: Val Loss 215.17366
Epoch 69: Val Loss 214.89914
Epoch 70: Val Loss 214.62054
Epoch 71: Val Loss 214.33614
Epoch 72: Val Loss 214.04756
Epoch 73: Val Loss 213.75319
Epoch 74: Val Loss 213.45424
Epoch 75: Val Loss 213.15097
Epoch 76: Val Loss 212.84248
Epoch 77: Val Loss 212.52817
Epoch 78: Val Loss 212.20776
Epoch 79: Val Loss 211.88004
Epoch 80: Val Loss 211.54620
Epoch 81: Val Loss 211.20676
Epoch 82: Val Loss 210.86157
Epoch 83: Val Loss 210.51033
Epoch 84: Val Loss 210.15361
Epoch 85: Val Loss 209.79160
Epoch 86: Val Loss 209.42197
Epoch 87: Val Loss 209.04605
Epoch 88: Val Loss 208.66348
Epoch 89: Val Loss 208.27115
Epoch 90: Val Loss 207.87285
Epoch 91: Val Loss 207.46840
Epoch 92: Val Loss 207.05618
Epoch 93: Val Loss 206.63620
Epoch 94: Val Loss 206.20712
Epoch 95: Val Loss 205.77148
Epoch 96: Val Loss 205.32732
Epoch 97: Val Loss 204.87514
Epoch 98: Val Loss 204.41718
Epoch 99: Val Loss 203.94911
{'MSE - mean': 212.69678667972374, 'MSE - std': 7.26467542574974, 'R2 - mean': -320.28446820087737, 'R2 - std': 41.22115672739664} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 212.69678667972374 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 17 with value: 0.5934298657769321.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.94330
Epoch 1: Val Loss 219.42146
Epoch 2: Val Loss 217.71074
Epoch 3: Val Loss 215.78600
Epoch 4: Val Loss 213.63156
Epoch 5: Val Loss 211.20932
Epoch 6: Val Loss 208.46973
Epoch 7: Val Loss 205.33621
Epoch 8: Val Loss 201.79520
Epoch 9: Val Loss 197.81783
Epoch 10: Val Loss 193.32451
Epoch 11: Val Loss 188.19838
Epoch 12: Val Loss 182.36528
Epoch 13: Val Loss 175.75192
Epoch 14: Val Loss 168.25600
Epoch 15: Val Loss 159.82991
Epoch 16: Val Loss 150.48103
Epoch 17: Val Loss 140.17548
Epoch 18: Val Loss 128.91342
Epoch 19: Val Loss 116.72298
Epoch 20: Val Loss 103.72892
Epoch 21: Val Loss 90.07859
Epoch 22: Val Loss 76.00813
Epoch 23: Val Loss 61.91785
Epoch 24: Val Loss 48.25571
Epoch 25: Val Loss 35.55990
Epoch 26: Val Loss 24.44043
Epoch 27: Val Loss 15.46288
Epoch 28: Val Loss 8.91966
Epoch 29: Val Loss 4.80455
Epoch 30: Val Loss 2.78339
Epoch 31: Val Loss 2.11730
Epoch 32: Val Loss 2.05481
Epoch 33: Val Loss 2.03885
Epoch 34: Val Loss 1.87448
Epoch 35: Val Loss 1.59138
Epoch 36: Val Loss 1.29490
Epoch 37: Val Loss 1.07759
Epoch 38: Val Loss 0.94586
Epoch 39: Val Loss 0.87179
Epoch 40: Val Loss 0.82405
Epoch 41: Val Loss 0.78001
Epoch 42: Val Loss 0.73374
Epoch 43: Val Loss 0.68656
Epoch 44: Val Loss 0.64364
Epoch 45: Val Loss 0.61125
Epoch 46: Val Loss 0.58632
Epoch 47: Val Loss 0.56889
Epoch 48: Val Loss 0.55547
Epoch 49: Val Loss 0.54377
Epoch 50: Val Loss 0.53306
Epoch 51: Val Loss 0.52354
Epoch 52: Val Loss 0.51611
Epoch 53: Val Loss 0.50987
Epoch 54: Val Loss 0.50595
Epoch 55: Val Loss 0.50254
Epoch 56: Val Loss 0.49956
Epoch 57: Val Loss 0.49757
Epoch 58: Val Loss 0.49597
Epoch 59: Val Loss 0.49439
Epoch 60: Val Loss 0.49403
Epoch 61: Val Loss 0.49489
Epoch 62: Val Loss 0.49399
Epoch 63: Val Loss 0.49492
Epoch 64: Val Loss 0.49424
Epoch 65: Val Loss 0.49438
Epoch 66: Val Loss 0.49525
Epoch 67: Val Loss 0.49584
Epoch 68: Val Loss 0.49694
Epoch 69: Val Loss 0.49905
Epoch 70: Val Loss 0.49993
Epoch 71: Val Loss 0.50027
Epoch 72: Val Loss 0.50151
Epoch 73: Val Loss 0.50393
Epoch 74: Val Loss 0.50543
Epoch 75: Val Loss 0.50713
Epoch 76: Val Loss 0.50826
Epoch 77: Val Loss 0.50808
Epoch 78: Val Loss 0.50771
Epoch 79: Val Loss 0.50770
Epoch 80: Val Loss 0.50718
Epoch 81: Val Loss 0.50667
Epoch 82: Val Loss 0.50551
Epoch 83: Val Loss 0.50605
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 0.4939927542703995, 'MSE - std': 0.0, 'R2 - mean': 0.10109652098279254, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.44785
Epoch 1: Val Loss 219.49646
Epoch 2: Val Loss 218.49973
Epoch 3: Val Loss 217.43881
Epoch 4: Val Loss 216.28154
Epoch 5: Val Loss 215.00879
Epoch 6: Val Loss 213.62251
Epoch 7: Val Loss 212.07054
Epoch 8: Val Loss 210.33963
Epoch 9: Val Loss 208.39369
Epoch 10: Val Loss 206.19943
Epoch 11: Val Loss 203.70250
Epoch 12: Val Loss 200.87778
Epoch 13: Val Loss 197.68890
Epoch 14: Val Loss 194.09669
Epoch 15: Val Loss 190.05962
Epoch 16: Val Loss 185.51564
Epoch 17: Val Loss 180.40906
Epoch 18: Val Loss 174.73225
Epoch 19: Val Loss 168.39375
Epoch 20: Val Loss 161.40479
Epoch 21: Val Loss 153.71538
Epoch 22: Val Loss 145.33063
Epoch 23: Val Loss 136.20253
Epoch 24: Val Loss 126.40025
Epoch 25: Val Loss 115.96455
Epoch 26: Val Loss 104.99611
Epoch 27: Val Loss 93.59739
Epoch 28: Val Loss 81.98063
Epoch 29: Val Loss 70.33173
Epoch 30: Val Loss 58.96334
Epoch 31: Val Loss 48.16638
Epoch 32: Val Loss 38.23771
Epoch 33: Val Loss 29.49863
Epoch 34: Val Loss 22.19901
Epoch 35: Val Loss 16.52338
Epoch 36: Val Loss 12.44618
Epoch 37: Val Loss 9.80516
Epoch 38: Val Loss 8.24751
Epoch 39: Val Loss 7.38563
Epoch 40: Val Loss 6.86813
Epoch 41: Val Loss 6.42198
Epoch 42: Val Loss 5.93875
Epoch 43: Val Loss 5.41204
Epoch 44: Val Loss 4.87365
Epoch 45: Val Loss 4.37024
Epoch 46: Val Loss 3.93719
Epoch 47: Val Loss 3.55910
Epoch 48: Val Loss 3.23477
Epoch 49: Val Loss 2.95256
Epoch 50: Val Loss 2.70434
Epoch 51: Val Loss 2.47861
Epoch 52: Val Loss 2.27602
Epoch 53: Val Loss 2.09174
Epoch 54: Val Loss 1.92851
Epoch 55: Val Loss 1.78119
Epoch 56: Val Loss 1.65112
Epoch 57: Val Loss 1.53425
Epoch 58: Val Loss 1.42923
Epoch 59: Val Loss 1.33504
Epoch 60: Val Loss 1.24993
Epoch 61: Val Loss 1.17336
Epoch 62: Val Loss 1.10677
Epoch 63: Val Loss 1.04533
Epoch 64: Val Loss 0.99038
Epoch 65: Val Loss 0.94077
Epoch 66: Val Loss 0.89699
Epoch 67: Val Loss 0.85850
Epoch 68: Val Loss 0.82444
Epoch 69: Val Loss 0.79337
Epoch 70: Val Loss 0.76638
Epoch 71: Val Loss 0.74165
Epoch 72: Val Loss 0.72065
Epoch 73: Val Loss 0.70243
Epoch 74: Val Loss 0.68618
Epoch 75: Val Loss 0.67255
Epoch 76: Val Loss 0.66075
Epoch 77: Val Loss 0.65027
Epoch 78: Val Loss 0.64152
Epoch 79: Val Loss 0.63431
Epoch 80: Val Loss 0.62798
Epoch 81: Val Loss 0.62149
Epoch 82: Val Loss 0.61627
Epoch 83: Val Loss 0.61159
Epoch 84: Val Loss 0.60786
Epoch 85: Val Loss 0.60392
Epoch 86: Val Loss 0.60012
Epoch 87: Val Loss 0.59713
Epoch 88: Val Loss 0.59493
Epoch 89: Val Loss 0.59332
Epoch 90: Val Loss 0.59150
Epoch 91: Val Loss 0.58986
Epoch 92: Val Loss 0.58834
Epoch 93: Val Loss 0.58682
Epoch 94: Val Loss 0.58581
Epoch 95: Val Loss 0.58501
Epoch 96: Val Loss 0.58407
Epoch 97: Val Loss 0.58386
Epoch 98: Val Loss 0.58336
Epoch 99: Val Loss 0.58308
{'MSE - mean': 0.5385363064924114, 'MSE - std': 0.04454355222201187, 'R2 - mean': 0.15574838687383297, 'R2 - std': 0.05465186589104043} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.04207
Epoch 1: Val Loss 233.49768
Epoch 2: Val Loss 232.96748
Epoch 3: Val Loss 232.44357
Epoch 4: Val Loss 231.88321
Epoch 5: Val Loss 231.26151
Epoch 6: Val Loss 230.54436
Epoch 7: Val Loss 229.72185
Epoch 8: Val Loss 228.77707
Epoch 9: Val Loss 227.68231
Epoch 10: Val Loss 226.42426
Epoch 11: Val Loss 224.98029
Epoch 12: Val Loss 223.32472
Epoch 13: Val Loss 221.43416
Epoch 14: Val Loss 219.24683
Epoch 15: Val Loss 216.74959
Epoch 16: Val Loss 213.91696
Epoch 17: Val Loss 210.66841
Epoch 18: Val Loss 206.96399
Epoch 19: Val Loss 202.71729
Epoch 20: Val Loss 197.86415
Epoch 21: Val Loss 192.31996
Epoch 22: Val Loss 186.03664
Epoch 23: Val Loss 178.97147
Epoch 24: Val Loss 171.02414
Epoch 25: Val Loss 162.12459
Epoch 26: Val Loss 152.35326
Epoch 27: Val Loss 141.76918
Epoch 28: Val Loss 130.44682
Epoch 29: Val Loss 118.55083
Epoch 30: Val Loss 106.22569
Epoch 31: Val Loss 93.66943
Epoch 32: Val Loss 81.09665
Epoch 33: Val Loss 68.80276
Epoch 34: Val Loss 57.10222
Epoch 35: Val Loss 46.31084
Epoch 36: Val Loss 36.74521
Epoch 37: Val Loss 28.69954
Epoch 38: Val Loss 22.23120
Epoch 39: Val Loss 17.37288
Epoch 40: Val Loss 14.02817
Epoch 41: Val Loss 11.88945
Epoch 42: Val Loss 10.60692
Epoch 43: Val Loss 9.83904
Epoch 44: Val Loss 9.29859
Epoch 45: Val Loss 8.82354
Epoch 46: Val Loss 8.33472
Epoch 47: Val Loss 7.81003
Epoch 48: Val Loss 7.29410
Epoch 49: Val Loss 6.80746
Epoch 50: Val Loss 6.36646
Epoch 51: Val Loss 5.98250
Epoch 52: Val Loss 5.63485
Epoch 53: Val Loss 5.31744
Epoch 54: Val Loss 5.02358
Epoch 55: Val Loss 4.75389
Epoch 56: Val Loss 4.50345
Epoch 57: Val Loss 4.26716
Epoch 58: Val Loss 4.04407
Epoch 59: Val Loss 3.83770
Epoch 60: Val Loss 3.64215
Epoch 61: Val Loss 3.45750
Epoch 62: Val Loss 3.28266
Epoch 63: Val Loss 3.11473
Epoch 64: Val Loss 2.96136
Epoch 65: Val Loss 2.81467
Epoch 66: Val Loss 2.67544
Epoch 67: Val Loss 2.54421
Epoch 68: Val Loss 2.42186
Epoch 69: Val Loss 2.30655
Epoch 70: Val Loss 2.19828
Epoch 71: Val Loss 2.09388
Epoch 72: Val Loss 1.99685
Epoch 73: Val Loss 1.90825
Epoch 74: Val Loss 1.82311
Epoch 75: Val Loss 1.74302
Epoch 76: Val Loss 1.66655
Epoch 77: Val Loss 1.59652
Epoch 78: Val Loss 1.53107
Epoch 79: Val Loss 1.46859
Epoch 80: Val Loss 1.41026
Epoch 81: Val Loss 1.35742
Epoch 82: Val Loss 1.30689
Epoch 83: Val Loss 1.25943
Epoch 84: Val Loss 1.21553
Epoch 85: Val Loss 1.17365
Epoch 86: Val Loss 1.13508
Epoch 87: Val Loss 1.09949
Epoch 88: Val Loss 1.06540
Epoch 89: Val Loss 1.03352
Epoch 90: Val Loss 1.00403
Epoch 91: Val Loss 0.97757
Epoch 92: Val Loss 0.95323
Epoch 93: Val Loss 0.93085
Epoch 94: Val Loss 0.90965
Epoch 95: Val Loss 0.89019
Epoch 96: Val Loss 0.87243
Epoch 97: Val Loss 0.85585
Epoch 98: Val Loss 0.84065
Epoch 99: Val Loss 0.82652
{'MSE - mean': 0.6345313931062148, 'MSE - std': 0.1405448872698209, 'R2 - mean': 0.03727725545508657, 'R2 - std': 0.17338406959557712} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.43257
Epoch 1: Val Loss 232.54527
Epoch 2: Val Loss 231.55432
Epoch 3: Val Loss 230.45654
Epoch 4: Val Loss 229.24211
Epoch 5: Val Loss 227.90132
Epoch 6: Val Loss 226.40767
Epoch 7: Val Loss 224.73923
Epoch 8: Val Loss 222.87241
Epoch 9: Val Loss 220.78099
Epoch 10: Val Loss 218.43033
Epoch 11: Val Loss 215.78508
Epoch 12: Val Loss 212.81721
Epoch 13: Val Loss 209.47498
Epoch 14: Val Loss 205.69333
Epoch 15: Val Loss 201.43239
Epoch 16: Val Loss 196.62665
Epoch 17: Val Loss 191.24092
Epoch 18: Val Loss 185.20679
Epoch 19: Val Loss 178.48691
Epoch 20: Val Loss 171.02916
Epoch 21: Val Loss 162.81300
Epoch 22: Val Loss 153.83223
Epoch 23: Val Loss 144.07494
Epoch 24: Val Loss 133.58984
Epoch 25: Val Loss 122.43089
Epoch 26: Val Loss 110.65927
Epoch 27: Val Loss 98.42696
Epoch 28: Val Loss 85.96468
Epoch 29: Val Loss 73.48931
Epoch 30: Val Loss 61.30020
Epoch 31: Val Loss 49.71753
Epoch 32: Val Loss 39.07120
Epoch 33: Val Loss 29.66727
Epoch 34: Val Loss 21.75931
Epoch 35: Val Loss 15.53797
Epoch 36: Val Loss 11.01974
Epoch 37: Val Loss 8.00925
Epoch 38: Val Loss 6.22713
Epoch 39: Val Loss 5.25784
Epoch 40: Val Loss 4.73733
Epoch 41: Val Loss 4.39500
Epoch 42: Val Loss 4.07994
Epoch 43: Val Loss 3.76685
Epoch 44: Val Loss 3.47040
Epoch 45: Val Loss 3.21091
Epoch 46: Val Loss 2.99527
Epoch 47: Val Loss 2.81361
Epoch 48: Val Loss 2.65077
Epoch 49: Val Loss 2.50218
Epoch 50: Val Loss 2.35749
Epoch 51: Val Loss 2.21163
Epoch 52: Val Loss 2.07242
Epoch 53: Val Loss 1.93941
Epoch 54: Val Loss 1.82029
Epoch 55: Val Loss 1.70787
Epoch 56: Val Loss 1.60401
Epoch 57: Val Loss 1.51203
Epoch 58: Val Loss 1.43025
Epoch 59: Val Loss 1.35635
Epoch 60: Val Loss 1.28883
Epoch 61: Val Loss 1.22527
Epoch 62: Val Loss 1.16828
Epoch 63: Val Loss 1.11789
Epoch 64: Val Loss 1.07035
Epoch 65: Val Loss 1.02938
Epoch 66: Val Loss 0.99166
Epoch 67: Val Loss 0.95600
Epoch 68: Val Loss 0.92371
Epoch 69: Val Loss 0.89468
Epoch 70: Val Loss 0.86832
Epoch 71: Val Loss 0.84413
Epoch 72: Val Loss 0.82174
Epoch 73: Val Loss 0.80140
Epoch 74: Val Loss 0.78365
Epoch 75: Val Loss 0.76794
Epoch 76: Val Loss 0.75263
Epoch 77: Val Loss 0.73887
Epoch 78: Val Loss 0.72646
Epoch 79: Val Loss 0.71549
Epoch 80: Val Loss 0.70586
Epoch 81: Val Loss 0.69610
Epoch 82: Val Loss 0.68702
Epoch 83: Val Loss 0.67854
Epoch 84: Val Loss 0.67208
Epoch 85: Val Loss 0.66597
Epoch 86: Val Loss 0.66046
Epoch 87: Val Loss 0.65598
Epoch 88: Val Loss 0.65150
Epoch 89: Val Loss 0.64731
Epoch 90: Val Loss 0.64312
Epoch 91: Val Loss 0.63958
Epoch 92: Val Loss 0.63658
Epoch 93: Val Loss 0.63394
Epoch 94: Val Loss 0.63127
Epoch 95: Val Loss 0.62876
Epoch 96: Val Loss 0.62606
Epoch 97: Val Loss 0.62345
Epoch 98: Val Loss 0.62098
Epoch 99: Val Loss 0.61892
{'MSE - mean': 0.6306281057271672, 'MSE - std': 0.12190305968414121, 'R2 - mean': 0.08061022793816447, 'R2 - std': 0.16786830017844498} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.15636
Epoch 1: Val Loss 233.61429
Epoch 2: Val Loss 233.12715
Epoch 3: Val Loss 232.65308
Epoch 4: Val Loss 232.16690
Epoch 5: Val Loss 231.63864
Epoch 6: Val Loss 231.05173
Epoch 7: Val Loss 230.38992
Epoch 8: Val Loss 229.61850
Epoch 9: Val Loss 228.69843
Epoch 10: Val Loss 227.60742
Epoch 11: Val Loss 226.33826
Epoch 12: Val Loss 224.87059
Epoch 13: Val Loss 223.10919
Epoch 14: Val Loss 221.02106
Epoch 15: Val Loss 218.55705
Epoch 16: Val Loss 215.62856
Epoch 17: Val Loss 212.19742
Epoch 18: Val Loss 208.15268
Epoch 19: Val Loss 203.38579
Epoch 20: Val Loss 197.88046
Epoch 21: Val Loss 191.56593
Epoch 22: Val Loss 184.36960
Epoch 23: Val Loss 176.24446
Epoch 24: Val Loss 167.11841
Epoch 25: Val Loss 156.91037
Epoch 26: Val Loss 145.60982
Epoch 27: Val Loss 133.30196
Epoch 28: Val Loss 120.01908
Epoch 29: Val Loss 105.98148
Epoch 30: Val Loss 91.52271
Epoch 31: Val Loss 76.96114
Epoch 32: Val Loss 62.78027
Epoch 33: Val Loss 49.51960
Epoch 34: Val Loss 37.69938
Epoch 35: Val Loss 27.81982
Epoch 36: Val Loss 20.20231
Epoch 37: Val Loss 14.96322
Epoch 38: Val Loss 11.79620
Epoch 39: Val Loss 10.17167
Epoch 40: Val Loss 9.40063
Epoch 41: Val Loss 8.88079
Epoch 42: Val Loss 8.28482
Epoch 43: Val Loss 7.57038
Epoch 44: Val Loss 6.77867
Epoch 45: Val Loss 6.02718
Epoch 46: Val Loss 5.37232
Epoch 47: Val Loss 4.84209
Epoch 48: Val Loss 4.40370
Epoch 49: Val Loss 4.03517
Epoch 50: Val Loss 3.71351
Epoch 51: Val Loss 3.42506
Epoch 52: Val Loss 3.16614
Epoch 53: Val Loss 2.93012
Epoch 54: Val Loss 2.71810
Epoch 55: Val Loss 2.52448
Epoch 56: Val Loss 2.34886
Epoch 57: Val Loss 2.19208
Epoch 58: Val Loss 2.05159
Epoch 59: Val Loss 1.91965
Epoch 60: Val Loss 1.80194
Epoch 61: Val Loss 1.69320
Epoch 62: Val Loss 1.59412
Epoch 63: Val Loss 1.50469
Epoch 64: Val Loss 1.42168
Epoch 65: Val Loss 1.34692
Epoch 66: Val Loss 1.27893
Epoch 67: Val Loss 1.21600
Epoch 68: Val Loss 1.16013
Epoch 69: Val Loss 1.10923
Epoch 70: Val Loss 1.06177
Epoch 71: Val Loss 1.01916
Epoch 72: Val Loss 0.98005
Epoch 73: Val Loss 0.94402
Epoch 74: Val Loss 0.91133
Epoch 75: Val Loss 0.88195
Epoch 76: Val Loss 0.85512
Epoch 77: Val Loss 0.83040
Epoch 78: Val Loss 0.80834
Epoch 79: Val Loss 0.78790
Epoch 80: Val Loss 0.76852
Epoch 81: Val Loss 0.75126
Epoch 82: Val Loss 0.73647
Epoch 83: Val Loss 0.72209
Epoch 84: Val Loss 0.71055
Epoch 85: Val Loss 0.70042
Epoch 86: Val Loss 0.69083
Epoch 87: Val Loss 0.68222
Epoch 88: Val Loss 0.67450
Epoch 89: Val Loss 0.66655
Epoch 90: Val Loss 0.66015
Epoch 91: Val Loss 0.65436
Epoch 92: Val Loss 0.64856
Epoch 93: Val Loss 0.64314
Epoch 94: Val Loss 0.63815
Epoch 95: Val Loss 0.63415
Epoch 96: Val Loss 0.63105
Epoch 97: Val Loss 0.62755
Epoch 98: Val Loss 0.62380
Epoch 99: Val Loss 0.62140
{'MSE - mean': 0.6287818512940075, 'MSE - std': 0.10909591830133739, 'R2 - mean': 0.05823964115717968, 'R2 - std': 0.15667030853209774} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 0.6287818512940075 and parameters: {'dim': 32, 'depth': 3, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 17 with value: 0.5934298657769321.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.70428
Epoch 1: Val Loss 230.61942
Epoch 2: Val Loss 229.57878
Epoch 3: Val Loss 228.55804
Epoch 4: Val Loss 227.51634
Epoch 5: Val Loss 226.40198
Epoch 6: Val Loss 225.18416
Epoch 7: Val Loss 223.84877
Epoch 8: Val Loss 222.38103
Epoch 9: Val Loss 220.73508
Epoch 10: Val Loss 218.89099
Epoch 11: Val Loss 216.79836
Epoch 12: Val Loss 214.43542
Epoch 13: Val Loss 211.77350
Epoch 14: Val Loss 208.76662
Epoch 15: Val Loss 205.37527
Epoch 16: Val Loss 201.54636
Epoch 17: Val Loss 197.21541
Epoch 18: Val Loss 192.27435
Epoch 19: Val Loss 186.65762
Epoch 20: Val Loss 180.24229
Epoch 21: Val Loss 173.00722
Epoch 22: Val Loss 164.92946
Epoch 23: Val Loss 155.95818
Epoch 24: Val Loss 146.01199
Epoch 25: Val Loss 135.17377
Epoch 26: Val Loss 123.45183
Epoch 27: Val Loss 110.99291
Epoch 28: Val Loss 97.93122
Epoch 29: Val Loss 84.55003
Epoch 30: Val Loss 71.10145
Epoch 31: Val Loss 57.97974
Epoch 32: Val Loss 45.57979
Epoch 33: Val Loss 34.32102
Epoch 34: Val Loss 24.61176
Epoch 35: Val Loss 16.77190
Epoch 36: Val Loss 10.96872
Epoch 37: Val Loss 7.12866
Epoch 38: Val Loss 4.96836
Epoch 39: Val Loss 3.97021
Epoch 40: Val Loss 3.60234
Epoch 41: Val Loss 3.40150
Epoch 42: Val Loss 3.15796
Epoch 43: Val Loss 2.78467
Epoch 44: Val Loss 2.36460
Epoch 45: Val Loss 1.98065
Epoch 46: Val Loss 1.66096
Epoch 47: Val Loss 1.41762
Epoch 48: Val Loss 1.23261
Epoch 49: Val Loss 1.09211
Epoch 50: Val Loss 0.97847
Epoch 51: Val Loss 0.88220
Epoch 52: Val Loss 0.80007
Epoch 53: Val Loss 0.73104
Epoch 54: Val Loss 0.67445
Epoch 55: Val Loss 0.62763
Epoch 56: Val Loss 0.58987
Epoch 57: Val Loss 0.55859
Epoch 58: Val Loss 0.53362
Epoch 59: Val Loss 0.51340
Epoch 60: Val Loss 0.49633
Epoch 61: Val Loss 0.48283
Epoch 62: Val Loss 0.47187
Epoch 63: Val Loss 0.46409
Epoch 64: Val Loss 0.45805
Epoch 65: Val Loss 0.45315
Epoch 66: Val Loss 0.45047
Epoch 67: Val Loss 0.44941
Epoch 68: Val Loss 0.44923
Epoch 69: Val Loss 0.44991
Epoch 70: Val Loss 0.45119
Epoch 71: Val Loss 0.45338
Epoch 72: Val Loss 0.45570
Epoch 73: Val Loss 0.45833
Epoch 74: Val Loss 0.46003
Epoch 75: Val Loss 0.46174
Epoch 76: Val Loss 0.46395
Epoch 77: Val Loss 0.46573
Epoch 78: Val Loss 0.46750
Epoch 79: Val Loss 0.46940
Epoch 80: Val Loss 0.47058
Epoch 81: Val Loss 0.47237
Epoch 82: Val Loss 0.47392
Epoch 83: Val Loss 0.47619
Epoch 84: Val Loss 0.47737
Epoch 85: Val Loss 0.47886
Epoch 86: Val Loss 0.48006
Epoch 87: Val Loss 0.48201
Epoch 88: Val Loss 0.48317
Epoch 89: Val Loss 0.48447
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 0.4492301918318829, 'MSE - std': 0.0, 'R2 - mean': 0.18254958432809565, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.60300
Epoch 1: Val Loss 216.44327
Epoch 2: Val Loss 215.13249
Epoch 3: Val Loss 213.64711
Epoch 4: Val Loss 211.96312
Epoch 5: Val Loss 210.04428
Epoch 6: Val Loss 207.85051
Epoch 7: Val Loss 205.33096
Epoch 8: Val Loss 202.42784
Epoch 9: Val Loss 199.04924
Epoch 10: Val Loss 195.11902
Epoch 11: Val Loss 190.59911
Epoch 12: Val Loss 185.42024
Epoch 13: Val Loss 179.49788
Epoch 14: Val Loss 172.71771
Epoch 15: Val Loss 165.03859
Epoch 16: Val Loss 156.40411
Epoch 17: Val Loss 146.81737
Epoch 18: Val Loss 136.23241
Epoch 19: Val Loss 124.72852
Epoch 20: Val Loss 112.35594
Epoch 21: Val Loss 99.29311
Epoch 22: Val Loss 85.75040
Epoch 23: Val Loss 71.99388
Epoch 24: Val Loss 58.46361
Epoch 25: Val Loss 45.63103
Epoch 26: Val Loss 34.03955
Epoch 27: Val Loss 24.18245
Epoch 28: Val Loss 16.42722
Epoch 29: Val Loss 10.91687
Epoch 30: Val Loss 7.50820
Epoch 31: Val Loss 5.74901
Epoch 32: Val Loss 4.95663
Epoch 33: Val Loss 4.54252
Epoch 34: Val Loss 4.10079
Epoch 35: Val Loss 3.56408
Epoch 36: Val Loss 2.97925
Epoch 37: Val Loss 2.46191
Epoch 38: Val Loss 2.04797
Epoch 39: Val Loss 1.74748
Epoch 40: Val Loss 1.52722
Epoch 41: Val Loss 1.35692
Epoch 42: Val Loss 1.21826
Epoch 43: Val Loss 1.10085
Epoch 44: Val Loss 1.00135
Epoch 45: Val Loss 0.91805
Epoch 46: Val Loss 0.85208
Epoch 47: Val Loss 0.79989
Epoch 48: Val Loss 0.75905
Epoch 49: Val Loss 0.72549
Epoch 50: Val Loss 0.69873
Epoch 51: Val Loss 0.67658
Epoch 52: Val Loss 0.65793
Epoch 53: Val Loss 0.64247
Epoch 54: Val Loss 0.63095
Epoch 55: Val Loss 0.62117
Epoch 56: Val Loss 0.61375
Epoch 57: Val Loss 0.60702
Epoch 58: Val Loss 0.60106
Epoch 59: Val Loss 0.59616
Epoch 60: Val Loss 0.59261
Epoch 61: Val Loss 0.58918
Epoch 62: Val Loss 0.58639
Epoch 63: Val Loss 0.58435
Epoch 64: Val Loss 0.58252
Epoch 65: Val Loss 0.58049
Epoch 66: Val Loss 0.57895
Epoch 67: Val Loss 0.57855
Epoch 68: Val Loss 0.57821
Epoch 69: Val Loss 0.57820
Epoch 70: Val Loss 0.57796
Epoch 71: Val Loss 0.57781
Epoch 72: Val Loss 0.57805
Epoch 73: Val Loss 0.57894
Epoch 74: Val Loss 0.57886
Epoch 75: Val Loss 0.57842
Epoch 76: Val Loss 0.57816
Epoch 77: Val Loss 0.57808
Epoch 78: Val Loss 0.57745
Epoch 79: Val Loss 0.57768
Epoch 80: Val Loss 0.57735
Epoch 81: Val Loss 0.57761
Epoch 82: Val Loss 0.57737
Epoch 83: Val Loss 0.57688
Epoch 84: Val Loss 0.57610
Epoch 85: Val Loss 0.57544
Epoch 86: Val Loss 0.57517
Epoch 87: Val Loss 0.57571
Epoch 88: Val Loss 0.57576
Epoch 89: Val Loss 0.57535
Epoch 90: Val Loss 0.57456
Epoch 91: Val Loss 0.57292
Epoch 92: Val Loss 0.57130
Epoch 93: Val Loss 0.57127
Epoch 94: Val Loss 0.57091
Epoch 95: Val Loss 0.57006
Epoch 96: Val Loss 0.56964
Epoch 97: Val Loss 0.57001
Epoch 98: Val Loss 0.57034
Epoch 99: Val Loss 0.57152
{'MSE - mean': 0.5094330314741685, 'MSE - std': 0.060202839642285594, 'R2 - mean': 0.20557776187963744, 'R2 - std': 0.023028177551541795} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.71901
Epoch 1: Val Loss 233.88535
Epoch 2: Val Loss 233.05481
Epoch 3: Val Loss 232.20732
Epoch 4: Val Loss 231.32672
Epoch 5: Val Loss 230.39113
Epoch 6: Val Loss 229.37720
Epoch 7: Val Loss 228.27129
Epoch 8: Val Loss 227.03952
Epoch 9: Val Loss 225.64291
Epoch 10: Val Loss 224.05092
Epoch 11: Val Loss 222.23453
Epoch 12: Val Loss 220.13776
Epoch 13: Val Loss 217.71901
Epoch 14: Val Loss 214.91466
Epoch 15: Val Loss 211.68037
Epoch 16: Val Loss 207.92778
Epoch 17: Val Loss 203.59998
Epoch 18: Val Loss 198.58427
Epoch 19: Val Loss 192.82855
Epoch 20: Val Loss 186.24007
Epoch 21: Val Loss 178.73228
Epoch 22: Val Loss 170.26939
Epoch 23: Val Loss 160.83124
Epoch 24: Val Loss 150.37831
Epoch 25: Val Loss 138.93323
Epoch 26: Val Loss 126.54171
Epoch 27: Val Loss 113.34271
Epoch 28: Val Loss 99.56779
Epoch 29: Val Loss 85.47527
Epoch 30: Val Loss 71.41911
Epoch 31: Val Loss 57.79839
Epoch 32: Val Loss 45.05480
Epoch 33: Val Loss 33.69421
Epoch 34: Val Loss 24.13641
Epoch 35: Val Loss 16.64339
Epoch 36: Val Loss 11.30478
Epoch 37: Val Loss 7.95543
Epoch 38: Val Loss 6.13849
Epoch 39: Val Loss 5.30433
Epoch 40: Val Loss 4.90070
Epoch 41: Val Loss 4.58509
Epoch 42: Val Loss 4.22950
Epoch 43: Val Loss 3.82578
Epoch 44: Val Loss 3.41975
Epoch 45: Val Loss 3.07254
Epoch 46: Val Loss 2.79497
Epoch 47: Val Loss 2.57085
Epoch 48: Val Loss 2.38615
Epoch 49: Val Loss 2.22192
Epoch 50: Val Loss 2.07133
Epoch 51: Val Loss 1.93019
Epoch 52: Val Loss 1.80013
Epoch 53: Val Loss 1.67966
Epoch 54: Val Loss 1.56965
Epoch 55: Val Loss 1.47114
Epoch 56: Val Loss 1.38362
Epoch 57: Val Loss 1.30651
Epoch 58: Val Loss 1.23578
Epoch 59: Val Loss 1.17344
Epoch 60: Val Loss 1.11713
Epoch 61: Val Loss 1.06694
Epoch 62: Val Loss 1.02004
Epoch 63: Val Loss 0.97881
Epoch 64: Val Loss 0.94133
Epoch 65: Val Loss 0.90772
Epoch 66: Val Loss 0.87785
Epoch 67: Val Loss 0.85156
Epoch 68: Val Loss 0.82767
Epoch 69: Val Loss 0.80648
Epoch 70: Val Loss 0.78733
Epoch 71: Val Loss 0.77047
Epoch 72: Val Loss 0.75542
Epoch 73: Val Loss 0.74201
Epoch 74: Val Loss 0.73019
Epoch 75: Val Loss 0.71989
Epoch 76: Val Loss 0.71063
Epoch 77: Val Loss 0.70245
Epoch 78: Val Loss 0.69537
Epoch 79: Val Loss 0.68914
Epoch 80: Val Loss 0.68398
Epoch 81: Val Loss 0.67983
Epoch 82: Val Loss 0.67595
Epoch 83: Val Loss 0.67231
Epoch 84: Val Loss 0.66961
Epoch 85: Val Loss 0.66688
Epoch 86: Val Loss 0.66547
Epoch 87: Val Loss 0.66398
Epoch 88: Val Loss 0.66249
Epoch 89: Val Loss 0.66153
Epoch 90: Val Loss 0.66080
Epoch 91: Val Loss 0.66064
Epoch 92: Val Loss 0.66007
Epoch 93: Val Loss 0.65999
Epoch 94: Val Loss 0.66017
Epoch 95: Val Loss 0.65993
Epoch 96: Val Loss 0.65970
Epoch 97: Val Loss 0.65952
Epoch 98: Val Loss 0.65903
Epoch 99: Val Loss 0.65881
{'MSE - mean': 0.5592238191476081, 'MSE - std': 0.08587490713209685, 'R2 - mean': 0.15164141332539605, 'R2 - std': 0.0785607452758233} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.09010
Epoch 1: Val Loss 222.10582
Epoch 2: Val Loss 221.15060
Epoch 3: Val Loss 220.21127
Epoch 4: Val Loss 219.26015
Epoch 5: Val Loss 218.26697
Epoch 6: Val Loss 217.21364
Epoch 7: Val Loss 216.05664
Epoch 8: Val Loss 214.78250
Epoch 9: Val Loss 213.34656
Epoch 10: Val Loss 211.70572
Epoch 11: Val Loss 209.81142
Epoch 12: Val Loss 207.63290
Epoch 13: Val Loss 205.11897
Epoch 14: Val Loss 202.22356
Epoch 15: Val Loss 198.88148
Epoch 16: Val Loss 195.04353
Epoch 17: Val Loss 190.65471
Epoch 18: Val Loss 185.65523
Epoch 19: Val Loss 179.96397
Epoch 20: Val Loss 173.57390
Epoch 21: Val Loss 166.41284
Epoch 22: Val Loss 158.41518
Epoch 23: Val Loss 149.55615
Epoch 24: Val Loss 139.83728
Epoch 25: Val Loss 129.28441
Epoch 26: Val Loss 117.97808
Epoch 27: Val Loss 105.97287
Epoch 28: Val Loss 93.43873
Epoch 29: Val Loss 80.61363
Epoch 30: Val Loss 67.76739
Epoch 31: Val Loss 55.27753
Epoch 32: Val Loss 43.55558
Epoch 33: Val Loss 33.06846
Epoch 34: Val Loss 24.14756
Epoch 35: Val Loss 17.07596
Epoch 36: Val Loss 11.94290
Epoch 37: Val Loss 8.55359
Epoch 38: Val Loss 6.53828
Epoch 39: Val Loss 5.43357
Epoch 40: Val Loss 4.78921
Epoch 41: Val Loss 4.30935
Epoch 42: Val Loss 3.87426
Epoch 43: Val Loss 3.46383
Epoch 44: Val Loss 3.10762
Epoch 45: Val Loss 2.82008
Epoch 46: Val Loss 2.59204
Epoch 47: Val Loss 2.40365
Epoch 48: Val Loss 2.23957
Epoch 49: Val Loss 2.08107
Epoch 50: Val Loss 1.92906
Epoch 51: Val Loss 1.78105
Epoch 52: Val Loss 1.64107
Epoch 53: Val Loss 1.51909
Epoch 54: Val Loss 1.41183
Epoch 55: Val Loss 1.32131
Epoch 56: Val Loss 1.24513
Epoch 57: Val Loss 1.17937
Epoch 58: Val Loss 1.12212
Epoch 59: Val Loss 1.07247
Epoch 60: Val Loss 1.03010
Epoch 61: Val Loss 0.99327
Epoch 62: Val Loss 0.96022
Epoch 63: Val Loss 0.92840
Epoch 64: Val Loss 0.89985
Epoch 65: Val Loss 0.87374
Epoch 66: Val Loss 0.85192
Epoch 67: Val Loss 0.83062
Epoch 68: Val Loss 0.81111
Epoch 69: Val Loss 0.79349
Epoch 70: Val Loss 0.77806
Epoch 71: Val Loss 0.76560
Epoch 72: Val Loss 0.75360
Epoch 73: Val Loss 0.74253
Epoch 74: Val Loss 0.73400
Epoch 75: Val Loss 0.72597
Epoch 76: Val Loss 0.71851
Epoch 77: Val Loss 0.71063
Epoch 78: Val Loss 0.70390
Epoch 79: Val Loss 0.69853
Epoch 80: Val Loss 0.69286
Epoch 81: Val Loss 0.68791
Epoch 82: Val Loss 0.68352
Epoch 83: Val Loss 0.68052
Epoch 84: Val Loss 0.67800
Epoch 85: Val Loss 0.67531
Epoch 86: Val Loss 0.67166
Epoch 87: Val Loss 0.66879
Epoch 88: Val Loss 0.66536
Epoch 89: Val Loss 0.66262
Epoch 90: Val Loss 0.65978
Epoch 91: Val Loss 0.65773
Epoch 92: Val Loss 0.65559
Epoch 93: Val Loss 0.65293
Epoch 94: Val Loss 0.65078
Epoch 95: Val Loss 0.64915
Epoch 96: Val Loss 0.64815
Epoch 97: Val Loss 0.64710
Epoch 98: Val Loss 0.64702
Epoch 99: Val Loss 0.64606
{'MSE - mean': 0.5809329724776644, 'MSE - std': 0.08333508727500906, 'R2 - mean': 0.15772881244396764, 'R2 - std': 0.06884774729652346} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.01717
Epoch 1: Val Loss 233.22610
Epoch 2: Val Loss 232.43616
Epoch 3: Val Loss 231.64636
Epoch 4: Val Loss 230.82974
Epoch 5: Val Loss 229.96535
Epoch 6: Val Loss 229.04295
Epoch 7: Val Loss 228.03915
Epoch 8: Val Loss 226.94299
Epoch 9: Val Loss 225.74687
Epoch 10: Val Loss 224.43620
Epoch 11: Val Loss 222.99374
Epoch 12: Val Loss 221.38966
Epoch 13: Val Loss 219.60617
Epoch 14: Val Loss 217.60991
Epoch 15: Val Loss 215.38470
Epoch 16: Val Loss 212.88705
Epoch 17: Val Loss 210.07004
Epoch 18: Val Loss 206.91696
Epoch 19: Val Loss 203.33974
Epoch 20: Val Loss 199.32401
Epoch 21: Val Loss 194.81247
Epoch 22: Val Loss 189.74788
Epoch 23: Val Loss 184.06714
Epoch 24: Val Loss 177.70166
Epoch 25: Val Loss 170.63158
Epoch 26: Val Loss 162.83090
Epoch 27: Val Loss 154.29044
Epoch 28: Val Loss 144.99641
Epoch 29: Val Loss 135.00049
Epoch 30: Val Loss 124.37331
Epoch 31: Val Loss 113.17265
Epoch 32: Val Loss 101.53719
Epoch 33: Val Loss 89.64272
Epoch 34: Val Loss 77.75561
Epoch 35: Val Loss 66.14519
Epoch 36: Val Loss 55.09066
Epoch 37: Val Loss 44.91894
Epoch 38: Val Loss 35.94274
Epoch 39: Val Loss 28.31882
Epoch 40: Val Loss 22.22851
Epoch 41: Val Loss 17.58540
Epoch 42: Val Loss 14.25632
Epoch 43: Val Loss 11.94013
Epoch 44: Val Loss 10.36602
Epoch 45: Val Loss 9.25009
Epoch 46: Val Loss 8.38202
Epoch 47: Val Loss 7.66745
Epoch 48: Val Loss 7.05820
Epoch 49: Val Loss 6.52469
Epoch 50: Val Loss 6.07238
Epoch 51: Val Loss 5.68553
Epoch 52: Val Loss 5.34085
Epoch 53: Val Loss 5.03370
Epoch 54: Val Loss 4.73338
Epoch 55: Val Loss 4.45551
Epoch 56: Val Loss 4.18529
Epoch 57: Val Loss 3.92754
Epoch 58: Val Loss 3.68504
Epoch 59: Val Loss 3.45802
Epoch 60: Val Loss 3.25426
Epoch 61: Val Loss 3.07089
Epoch 62: Val Loss 2.89624
Epoch 63: Val Loss 2.74257
Epoch 64: Val Loss 2.60141
Epoch 65: Val Loss 2.47622
Epoch 66: Val Loss 2.36224
Epoch 67: Val Loss 2.25153
Epoch 68: Val Loss 2.14541
Epoch 69: Val Loss 2.04641
Epoch 70: Val Loss 1.95586
Epoch 71: Val Loss 1.87139
Epoch 72: Val Loss 1.79459
Epoch 73: Val Loss 1.71814
Epoch 74: Val Loss 1.64153
Epoch 75: Val Loss 1.57180
Epoch 76: Val Loss 1.50397
Epoch 77: Val Loss 1.44009
Epoch 78: Val Loss 1.38137
Epoch 79: Val Loss 1.32831
Epoch 80: Val Loss 1.27713
Epoch 81: Val Loss 1.23224
Epoch 82: Val Loss 1.18658
Epoch 83: Val Loss 1.14351
Epoch 84: Val Loss 1.10522
Epoch 85: Val Loss 1.06831
Epoch 86: Val Loss 1.03220
Epoch 87: Val Loss 0.99853
Epoch 88: Val Loss 0.96598
Epoch 89: Val Loss 0.93667
Epoch 90: Val Loss 0.90884
Epoch 91: Val Loss 0.88228
Epoch 92: Val Loss 0.85773
Epoch 93: Val Loss 0.83659
Epoch 94: Val Loss 0.81630
Epoch 95: Val Loss 0.79838
Epoch 96: Val Loss 0.78346
Epoch 97: Val Loss 0.76681
Epoch 98: Val Loss 0.75320
Epoch 99: Val Loss 0.73989
{'MSE - mean': 0.6127235304211145, 'MSE - std': 0.09797115756348977, 'R2 - mean': 0.08060671151803644, 'R2 - std': 0.16608215934037834} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 0.6127235304211145 and parameters: {'dim': 32, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0}. Best is trial 17 with value: 0.5934298657769321.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.29691
Epoch 1: Val Loss 222.18188
Epoch 2: Val Loss 222.06783
Epoch 3: Val Loss 221.95496
Epoch 4: Val Loss 221.84239
Epoch 5: Val Loss 221.73131
Epoch 6: Val Loss 221.62157
Epoch 7: Val Loss 221.51241
Epoch 8: Val Loss 221.40385
Epoch 9: Val Loss 221.29593
Epoch 10: Val Loss 221.18965
Epoch 11: Val Loss 221.08478
Epoch 12: Val Loss 220.98138
Epoch 13: Val Loss 220.87941
Epoch 14: Val Loss 220.77933
Epoch 15: Val Loss 220.67952
Epoch 16: Val Loss 220.57976
Epoch 17: Val Loss 220.48100
Epoch 18: Val Loss 220.38345
Epoch 19: Val Loss 220.28798
Epoch 20: Val Loss 220.19359
Epoch 21: Val Loss 220.09950
Epoch 22: Val Loss 220.00609
Epoch 23: Val Loss 219.91316
Epoch 24: Val Loss 219.82077
Epoch 25: Val Loss 219.72842
Epoch 26: Val Loss 219.63652
Epoch 27: Val Loss 219.54474
Epoch 28: Val Loss 219.45329
Epoch 29: Val Loss 219.36176
Epoch 30: Val Loss 219.27135
Epoch 31: Val Loss 219.18193
Epoch 32: Val Loss 219.09218
Epoch 33: Val Loss 219.00273
Epoch 34: Val Loss 218.91342
Epoch 35: Val Loss 218.82391
Epoch 36: Val Loss 218.73427
Epoch 37: Val Loss 218.64450
Epoch 38: Val Loss 218.55432
Epoch 39: Val Loss 218.46307
Epoch 40: Val Loss 218.37148
Epoch 41: Val Loss 218.27910
Epoch 42: Val Loss 218.18591
Epoch 43: Val Loss 218.09229
Epoch 44: Val Loss 217.99774
Epoch 45: Val Loss 217.90215
Epoch 46: Val Loss 217.80513
Epoch 47: Val Loss 217.70709
Epoch 48: Val Loss 217.60829
Epoch 49: Val Loss 217.50919
Epoch 50: Val Loss 217.40869
Epoch 51: Val Loss 217.30717
Epoch 52: Val Loss 217.20363
Epoch 53: Val Loss 217.09860
Epoch 54: Val Loss 216.99246
Epoch 55: Val Loss 216.88521
Epoch 56: Val Loss 216.77641
Epoch 57: Val Loss 216.66624
Epoch 58: Val Loss 216.55472
Epoch 59: Val Loss 216.44168
Epoch 60: Val Loss 216.32776
Epoch 61: Val Loss 216.21283
Epoch 62: Val Loss 216.09682
Epoch 63: Val Loss 215.97902
Epoch 64: Val Loss 215.85796
Epoch 65: Val Loss 215.73534
Epoch 66: Val Loss 215.61183
Epoch 67: Val Loss 215.48758
Epoch 68: Val Loss 215.36130
Epoch 69: Val Loss 215.23393
Epoch 70: Val Loss 215.10382
Epoch 71: Val Loss 214.97162
Epoch 72: Val Loss 214.83708
Epoch 73: Val Loss 214.70123
Epoch 74: Val Loss 214.56279
Epoch 75: Val Loss 214.42216
Epoch 76: Val Loss 214.27953
Epoch 77: Val Loss 214.13470
Epoch 78: Val Loss 213.98859
Epoch 79: Val Loss 213.84012
Epoch 80: Val Loss 213.68980
Epoch 81: Val Loss 213.53653
Epoch 82: Val Loss 213.38153
Epoch 83: Val Loss 213.22357
Epoch 84: Val Loss 213.06380
Epoch 85: Val Loss 212.90067
Epoch 86: Val Loss 212.73558
Epoch 87: Val Loss 212.56850
Epoch 88: Val Loss 212.39915
Epoch 89: Val Loss 212.22783
Epoch 90: Val Loss 212.05391
Epoch 91: Val Loss 211.87831
Epoch 92: Val Loss 211.69962
Epoch 93: Val Loss 211.51811
Epoch 94: Val Loss 211.33337
Epoch 95: Val Loss 211.14583
Epoch 96: Val Loss 210.95576
Epoch 97: Val Loss 210.76370
Epoch 98: Val Loss 210.56847
Epoch 99: Val Loss 210.37029
{'MSE - mean': 210.37027744367032, 'MSE - std': 0.0, 'R2 - mean': -381.80434812292873, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 225.89047
Epoch 1: Val Loss 225.80237
Epoch 2: Val Loss 225.71539
Epoch 3: Val Loss 225.62917
Epoch 4: Val Loss 225.54486
Epoch 5: Val Loss 225.46165
Epoch 6: Val Loss 225.37953
Epoch 7: Val Loss 225.29823
Epoch 8: Val Loss 225.21895
Epoch 9: Val Loss 225.14078
Epoch 10: Val Loss 225.06442
Epoch 11: Val Loss 224.99001
Epoch 12: Val Loss 224.91563
Epoch 13: Val Loss 224.84213
Epoch 14: Val Loss 224.76942
Epoch 15: Val Loss 224.69696
Epoch 16: Val Loss 224.62567
Epoch 17: Val Loss 224.55589
Epoch 18: Val Loss 224.48657
Epoch 19: Val Loss 224.41830
Epoch 20: Val Loss 224.35071
Epoch 21: Val Loss 224.28464
Epoch 22: Val Loss 224.21870
Epoch 23: Val Loss 224.15349
Epoch 24: Val Loss 224.08928
Epoch 25: Val Loss 224.02557
Epoch 26: Val Loss 223.96239
Epoch 27: Val Loss 223.89880
Epoch 28: Val Loss 223.83592
Epoch 29: Val Loss 223.77382
Epoch 30: Val Loss 223.71161
Epoch 31: Val Loss 223.64975
Epoch 32: Val Loss 223.58780
Epoch 33: Val Loss 223.52534
Epoch 34: Val Loss 223.46407
Epoch 35: Val Loss 223.40170
Epoch 36: Val Loss 223.33873
Epoch 37: Val Loss 223.27544
Epoch 38: Val Loss 223.21103
Epoch 39: Val Loss 223.14627
Epoch 40: Val Loss 223.08070
Epoch 41: Val Loss 223.01485
Epoch 42: Val Loss 222.94910
Epoch 43: Val Loss 222.88205
Epoch 44: Val Loss 222.81528
Epoch 45: Val Loss 222.74768
Epoch 46: Val Loss 222.67953
Epoch 47: Val Loss 222.60991
Epoch 48: Val Loss 222.53979
Epoch 49: Val Loss 222.46799
Epoch 50: Val Loss 222.39452
Epoch 51: Val Loss 222.32034
Epoch 52: Val Loss 222.24542
Epoch 53: Val Loss 222.16891
Epoch 54: Val Loss 222.09219
Epoch 55: Val Loss 222.01416
Epoch 56: Val Loss 221.93457
Epoch 57: Val Loss 221.85445
Epoch 58: Val Loss 221.77290
Epoch 59: Val Loss 221.68983
Epoch 60: Val Loss 221.60454
Epoch 61: Val Loss 221.51801
Epoch 62: Val Loss 221.43124
Epoch 63: Val Loss 221.34404
Epoch 64: Val Loss 221.25655
Epoch 65: Val Loss 221.16785
Epoch 66: Val Loss 221.07796
Epoch 67: Val Loss 220.98680
Epoch 68: Val Loss 220.89496
Epoch 69: Val Loss 220.80145
Epoch 70: Val Loss 220.70607
Epoch 71: Val Loss 220.60944
Epoch 72: Val Loss 220.51184
Epoch 73: Val Loss 220.41200
Epoch 74: Val Loss 220.31084
Epoch 75: Val Loss 220.20677
Epoch 76: Val Loss 220.10168
Epoch 77: Val Loss 219.99445
Epoch 78: Val Loss 219.88573
Epoch 79: Val Loss 219.77655
Epoch 80: Val Loss 219.66469
Epoch 81: Val Loss 219.55040
Epoch 82: Val Loss 219.43394
Epoch 83: Val Loss 219.31548
Epoch 84: Val Loss 219.19574
Epoch 85: Val Loss 219.07547
Epoch 86: Val Loss 218.95369
Epoch 87: Val Loss 218.83104
Epoch 88: Val Loss 218.70761
Epoch 89: Val Loss 218.58202
Epoch 90: Val Loss 218.45576
Epoch 91: Val Loss 218.32817
Epoch 92: Val Loss 218.20026
Epoch 93: Val Loss 218.07095
Epoch 94: Val Loss 217.94061
Epoch 95: Val Loss 217.80763
Epoch 96: Val Loss 217.67091
Epoch 97: Val Loss 217.52979
Epoch 98: Val Loss 217.38551
Epoch 99: Val Loss 217.24005
{'MSE - mean': 213.80517101147353, 'MSE - std': 3.434893567803215, 'R2 - mean': -337.494118258727, 'R2 - std': 44.310229864201716} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 218.94063
Epoch 1: Val Loss 218.78972
Epoch 2: Val Loss 218.63780
Epoch 3: Val Loss 218.48480
Epoch 4: Val Loss 218.33110
Epoch 5: Val Loss 218.17584
Epoch 6: Val Loss 218.01932
Epoch 7: Val Loss 217.86157
Epoch 8: Val Loss 217.70302
Epoch 9: Val Loss 217.54292
Epoch 10: Val Loss 217.38121
Epoch 11: Val Loss 217.21747
Epoch 12: Val Loss 217.05173
Epoch 13: Val Loss 216.88426
Epoch 14: Val Loss 216.71495
Epoch 15: Val Loss 216.54466
Epoch 16: Val Loss 216.37283
Epoch 17: Val Loss 216.19759
Epoch 18: Val Loss 216.02017
Epoch 19: Val Loss 215.84033
Epoch 20: Val Loss 215.65895
Epoch 21: Val Loss 215.47601
Epoch 22: Val Loss 215.29115
Epoch 23: Val Loss 215.10321
Epoch 24: Val Loss 214.91225
Epoch 25: Val Loss 214.71820
Epoch 26: Val Loss 214.52071
Epoch 27: Val Loss 214.32169
Epoch 28: Val Loss 214.12016
Epoch 29: Val Loss 213.91660
Epoch 30: Val Loss 213.71028
Epoch 31: Val Loss 213.50194
Epoch 32: Val Loss 213.29102
Epoch 33: Val Loss 213.07835
Epoch 34: Val Loss 212.86304
Epoch 35: Val Loss 212.64445
Epoch 36: Val Loss 212.42291
Epoch 37: Val Loss 212.19829
Epoch 38: Val Loss 211.96959
Epoch 39: Val Loss 211.73828
Epoch 40: Val Loss 211.50424
Epoch 41: Val Loss 211.26837
Epoch 42: Val Loss 211.02988
Epoch 43: Val Loss 210.78680
Epoch 44: Val Loss 210.54054
Epoch 45: Val Loss 210.29160
Epoch 46: Val Loss 210.03954
Epoch 47: Val Loss 209.78462
Epoch 48: Val Loss 209.52696
Epoch 49: Val Loss 209.26553
Epoch 50: Val Loss 209.00043
Epoch 51: Val Loss 208.73195
Epoch 52: Val Loss 208.45970
Epoch 53: Val Loss 208.18417
Epoch 54: Val Loss 207.90515
Epoch 55: Val Loss 207.62244
Epoch 56: Val Loss 207.33606
Epoch 57: Val Loss 207.04556
Epoch 58: Val Loss 206.75217
Epoch 59: Val Loss 206.45424
Epoch 60: Val Loss 206.15186
Epoch 61: Val Loss 205.84573
Epoch 62: Val Loss 205.53590
Epoch 63: Val Loss 205.22153
Epoch 64: Val Loss 204.90207
Epoch 65: Val Loss 204.57811
Epoch 66: Val Loss 204.25040
Epoch 67: Val Loss 203.91884
Epoch 68: Val Loss 203.58267
Epoch 69: Val Loss 203.24240
Epoch 70: Val Loss 202.89656
Epoch 71: Val Loss 202.54614
Epoch 72: Val Loss 202.19115
Epoch 73: Val Loss 201.83182
Epoch 74: Val Loss 201.46751
Epoch 75: Val Loss 201.09850
Epoch 76: Val Loss 200.72455
Epoch 77: Val Loss 200.34561
Epoch 78: Val Loss 199.96167
Epoch 79: Val Loss 199.57147
Epoch 80: Val Loss 199.17595
Epoch 81: Val Loss 198.77478
Epoch 82: Val Loss 198.36746
Epoch 83: Val Loss 197.95348
Epoch 84: Val Loss 197.53355
Epoch 85: Val Loss 197.10625
Epoch 86: Val Loss 196.67442
Epoch 87: Val Loss 196.23686
Epoch 88: Val Loss 195.79195
Epoch 89: Val Loss 195.34102
Epoch 90: Val Loss 194.88368
Epoch 91: Val Loss 194.41898
Epoch 92: Val Loss 193.94864
Epoch 93: Val Loss 193.47275
Epoch 94: Val Loss 192.98907
Epoch 95: Val Loss 192.49884
Epoch 96: Val Loss 192.00209
Epoch 97: Val Loss 191.49959
Epoch 98: Val Loss 190.99048
Epoch 99: Val Loss 190.47394
{'MSE - mean': 206.02810188794422, 'MSE - std': 11.350386374586403, 'R2 - mean': -316.8180133596552, 'R2 - std': 46.518099789298205} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.30887
Epoch 1: Val Loss 236.26497
Epoch 2: Val Loss 236.22020
Epoch 3: Val Loss 236.17509
Epoch 4: Val Loss 236.12944
Epoch 5: Val Loss 236.08362
Epoch 6: Val Loss 236.03746
Epoch 7: Val Loss 235.99054
Epoch 8: Val Loss 235.94310
Epoch 9: Val Loss 235.89496
Epoch 10: Val Loss 235.84613
Epoch 11: Val Loss 235.79648
Epoch 12: Val Loss 235.74622
Epoch 13: Val Loss 235.69510
Epoch 14: Val Loss 235.64346
Epoch 15: Val Loss 235.59079
Epoch 16: Val Loss 235.53726
Epoch 17: Val Loss 235.48236
Epoch 18: Val Loss 235.42613
Epoch 19: Val Loss 235.36902
Epoch 20: Val Loss 235.31024
Epoch 21: Val Loss 235.25018
Epoch 22: Val Loss 235.18892
Epoch 23: Val Loss 235.12651
Epoch 24: Val Loss 235.06288
Epoch 25: Val Loss 234.99817
Epoch 26: Val Loss 234.93248
Epoch 27: Val Loss 234.86534
Epoch 28: Val Loss 234.79729
Epoch 29: Val Loss 234.72810
Epoch 30: Val Loss 234.65695
Epoch 31: Val Loss 234.58423
Epoch 32: Val Loss 234.51038
Epoch 33: Val Loss 234.43506
Epoch 34: Val Loss 234.35814
Epoch 35: Val Loss 234.28018
Epoch 36: Val Loss 234.20045
Epoch 37: Val Loss 234.11928
Epoch 38: Val Loss 234.03615
Epoch 39: Val Loss 233.95174
Epoch 40: Val Loss 233.86592
Epoch 41: Val Loss 233.77798
Epoch 42: Val Loss 233.68712
Epoch 43: Val Loss 233.59409
Epoch 44: Val Loss 233.49869
Epoch 45: Val Loss 233.40077
Epoch 46: Val Loss 233.30098
Epoch 47: Val Loss 233.19765
Epoch 48: Val Loss 233.09238
Epoch 49: Val Loss 232.98494
Epoch 50: Val Loss 232.87354
Epoch 51: Val Loss 232.75801
Epoch 52: Val Loss 232.63872
Epoch 53: Val Loss 232.51633
Epoch 54: Val Loss 232.39127
Epoch 55: Val Loss 232.26335
Epoch 56: Val Loss 232.13258
Epoch 57: Val Loss 231.99948
Epoch 58: Val Loss 231.86235
Epoch 59: Val Loss 231.72235
Epoch 60: Val Loss 231.57890
Epoch 61: Val Loss 231.43222
Epoch 62: Val Loss 231.28238
Epoch 63: Val Loss 231.13002
Epoch 64: Val Loss 230.97469
Epoch 65: Val Loss 230.81613
Epoch 66: Val Loss 230.65382
Epoch 67: Val Loss 230.48790
Epoch 68: Val Loss 230.31889
Epoch 69: Val Loss 230.14684
Epoch 70: Val Loss 229.97064
Epoch 71: Val Loss 229.79089
Epoch 72: Val Loss 229.60698
Epoch 73: Val Loss 229.41866
Epoch 74: Val Loss 229.22530
Epoch 75: Val Loss 229.02809
Epoch 76: Val Loss 228.82590
Epoch 77: Val Loss 228.61839
Epoch 78: Val Loss 228.40562
Epoch 79: Val Loss 228.18684
Epoch 80: Val Loss 227.96249
Epoch 81: Val Loss 227.73419
Epoch 82: Val Loss 227.50206
Epoch 83: Val Loss 227.26337
Epoch 84: Val Loss 227.02010
Epoch 85: Val Loss 226.77193
Epoch 86: Val Loss 226.51779
Epoch 87: Val Loss 226.26012
Epoch 88: Val Loss 225.99675
Epoch 89: Val Loss 225.72855
Epoch 90: Val Loss 225.45503
Epoch 91: Val Loss 225.17673
Epoch 92: Val Loss 224.89326
Epoch 93: Val Loss 224.60371
Epoch 94: Val Loss 224.30855
Epoch 95: Val Loss 224.00868
Epoch 96: Val Loss 223.70291
Epoch 97: Val Loss 223.39172
Epoch 98: Val Loss 223.07268
Epoch 99: Val Loss 222.74915
{'MSE - mean': 210.2083654679969, 'MSE - std': 12.208491468459263, 'R2 - mean': -308.38910532432357, 'R2 - std': 42.849617011235516} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 238.75148
Epoch 1: Val Loss 238.64099
Epoch 2: Val Loss 238.53215
Epoch 3: Val Loss 238.42590
Epoch 4: Val Loss 238.32172
Epoch 5: Val Loss 238.21846
Epoch 6: Val Loss 238.11653
Epoch 7: Val Loss 238.01622
Epoch 8: Val Loss 237.91701
Epoch 9: Val Loss 237.81938
Epoch 10: Val Loss 237.72406
Epoch 11: Val Loss 237.63080
Epoch 12: Val Loss 237.53902
Epoch 13: Val Loss 237.44820
Epoch 14: Val Loss 237.35977
Epoch 15: Val Loss 237.27258
Epoch 16: Val Loss 237.18674
Epoch 17: Val Loss 237.10226
Epoch 18: Val Loss 237.01842
Epoch 19: Val Loss 236.93546
Epoch 20: Val Loss 236.85365
Epoch 21: Val Loss 236.77309
Epoch 22: Val Loss 236.69405
Epoch 23: Val Loss 236.61674
Epoch 24: Val Loss 236.54115
Epoch 25: Val Loss 236.46571
Epoch 26: Val Loss 236.39078
Epoch 27: Val Loss 236.31635
Epoch 28: Val Loss 236.24242
Epoch 29: Val Loss 236.16911
Epoch 30: Val Loss 236.09650
Epoch 31: Val Loss 236.02452
Epoch 32: Val Loss 235.95355
Epoch 33: Val Loss 235.88281
Epoch 34: Val Loss 235.81078
Epoch 35: Val Loss 235.73877
Epoch 36: Val Loss 235.66629
Epoch 37: Val Loss 235.59462
Epoch 38: Val Loss 235.52333
Epoch 39: Val Loss 235.45274
Epoch 40: Val Loss 235.38138
Epoch 41: Val Loss 235.31052
Epoch 42: Val Loss 235.24089
Epoch 43: Val Loss 235.17172
Epoch 44: Val Loss 235.10246
Epoch 45: Val Loss 235.03340
Epoch 46: Val Loss 234.96388
Epoch 47: Val Loss 234.89435
Epoch 48: Val Loss 234.82486
Epoch 49: Val Loss 234.75566
Epoch 50: Val Loss 234.68657
Epoch 51: Val Loss 234.61626
Epoch 52: Val Loss 234.54460
Epoch 53: Val Loss 234.47243
Epoch 54: Val Loss 234.40030
Epoch 55: Val Loss 234.32700
Epoch 56: Val Loss 234.25389
Epoch 57: Val Loss 234.18034
Epoch 58: Val Loss 234.10657
Epoch 59: Val Loss 234.03218
Epoch 60: Val Loss 233.95737
Epoch 61: Val Loss 233.88252
Epoch 62: Val Loss 233.80699
Epoch 63: Val Loss 233.73131
Epoch 64: Val Loss 233.65535
Epoch 65: Val Loss 233.57866
Epoch 66: Val Loss 233.50110
Epoch 67: Val Loss 233.42302
Epoch 68: Val Loss 233.34492
Epoch 69: Val Loss 233.26593
Epoch 70: Val Loss 233.18568
Epoch 71: Val Loss 233.10349
Epoch 72: Val Loss 233.02083
Epoch 73: Val Loss 232.93712
Epoch 74: Val Loss 232.85257
Epoch 75: Val Loss 232.76767
Epoch 76: Val Loss 232.68259
Epoch 77: Val Loss 232.59668
Epoch 78: Val Loss 232.51001
Epoch 79: Val Loss 232.42287
Epoch 80: Val Loss 232.33519
Epoch 81: Val Loss 232.24669
Epoch 82: Val Loss 232.15804
Epoch 83: Val Loss 232.06871
Epoch 84: Val Loss 231.97893
Epoch 85: Val Loss 231.88863
Epoch 86: Val Loss 231.79768
Epoch 87: Val Loss 231.70538
Epoch 88: Val Loss 231.61214
Epoch 89: Val Loss 231.51857
Epoch 90: Val Loss 231.42436
Epoch 91: Val Loss 231.32913
Epoch 92: Val Loss 231.23264
Epoch 93: Val Loss 231.13559
Epoch 94: Val Loss 231.03706
Epoch 95: Val Loss 230.93723
Epoch 96: Val Loss 230.83539
Epoch 97: Val Loss 230.73181
Epoch 98: Val Loss 230.62640
Epoch 99: Val Loss 230.51958
{'MSE - mean': 214.27060843311787, 'MSE - std': 13.610476948992407, 'R2 - mean': -323.0233045727449, 'R2 - std': 48.22355122735818} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 214.27060843311787 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -4, 'dropout': 0}. Best is trial 17 with value: 0.5934298657769321.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.21127
Epoch 1: Val Loss 221.11694
Epoch 2: Val Loss 219.92085
Epoch 3: Val Loss 218.59441
Epoch 4: Val Loss 217.11740
Epoch 5: Val Loss 215.45804
Epoch 6: Val Loss 213.58089
Epoch 7: Val Loss 211.46524
Epoch 8: Val Loss 209.09064
Epoch 9: Val Loss 206.41289
Epoch 10: Val Loss 203.36392
Epoch 11: Val Loss 199.88588
Epoch 12: Val Loss 195.92439
Epoch 13: Val Loss 191.42999
Epoch 14: Val Loss 186.34833
Epoch 15: Val Loss 180.62354
Epoch 16: Val Loss 174.18864
Epoch 17: Val Loss 166.97997
Epoch 18: Val Loss 158.96222
Epoch 19: Val Loss 150.05492
Epoch 20: Val Loss 140.32094
Epoch 21: Val Loss 129.72475
Epoch 22: Val Loss 118.34195
Epoch 23: Val Loss 106.26987
Epoch 24: Val Loss 93.69001
Epoch 25: Val Loss 80.85753
Epoch 26: Val Loss 68.08509
Epoch 27: Val Loss 55.69593
Epoch 28: Val Loss 44.13726
Epoch 29: Val Loss 33.77840
Epoch 30: Val Loss 25.01742
Epoch 31: Val Loss 18.16895
Epoch 32: Val Loss 13.21398
Epoch 33: Val Loss 9.97684
Epoch 34: Val Loss 8.08096
Epoch 35: Val Loss 7.04816
Epoch 36: Val Loss 6.40541
Epoch 37: Val Loss 5.85935
Epoch 38: Val Loss 5.29657
Epoch 39: Val Loss 4.73511
Epoch 40: Val Loss 4.21043
Epoch 41: Val Loss 3.75623
Epoch 42: Val Loss 3.38024
Epoch 43: Val Loss 3.06772
Epoch 44: Val Loss 2.79347
Epoch 45: Val Loss 2.55056
Epoch 46: Val Loss 2.32824
Epoch 47: Val Loss 2.12094
Epoch 48: Val Loss 1.93426
Epoch 49: Val Loss 1.76556
Epoch 50: Val Loss 1.61017
Epoch 51: Val Loss 1.47963
Epoch 52: Val Loss 1.36402
Epoch 53: Val Loss 1.26484
Epoch 54: Val Loss 1.17660
Epoch 55: Val Loss 1.09974
Epoch 56: Val Loss 1.03354
Epoch 57: Val Loss 0.97393
Epoch 58: Val Loss 0.92103
Epoch 59: Val Loss 0.87339
Epoch 60: Val Loss 0.83187
Epoch 61: Val Loss 0.79414
Epoch 62: Val Loss 0.76131
Epoch 63: Val Loss 0.73241
Epoch 64: Val Loss 0.70740
Epoch 65: Val Loss 0.68484
Epoch 66: Val Loss 0.66537
Epoch 67: Val Loss 0.64898
Epoch 68: Val Loss 0.63401
Epoch 69: Val Loss 0.62099
Epoch 70: Val Loss 0.61088
Epoch 71: Val Loss 0.59959
Epoch 72: Val Loss 0.59057
Epoch 73: Val Loss 0.58252
Epoch 74: Val Loss 0.57537
Epoch 75: Val Loss 0.56887
Epoch 76: Val Loss 0.56425
Epoch 77: Val Loss 0.56009
Epoch 78: Val Loss 0.55624
Epoch 79: Val Loss 0.55259
Epoch 80: Val Loss 0.54869
Epoch 81: Val Loss 0.54540
Epoch 82: Val Loss 0.54170
Epoch 83: Val Loss 0.53933
Epoch 84: Val Loss 0.53651
Epoch 85: Val Loss 0.53535
Epoch 86: Val Loss 0.53317
Epoch 87: Val Loss 0.53124
Epoch 88: Val Loss 0.53002
Epoch 89: Val Loss 0.52996
Epoch 90: Val Loss 0.52954
Epoch 91: Val Loss 0.52842
Epoch 92: Val Loss 0.52816
Epoch 93: Val Loss 0.52724
Epoch 94: Val Loss 0.52596
Epoch 95: Val Loss 0.52435
Epoch 96: Val Loss 0.52351
Epoch 97: Val Loss 0.52217
Epoch 98: Val Loss 0.52187
Epoch 99: Val Loss 0.52131
{'MSE - mean': 0.5213099290068887, 'MSE - std': 0.0, 'R2 - mean': 0.051388295112519966, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.54276
Epoch 1: Val Loss 224.05400
Epoch 2: Val Loss 223.54475
Epoch 3: Val Loss 222.99159
Epoch 4: Val Loss 222.36583
Epoch 5: Val Loss 221.64340
Epoch 6: Val Loss 220.81137
Epoch 7: Val Loss 219.85088
Epoch 8: Val Loss 218.73178
Epoch 9: Val Loss 217.42230
Epoch 10: Val Loss 215.88307
Epoch 11: Val Loss 214.07899
Epoch 12: Val Loss 211.95729
Epoch 13: Val Loss 209.46986
Epoch 14: Val Loss 206.54964
Epoch 15: Val Loss 203.13651
Epoch 16: Val Loss 199.15939
Epoch 17: Val Loss 194.52153
Epoch 18: Val Loss 189.14856
Epoch 19: Val Loss 182.97531
Epoch 20: Val Loss 175.90247
Epoch 21: Val Loss 167.88600
Epoch 22: Val Loss 158.87730
Epoch 23: Val Loss 148.79308
Epoch 24: Val Loss 137.65456
Epoch 25: Val Loss 125.52369
Epoch 26: Val Loss 112.52174
Epoch 27: Val Loss 98.82290
Epoch 28: Val Loss 84.64178
Epoch 29: Val Loss 70.32016
Epoch 30: Val Loss 56.30673
Epoch 31: Val Loss 43.14857
Epoch 32: Val Loss 31.37663
Epoch 33: Val Loss 21.46399
Epoch 34: Val Loss 13.77853
Epoch 35: Val Loss 8.46208
Epoch 36: Val Loss 5.29609
Epoch 37: Val Loss 3.75834
Epoch 38: Val Loss 3.20422
Epoch 39: Val Loss 3.01912
Epoch 40: Val Loss 2.83597
Epoch 41: Val Loss 2.55257
Epoch 42: Val Loss 2.20718
Epoch 43: Val Loss 1.88713
Epoch 44: Val Loss 1.63136
Epoch 45: Val Loss 1.44538
Epoch 46: Val Loss 1.31588
Epoch 47: Val Loss 1.21738
Epoch 48: Val Loss 1.13177
Epoch 49: Val Loss 1.05261
Epoch 50: Val Loss 0.97840
Epoch 51: Val Loss 0.91294
Epoch 52: Val Loss 0.85602
Epoch 53: Val Loss 0.80743
Epoch 54: Val Loss 0.76776
Epoch 55: Val Loss 0.73470
Epoch 56: Val Loss 0.70734
Epoch 57: Val Loss 0.68498
Epoch 58: Val Loss 0.66645
Epoch 59: Val Loss 0.65001
Epoch 60: Val Loss 0.63647
Epoch 61: Val Loss 0.62520
Epoch 62: Val Loss 0.61509
Epoch 63: Val Loss 0.60664
Epoch 64: Val Loss 0.59953
Epoch 65: Val Loss 0.59351
Epoch 66: Val Loss 0.58826
Epoch 67: Val Loss 0.58444
Epoch 68: Val Loss 0.58057
Epoch 69: Val Loss 0.57687
Epoch 70: Val Loss 0.57458
Epoch 71: Val Loss 0.57242
Epoch 72: Val Loss 0.57026
Epoch 73: Val Loss 0.56881
Epoch 74: Val Loss 0.56787
Epoch 75: Val Loss 0.56604
Epoch 76: Val Loss 0.56499
Epoch 77: Val Loss 0.56404
Epoch 78: Val Loss 0.56299
Epoch 79: Val Loss 0.56246
Epoch 80: Val Loss 0.56247
Epoch 81: Val Loss 0.56190
Epoch 82: Val Loss 0.56193
Epoch 83: Val Loss 0.56192
Epoch 84: Val Loss 0.56176
Epoch 85: Val Loss 0.56168
Epoch 86: Val Loss 0.56205
Epoch 87: Val Loss 0.56243
Epoch 88: Val Loss 0.56348
Epoch 89: Val Loss 0.56344
Epoch 90: Val Loss 0.56297
Epoch 91: Val Loss 0.56300
Epoch 92: Val Loss 0.56324
Epoch 93: Val Loss 0.56369
Epoch 94: Val Loss 0.56444
Epoch 95: Val Loss 0.56483
Epoch 96: Val Loss 0.56539
Epoch 97: Val Loss 0.56520
Epoch 98: Val Loss 0.56487
Epoch 99: Val Loss 0.56517
{'MSE - mean': 0.5414953147256744, 'MSE - std': 0.020185385718785698, 'R2 - mean': 0.14538351559957435, 'R2 - std': 0.09399522048705439} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 221.61557
Epoch 1: Val Loss 220.06392
Epoch 2: Val Loss 218.45932
Epoch 3: Val Loss 216.80888
Epoch 4: Val Loss 215.04378
Epoch 5: Val Loss 213.13304
Epoch 6: Val Loss 211.05453
Epoch 7: Val Loss 208.77013
Epoch 8: Val Loss 206.25157
Epoch 9: Val Loss 203.45670
Epoch 10: Val Loss 200.34123
Epoch 11: Val Loss 196.86537
Epoch 12: Val Loss 193.01373
Epoch 13: Val Loss 188.74754
Epoch 14: Val Loss 183.99973
Epoch 15: Val Loss 178.74696
Epoch 16: Val Loss 172.93526
Epoch 17: Val Loss 166.51192
Epoch 18: Val Loss 159.45155
Epoch 19: Val Loss 151.72313
Epoch 20: Val Loss 143.29990
Epoch 21: Val Loss 134.16100
Epoch 22: Val Loss 124.40486
Epoch 23: Val Loss 114.09255
Epoch 24: Val Loss 103.27498
Epoch 25: Val Loss 92.09852
Epoch 26: Val Loss 80.74634
Epoch 27: Val Loss 69.39430
Epoch 28: Val Loss 58.38112
Epoch 29: Val Loss 47.99878
Epoch 30: Val Loss 38.56284
Epoch 31: Val Loss 30.29436
Epoch 32: Val Loss 23.44088
Epoch 33: Val Loss 18.05451
Epoch 34: Val Loss 14.08997
Epoch 35: Val Loss 11.31953
Epoch 36: Val Loss 9.48908
Epoch 37: Val Loss 8.26159
Epoch 38: Val Loss 7.35181
Epoch 39: Val Loss 6.59326
Epoch 40: Val Loss 5.91408
Epoch 41: Val Loss 5.30246
Epoch 42: Val Loss 4.75294
Epoch 43: Val Loss 4.27478
Epoch 44: Val Loss 3.85724
Epoch 45: Val Loss 3.50387
Epoch 46: Val Loss 3.20577
Epoch 47: Val Loss 2.93701
Epoch 48: Val Loss 2.69050
Epoch 49: Val Loss 2.46842
Epoch 50: Val Loss 2.26491
Epoch 51: Val Loss 2.08553
Epoch 52: Val Loss 1.92588
Epoch 53: Val Loss 1.78282
Epoch 54: Val Loss 1.65313
Epoch 55: Val Loss 1.53756
Epoch 56: Val Loss 1.43454
Epoch 57: Val Loss 1.34646
Epoch 58: Val Loss 1.26894
Epoch 59: Val Loss 1.20061
Epoch 60: Val Loss 1.14044
Epoch 61: Val Loss 1.08599
Epoch 62: Val Loss 1.03852
Epoch 63: Val Loss 0.99596
Epoch 64: Val Loss 0.95756
Epoch 65: Val Loss 0.92361
Epoch 66: Val Loss 0.89302
Epoch 67: Val Loss 0.86628
Epoch 68: Val Loss 0.84399
Epoch 69: Val Loss 0.82338
Epoch 70: Val Loss 0.80466
Epoch 71: Val Loss 0.78755
Epoch 72: Val Loss 0.77177
Epoch 73: Val Loss 0.75948
Epoch 74: Val Loss 0.74798
Epoch 75: Val Loss 0.73726
Epoch 76: Val Loss 0.72725
Epoch 77: Val Loss 0.71927
Epoch 78: Val Loss 0.71158
Epoch 79: Val Loss 0.70501
Epoch 80: Val Loss 0.69889
Epoch 81: Val Loss 0.69436
Epoch 82: Val Loss 0.69033
Epoch 83: Val Loss 0.68645
Epoch 84: Val Loss 0.68347
Epoch 85: Val Loss 0.68009
Epoch 86: Val Loss 0.67770
Epoch 87: Val Loss 0.67445
Epoch 88: Val Loss 0.67193
Epoch 89: Val Loss 0.67011
Epoch 90: Val Loss 0.66854
Epoch 91: Val Loss 0.66735
Epoch 92: Val Loss 0.66674
Epoch 93: Val Loss 0.66612
Epoch 94: Val Loss 0.66479
Epoch 95: Val Loss 0.66399
Epoch 96: Val Loss 0.66342
Epoch 97: Val Loss 0.66300
Epoch 98: Val Loss 0.66259
Epoch 99: Val Loss 0.66242
{'MSE - mean': 0.5818025566422819, 'MSE - std': 0.05933785216530795, 'R2 - mean': 0.10976452631226463, 'R2 - std': 0.09180137496413447} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 218.44060
Epoch 1: Val Loss 217.27487
Epoch 2: Val Loss 216.05400
Epoch 3: Val Loss 214.75967
Epoch 4: Val Loss 213.37074
Epoch 5: Val Loss 211.85780
Epoch 6: Val Loss 210.18819
Epoch 7: Val Loss 208.32866
Epoch 8: Val Loss 206.23993
Epoch 9: Val Loss 203.87560
Epoch 10: Val Loss 201.18617
Epoch 11: Val Loss 198.11935
Epoch 12: Val Loss 194.60445
Epoch 13: Val Loss 190.57979
Epoch 14: Val Loss 185.98221
Epoch 15: Val Loss 180.73421
Epoch 16: Val Loss 174.77071
Epoch 17: Val Loss 167.99419
Epoch 18: Val Loss 160.35464
Epoch 19: Val Loss 151.77570
Epoch 20: Val Loss 142.22893
Epoch 21: Val Loss 131.71965
Epoch 22: Val Loss 120.27675
Epoch 23: Val Loss 107.99139
Epoch 24: Val Loss 95.04195
Epoch 25: Val Loss 81.64983
Epoch 26: Val Loss 68.16753
Epoch 27: Val Loss 54.98587
Epoch 28: Val Loss 42.53910
Epoch 29: Val Loss 31.33065
Epoch 30: Val Loss 21.85906
Epoch 31: Val Loss 14.48531
Epoch 32: Val Loss 9.35235
Epoch 33: Val Loss 6.28775
Epoch 34: Val Loss 4.82445
Epoch 35: Val Loss 4.29240
Epoch 36: Val Loss 4.09929
Epoch 37: Val Loss 3.87071
Epoch 38: Val Loss 3.50055
Epoch 39: Val Loss 3.04800
Epoch 40: Val Loss 2.61666
Epoch 41: Val Loss 2.26295
Epoch 42: Val Loss 1.99604
Epoch 43: Val Loss 1.79110
Epoch 44: Val Loss 1.62592
Epoch 45: Val Loss 1.48530
Epoch 46: Val Loss 1.35705
Epoch 47: Val Loss 1.24206
Epoch 48: Val Loss 1.14041
Epoch 49: Val Loss 1.05490
Epoch 50: Val Loss 0.98379
Epoch 51: Val Loss 0.92306
Epoch 52: Val Loss 0.87141
Epoch 53: Val Loss 0.82734
Epoch 54: Val Loss 0.78904
Epoch 55: Val Loss 0.75758
Epoch 56: Val Loss 0.73065
Epoch 57: Val Loss 0.70840
Epoch 58: Val Loss 0.68943
Epoch 59: Val Loss 0.67344
Epoch 60: Val Loss 0.66011
Epoch 61: Val Loss 0.64928
Epoch 62: Val Loss 0.64070
Epoch 63: Val Loss 0.63322
Epoch 64: Val Loss 0.62730
Epoch 65: Val Loss 0.62288
Epoch 66: Val Loss 0.61884
Epoch 67: Val Loss 0.61579
Epoch 68: Val Loss 0.61337
Epoch 69: Val Loss 0.61098
Epoch 70: Val Loss 0.60869
Epoch 71: Val Loss 0.60660
Epoch 72: Val Loss 0.60429
Epoch 73: Val Loss 0.60292
Epoch 74: Val Loss 0.60170
Epoch 75: Val Loss 0.60009
Epoch 76: Val Loss 0.59838
Epoch 77: Val Loss 0.59712
Epoch 78: Val Loss 0.59658
Epoch 79: Val Loss 0.59592
Epoch 80: Val Loss 0.59557
Epoch 81: Val Loss 0.59422
Epoch 82: Val Loss 0.59347
Epoch 83: Val Loss 0.59291
Epoch 84: Val Loss 0.59198
Epoch 85: Val Loss 0.59176
Epoch 86: Val Loss 0.59130
Epoch 87: Val Loss 0.59129
Epoch 88: Val Loss 0.59159
Epoch 89: Val Loss 0.59145
Epoch 90: Val Loss 0.59187
Epoch 91: Val Loss 0.59147
Epoch 92: Val Loss 0.59093
Epoch 93: Val Loss 0.59071
Epoch 94: Val Loss 0.58997
Epoch 95: Val Loss 0.58949
Epoch 96: Val Loss 0.58970
Epoch 97: Val Loss 0.58980
Epoch 98: Val Loss 0.58982
Epoch 99: Val Loss 0.58962
{'MSE - mean': 0.5837239420721226, 'MSE - std': 0.05149573468360239, 'R2 - mean': 0.14435975067901582, 'R2 - std': 0.099554547766221} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.70555
Epoch 1: Val Loss 216.87367
Epoch 2: Val Loss 215.99068
Epoch 3: Val Loss 215.03520
Epoch 4: Val Loss 213.99013
Epoch 5: Val Loss 212.81929
Epoch 6: Val Loss 211.51276
Epoch 7: Val Loss 210.05949
Epoch 8: Val Loss 208.42807
Epoch 9: Val Loss 206.56567
Epoch 10: Val Loss 204.45738
Epoch 11: Val Loss 202.05365
Epoch 12: Val Loss 199.32184
Epoch 13: Val Loss 196.19930
Epoch 14: Val Loss 192.62224
Epoch 15: Val Loss 188.52969
Epoch 16: Val Loss 183.86391
Epoch 17: Val Loss 178.57320
Epoch 18: Val Loss 172.54971
Epoch 19: Val Loss 165.69435
Epoch 20: Val Loss 157.99394
Epoch 21: Val Loss 149.37553
Epoch 22: Val Loss 139.85213
Epoch 23: Val Loss 129.41701
Epoch 24: Val Loss 118.11824
Epoch 25: Val Loss 106.05589
Epoch 26: Val Loss 93.39048
Epoch 27: Val Loss 80.37010
Epoch 28: Val Loss 67.25888
Epoch 29: Val Loss 54.44810
Epoch 30: Val Loss 42.44534
Epoch 31: Val Loss 31.71194
Epoch 32: Val Loss 22.69664
Epoch 33: Val Loss 15.72116
Epoch 34: Val Loss 10.87116
Epoch 35: Val Loss 7.89260
Epoch 36: Val Loss 6.34044
Epoch 37: Val Loss 5.60204
Epoch 38: Val Loss 5.15723
Epoch 39: Val Loss 4.72827
Epoch 40: Val Loss 4.25892
Epoch 41: Val Loss 3.77976
Epoch 42: Val Loss 3.36150
Epoch 43: Val Loss 3.02538
Epoch 44: Val Loss 2.75770
Epoch 45: Val Loss 2.53299
Epoch 46: Val Loss 2.33453
Epoch 47: Val Loss 2.15071
Epoch 48: Val Loss 1.97551
Epoch 49: Val Loss 1.81245
Epoch 50: Val Loss 1.65785
Epoch 51: Val Loss 1.52254
Epoch 52: Val Loss 1.40293
Epoch 53: Val Loss 1.30180
Epoch 54: Val Loss 1.21405
Epoch 55: Val Loss 1.13489
Epoch 56: Val Loss 1.06517
Epoch 57: Val Loss 1.00418
Epoch 58: Val Loss 0.95013
Epoch 59: Val Loss 0.90408
Epoch 60: Val Loss 0.86459
Epoch 61: Val Loss 0.82940
Epoch 62: Val Loss 0.79872
Epoch 63: Val Loss 0.77113
Epoch 64: Val Loss 0.74721
Epoch 65: Val Loss 0.72769
Epoch 66: Val Loss 0.71034
Epoch 67: Val Loss 0.69525
Epoch 68: Val Loss 0.68211
Epoch 69: Val Loss 0.67055
Epoch 70: Val Loss 0.66158
Epoch 71: Val Loss 0.65331
Epoch 72: Val Loss 0.64792
Epoch 73: Val Loss 0.64276
Epoch 74: Val Loss 0.63753
Epoch 75: Val Loss 0.63361
Epoch 76: Val Loss 0.63076
Epoch 77: Val Loss 0.62853
Epoch 78: Val Loss 0.62705
Epoch 79: Val Loss 0.62614
Epoch 80: Val Loss 0.62588
Epoch 81: Val Loss 0.62540
Epoch 82: Val Loss 0.62456
Epoch 83: Val Loss 0.62465
Epoch 84: Val Loss 0.62425
Epoch 85: Val Loss 0.62438
Epoch 86: Val Loss 0.62420
Epoch 87: Val Loss 0.62396
Epoch 88: Val Loss 0.62415
Epoch 89: Val Loss 0.62449
Epoch 90: Val Loss 0.62485
Epoch 91: Val Loss 0.62553
Epoch 92: Val Loss 0.62535
Epoch 93: Val Loss 0.62503
Epoch 94: Val Loss 0.62564
Epoch 95: Val Loss 0.62510
Epoch 96: Val Loss 0.62510
Epoch 97: Val Loss 0.62371
Epoch 98: Val Loss 0.62295
Epoch 99: Val Loss 0.62325
{'MSE - mean': 0.5915682181299382, 'MSE - std': 0.048657776562108256, 'R2 - mean': 0.10872529864267613, 'R2 - std': 0.11405324665466653} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 0.5915682181299382 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 218.03453
Epoch 1: Val Loss 217.29878
Epoch 2: Val Loss 216.52383
Epoch 3: Val Loss 215.69601
Epoch 4: Val Loss 214.78987
Epoch 5: Val Loss 213.78870
Epoch 6: Val Loss 212.66457
Epoch 7: Val Loss 211.39931
Epoch 8: Val Loss 209.94525
Epoch 9: Val Loss 208.29936
Epoch 10: Val Loss 206.42224
Epoch 11: Val Loss 204.27263
Epoch 12: Val Loss 201.84801
Epoch 13: Val Loss 199.09412
Epoch 14: Val Loss 195.97997
Epoch 15: Val Loss 192.46388
Epoch 16: Val Loss 188.50206
Epoch 17: Val Loss 184.03484
Epoch 18: Val Loss 179.00890
Epoch 19: Val Loss 173.37941
Epoch 20: Val Loss 167.07909
Epoch 21: Val Loss 160.05862
Epoch 22: Val Loss 152.30853
Epoch 23: Val Loss 143.78401
Epoch 24: Val Loss 134.49704
Epoch 25: Val Loss 124.44572
Epoch 26: Val Loss 113.71851
Epoch 27: Val Loss 102.41586
Epoch 28: Val Loss 90.69396
Epoch 29: Val Loss 78.76830
Epoch 30: Val Loss 66.89098
Epoch 31: Val Loss 55.33786
Epoch 32: Val Loss 44.44490
Epoch 33: Val Loss 34.52013
Epoch 34: Val Loss 25.87388
Epoch 35: Val Loss 18.73269
Epoch 36: Val Loss 13.29600
Epoch 37: Val Loss 9.50033
Epoch 38: Val Loss 7.14902
Epoch 39: Val Loss 5.90695
Epoch 40: Val Loss 5.34467
Epoch 41: Val Loss 5.05927
Epoch 42: Val Loss 4.78195
Epoch 43: Val Loss 4.42552
Epoch 44: Val Loss 4.01660
Epoch 45: Val Loss 3.60888
Epoch 46: Val Loss 3.24724
Epoch 47: Val Loss 2.94470
Epoch 48: Val Loss 2.68627
Epoch 49: Val Loss 2.46745
Epoch 50: Val Loss 2.27828
Epoch 51: Val Loss 2.10663
Epoch 52: Val Loss 1.94813
Epoch 53: Val Loss 1.80604
Epoch 54: Val Loss 1.67333
Epoch 55: Val Loss 1.55424
Epoch 56: Val Loss 1.44636
Epoch 57: Val Loss 1.34670
Epoch 58: Val Loss 1.25548
Epoch 59: Val Loss 1.17298
Epoch 60: Val Loss 1.09832
Epoch 61: Val Loss 1.02951
Epoch 62: Val Loss 0.96854
Epoch 63: Val Loss 0.91389
Epoch 64: Val Loss 0.86381
Epoch 65: Val Loss 0.81948
Epoch 66: Val Loss 0.78030
Epoch 67: Val Loss 0.74547
Epoch 68: Val Loss 0.71342
Epoch 69: Val Loss 0.68538
Epoch 70: Val Loss 0.66021
Epoch 71: Val Loss 0.63927
Epoch 72: Val Loss 0.62052
Epoch 73: Val Loss 0.60449
Epoch 74: Val Loss 0.59033
Epoch 75: Val Loss 0.57879
Epoch 76: Val Loss 0.56723
Epoch 77: Val Loss 0.55807
Epoch 78: Val Loss 0.55015
Epoch 79: Val Loss 0.54324
Epoch 80: Val Loss 0.53688
Epoch 81: Val Loss 0.53261
Epoch 82: Val Loss 0.52842
Epoch 83: Val Loss 0.52531
Epoch 84: Val Loss 0.52287
Epoch 85: Val Loss 0.51992
Epoch 86: Val Loss 0.51738
Epoch 87: Val Loss 0.51591
Epoch 88: Val Loss 0.51417
Epoch 89: Val Loss 0.51268
Epoch 90: Val Loss 0.51186
Epoch 91: Val Loss 0.51019
Epoch 92: Val Loss 0.50968
Epoch 93: Val Loss 0.50876
Epoch 94: Val Loss 0.50854
Epoch 95: Val Loss 0.50903
Epoch 96: Val Loss 0.50848
Epoch 97: Val Loss 0.50739
Epoch 98: Val Loss 0.50831
Epoch 99: Val Loss 0.50862
{'MSE - mean': 0.5073876011077134, 'MSE - std': 0.0, 'R2 - mean': 0.0767223286107861, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 217.72313
Epoch 1: Val Loss 216.81721
Epoch 2: Val Loss 215.85011
Epoch 3: Val Loss 214.80583
Epoch 4: Val Loss 213.67178
Epoch 5: Val Loss 212.40546
Epoch 6: Val Loss 210.98386
Epoch 7: Val Loss 209.35115
Epoch 8: Val Loss 207.49043
Epoch 9: Val Loss 205.33110
Epoch 10: Val Loss 202.84224
Epoch 11: Val Loss 200.00894
Epoch 12: Val Loss 196.77147
Epoch 13: Val Loss 193.10130
Epoch 14: Val Loss 188.95204
Epoch 15: Val Loss 184.26579
Epoch 16: Val Loss 178.98793
Epoch 17: Val Loss 173.07372
Epoch 18: Val Loss 166.45302
Epoch 19: Val Loss 159.08823
Epoch 20: Val Loss 150.93269
Epoch 21: Val Loss 141.94751
Epoch 22: Val Loss 132.13391
Epoch 23: Val Loss 121.57603
Epoch 24: Val Loss 110.31870
Epoch 25: Val Loss 98.43980
Epoch 26: Val Loss 86.18439
Epoch 27: Val Loss 73.81188
Epoch 28: Val Loss 61.61259
Epoch 29: Val Loss 49.96757
Epoch 30: Val Loss 39.28986
Epoch 31: Val Loss 29.96766
Epoch 32: Val Loss 22.30651
Epoch 33: Val Loss 16.43054
Epoch 34: Val Loss 12.28006
Epoch 35: Val Loss 9.62284
Epoch 36: Val Loss 8.04434
Epoch 37: Val Loss 7.06975
Epoch 38: Val Loss 6.34915
Epoch 39: Val Loss 5.72931
Epoch 40: Val Loss 5.15947
Epoch 41: Val Loss 4.65417
Epoch 42: Val Loss 4.23157
Epoch 43: Val Loss 3.88146
Epoch 44: Val Loss 3.58803
Epoch 45: Val Loss 3.32694
Epoch 46: Val Loss 3.07747
Epoch 47: Val Loss 2.84461
Epoch 48: Val Loss 2.62088
Epoch 49: Val Loss 2.40815
Epoch 50: Val Loss 2.21582
Epoch 51: Val Loss 2.04794
Epoch 52: Val Loss 1.89960
Epoch 53: Val Loss 1.76603
Epoch 54: Val Loss 1.65104
Epoch 55: Val Loss 1.54614
Epoch 56: Val Loss 1.45597
Epoch 57: Val Loss 1.37639
Epoch 58: Val Loss 1.29959
Epoch 59: Val Loss 1.23075
Epoch 60: Val Loss 1.16866
Epoch 61: Val Loss 1.11654
Epoch 62: Val Loss 1.06846
Epoch 63: Val Loss 1.02653
Epoch 64: Val Loss 0.98831
Epoch 65: Val Loss 0.95271
Epoch 66: Val Loss 0.92222
Epoch 67: Val Loss 0.89448
Epoch 68: Val Loss 0.86946
Epoch 69: Val Loss 0.84688
Epoch 70: Val Loss 0.82744
Epoch 71: Val Loss 0.80960
Epoch 72: Val Loss 0.79244
Epoch 73: Val Loss 0.77681
Epoch 74: Val Loss 0.76353
Epoch 75: Val Loss 0.75137
Epoch 76: Val Loss 0.74142
Epoch 77: Val Loss 0.73259
Epoch 78: Val Loss 0.72444
Epoch 79: Val Loss 0.71772
Epoch 80: Val Loss 0.71049
Epoch 81: Val Loss 0.70451
Epoch 82: Val Loss 0.69973
Epoch 83: Val Loss 0.69505
Epoch 84: Val Loss 0.69046
Epoch 85: Val Loss 0.68633
Epoch 86: Val Loss 0.68328
Epoch 87: Val Loss 0.68076
Epoch 88: Val Loss 0.67741
Epoch 89: Val Loss 0.67377
Epoch 90: Val Loss 0.66927
Epoch 91: Val Loss 0.66607
Epoch 92: Val Loss 0.66420
Epoch 93: Val Loss 0.66066
Epoch 94: Val Loss 0.65959
Epoch 95: Val Loss 0.65836
Epoch 96: Val Loss 0.65756
Epoch 97: Val Loss 0.65648
Epoch 98: Val Loss 0.65570
Epoch 99: Val Loss 0.65413
{'MSE - mean': 0.5807566262804227, 'MSE - std': 0.07336902517270927, 'R2 - mean': 0.0954566107283058, 'R2 - std': 0.018734282117519707} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.95937
Epoch 1: Val Loss 236.14529
Epoch 2: Val Loss 235.34540
Epoch 3: Val Loss 234.53481
Epoch 4: Val Loss 233.64870
Epoch 5: Val Loss 232.62985
Epoch 6: Val Loss 231.46690
Epoch 7: Val Loss 230.13174
Epoch 8: Val Loss 228.60687
Epoch 9: Val Loss 226.87387
Epoch 10: Val Loss 224.86523
Epoch 11: Val Loss 222.56499
Epoch 12: Val Loss 219.94479
Epoch 13: Val Loss 216.98813
Epoch 14: Val Loss 213.64891
Epoch 15: Val Loss 209.87091
Epoch 16: Val Loss 205.62781
Epoch 17: Val Loss 200.86385
Epoch 18: Val Loss 195.50497
Epoch 19: Val Loss 189.49466
Epoch 20: Val Loss 182.78955
Epoch 21: Val Loss 175.34908
Epoch 22: Val Loss 167.12370
Epoch 23: Val Loss 158.09146
Epoch 24: Val Loss 148.23665
Epoch 25: Val Loss 137.53691
Epoch 26: Val Loss 126.02348
Epoch 27: Val Loss 113.78658
Epoch 28: Val Loss 100.98495
Epoch 29: Val Loss 87.84634
Epoch 30: Val Loss 74.62120
Epoch 31: Val Loss 61.61591
Epoch 32: Val Loss 49.20482
Epoch 33: Val Loss 37.76067
Epoch 34: Val Loss 27.68808
Epoch 35: Val Loss 19.28916
Epoch 36: Val Loss 12.80976
Epoch 37: Val Loss 8.26817
Epoch 38: Val Loss 5.48626
Epoch 39: Val Loss 4.08304
Epoch 40: Val Loss 3.58103
Epoch 41: Val Loss 3.48751
Epoch 42: Val Loss 3.44115
Epoch 43: Val Loss 3.28995
Epoch 44: Val Loss 3.01501
Epoch 45: Val Loss 2.67349
Epoch 46: Val Loss 2.34196
Epoch 47: Val Loss 2.05155
Epoch 48: Val Loss 1.81744
Epoch 49: Val Loss 1.63914
Epoch 50: Val Loss 1.49774
Epoch 51: Val Loss 1.38244
Epoch 52: Val Loss 1.28945
Epoch 53: Val Loss 1.21043
Epoch 54: Val Loss 1.14294
Epoch 55: Val Loss 1.08352
Epoch 56: Val Loss 1.03377
Epoch 57: Val Loss 0.98898
Epoch 58: Val Loss 0.94959
Epoch 59: Val Loss 0.91553
Epoch 60: Val Loss 0.88430
Epoch 61: Val Loss 0.85446
Epoch 62: Val Loss 0.82784
Epoch 63: Val Loss 0.80363
Epoch 64: Val Loss 0.78330
Epoch 65: Val Loss 0.76470
Epoch 66: Val Loss 0.74804
Epoch 67: Val Loss 0.73455
Epoch 68: Val Loss 0.72338
Epoch 69: Val Loss 0.71393
Epoch 70: Val Loss 0.70641
Epoch 71: Val Loss 0.69948
Epoch 72: Val Loss 0.69429
Epoch 73: Val Loss 0.68991
Epoch 74: Val Loss 0.68608
Epoch 75: Val Loss 0.68240
Epoch 76: Val Loss 0.67971
Epoch 77: Val Loss 0.67741
Epoch 78: Val Loss 0.67628
Epoch 79: Val Loss 0.67522
Epoch 80: Val Loss 0.67359
Epoch 81: Val Loss 0.67291
Epoch 82: Val Loss 0.67187
Epoch 83: Val Loss 0.67098
Epoch 84: Val Loss 0.67182
Epoch 85: Val Loss 0.67183
Epoch 86: Val Loss 0.67161
Epoch 87: Val Loss 0.67182
Epoch 88: Val Loss 0.67236
Epoch 89: Val Loss 0.67256
Epoch 90: Val Loss 0.67267
Epoch 91: Val Loss 0.67223
Epoch 92: Val Loss 0.67210
Epoch 93: Val Loss 0.67269
Epoch 94: Val Loss 0.67312
Epoch 95: Val Loss 0.67246
Epoch 96: Val Loss 0.67111
Epoch 97: Val Loss 0.67090
Epoch 98: Val Loss 0.67036
Epoch 99: Val Loss 0.66985
{'MSE - mean': 0.6104544343336368, 'MSE - std': 0.0731614345781085, 'R2 - mean': 0.07288367814417891, 'R2 - std': 0.03539854218091879} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.23074
Epoch 1: Val Loss 223.11351
Epoch 2: Val Loss 222.01581
Epoch 3: Val Loss 220.90768
Epoch 4: Val Loss 219.76495
Epoch 5: Val Loss 218.55467
Epoch 6: Val Loss 217.24257
Epoch 7: Val Loss 215.81433
Epoch 8: Val Loss 214.24591
Epoch 9: Val Loss 212.50525
Epoch 10: Val Loss 210.56392
Epoch 11: Val Loss 208.37137
Epoch 12: Val Loss 205.89276
Epoch 13: Val Loss 203.09334
Epoch 14: Val Loss 199.91489
Epoch 15: Val Loss 196.33325
Epoch 16: Val Loss 192.28642
Epoch 17: Val Loss 187.73473
Epoch 18: Val Loss 182.60455
Epoch 19: Val Loss 176.85526
Epoch 20: Val Loss 170.39650
Epoch 21: Val Loss 163.19588
Epoch 22: Val Loss 155.22733
Epoch 23: Val Loss 146.44479
Epoch 24: Val Loss 136.87883
Epoch 25: Val Loss 126.54901
Epoch 26: Val Loss 115.52380
Epoch 27: Val Loss 103.93787
Epoch 28: Val Loss 91.95769
Epoch 29: Val Loss 79.77765
Epoch 30: Val Loss 67.67085
Epoch 31: Val Loss 55.94621
Epoch 32: Val Loss 44.97953
Epoch 33: Val Loss 35.10318
Epoch 34: Val Loss 26.62864
Epoch 35: Val Loss 19.80117
Epoch 36: Val Loss 14.70596
Epoch 37: Val Loss 11.17305
Epoch 38: Val Loss 8.95315
Epoch 39: Val Loss 7.65084
Epoch 40: Val Loss 6.85337
Epoch 41: Val Loss 6.28103
Epoch 42: Val Loss 5.76941
Epoch 43: Val Loss 5.27987
Epoch 44: Val Loss 4.81248
Epoch 45: Val Loss 4.40081
Epoch 46: Val Loss 4.04425
Epoch 47: Val Loss 3.74343
Epoch 48: Val Loss 3.48388
Epoch 49: Val Loss 3.24721
Epoch 50: Val Loss 3.02681
Epoch 51: Val Loss 2.81382
Epoch 52: Val Loss 2.61559
Epoch 53: Val Loss 2.42531
Epoch 54: Val Loss 2.25166
Epoch 55: Val Loss 2.09098
Epoch 56: Val Loss 1.94260
Epoch 57: Val Loss 1.81259
Epoch 58: Val Loss 1.69877
Epoch 59: Val Loss 1.59543
Epoch 60: Val Loss 1.50376
Epoch 61: Val Loss 1.41986
Epoch 62: Val Loss 1.34532
Epoch 63: Val Loss 1.27812
Epoch 64: Val Loss 1.21703
Epoch 65: Val Loss 1.16262
Epoch 66: Val Loss 1.11299
Epoch 67: Val Loss 1.06702
Epoch 68: Val Loss 1.02600
Epoch 69: Val Loss 0.99011
Epoch 70: Val Loss 0.95732
Epoch 71: Val Loss 0.92736
Epoch 72: Val Loss 0.90188
Epoch 73: Val Loss 0.87761
Epoch 74: Val Loss 0.85446
Epoch 75: Val Loss 0.83444
Epoch 76: Val Loss 0.81613
Epoch 77: Val Loss 0.79962
Epoch 78: Val Loss 0.78525
Epoch 79: Val Loss 0.77188
Epoch 80: Val Loss 0.75967
Epoch 81: Val Loss 0.75017
Epoch 82: Val Loss 0.74092
Epoch 83: Val Loss 0.73191
Epoch 84: Val Loss 0.72315
Epoch 85: Val Loss 0.71524
Epoch 86: Val Loss 0.70870
Epoch 87: Val Loss 0.70255
Epoch 88: Val Loss 0.69719
Epoch 89: Val Loss 0.69206
Epoch 90: Val Loss 0.68832
Epoch 91: Val Loss 0.68327
Epoch 92: Val Loss 0.67833
Epoch 93: Val Loss 0.67548
Epoch 94: Val Loss 0.67274
Epoch 95: Val Loss 0.66920
Epoch 96: Val Loss 0.66614
Epoch 97: Val Loss 0.66250
Epoch 98: Val Loss 0.65947
Epoch 99: Val Loss 0.65681
{'MSE - mean': 0.6220431867099957, 'MSE - std': 0.0664631038777669, 'R2 - mean': 0.0952330909745212, 'R2 - std': 0.049378956580550804} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.45892
Epoch 1: Val Loss 222.86839
Epoch 2: Val Loss 221.24336
Epoch 3: Val Loss 219.56238
Epoch 4: Val Loss 217.81772
Epoch 5: Val Loss 215.97513
Epoch 6: Val Loss 213.99712
Epoch 7: Val Loss 211.83023
Epoch 8: Val Loss 209.42244
Epoch 9: Val Loss 206.73264
Epoch 10: Val Loss 203.72145
Epoch 11: Val Loss 200.27663
Epoch 12: Val Loss 196.34293
Epoch 13: Val Loss 191.85501
Epoch 14: Val Loss 186.71526
Epoch 15: Val Loss 180.86023
Epoch 16: Val Loss 174.20279
Epoch 17: Val Loss 166.68654
Epoch 18: Val Loss 158.27632
Epoch 19: Val Loss 148.91338
Epoch 20: Val Loss 138.60037
Epoch 21: Val Loss 127.37701
Epoch 22: Val Loss 115.33270
Epoch 23: Val Loss 102.62481
Epoch 24: Val Loss 89.39290
Epoch 25: Val Loss 75.98461
Epoch 26: Val Loss 62.74720
Epoch 27: Val Loss 50.05078
Epoch 28: Val Loss 38.38921
Epoch 29: Val Loss 28.27150
Epoch 30: Val Loss 20.01020
Epoch 31: Val Loss 13.86615
Epoch 32: Val Loss 9.69137
Epoch 33: Val Loss 7.18743
Epoch 34: Val Loss 5.83927
Epoch 35: Val Loss 5.11260
Epoch 36: Val Loss 4.57004
Epoch 37: Val Loss 4.03149
Epoch 38: Val Loss 3.48958
Epoch 39: Val Loss 2.96866
Epoch 40: Val Loss 2.54365
Epoch 41: Val Loss 2.20977
Epoch 42: Val Loss 1.95339
Epoch 43: Val Loss 1.75292
Epoch 44: Val Loss 1.57930
Epoch 45: Val Loss 1.43066
Epoch 46: Val Loss 1.29840
Epoch 47: Val Loss 1.18521
Epoch 48: Val Loss 1.08817
Epoch 49: Val Loss 1.00813
Epoch 50: Val Loss 0.94188
Epoch 51: Val Loss 0.88451
Epoch 52: Val Loss 0.83675
Epoch 53: Val Loss 0.79521
Epoch 54: Val Loss 0.75993
Epoch 55: Val Loss 0.73012
Epoch 56: Val Loss 0.70414
Epoch 57: Val Loss 0.68189
Epoch 58: Val Loss 0.66386
Epoch 59: Val Loss 0.64739
Epoch 60: Val Loss 0.63267
Epoch 61: Val Loss 0.62057
Epoch 62: Val Loss 0.61022
Epoch 63: Val Loss 0.60068
Epoch 64: Val Loss 0.59354
Epoch 65: Val Loss 0.58721
Epoch 66: Val Loss 0.58198
Epoch 67: Val Loss 0.57840
Epoch 68: Val Loss 0.57506
Epoch 69: Val Loss 0.57178
Epoch 70: Val Loss 0.56976
Epoch 71: Val Loss 0.56785
Epoch 72: Val Loss 0.56648
Epoch 73: Val Loss 0.56547
Epoch 74: Val Loss 0.56493
Epoch 75: Val Loss 0.56368
Epoch 76: Val Loss 0.56257
Epoch 77: Val Loss 0.56215
Epoch 78: Val Loss 0.56296
Epoch 79: Val Loss 0.56365
Epoch 80: Val Loss 0.56520
Epoch 81: Val Loss 0.56675
Epoch 82: Val Loss 0.56728
Epoch 83: Val Loss 0.56691
Epoch 84: Val Loss 0.56647
Epoch 85: Val Loss 0.56660
Epoch 86: Val Loss 0.56634
Epoch 87: Val Loss 0.56642
Epoch 88: Val Loss 0.56701
Epoch 89: Val Loss 0.56758
Epoch 90: Val Loss 0.56854
Epoch 91: Val Loss 0.56996
Epoch 92: Val Loss 0.57089
Epoch 93: Val Loss 0.57080
Epoch 94: Val Loss 0.57045
Epoch 95: Val Loss 0.57040
Epoch 96: Val Loss 0.56989
Epoch 97: Val Loss 0.56955
Epoch 98: Val Loss 0.56945
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 0.6100642053957241, 'MSE - std': 0.06409258393109242, 'R2 - mean': 0.08960318742804232, 'R2 - std': 0.045578595175342546} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 0.6100642053957241 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 229.29944
Epoch 1: Val Loss 228.30453
Epoch 2: Val Loss 227.42920
Epoch 3: Val Loss 226.62929
Epoch 4: Val Loss 225.88268
Epoch 5: Val Loss 225.14294
Epoch 6: Val Loss 224.37685
Epoch 7: Val Loss 223.57909
Epoch 8: Val Loss 222.71555
Epoch 9: Val Loss 221.77327
Epoch 10: Val Loss 220.72992
Epoch 11: Val Loss 219.56125
Epoch 12: Val Loss 218.24310
Epoch 13: Val Loss 216.74832
Epoch 14: Val Loss 215.04756
Epoch 15: Val Loss 213.10072
Epoch 16: Val Loss 210.86557
Epoch 17: Val Loss 208.30226
Epoch 18: Val Loss 205.38205
Epoch 19: Val Loss 202.06088
Epoch 20: Val Loss 198.28024
Epoch 21: Val Loss 193.97301
Epoch 22: Val Loss 189.10274
Epoch 23: Val Loss 183.58011
Epoch 24: Val Loss 177.37115
Epoch 25: Val Loss 170.42691
Epoch 26: Val Loss 162.69168
Epoch 27: Val Loss 154.14337
Epoch 28: Val Loss 144.75534
Epoch 29: Val Loss 134.60692
Epoch 30: Val Loss 123.71343
Epoch 31: Val Loss 112.19514
Epoch 32: Val Loss 100.15520
Epoch 33: Val Loss 87.75530
Epoch 34: Val Loss 75.24718
Epoch 35: Val Loss 62.88901
Epoch 36: Val Loss 51.03456
Epoch 37: Val Loss 40.04850
Epoch 38: Val Loss 30.30002
Epoch 39: Val Loss 22.09431
Epoch 40: Val Loss 15.63462
Epoch 41: Val Loss 10.96301
Epoch 42: Val Loss 7.92639
Epoch 43: Val Loss 6.16445
Epoch 44: Val Loss 5.28634
Epoch 45: Val Loss 4.86919
Epoch 46: Val Loss 4.59968
Epoch 47: Val Loss 4.32492
Epoch 48: Val Loss 3.98651
Epoch 49: Val Loss 3.61823
Epoch 50: Val Loss 3.25479
Epoch 51: Val Loss 2.93782
Epoch 52: Val Loss 2.67448
Epoch 53: Val Loss 2.45337
Epoch 54: Val Loss 2.26835
Epoch 55: Val Loss 2.10763
Epoch 56: Val Loss 1.96330
Epoch 57: Val Loss 1.83382
Epoch 58: Val Loss 1.71544
Epoch 59: Val Loss 1.60928
Epoch 60: Val Loss 1.51530
Epoch 61: Val Loss 1.42874
Epoch 62: Val Loss 1.35201
Epoch 63: Val Loss 1.28298
Epoch 64: Val Loss 1.21928
Epoch 65: Val Loss 1.16193
Epoch 66: Val Loss 1.10827
Epoch 67: Val Loss 1.06014
Epoch 68: Val Loss 1.01637
Epoch 69: Val Loss 0.97546
Epoch 70: Val Loss 0.93833
Epoch 71: Val Loss 0.90469
Epoch 72: Val Loss 0.87371
Epoch 73: Val Loss 0.84612
Epoch 74: Val Loss 0.82050
Epoch 75: Val Loss 0.79842
Epoch 76: Val Loss 0.77763
Epoch 77: Val Loss 0.75781
Epoch 78: Val Loss 0.73932
Epoch 79: Val Loss 0.72278
Epoch 80: Val Loss 0.70778
Epoch 81: Val Loss 0.69388
Epoch 82: Val Loss 0.68139
Epoch 83: Val Loss 0.66805
Epoch 84: Val Loss 0.65612
Epoch 85: Val Loss 0.64512
Epoch 86: Val Loss 0.63532
Epoch 87: Val Loss 0.62748
Epoch 88: Val Loss 0.61865
Epoch 89: Val Loss 0.61156
Epoch 90: Val Loss 0.60416
Epoch 91: Val Loss 0.59704
Epoch 92: Val Loss 0.58927
Epoch 93: Val Loss 0.58340
Epoch 94: Val Loss 0.57737
Epoch 95: Val Loss 0.57184
Epoch 96: Val Loss 0.56802
Epoch 97: Val Loss 0.56400
Epoch 98: Val Loss 0.56041
Epoch 99: Val Loss 0.55679
{'MSE - mean': 0.5567917296169635, 'MSE - std': 0.0, 'R2 - mean': -0.013176850295934184, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.68938
Epoch 1: Val Loss 231.67703
Epoch 2: Val Loss 230.59676
Epoch 3: Val Loss 229.43871
Epoch 4: Val Loss 228.18134
Epoch 5: Val Loss 226.80165
Epoch 6: Val Loss 225.29932
Epoch 7: Val Loss 223.65643
Epoch 8: Val Loss 221.84781
Epoch 9: Val Loss 219.85077
Epoch 10: Val Loss 217.63820
Epoch 11: Val Loss 215.17461
Epoch 12: Val Loss 212.45636
Epoch 13: Val Loss 209.44633
Epoch 14: Val Loss 206.06580
Epoch 15: Val Loss 202.26279
Epoch 16: Val Loss 197.99089
Epoch 17: Val Loss 193.19072
Epoch 18: Val Loss 187.78648
Epoch 19: Val Loss 181.67682
Epoch 20: Val Loss 174.78569
Epoch 21: Val Loss 167.01385
Epoch 22: Val Loss 158.28362
Epoch 23: Val Loss 148.56342
Epoch 24: Val Loss 137.86099
Epoch 25: Val Loss 126.13686
Epoch 26: Val Loss 113.56601
Epoch 27: Val Loss 100.38819
Epoch 28: Val Loss 86.88337
Epoch 29: Val Loss 73.38079
Epoch 30: Val Loss 60.28890
Epoch 31: Val Loss 48.04073
Epoch 32: Val Loss 37.04044
Epoch 33: Val Loss 27.74046
Epoch 34: Val Loss 20.40524
Epoch 35: Val Loss 15.04453
Epoch 36: Val Loss 11.54319
Epoch 37: Val Loss 9.40931
Epoch 38: Val Loss 8.13308
Epoch 39: Val Loss 7.27365
Epoch 40: Val Loss 6.54045
Epoch 41: Val Loss 5.81871
Epoch 42: Val Loss 5.13222
Epoch 43: Val Loss 4.51498
Epoch 44: Val Loss 3.99196
Epoch 45: Val Loss 3.56358
Epoch 46: Val Loss 3.20868
Epoch 47: Val Loss 2.89472
Epoch 48: Val Loss 2.61830
Epoch 49: Val Loss 2.36476
Epoch 50: Val Loss 2.13575
Epoch 51: Val Loss 1.92991
Epoch 52: Val Loss 1.75233
Epoch 53: Val Loss 1.59781
Epoch 54: Val Loss 1.46374
Epoch 55: Val Loss 1.34945
Epoch 56: Val Loss 1.25333
Epoch 57: Val Loss 1.16934
Epoch 58: Val Loss 1.09868
Epoch 59: Val Loss 1.03605
Epoch 60: Val Loss 0.98384
Epoch 61: Val Loss 0.93970
Epoch 62: Val Loss 0.90039
Epoch 63: Val Loss 0.86638
Epoch 64: Val Loss 0.83610
Epoch 65: Val Loss 0.80842
Epoch 66: Val Loss 0.78524
Epoch 67: Val Loss 0.76460
Epoch 68: Val Loss 0.74671
Epoch 69: Val Loss 0.73077
Epoch 70: Val Loss 0.71890
Epoch 71: Val Loss 0.70875
Epoch 72: Val Loss 0.69965
Epoch 73: Val Loss 0.69206
Epoch 74: Val Loss 0.68629
Epoch 75: Val Loss 0.67967
Epoch 76: Val Loss 0.67235
Epoch 77: Val Loss 0.66651
Epoch 78: Val Loss 0.66068
Epoch 79: Val Loss 0.65684
Epoch 80: Val Loss 0.65342
Epoch 81: Val Loss 0.65025
Epoch 82: Val Loss 0.64727
Epoch 83: Val Loss 0.64525
Epoch 84: Val Loss 0.64268
Epoch 85: Val Loss 0.64138
Epoch 86: Val Loss 0.63789
Epoch 87: Val Loss 0.63549
Epoch 88: Val Loss 0.63379
Epoch 89: Val Loss 0.63223
Epoch 90: Val Loss 0.63029
Epoch 91: Val Loss 0.62870
Epoch 92: Val Loss 0.62661
Epoch 93: Val Loss 0.62695
Epoch 94: Val Loss 0.62491
Epoch 95: Val Loss 0.62352
Epoch 96: Val Loss 0.62178
Epoch 97: Val Loss 0.61853
Epoch 98: Val Loss 0.61859
Epoch 99: Val Loss 0.61841
{'MSE - mean': 0.5876003937703727, 'MSE - std': 0.030808664153409104, 'R2 - mean': 0.0746905124394785, 'R2 - std': 0.08786736273541268} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.86057
Epoch 1: Val Loss 220.13495
Epoch 2: Val Loss 219.48053
Epoch 3: Val Loss 218.88712
Epoch 4: Val Loss 218.36531
Epoch 5: Val Loss 217.89313
Epoch 6: Val Loss 217.44389
Epoch 7: Val Loss 217.01328
Epoch 8: Val Loss 216.57741
Epoch 9: Val Loss 216.11694
Epoch 10: Val Loss 215.62357
Epoch 11: Val Loss 215.08598
Epoch 12: Val Loss 214.49873
Epoch 13: Val Loss 213.83104
Epoch 14: Val Loss 213.07866
Epoch 15: Val Loss 212.23120
Epoch 16: Val Loss 211.27235
Epoch 17: Val Loss 210.18933
Epoch 18: Val Loss 208.96181
Epoch 19: Val Loss 207.57890
Epoch 20: Val Loss 206.02426
Epoch 21: Val Loss 204.27307
Epoch 22: Val Loss 202.30162
Epoch 23: Val Loss 200.07870
Epoch 24: Val Loss 197.58148
Epoch 25: Val Loss 194.76727
Epoch 26: Val Loss 191.61290
Epoch 27: Val Loss 188.07866
Epoch 28: Val Loss 184.13060
Epoch 29: Val Loss 179.71992
Epoch 30: Val Loss 174.81567
Epoch 31: Val Loss 169.38559
Epoch 32: Val Loss 163.39653
Epoch 33: Val Loss 156.81926
Epoch 34: Val Loss 149.66173
Epoch 35: Val Loss 141.92654
Epoch 36: Val Loss 133.58783
Epoch 37: Val Loss 124.69207
Epoch 38: Val Loss 115.26902
Epoch 39: Val Loss 105.44312
Epoch 40: Val Loss 95.28534
Epoch 41: Val Loss 84.92725
Epoch 42: Val Loss 74.52370
Epoch 43: Val Loss 64.27027
Epoch 44: Val Loss 54.37435
Epoch 45: Val Loss 45.03617
Epoch 46: Val Loss 36.48776
Epoch 47: Val Loss 28.91497
Epoch 48: Val Loss 22.45363
Epoch 49: Val Loss 17.19407
Epoch 50: Val Loss 13.13377
Epoch 51: Val Loss 10.22782
Epoch 52: Val Loss 8.27197
Epoch 53: Val Loss 7.07257
Epoch 54: Val Loss 6.38422
Epoch 55: Val Loss 6.00646
Epoch 56: Val Loss 5.77288
Epoch 57: Val Loss 5.58408
Epoch 58: Val Loss 5.37652
Epoch 59: Val Loss 5.14255
Epoch 60: Val Loss 4.90605
Epoch 61: Val Loss 4.66906
Epoch 62: Val Loss 4.44322
Epoch 63: Val Loss 4.23346
Epoch 64: Val Loss 4.04313
Epoch 65: Val Loss 3.86957
Epoch 66: Val Loss 3.71208
Epoch 67: Val Loss 3.56977
Epoch 68: Val Loss 3.43463
Epoch 69: Val Loss 3.30708
Epoch 70: Val Loss 3.18672
Epoch 71: Val Loss 3.07472
Epoch 72: Val Loss 2.96962
Epoch 73: Val Loss 2.87094
Epoch 74: Val Loss 2.77628
Epoch 75: Val Loss 2.68692
Epoch 76: Val Loss 2.59943
Epoch 77: Val Loss 2.51608
Epoch 78: Val Loss 2.43582
Epoch 79: Val Loss 2.35980
Epoch 80: Val Loss 2.28568
Epoch 81: Val Loss 2.21482
Epoch 82: Val Loss 2.14873
Epoch 83: Val Loss 2.08452
Epoch 84: Val Loss 2.02484
Epoch 85: Val Loss 1.96594
Epoch 86: Val Loss 1.90986
Epoch 87: Val Loss 1.85735
Epoch 88: Val Loss 1.80496
Epoch 89: Val Loss 1.75556
Epoch 90: Val Loss 1.70816
Epoch 91: Val Loss 1.66209
Epoch 92: Val Loss 1.61827
Epoch 93: Val Loss 1.57546
Epoch 94: Val Loss 1.53505
Epoch 95: Val Loss 1.49474
Epoch 96: Val Loss 1.45702
Epoch 97: Val Loss 1.41961
Epoch 98: Val Loss 1.38411
Epoch 99: Val Loss 1.35070
{'MSE - mean': 0.8419656161171051, 'MSE - std': 0.3606052068448499, 'R2 - mean': -0.27036785732444985, 'R2 - std': 0.4932318650604374} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.23273
Epoch 1: Val Loss 219.73024
Epoch 2: Val Loss 219.27281
Epoch 3: Val Loss 218.82802
Epoch 4: Val Loss 218.35661
Epoch 5: Val Loss 217.83231
Epoch 6: Val Loss 217.24768
Epoch 7: Val Loss 216.55885
Epoch 8: Val Loss 215.74228
Epoch 9: Val Loss 214.77139
Epoch 10: Val Loss 213.59299
Epoch 11: Val Loss 212.20396
Epoch 12: Val Loss 210.55746
Epoch 13: Val Loss 208.57771
Epoch 14: Val Loss 206.22241
Epoch 15: Val Loss 203.41890
Epoch 16: Val Loss 200.05165
Epoch 17: Val Loss 196.09193
Epoch 18: Val Loss 191.49225
Epoch 19: Val Loss 186.12253
Epoch 20: Val Loss 179.95216
Epoch 21: Val Loss 172.91991
Epoch 22: Val Loss 164.91570
Epoch 23: Val Loss 155.90680
Epoch 24: Val Loss 145.95213
Epoch 25: Val Loss 135.06737
Epoch 26: Val Loss 123.32581
Epoch 27: Val Loss 110.85977
Epoch 28: Val Loss 97.78758
Epoch 29: Val Loss 84.38738
Epoch 30: Val Loss 70.99442
Epoch 31: Val Loss 57.93148
Epoch 32: Val Loss 45.66569
Epoch 33: Val Loss 34.66626
Epoch 34: Val Loss 25.36887
Epoch 35: Val Loss 18.02018
Epoch 36: Val Loss 12.75186
Epoch 37: Val Loss 9.40106
Epoch 38: Val Loss 7.51794
Epoch 39: Val Loss 6.55356
Epoch 40: Val Loss 6.00195
Epoch 41: Val Loss 5.53908
Epoch 42: Val Loss 5.03363
Epoch 43: Val Loss 4.49749
Epoch 44: Val Loss 3.99721
Epoch 45: Val Loss 3.57331
Epoch 46: Val Loss 3.23192
Epoch 47: Val Loss 2.95641
Epoch 48: Val Loss 2.72240
Epoch 49: Val Loss 2.52086
Epoch 50: Val Loss 2.33531
Epoch 51: Val Loss 2.15334
Epoch 52: Val Loss 1.98806
Epoch 53: Val Loss 1.83959
Epoch 54: Val Loss 1.70956
Epoch 55: Val Loss 1.59508
Epoch 56: Val Loss 1.49697
Epoch 57: Val Loss 1.41050
Epoch 58: Val Loss 1.33256
Epoch 59: Val Loss 1.26373
Epoch 60: Val Loss 1.20116
Epoch 61: Val Loss 1.14493
Epoch 62: Val Loss 1.09612
Epoch 63: Val Loss 1.05141
Epoch 64: Val Loss 1.01253
Epoch 65: Val Loss 0.97572
Epoch 66: Val Loss 0.94307
Epoch 67: Val Loss 0.91469
Epoch 68: Val Loss 0.88801
Epoch 69: Val Loss 0.86479
Epoch 70: Val Loss 0.84330
Epoch 71: Val Loss 0.82227
Epoch 72: Val Loss 0.80384
Epoch 73: Val Loss 0.78768
Epoch 74: Val Loss 0.77352
Epoch 75: Val Loss 0.76244
Epoch 76: Val Loss 0.75080
Epoch 77: Val Loss 0.74188
Epoch 78: Val Loss 0.73393
Epoch 79: Val Loss 0.72663
Epoch 80: Val Loss 0.71940
Epoch 81: Val Loss 0.71134
Epoch 82: Val Loss 0.70572
Epoch 83: Val Loss 0.69980
Epoch 84: Val Loss 0.69474
Epoch 85: Val Loss 0.68940
Epoch 86: Val Loss 0.68631
Epoch 87: Val Loss 0.68345
Epoch 88: Val Loss 0.68034
Epoch 89: Val Loss 0.67792
Epoch 90: Val Loss 0.67668
Epoch 91: Val Loss 0.67519
Epoch 92: Val Loss 0.67308
Epoch 93: Val Loss 0.67175
Epoch 94: Val Loss 0.66996
Epoch 95: Val Loss 0.66843
Epoch 96: Val Loss 0.66691
Epoch 97: Val Loss 0.66612
Epoch 98: Val Loss 0.66487
Epoch 99: Val Loss 0.66342
{'MSE - mean': 0.7973280256690005, 'MSE - std': 0.3217213543514805, 'R2 - mean': -0.164311883408131, 'R2 - std': 0.46497512120893614} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.22681
Epoch 1: Val Loss 233.12814
Epoch 2: Val Loss 232.00385
Epoch 3: Val Loss 230.81596
Epoch 4: Val Loss 229.52957
Epoch 5: Val Loss 228.14246
Epoch 6: Val Loss 226.61365
Epoch 7: Val Loss 224.91490
Epoch 8: Val Loss 223.03345
Epoch 9: Val Loss 220.96094
Epoch 10: Val Loss 218.64613
Epoch 11: Val Loss 216.02820
Epoch 12: Val Loss 213.08508
Epoch 13: Val Loss 209.78764
Epoch 14: Val Loss 206.08141
Epoch 15: Val Loss 201.85887
Epoch 16: Val Loss 197.04616
Epoch 17: Val Loss 191.57774
Epoch 18: Val Loss 185.38806
Epoch 19: Val Loss 178.36981
Epoch 20: Val Loss 170.48912
Epoch 21: Val Loss 161.72726
Epoch 22: Val Loss 152.07115
Epoch 23: Val Loss 141.52161
Epoch 24: Val Loss 130.07343
Epoch 25: Val Loss 117.84765
Epoch 26: Val Loss 104.89278
Epoch 27: Val Loss 91.42224
Epoch 28: Val Loss 77.72563
Epoch 29: Val Loss 64.14984
Epoch 30: Val Loss 51.13934
Epoch 31: Val Loss 39.12019
Epoch 32: Val Loss 28.56283
Epoch 33: Val Loss 19.82797
Epoch 34: Val Loss 13.16576
Epoch 35: Val Loss 8.62827
Epoch 36: Val Loss 5.93623
Epoch 37: Val Loss 4.62254
Epoch 38: Val Loss 4.08696
Epoch 39: Val Loss 3.83022
Epoch 40: Val Loss 3.55639
Epoch 41: Val Loss 3.19733
Epoch 42: Val Loss 2.81204
Epoch 43: Val Loss 2.45567
Epoch 44: Val Loss 2.17448
Epoch 45: Val Loss 1.96298
Epoch 46: Val Loss 1.80125
Epoch 47: Val Loss 1.66766
Epoch 48: Val Loss 1.54797
Epoch 49: Val Loss 1.43918
Epoch 50: Val Loss 1.33787
Epoch 51: Val Loss 1.24626
Epoch 52: Val Loss 1.16600
Epoch 53: Val Loss 1.09522
Epoch 54: Val Loss 1.03361
Epoch 55: Val Loss 0.98002
Epoch 56: Val Loss 0.93411
Epoch 57: Val Loss 0.89356
Epoch 58: Val Loss 0.85858
Epoch 59: Val Loss 0.82740
Epoch 60: Val Loss 0.80083
Epoch 61: Val Loss 0.77755
Epoch 62: Val Loss 0.75624
Epoch 63: Val Loss 0.73804
Epoch 64: Val Loss 0.72170
Epoch 65: Val Loss 0.70710
Epoch 66: Val Loss 0.69355
Epoch 67: Val Loss 0.68204
Epoch 68: Val Loss 0.67082
Epoch 69: Val Loss 0.66162
Epoch 70: Val Loss 0.65364
Epoch 71: Val Loss 0.64670
Epoch 72: Val Loss 0.64022
Epoch 73: Val Loss 0.63465
Epoch 74: Val Loss 0.62918
Epoch 75: Val Loss 0.62445
Epoch 76: Val Loss 0.61978
Epoch 77: Val Loss 0.61592
Epoch 78: Val Loss 0.61294
Epoch 79: Val Loss 0.61047
Epoch 80: Val Loss 0.60790
Epoch 81: Val Loss 0.60602
Epoch 82: Val Loss 0.60392
Epoch 83: Val Loss 0.60291
Epoch 84: Val Loss 0.60144
Epoch 85: Val Loss 0.60043
Epoch 86: Val Loss 0.59898
Epoch 87: Val Loss 0.59753
Epoch 88: Val Loss 0.59671
Epoch 89: Val Loss 0.59628
Epoch 90: Val Loss 0.59645
Epoch 91: Val Loss 0.59515
Epoch 92: Val Loss 0.59455
Epoch 93: Val Loss 0.59387
Epoch 94: Val Loss 0.59324
Epoch 95: Val Loss 0.59187
Epoch 96: Val Loss 0.59146
Epoch 97: Val Loss 0.59132
Epoch 98: Val Loss 0.59032
Epoch 99: Val Loss 0.59055
{'MSE - mean': 0.7559259885199883, 'MSE - std': 0.2994331621530145, 'R2 - mean': -0.12738258322278026, 'R2 - std': 0.4223938724973776} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 0.7559259885199883 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.85207
Epoch 1: Val Loss 224.77316
Epoch 2: Val Loss 224.69495
Epoch 3: Val Loss 224.61682
Epoch 4: Val Loss 224.53909
Epoch 5: Val Loss 224.46196
Epoch 6: Val Loss 224.38538
Epoch 7: Val Loss 224.30937
Epoch 8: Val Loss 224.23375
Epoch 9: Val Loss 224.15791
Epoch 10: Val Loss 224.08240
Epoch 11: Val Loss 224.00740
Epoch 12: Val Loss 223.93323
Epoch 13: Val Loss 223.85852
Epoch 14: Val Loss 223.78322
Epoch 15: Val Loss 223.70822
Epoch 16: Val Loss 223.63359
Epoch 17: Val Loss 223.55942
Epoch 18: Val Loss 223.48480
Epoch 19: Val Loss 223.41136
Epoch 20: Val Loss 223.33797
Epoch 21: Val Loss 223.26492
Epoch 22: Val Loss 223.19099
Epoch 23: Val Loss 223.11627
Epoch 24: Val Loss 223.04088
Epoch 25: Val Loss 222.96426
Epoch 26: Val Loss 222.88599
Epoch 27: Val Loss 222.80518
Epoch 28: Val Loss 222.72247
Epoch 29: Val Loss 222.63907
Epoch 30: Val Loss 222.55371
Epoch 31: Val Loss 222.46741
Epoch 32: Val Loss 222.38187
Epoch 33: Val Loss 222.29570
Epoch 34: Val Loss 222.20853
Epoch 35: Val Loss 222.12106
Epoch 36: Val Loss 222.03337
Epoch 37: Val Loss 221.94447
Epoch 38: Val Loss 221.85338
Epoch 39: Val Loss 221.76057
Epoch 40: Val Loss 221.66713
Epoch 41: Val Loss 221.57196
Epoch 42: Val Loss 221.47542
Epoch 43: Val Loss 221.37756
Epoch 44: Val Loss 221.27832
Epoch 45: Val Loss 221.17737
Epoch 46: Val Loss 221.07469
Epoch 47: Val Loss 220.97044
Epoch 48: Val Loss 220.86459
Epoch 49: Val Loss 220.75719
Epoch 50: Val Loss 220.64716
Epoch 51: Val Loss 220.53465
Epoch 52: Val Loss 220.41939
Epoch 53: Val Loss 220.30226
Epoch 54: Val Loss 220.18242
Epoch 55: Val Loss 220.06117
Epoch 56: Val Loss 219.93820
Epoch 57: Val Loss 219.81313
Epoch 58: Val Loss 219.68588
Epoch 59: Val Loss 219.55713
Epoch 60: Val Loss 219.42793
Epoch 61: Val Loss 219.29735
Epoch 62: Val Loss 219.16463
Epoch 63: Val Loss 219.02943
Epoch 64: Val Loss 218.89272
Epoch 65: Val Loss 218.75349
Epoch 66: Val Loss 218.61096
Epoch 67: Val Loss 218.46722
Epoch 68: Val Loss 218.31970
Epoch 69: Val Loss 218.16933
Epoch 70: Val Loss 218.01648
Epoch 71: Val Loss 217.86130
Epoch 72: Val Loss 217.70247
Epoch 73: Val Loss 217.54135
Epoch 74: Val Loss 217.37799
Epoch 75: Val Loss 217.21034
Epoch 76: Val Loss 217.03900
Epoch 77: Val Loss 216.86423
Epoch 78: Val Loss 216.68723
Epoch 79: Val Loss 216.50690
Epoch 80: Val Loss 216.32246
Epoch 81: Val Loss 216.13361
Epoch 82: Val Loss 215.94147
Epoch 83: Val Loss 215.74597
Epoch 84: Val Loss 215.54715
Epoch 85: Val Loss 215.34555
Epoch 86: Val Loss 215.14188
Epoch 87: Val Loss 214.93460
Epoch 88: Val Loss 214.72366
Epoch 89: Val Loss 214.50932
Epoch 90: Val Loss 214.29158
Epoch 91: Val Loss 214.07004
Epoch 92: Val Loss 213.84546
Epoch 93: Val Loss 213.61624
Epoch 94: Val Loss 213.38333
Epoch 95: Val Loss 213.14827
Epoch 96: Val Loss 212.90918
Epoch 97: Val Loss 212.66652
Epoch 98: Val Loss 212.41982
Epoch 99: Val Loss 212.16994
{'MSE - mean': 212.1699367257688, 'MSE - std': 0.0, 'R2 - mean': -385.079132977037, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 237.10611
Epoch 1: Val Loss 236.99306
Epoch 2: Val Loss 236.87994
Epoch 3: Val Loss 236.76704
Epoch 4: Val Loss 236.65413
Epoch 5: Val Loss 236.54155
Epoch 6: Val Loss 236.42905
Epoch 7: Val Loss 236.31601
Epoch 8: Val Loss 236.20287
Epoch 9: Val Loss 236.08929
Epoch 10: Val Loss 235.97499
Epoch 11: Val Loss 235.85994
Epoch 12: Val Loss 235.74364
Epoch 13: Val Loss 235.62660
Epoch 14: Val Loss 235.50867
Epoch 15: Val Loss 235.38991
Epoch 16: Val Loss 235.26987
Epoch 17: Val Loss 235.14859
Epoch 18: Val Loss 235.02658
Epoch 19: Val Loss 234.90410
Epoch 20: Val Loss 234.77998
Epoch 21: Val Loss 234.65482
Epoch 22: Val Loss 234.52724
Epoch 23: Val Loss 234.39903
Epoch 24: Val Loss 234.26892
Epoch 25: Val Loss 234.13756
Epoch 26: Val Loss 234.00502
Epoch 27: Val Loss 233.87050
Epoch 28: Val Loss 233.73384
Epoch 29: Val Loss 233.59612
Epoch 30: Val Loss 233.45624
Epoch 31: Val Loss 233.31456
Epoch 32: Val Loss 233.17050
Epoch 33: Val Loss 233.02455
Epoch 34: Val Loss 232.87665
Epoch 35: Val Loss 232.72557
Epoch 36: Val Loss 232.57211
Epoch 37: Val Loss 232.41701
Epoch 38: Val Loss 232.25914
Epoch 39: Val Loss 232.09921
Epoch 40: Val Loss 231.93616
Epoch 41: Val Loss 231.77017
Epoch 42: Val Loss 231.60210
Epoch 43: Val Loss 231.43050
Epoch 44: Val Loss 231.25555
Epoch 45: Val Loss 231.07832
Epoch 46: Val Loss 230.89804
Epoch 47: Val Loss 230.71594
Epoch 48: Val Loss 230.53120
Epoch 49: Val Loss 230.34331
Epoch 50: Val Loss 230.15343
Epoch 51: Val Loss 229.96150
Epoch 52: Val Loss 229.76675
Epoch 53: Val Loss 229.56825
Epoch 54: Val Loss 229.36719
Epoch 55: Val Loss 229.16394
Epoch 56: Val Loss 228.95842
Epoch 57: Val Loss 228.74928
Epoch 58: Val Loss 228.53699
Epoch 59: Val Loss 228.32187
Epoch 60: Val Loss 228.10350
Epoch 61: Val Loss 227.88277
Epoch 62: Val Loss 227.65790
Epoch 63: Val Loss 227.43077
Epoch 64: Val Loss 227.19974
Epoch 65: Val Loss 226.96571
Epoch 66: Val Loss 226.72801
Epoch 67: Val Loss 226.48621
Epoch 68: Val Loss 226.24133
Epoch 69: Val Loss 225.99359
Epoch 70: Val Loss 225.74269
Epoch 71: Val Loss 225.48929
Epoch 72: Val Loss 225.23166
Epoch 73: Val Loss 224.96992
Epoch 74: Val Loss 224.70410
Epoch 75: Val Loss 224.43288
Epoch 76: Val Loss 224.15863
Epoch 77: Val Loss 223.88091
Epoch 78: Val Loss 223.59906
Epoch 79: Val Loss 223.31410
Epoch 80: Val Loss 223.02440
Epoch 81: Val Loss 222.73087
Epoch 82: Val Loss 222.43349
Epoch 83: Val Loss 222.13086
Epoch 84: Val Loss 221.82469
Epoch 85: Val Loss 221.51442
Epoch 86: Val Loss 221.19826
Epoch 87: Val Loss 220.87994
Epoch 88: Val Loss 220.55562
Epoch 89: Val Loss 220.22809
Epoch 90: Val Loss 219.89557
Epoch 91: Val Loss 219.55812
Epoch 92: Val Loss 219.21677
Epoch 93: Val Loss 218.86972
Epoch 94: Val Loss 218.51793
Epoch 95: Val Loss 218.16046
Epoch 96: Val Loss 217.79807
Epoch 97: Val Loss 217.42906
Epoch 98: Val Loss 217.05612
Epoch 99: Val Loss 216.67668
{'MSE - mean': 214.42332709762556, 'MSE - std': 2.253390371856767, 'R2 - mean': -338.7500717380527, 'R2 - std': 46.329061238984224} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.52647
Epoch 1: Val Loss 224.38870
Epoch 2: Val Loss 224.25105
Epoch 3: Val Loss 224.11310
Epoch 4: Val Loss 223.97491
Epoch 5: Val Loss 223.83638
Epoch 6: Val Loss 223.69690
Epoch 7: Val Loss 223.55733
Epoch 8: Val Loss 223.41737
Epoch 9: Val Loss 223.27666
Epoch 10: Val Loss 223.13586
Epoch 11: Val Loss 222.99419
Epoch 12: Val Loss 222.85199
Epoch 13: Val Loss 222.70921
Epoch 14: Val Loss 222.56606
Epoch 15: Val Loss 222.42203
Epoch 16: Val Loss 222.27745
Epoch 17: Val Loss 222.13290
Epoch 18: Val Loss 221.98744
Epoch 19: Val Loss 221.84096
Epoch 20: Val Loss 221.69357
Epoch 21: Val Loss 221.54506
Epoch 22: Val Loss 221.39537
Epoch 23: Val Loss 221.24561
Epoch 24: Val Loss 221.09476
Epoch 25: Val Loss 220.94327
Epoch 26: Val Loss 220.79112
Epoch 27: Val Loss 220.63731
Epoch 28: Val Loss 220.48282
Epoch 29: Val Loss 220.32718
Epoch 30: Val Loss 220.17001
Epoch 31: Val Loss 220.01120
Epoch 32: Val Loss 219.85023
Epoch 33: Val Loss 219.68777
Epoch 34: Val Loss 219.52516
Epoch 35: Val Loss 219.36113
Epoch 36: Val Loss 219.19637
Epoch 37: Val Loss 219.03023
Epoch 38: Val Loss 218.86317
Epoch 39: Val Loss 218.69441
Epoch 40: Val Loss 218.52470
Epoch 41: Val Loss 218.35425
Epoch 42: Val Loss 218.18185
Epoch 43: Val Loss 218.00743
Epoch 44: Val Loss 217.83134
Epoch 45: Val Loss 217.65396
Epoch 46: Val Loss 217.47444
Epoch 47: Val Loss 217.29311
Epoch 48: Val Loss 217.10966
Epoch 49: Val Loss 216.92537
Epoch 50: Val Loss 216.73871
Epoch 51: Val Loss 216.55115
Epoch 52: Val Loss 216.36317
Epoch 53: Val Loss 216.17291
Epoch 54: Val Loss 215.98024
Epoch 55: Val Loss 215.78676
Epoch 56: Val Loss 215.59149
Epoch 57: Val Loss 215.39433
Epoch 58: Val Loss 215.19635
Epoch 59: Val Loss 214.99625
Epoch 60: Val Loss 214.79514
Epoch 61: Val Loss 214.59230
Epoch 62: Val Loss 214.38803
Epoch 63: Val Loss 214.18161
Epoch 64: Val Loss 213.97234
Epoch 65: Val Loss 213.76059
Epoch 66: Val Loss 213.54646
Epoch 67: Val Loss 213.33057
Epoch 68: Val Loss 213.11281
Epoch 69: Val Loss 212.89204
Epoch 70: Val Loss 212.66982
Epoch 71: Val Loss 212.44579
Epoch 72: Val Loss 212.21883
Epoch 73: Val Loss 211.99017
Epoch 74: Val Loss 211.75940
Epoch 75: Val Loss 211.52597
Epoch 76: Val Loss 211.29036
Epoch 77: Val Loss 211.05283
Epoch 78: Val Loss 210.81262
Epoch 79: Val Loss 210.57027
Epoch 80: Val Loss 210.32533
Epoch 81: Val Loss 210.07826
Epoch 82: Val Loss 209.82785
Epoch 83: Val Loss 209.57442
Epoch 84: Val Loss 209.31805
Epoch 85: Val Loss 209.05862
Epoch 86: Val Loss 208.79591
Epoch 87: Val Loss 208.53043
Epoch 88: Val Loss 208.26285
Epoch 89: Val Loss 207.99330
Epoch 90: Val Loss 207.72092
Epoch 91: Val Loss 207.44513
Epoch 92: Val Loss 207.16586
Epoch 93: Val Loss 206.88385
Epoch 94: Val Loss 206.59856
Epoch 95: Val Loss 206.30974
Epoch 96: Val Loss 206.01901
Epoch 97: Val Loss 205.72510
Epoch 98: Val Loss 205.42738
Epoch 99: Val Loss 205.12775
{'MSE - mean': 211.32480586619945, 'MSE - std': 4.752562090267509, 'R2 - mean': -324.7451289868775, 'R2 - std': 42.69892410570115} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.50056
Epoch 1: Val Loss 233.43352
Epoch 2: Val Loss 233.36742
Epoch 3: Val Loss 233.30235
Epoch 4: Val Loss 233.23756
Epoch 5: Val Loss 233.17331
Epoch 6: Val Loss 233.10941
Epoch 7: Val Loss 233.04567
Epoch 8: Val Loss 232.98212
Epoch 9: Val Loss 232.91901
Epoch 10: Val Loss 232.85625
Epoch 11: Val Loss 232.79402
Epoch 12: Val Loss 232.73225
Epoch 13: Val Loss 232.67136
Epoch 14: Val Loss 232.61072
Epoch 15: Val Loss 232.55040
Epoch 16: Val Loss 232.49042
Epoch 17: Val Loss 232.43056
Epoch 18: Val Loss 232.37064
Epoch 19: Val Loss 232.31108
Epoch 20: Val Loss 232.25148
Epoch 21: Val Loss 232.19164
Epoch 22: Val Loss 232.13188
Epoch 23: Val Loss 232.07176
Epoch 24: Val Loss 232.01172
Epoch 25: Val Loss 231.95200
Epoch 26: Val Loss 231.89201
Epoch 27: Val Loss 231.83215
Epoch 28: Val Loss 231.77245
Epoch 29: Val Loss 231.71191
Epoch 30: Val Loss 231.65109
Epoch 31: Val Loss 231.59009
Epoch 32: Val Loss 231.52896
Epoch 33: Val Loss 231.46777
Epoch 34: Val Loss 231.40685
Epoch 35: Val Loss 231.34544
Epoch 36: Val Loss 231.28239
Epoch 37: Val Loss 231.21921
Epoch 38: Val Loss 231.15546
Epoch 39: Val Loss 231.09116
Epoch 40: Val Loss 231.02663
Epoch 41: Val Loss 230.96184
Epoch 42: Val Loss 230.89621
Epoch 43: Val Loss 230.82961
Epoch 44: Val Loss 230.76237
Epoch 45: Val Loss 230.69374
Epoch 46: Val Loss 230.62479
Epoch 47: Val Loss 230.55553
Epoch 48: Val Loss 230.48526
Epoch 49: Val Loss 230.41391
Epoch 50: Val Loss 230.34116
Epoch 51: Val Loss 230.26756
Epoch 52: Val Loss 230.19255
Epoch 53: Val Loss 230.11673
Epoch 54: Val Loss 230.04005
Epoch 55: Val Loss 229.96268
Epoch 56: Val Loss 229.88301
Epoch 57: Val Loss 229.80261
Epoch 58: Val Loss 229.72105
Epoch 59: Val Loss 229.63860
Epoch 60: Val Loss 229.55505
Epoch 61: Val Loss 229.46971
Epoch 62: Val Loss 229.38271
Epoch 63: Val Loss 229.29391
Epoch 64: Val Loss 229.20372
Epoch 65: Val Loss 229.11296
Epoch 66: Val Loss 229.01979
Epoch 67: Val Loss 228.92421
Epoch 68: Val Loss 228.82744
Epoch 69: Val Loss 228.72849
Epoch 70: Val Loss 228.62753
Epoch 71: Val Loss 228.52403
Epoch 72: Val Loss 228.41902
Epoch 73: Val Loss 228.31209
Epoch 74: Val Loss 228.20319
Epoch 75: Val Loss 228.09236
Epoch 76: Val Loss 227.97882
Epoch 77: Val Loss 227.86311
Epoch 78: Val Loss 227.74477
Epoch 79: Val Loss 227.62260
Epoch 80: Val Loss 227.49736
Epoch 81: Val Loss 227.37129
Epoch 82: Val Loss 227.24132
Epoch 83: Val Loss 227.10974
Epoch 84: Val Loss 226.97499
Epoch 85: Val Loss 226.83752
Epoch 86: Val Loss 226.69708
Epoch 87: Val Loss 226.55444
Epoch 88: Val Loss 226.40875
Epoch 89: Val Loss 226.26073
Epoch 90: Val Loss 226.11073
Epoch 91: Val Loss 225.95801
Epoch 92: Val Loss 225.80247
Epoch 93: Val Loss 225.64435
Epoch 94: Val Loss 225.48326
Epoch 95: Val Loss 225.32005
Epoch 96: Val Loss 225.15410
Epoch 97: Val Loss 224.98491
Epoch 98: Val Loss 224.81317
Epoch 99: Val Loss 224.63870
{'MSE - mean': 214.6532805489566, 'MSE - std': 7.083527790419913, 'R2 - mean': -314.9369417420469, 'R2 - std': 40.69398236751097} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 239.37744
Epoch 1: Val Loss 239.22142
Epoch 2: Val Loss 239.06635
Epoch 3: Val Loss 238.91144
Epoch 4: Val Loss 238.75726
Epoch 5: Val Loss 238.60335
Epoch 6: Val Loss 238.44937
Epoch 7: Val Loss 238.29489
Epoch 8: Val Loss 238.14102
Epoch 9: Val Loss 237.98735
Epoch 10: Val Loss 237.83395
Epoch 11: Val Loss 237.68045
Epoch 12: Val Loss 237.52702
Epoch 13: Val Loss 237.37325
Epoch 14: Val Loss 237.21925
Epoch 15: Val Loss 237.06438
Epoch 16: Val Loss 236.90991
Epoch 17: Val Loss 236.75493
Epoch 18: Val Loss 236.59975
Epoch 19: Val Loss 236.44400
Epoch 20: Val Loss 236.28798
Epoch 21: Val Loss 236.13199
Epoch 22: Val Loss 235.97624
Epoch 23: Val Loss 235.81995
Epoch 24: Val Loss 235.66286
Epoch 25: Val Loss 235.50615
Epoch 26: Val Loss 235.34930
Epoch 27: Val Loss 235.19194
Epoch 28: Val Loss 235.03419
Epoch 29: Val Loss 234.87680
Epoch 30: Val Loss 234.71846
Epoch 31: Val Loss 234.55943
Epoch 32: Val Loss 234.39955
Epoch 33: Val Loss 234.23883
Epoch 34: Val Loss 234.07672
Epoch 35: Val Loss 233.91365
Epoch 36: Val Loss 233.75040
Epoch 37: Val Loss 233.58598
Epoch 38: Val Loss 233.41992
Epoch 39: Val Loss 233.25331
Epoch 40: Val Loss 233.08511
Epoch 41: Val Loss 232.91548
Epoch 42: Val Loss 232.74500
Epoch 43: Val Loss 232.57343
Epoch 44: Val Loss 232.40105
Epoch 45: Val Loss 232.22829
Epoch 46: Val Loss 232.05420
Epoch 47: Val Loss 231.87988
Epoch 48: Val Loss 231.70486
Epoch 49: Val Loss 231.52838
Epoch 50: Val Loss 231.35007
Epoch 51: Val Loss 231.17151
Epoch 52: Val Loss 230.99092
Epoch 53: Val Loss 230.80835
Epoch 54: Val Loss 230.62418
Epoch 55: Val Loss 230.43968
Epoch 56: Val Loss 230.25366
Epoch 57: Val Loss 230.06650
Epoch 58: Val Loss 229.87723
Epoch 59: Val Loss 229.68729
Epoch 60: Val Loss 229.49544
Epoch 61: Val Loss 229.30238
Epoch 62: Val Loss 229.10770
Epoch 63: Val Loss 228.91133
Epoch 64: Val Loss 228.71347
Epoch 65: Val Loss 228.51390
Epoch 66: Val Loss 228.31190
Epoch 67: Val Loss 228.10727
Epoch 68: Val Loss 227.90102
Epoch 69: Val Loss 227.69261
Epoch 70: Val Loss 227.48218
Epoch 71: Val Loss 227.26988
Epoch 72: Val Loss 227.05630
Epoch 73: Val Loss 226.84027
Epoch 74: Val Loss 226.62178
Epoch 75: Val Loss 226.40135
Epoch 76: Val Loss 226.17883
Epoch 77: Val Loss 225.95322
Epoch 78: Val Loss 225.72557
Epoch 79: Val Loss 225.49486
Epoch 80: Val Loss 225.26074
Epoch 81: Val Loss 225.02373
Epoch 82: Val Loss 224.78433
Epoch 83: Val Loss 224.54326
Epoch 84: Val Loss 224.29881
Epoch 85: Val Loss 224.05199
Epoch 86: Val Loss 223.80309
Epoch 87: Val Loss 223.55057
Epoch 88: Val Loss 223.29482
Epoch 89: Val Loss 223.03569
Epoch 90: Val Loss 222.77386
Epoch 91: Val Loss 222.50871
Epoch 92: Val Loss 222.24049
Epoch 93: Val Loss 221.96915
Epoch 94: Val Loss 221.69336
Epoch 95: Val Loss 221.41399
Epoch 96: Val Loss 221.13191
Epoch 97: Val Loss 220.84633
Epoch 98: Val Loss 220.55679
Epoch 99: Val Loss 220.26389
{'MSE - mean': 215.77540260599463, 'MSE - std': 6.721437674313902, 'R2 - mean': -324.85759596564384, 'R2 - std': 41.45452546632906} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 215.77540260599463 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.32559
Epoch 1: Val Loss 218.66884
Epoch 2: Val Loss 218.06133
Epoch 3: Val Loss 217.47281
Epoch 4: Val Loss 216.87392
Epoch 5: Val Loss 216.23677
Epoch 6: Val Loss 215.51569
Epoch 7: Val Loss 214.70018
Epoch 8: Val Loss 213.74983
Epoch 9: Val Loss 212.64029
Epoch 10: Val Loss 211.35793
Epoch 11: Val Loss 209.87749
Epoch 12: Val Loss 208.18510
Epoch 13: Val Loss 206.19720
Epoch 14: Val Loss 203.89061
Epoch 15: Val Loss 201.24263
Epoch 16: Val Loss 198.22447
Epoch 17: Val Loss 194.80814
Epoch 18: Val Loss 190.94966
Epoch 19: Val Loss 186.57080
Epoch 20: Val Loss 181.65820
Epoch 21: Val Loss 176.19046
Epoch 22: Val Loss 170.14392
Epoch 23: Val Loss 163.47310
Epoch 24: Val Loss 156.16417
Epoch 25: Val Loss 148.21625
Epoch 26: Val Loss 139.64153
Epoch 27: Val Loss 130.44089
Epoch 28: Val Loss 120.66434
Epoch 29: Val Loss 110.37214
Epoch 30: Val Loss 99.66551
Epoch 31: Val Loss 88.67127
Epoch 32: Val Loss 77.51774
Epoch 33: Val Loss 66.44154
Epoch 34: Val Loss 55.65683
Epoch 35: Val Loss 45.44293
Epoch 36: Val Loss 36.10574
Epoch 37: Val Loss 27.89150
Epoch 38: Val Loss 20.98216
Epoch 39: Val Loss 15.50546
Epoch 40: Val Loss 11.46953
Epoch 41: Val Loss 8.73459
Epoch 42: Val Loss 7.05297
Epoch 43: Val Loss 6.07115
Epoch 44: Val Loss 5.49311
Epoch 45: Val Loss 5.09737
Epoch 46: Val Loss 4.74016
Epoch 47: Val Loss 4.37221
Epoch 48: Val Loss 3.98810
Epoch 49: Val Loss 3.61982
Epoch 50: Val Loss 3.28113
Epoch 51: Val Loss 2.99061
Epoch 52: Val Loss 2.73356
Epoch 53: Val Loss 2.50838
Epoch 54: Val Loss 2.31005
Epoch 55: Val Loss 2.13223
Epoch 56: Val Loss 1.96911
Epoch 57: Val Loss 1.81782
Epoch 58: Val Loss 1.68003
Epoch 59: Val Loss 1.55388
Epoch 60: Val Loss 1.43904
Epoch 61: Val Loss 1.33412
Epoch 62: Val Loss 1.23940
Epoch 63: Val Loss 1.15313
Epoch 64: Val Loss 1.07653
Epoch 65: Val Loss 1.00637
Epoch 66: Val Loss 0.94378
Epoch 67: Val Loss 0.88649
Epoch 68: Val Loss 0.83457
Epoch 69: Val Loss 0.78770
Epoch 70: Val Loss 0.74606
Epoch 71: Val Loss 0.70860
Epoch 72: Val Loss 0.67578
Epoch 73: Val Loss 0.64661
Epoch 74: Val Loss 0.62086
Epoch 75: Val Loss 0.59840
Epoch 76: Val Loss 0.57946
Epoch 77: Val Loss 0.56222
Epoch 78: Val Loss 0.54727
Epoch 79: Val Loss 0.53419
Epoch 80: Val Loss 0.52294
Epoch 81: Val Loss 0.51370
Epoch 82: Val Loss 0.50554
Epoch 83: Val Loss 0.49881
Epoch 84: Val Loss 0.49332
Epoch 85: Val Loss 0.48888
Epoch 86: Val Loss 0.48521
Epoch 87: Val Loss 0.48243
Epoch 88: Val Loss 0.48018
Epoch 89: Val Loss 0.47923
Epoch 90: Val Loss 0.47832
Epoch 91: Val Loss 0.47777
Epoch 92: Val Loss 0.47666
Epoch 93: Val Loss 0.47662
Epoch 94: Val Loss 0.47683
Epoch 95: Val Loss 0.47801
Epoch 96: Val Loss 0.47954
Epoch 97: Val Loss 0.47938
Epoch 98: Val Loss 0.48032
Epoch 99: Val Loss 0.48099
{'MSE - mean': 0.4766212554649632, 'MSE - std': 0.0, 'R2 - mean': 0.1327069050966505, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.77020
Epoch 1: Val Loss 219.77118
Epoch 2: Val Loss 218.71671
Epoch 3: Val Loss 217.61198
Epoch 4: Val Loss 216.43486
Epoch 5: Val Loss 215.16582
Epoch 6: Val Loss 213.76981
Epoch 7: Val Loss 212.21568
Epoch 8: Val Loss 210.48358
Epoch 9: Val Loss 208.51550
Epoch 10: Val Loss 206.28604
Epoch 11: Val Loss 203.74812
Epoch 12: Val Loss 200.81564
Epoch 13: Val Loss 197.46870
Epoch 14: Val Loss 193.61722
Epoch 15: Val Loss 189.21516
Epoch 16: Val Loss 184.21613
Epoch 17: Val Loss 178.56183
Epoch 18: Val Loss 172.16858
Epoch 19: Val Loss 164.98416
Epoch 20: Val Loss 156.97231
Epoch 21: Val Loss 148.11407
Epoch 22: Val Loss 138.41182
Epoch 23: Val Loss 127.95480
Epoch 24: Val Loss 116.76283
Epoch 25: Val Loss 104.95810
Epoch 26: Val Loss 92.69918
Epoch 27: Val Loss 80.19318
Epoch 28: Val Loss 67.74932
Epoch 29: Val Loss 55.71136
Epoch 30: Val Loss 44.39688
Epoch 31: Val Loss 34.11480
Epoch 32: Val Loss 25.26947
Epoch 33: Val Loss 18.05873
Epoch 34: Val Loss 12.61308
Epoch 35: Val Loss 8.86172
Epoch 36: Val Loss 6.52204
Epoch 37: Val Loss 5.19413
Epoch 38: Val Loss 4.45788
Epoch 39: Val Loss 3.99110
Epoch 40: Val Loss 3.60582
Epoch 41: Val Loss 3.25431
Epoch 42: Val Loss 2.94921
Epoch 43: Val Loss 2.70081
Epoch 44: Val Loss 2.50627
Epoch 45: Val Loss 2.34897
Epoch 46: Val Loss 2.20749
Epoch 47: Val Loss 2.07088
Epoch 48: Val Loss 1.93729
Epoch 49: Val Loss 1.80488
Epoch 50: Val Loss 1.67531
Epoch 51: Val Loss 1.55895
Epoch 52: Val Loss 1.45035
Epoch 53: Val Loss 1.35597
Epoch 54: Val Loss 1.27125
Epoch 55: Val Loss 1.20194
Epoch 56: Val Loss 1.14156
Epoch 57: Val Loss 1.08885
Epoch 58: Val Loss 1.04438
Epoch 59: Val Loss 1.00504
Epoch 60: Val Loss 0.96899
Epoch 61: Val Loss 0.93616
Epoch 62: Val Loss 0.90564
Epoch 63: Val Loss 0.87723
Epoch 64: Val Loss 0.84935
Epoch 65: Val Loss 0.82385
Epoch 66: Val Loss 0.80106
Epoch 67: Val Loss 0.78126
Epoch 68: Val Loss 0.76571
Epoch 69: Val Loss 0.74995
Epoch 70: Val Loss 0.73552
Epoch 71: Val Loss 0.72281
Epoch 72: Val Loss 0.71071
Epoch 73: Val Loss 0.69872
Epoch 74: Val Loss 0.68816
Epoch 75: Val Loss 0.67985
Epoch 76: Val Loss 0.67053
Epoch 77: Val Loss 0.66310
Epoch 78: Val Loss 0.65515
Epoch 79: Val Loss 0.64763
Epoch 80: Val Loss 0.63942
Epoch 81: Val Loss 0.63211
Epoch 82: Val Loss 0.62671
Epoch 83: Val Loss 0.62162
Epoch 84: Val Loss 0.61809
Epoch 85: Val Loss 0.61366
Epoch 86: Val Loss 0.60827
Epoch 87: Val Loss 0.60449
Epoch 88: Val Loss 0.60177
Epoch 89: Val Loss 0.59704
Epoch 90: Val Loss 0.59423
Epoch 91: Val Loss 0.59206
Epoch 92: Val Loss 0.59196
Epoch 93: Val Loss 0.59020
Epoch 94: Val Loss 0.58917
Epoch 95: Val Loss 0.58725
Epoch 96: Val Loss 0.58620
Epoch 97: Val Loss 0.58474
Epoch 98: Val Loss 0.58266
Epoch 99: Val Loss 0.58203
{'MSE - mean': 0.5293279353582127, 'MSE - std': 0.05270667989324948, 'R2 - mean': 0.17226130700764403, 'R2 - std': 0.03955440191099352} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 213.79176
Epoch 1: Val Loss 212.16878
Epoch 2: Val Loss 210.41338
Epoch 3: Val Loss 208.50204
Epoch 4: Val Loss 206.43674
Epoch 5: Val Loss 204.20227
Epoch 6: Val Loss 201.77020
Epoch 7: Val Loss 199.09731
Epoch 8: Val Loss 196.16646
Epoch 9: Val Loss 192.96417
Epoch 10: Val Loss 189.42851
Epoch 11: Val Loss 185.51158
Epoch 12: Val Loss 181.16124
Epoch 13: Val Loss 176.29997
Epoch 14: Val Loss 170.90617
Epoch 15: Val Loss 164.92339
Epoch 16: Val Loss 158.31389
Epoch 17: Val Loss 151.05426
Epoch 18: Val Loss 143.07898
Epoch 19: Val Loss 134.39236
Epoch 20: Val Loss 125.04174
Epoch 21: Val Loss 115.05589
Epoch 22: Val Loss 104.52535
Epoch 23: Val Loss 93.57746
Epoch 24: Val Loss 82.31591
Epoch 25: Val Loss 70.97735
Epoch 26: Val Loss 59.77616
Epoch 27: Val Loss 49.01036
Epoch 28: Val Loss 39.02040
Epoch 29: Val Loss 30.11013
Epoch 30: Val Loss 22.49074
Epoch 31: Val Loss 16.39251
Epoch 32: Val Loss 11.81491
Epoch 33: Val Loss 8.65539
Epoch 34: Val Loss 6.66527
Epoch 35: Val Loss 5.44995
Epoch 36: Val Loss 4.71261
Epoch 37: Val Loss 4.20020
Epoch 38: Val Loss 3.76218
Epoch 39: Val Loss 3.35699
Epoch 40: Val Loss 2.98573
Epoch 41: Val Loss 2.66348
Epoch 42: Val Loss 2.38508
Epoch 43: Val Loss 2.15713
Epoch 44: Val Loss 1.96740
Epoch 45: Val Loss 1.80995
Epoch 46: Val Loss 1.67269
Epoch 47: Val Loss 1.55124
Epoch 48: Val Loss 1.44263
Epoch 49: Val Loss 1.34672
Epoch 50: Val Loss 1.26204
Epoch 51: Val Loss 1.18766
Epoch 52: Val Loss 1.12337
Epoch 53: Val Loss 1.06798
Epoch 54: Val Loss 1.01962
Epoch 55: Val Loss 0.97723
Epoch 56: Val Loss 0.94154
Epoch 57: Val Loss 0.91050
Epoch 58: Val Loss 0.88303
Epoch 59: Val Loss 0.86016
Epoch 60: Val Loss 0.84055
Epoch 61: Val Loss 0.82224
Epoch 62: Val Loss 0.80678
Epoch 63: Val Loss 0.79284
Epoch 64: Val Loss 0.78110
Epoch 65: Val Loss 0.76973
Epoch 66: Val Loss 0.75933
Epoch 67: Val Loss 0.74987
Epoch 68: Val Loss 0.74181
Epoch 69: Val Loss 0.73423
Epoch 70: Val Loss 0.72737
Epoch 71: Val Loss 0.72169
Epoch 72: Val Loss 0.71627
Epoch 73: Val Loss 0.71182
Epoch 74: Val Loss 0.70777
Epoch 75: Val Loss 0.70337
Epoch 76: Val Loss 0.69974
Epoch 77: Val Loss 0.69567
Epoch 78: Val Loss 0.69254
Epoch 79: Val Loss 0.68958
Epoch 80: Val Loss 0.68685
Epoch 81: Val Loss 0.68426
Epoch 82: Val Loss 0.68206
Epoch 83: Val Loss 0.68012
Epoch 84: Val Loss 0.67842
Epoch 85: Val Loss 0.67629
Epoch 86: Val Loss 0.67528
Epoch 87: Val Loss 0.67378
Epoch 88: Val Loss 0.67203
Epoch 89: Val Loss 0.67022
Epoch 90: Val Loss 0.66903
Epoch 91: Val Loss 0.66755
Epoch 92: Val Loss 0.66619
Epoch 93: Val Loss 0.66478
Epoch 94: Val Loss 0.66319
Epoch 95: Val Loss 0.66161
Epoch 96: Val Loss 0.66115
Epoch 97: Val Loss 0.66007
Epoch 98: Val Loss 0.65972
Epoch 99: Val Loss 0.65926
{'MSE - mean': 0.5726383808005743, 'MSE - std': 0.07485710012469009, 'R2 - mean': 0.12921084817948877, 'R2 - std': 0.06891819656414157} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.95308
Epoch 1: Val Loss 230.85703
Epoch 2: Val Loss 229.79842
Epoch 3: Val Loss 228.77113
Epoch 4: Val Loss 227.76213
Epoch 5: Val Loss 226.75966
Epoch 6: Val Loss 225.71715
Epoch 7: Val Loss 224.60135
Epoch 8: Val Loss 223.40811
Epoch 9: Val Loss 222.09360
Epoch 10: Val Loss 220.60616
Epoch 11: Val Loss 218.90421
Epoch 12: Val Loss 216.94133
Epoch 13: Val Loss 214.65852
Epoch 14: Val Loss 212.03334
Epoch 15: Val Loss 208.99217
Epoch 16: Val Loss 205.48282
Epoch 17: Val Loss 201.44380
Epoch 18: Val Loss 196.79424
Epoch 19: Val Loss 191.47569
Epoch 20: Val Loss 185.45491
Epoch 21: Val Loss 178.66235
Epoch 22: Val Loss 171.03627
Epoch 23: Val Loss 162.56798
Epoch 24: Val Loss 153.24942
Epoch 25: Val Loss 143.04649
Epoch 26: Val Loss 132.02719
Epoch 27: Val Loss 120.22482
Epoch 28: Val Loss 107.79154
Epoch 29: Val Loss 94.90722
Epoch 30: Val Loss 81.77586
Epoch 31: Val Loss 68.69386
Epoch 32: Val Loss 55.99783
Epoch 33: Val Loss 44.13015
Epoch 34: Val Loss 33.42847
Epoch 35: Val Loss 24.28505
Epoch 36: Val Loss 16.94615
Epoch 37: Val Loss 11.53096
Epoch 38: Val Loss 7.94107
Epoch 39: Val Loss 5.88163
Epoch 40: Val Loss 4.87859
Epoch 41: Val Loss 4.47603
Epoch 42: Val Loss 4.29100
Epoch 43: Val Loss 4.09327
Epoch 44: Val Loss 3.82020
Epoch 45: Val Loss 3.48194
Epoch 46: Val Loss 3.12705
Epoch 47: Val Loss 2.80355
Epoch 48: Val Loss 2.53320
Epoch 49: Val Loss 2.31698
Epoch 50: Val Loss 2.14171
Epoch 51: Val Loss 1.99109
Epoch 52: Val Loss 1.86000
Epoch 53: Val Loss 1.74047
Epoch 54: Val Loss 1.62802
Epoch 55: Val Loss 1.52537
Epoch 56: Val Loss 1.43160
Epoch 57: Val Loss 1.34760
Epoch 58: Val Loss 1.27261
Epoch 59: Val Loss 1.20576
Epoch 60: Val Loss 1.14588
Epoch 61: Val Loss 1.09257
Epoch 62: Val Loss 1.04435
Epoch 63: Val Loss 1.00180
Epoch 64: Val Loss 0.96350
Epoch 65: Val Loss 0.92940
Epoch 66: Val Loss 0.89792
Epoch 67: Val Loss 0.86964
Epoch 68: Val Loss 0.84477
Epoch 69: Val Loss 0.82228
Epoch 70: Val Loss 0.80247
Epoch 71: Val Loss 0.78423
Epoch 72: Val Loss 0.76934
Epoch 73: Val Loss 0.75476
Epoch 74: Val Loss 0.74144
Epoch 75: Val Loss 0.73049
Epoch 76: Val Loss 0.71995
Epoch 77: Val Loss 0.71045
Epoch 78: Val Loss 0.70245
Epoch 79: Val Loss 0.69478
Epoch 80: Val Loss 0.68787
Epoch 81: Val Loss 0.68164
Epoch 82: Val Loss 0.67620
Epoch 83: Val Loss 0.67179
Epoch 84: Val Loss 0.66737
Epoch 85: Val Loss 0.66372
Epoch 86: Val Loss 0.66114
Epoch 87: Val Loss 0.65852
Epoch 88: Val Loss 0.65629
Epoch 89: Val Loss 0.65357
Epoch 90: Val Loss 0.65200
Epoch 91: Val Loss 0.65062
Epoch 92: Val Loss 0.64938
Epoch 93: Val Loss 0.64723
Epoch 94: Val Loss 0.64585
Epoch 95: Val Loss 0.64485
Epoch 96: Val Loss 0.64381
Epoch 97: Val Loss 0.64289
Epoch 98: Val Loss 0.64199
Epoch 99: Val Loss 0.64151
{'MSE - mean': 0.5898557738697888, 'MSE - std': 0.07135828579504915, 'R2 - mean': 0.1423574880622605, 'R2 - std': 0.06388106749156665} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 237.82639
Epoch 1: Val Loss 236.54797
Epoch 2: Val Loss 235.30162
Epoch 3: Val Loss 234.09505
Epoch 4: Val Loss 232.89224
Epoch 5: Val Loss 231.65430
Epoch 6: Val Loss 230.35071
Epoch 7: Val Loss 228.93494
Epoch 8: Val Loss 227.36038
Epoch 9: Val Loss 225.58237
Epoch 10: Val Loss 223.54619
Epoch 11: Val Loss 221.22166
Epoch 12: Val Loss 218.52296
Epoch 13: Val Loss 215.40332
Epoch 14: Val Loss 211.80345
Epoch 15: Val Loss 207.64537
Epoch 16: Val Loss 202.86606
Epoch 17: Val Loss 197.38388
Epoch 18: Val Loss 191.07564
Epoch 19: Val Loss 183.89546
Epoch 20: Val Loss 175.76654
Epoch 21: Val Loss 166.65971
Epoch 22: Val Loss 156.50363
Epoch 23: Val Loss 145.28748
Epoch 24: Val Loss 133.08118
Epoch 25: Val Loss 119.99641
Epoch 26: Val Loss 106.14610
Epoch 27: Val Loss 91.75489
Epoch 28: Val Loss 77.16423
Epoch 29: Val Loss 62.77798
Epoch 30: Val Loss 49.11091
Epoch 31: Val Loss 36.68871
Epoch 32: Val Loss 26.01697
Epoch 33: Val Loss 17.53271
Epoch 34: Val Loss 11.44393
Epoch 35: Val Loss 7.66063
Epoch 36: Val Loss 5.69152
Epoch 37: Val Loss 4.89511
Epoch 38: Val Loss 4.57927
Epoch 39: Val Loss 4.32315
Epoch 40: Val Loss 3.96677
Epoch 41: Val Loss 3.55181
Epoch 42: Val Loss 3.15892
Epoch 43: Val Loss 2.84333
Epoch 44: Val Loss 2.61525
Epoch 45: Val Loss 2.44244
Epoch 46: Val Loss 2.30158
Epoch 47: Val Loss 2.16783
Epoch 48: Val Loss 2.03337
Epoch 49: Val Loss 1.90458
Epoch 50: Val Loss 1.78080
Epoch 51: Val Loss 1.66368
Epoch 52: Val Loss 1.55981
Epoch 53: Val Loss 1.47038
Epoch 54: Val Loss 1.39118
Epoch 55: Val Loss 1.32206
Epoch 56: Val Loss 1.25965
Epoch 57: Val Loss 1.20390
Epoch 58: Val Loss 1.15351
Epoch 59: Val Loss 1.10685
Epoch 60: Val Loss 1.06423
Epoch 61: Val Loss 1.02447
Epoch 62: Val Loss 0.98875
Epoch 63: Val Loss 0.95393
Epoch 64: Val Loss 0.92226
Epoch 65: Val Loss 0.89307
Epoch 66: Val Loss 0.86618
Epoch 67: Val Loss 0.84151
Epoch 68: Val Loss 0.81958
Epoch 69: Val Loss 0.80107
Epoch 70: Val Loss 0.78363
Epoch 71: Val Loss 0.76717
Epoch 72: Val Loss 0.75405
Epoch 73: Val Loss 0.74137
Epoch 74: Val Loss 0.72922
Epoch 75: Val Loss 0.71818
Epoch 76: Val Loss 0.70906
Epoch 77: Val Loss 0.70097
Epoch 78: Val Loss 0.69457
Epoch 79: Val Loss 0.68850
Epoch 80: Val Loss 0.68340
Epoch 81: Val Loss 0.67808
Epoch 82: Val Loss 0.67353
Epoch 83: Val Loss 0.66927
Epoch 84: Val Loss 0.66495
Epoch 85: Val Loss 0.66241
Epoch 86: Val Loss 0.65918
Epoch 87: Val Loss 0.65578
Epoch 88: Val Loss 0.65415
Epoch 89: Val Loss 0.65262
Epoch 90: Val Loss 0.65168
Epoch 91: Val Loss 0.64993
Epoch 92: Val Loss 0.64916
Epoch 93: Val Loss 0.64884
Epoch 94: Val Loss 0.64702
Epoch 95: Val Loss 0.64585
Epoch 96: Val Loss 0.64518
Epoch 97: Val Loss 0.64416
Epoch 98: Val Loss 0.64308
Epoch 99: Val Loss 0.64353
{'MSE - mean': 0.600500668457617, 'MSE - std': 0.06728193728208358, 'R2 - mean': 0.10044048250532107, 'R2 - std': 0.10145330968742046} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 0.600500668457617 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.15202
Epoch 1: Val Loss 229.06879
Epoch 2: Val Loss 227.94679
Epoch 3: Val Loss 226.74567
Epoch 4: Val Loss 225.38393
Epoch 5: Val Loss 223.84680
Epoch 6: Val Loss 222.09053
Epoch 7: Val Loss 220.08311
Epoch 8: Val Loss 217.79243
Epoch 9: Val Loss 215.17729
Epoch 10: Val Loss 212.19290
Epoch 11: Val Loss 208.77707
Epoch 12: Val Loss 204.88268
Epoch 13: Val Loss 200.44759
Epoch 14: Val Loss 195.42853
Epoch 15: Val Loss 189.76056
Epoch 16: Val Loss 183.39143
Epoch 17: Val Loss 176.21420
Epoch 18: Val Loss 168.19365
Epoch 19: Val Loss 159.27007
Epoch 20: Val Loss 149.39665
Epoch 21: Val Loss 138.62224
Epoch 22: Val Loss 126.97682
Epoch 23: Val Loss 114.58705
Epoch 24: Val Loss 101.60609
Epoch 25: Val Loss 88.22344
Epoch 26: Val Loss 74.73882
Epoch 27: Val Loss 61.56062
Epoch 28: Val Loss 49.13294
Epoch 29: Val Loss 37.87322
Epoch 30: Val Loss 28.27337
Epoch 31: Val Loss 20.56087
Epoch 32: Val Loss 14.91377
Epoch 33: Val Loss 11.22694
Epoch 34: Val Loss 9.10628
Epoch 35: Val Loss 8.02767
Epoch 36: Val Loss 7.46585
Epoch 37: Val Loss 7.00611
Epoch 38: Val Loss 6.48332
Epoch 39: Val Loss 5.88918
Epoch 40: Val Loss 5.27022
Epoch 41: Val Loss 4.70258
Epoch 42: Val Loss 4.21785
Epoch 43: Val Loss 3.81291
Epoch 44: Val Loss 3.47378
Epoch 45: Val Loss 3.18596
Epoch 46: Val Loss 2.93538
Epoch 47: Val Loss 2.71054
Epoch 48: Val Loss 2.50896
Epoch 49: Val Loss 2.32821
Epoch 50: Val Loss 2.16397
Epoch 51: Val Loss 2.01714
Epoch 52: Val Loss 1.88380
Epoch 53: Val Loss 1.75780
Epoch 54: Val Loss 1.64318
Epoch 55: Val Loss 1.53759
Epoch 56: Val Loss 1.43850
Epoch 57: Val Loss 1.34563
Epoch 58: Val Loss 1.26320
Epoch 59: Val Loss 1.18703
Epoch 60: Val Loss 1.11941
Epoch 61: Val Loss 1.05747
Epoch 62: Val Loss 1.00271
Epoch 63: Val Loss 0.95412
Epoch 64: Val Loss 0.90886
Epoch 65: Val Loss 0.86831
Epoch 66: Val Loss 0.83055
Epoch 67: Val Loss 0.79802
Epoch 68: Val Loss 0.76798
Epoch 69: Val Loss 0.74202
Epoch 70: Val Loss 0.71749
Epoch 71: Val Loss 0.69615
Epoch 72: Val Loss 0.67796
Epoch 73: Val Loss 0.66075
Epoch 74: Val Loss 0.64517
Epoch 75: Val Loss 0.63110
Epoch 76: Val Loss 0.62119
Epoch 77: Val Loss 0.61166
Epoch 78: Val Loss 0.60374
Epoch 79: Val Loss 0.59497
Epoch 80: Val Loss 0.58765
Epoch 81: Val Loss 0.58132
Epoch 82: Val Loss 0.57541
Epoch 83: Val Loss 0.56969
Epoch 84: Val Loss 0.56498
Epoch 85: Val Loss 0.56144
Epoch 86: Val Loss 0.55947
Epoch 87: Val Loss 0.55771
Epoch 88: Val Loss 0.55579
Epoch 89: Val Loss 0.55450
Epoch 90: Val Loss 0.55356
Epoch 91: Val Loss 0.55081
Epoch 92: Val Loss 0.54926
Epoch 93: Val Loss 0.54811
Epoch 94: Val Loss 0.54763
Epoch 95: Val Loss 0.54575
Epoch 96: Val Loss 0.54482
Epoch 97: Val Loss 0.54390
Epoch 98: Val Loss 0.54393
Epoch 99: Val Loss 0.54346
{'MSE - mean': 0.5434616563984962, 'MSE - std': 0.0, 'R2 - mean': 0.011079475506519776, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.14569
Epoch 1: Val Loss 233.05583
Epoch 2: Val Loss 231.95015
Epoch 3: Val Loss 230.81897
Epoch 4: Val Loss 229.62794
Epoch 5: Val Loss 228.35379
Epoch 6: Val Loss 226.97696
Epoch 7: Val Loss 225.45401
Epoch 8: Val Loss 223.75258
Epoch 9: Val Loss 221.85617
Epoch 10: Val Loss 219.71181
Epoch 11: Val Loss 217.29051
Epoch 12: Val Loss 214.55281
Epoch 13: Val Loss 211.41072
Epoch 14: Val Loss 207.82974
Epoch 15: Val Loss 203.73836
Epoch 16: Val Loss 199.04776
Epoch 17: Val Loss 193.69930
Epoch 18: Val Loss 187.63379
Epoch 19: Val Loss 180.73814
Epoch 20: Val Loss 172.95503
Epoch 21: Val Loss 164.27129
Epoch 22: Val Loss 154.64824
Epoch 23: Val Loss 144.07591
Epoch 24: Val Loss 132.55876
Epoch 25: Val Loss 120.16429
Epoch 26: Val Loss 107.02815
Epoch 27: Val Loss 93.34561
Epoch 28: Val Loss 79.39574
Epoch 29: Val Loss 65.57143
Epoch 30: Val Loss 52.32517
Epoch 31: Val Loss 40.13287
Epoch 32: Val Loss 29.44315
Epoch 33: Val Loss 20.72642
Epoch 34: Val Loss 14.19224
Epoch 35: Val Loss 9.75496
Epoch 36: Val Loss 7.14809
Epoch 37: Val Loss 5.80844
Epoch 38: Val Loss 5.13715
Epoch 39: Val Loss 4.68769
Epoch 40: Val Loss 4.25055
Epoch 41: Val Loss 3.80200
Epoch 42: Val Loss 3.38496
Epoch 43: Val Loss 3.04574
Epoch 44: Val Loss 2.78055
Epoch 45: Val Loss 2.56820
Epoch 46: Val Loss 2.38407
Epoch 47: Val Loss 2.21470
Epoch 48: Val Loss 2.04950
Epoch 49: Val Loss 1.89402
Epoch 50: Val Loss 1.75074
Epoch 51: Val Loss 1.62710
Epoch 52: Val Loss 1.51442
Epoch 53: Val Loss 1.41806
Epoch 54: Val Loss 1.33625
Epoch 55: Val Loss 1.26460
Epoch 56: Val Loss 1.20091
Epoch 57: Val Loss 1.14517
Epoch 58: Val Loss 1.09714
Epoch 59: Val Loss 1.05446
Epoch 60: Val Loss 1.01494
Epoch 61: Val Loss 0.97898
Epoch 62: Val Loss 0.94694
Epoch 63: Val Loss 0.91758
Epoch 64: Val Loss 0.89087
Epoch 65: Val Loss 0.86491
Epoch 66: Val Loss 0.84200
Epoch 67: Val Loss 0.82064
Epoch 68: Val Loss 0.80086
Epoch 69: Val Loss 0.78509
Epoch 70: Val Loss 0.77054
Epoch 71: Val Loss 0.75720
Epoch 72: Val Loss 0.74501
Epoch 73: Val Loss 0.73325
Epoch 74: Val Loss 0.72269
Epoch 75: Val Loss 0.71263
Epoch 76: Val Loss 0.70449
Epoch 77: Val Loss 0.69581
Epoch 78: Val Loss 0.68856
Epoch 79: Val Loss 0.68184
Epoch 80: Val Loss 0.67644
Epoch 81: Val Loss 0.67152
Epoch 82: Val Loss 0.66521
Epoch 83: Val Loss 0.65998
Epoch 84: Val Loss 0.65502
Epoch 85: Val Loss 0.65019
Epoch 86: Val Loss 0.64598
Epoch 87: Val Loss 0.64259
Epoch 88: Val Loss 0.64072
Epoch 89: Val Loss 0.63855
Epoch 90: Val Loss 0.63655
Epoch 91: Val Loss 0.63574
Epoch 92: Val Loss 0.63278
Epoch 93: Val Loss 0.63069
Epoch 94: Val Loss 0.62881
Epoch 95: Val Loss 0.62632
Epoch 96: Val Loss 0.62442
Epoch 97: Val Loss 0.62332
Epoch 98: Val Loss 0.62298
Epoch 99: Val Loss 0.62228
{'MSE - mean': 0.5828721278235818, 'MSE - std': 0.039410471425085636, 'R2 - mean': 0.08419592375245993, 'R2 - std': 0.07311644824594016} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 221.37212
Epoch 1: Val Loss 220.71463
Epoch 2: Val Loss 220.03114
Epoch 3: Val Loss 219.30377
Epoch 4: Val Loss 218.52275
Epoch 5: Val Loss 217.66893
Epoch 6: Val Loss 216.71976
Epoch 7: Val Loss 215.65132
Epoch 8: Val Loss 214.43710
Epoch 9: Val Loss 213.04581
Epoch 10: Val Loss 211.43999
Epoch 11: Val Loss 209.57727
Epoch 12: Val Loss 207.42509
Epoch 13: Val Loss 204.94086
Epoch 14: Val Loss 202.08005
Epoch 15: Val Loss 198.76910
Epoch 16: Val Loss 194.96341
Epoch 17: Val Loss 190.60155
Epoch 18: Val Loss 185.61670
Epoch 19: Val Loss 179.94785
Epoch 20: Val Loss 173.52971
Epoch 21: Val Loss 166.30087
Epoch 22: Val Loss 158.19087
Epoch 23: Val Loss 149.12938
Epoch 24: Val Loss 139.10451
Epoch 25: Val Loss 128.09412
Epoch 26: Val Loss 116.16314
Epoch 27: Val Loss 103.43293
Epoch 28: Val Loss 90.09378
Epoch 29: Val Loss 76.44631
Epoch 30: Val Loss 62.86633
Epoch 31: Val Loss 49.79961
Epoch 32: Val Loss 37.84595
Epoch 33: Val Loss 27.48737
Epoch 34: Val Loss 19.17931
Epoch 35: Val Loss 13.12697
Epoch 36: Val Loss 9.28939
Epoch 37: Val Loss 7.23703
Epoch 38: Val Loss 6.30329
Epoch 39: Val Loss 5.83764
Epoch 40: Val Loss 5.40789
Epoch 41: Val Loss 4.86078
Epoch 42: Val Loss 4.24901
Epoch 43: Val Loss 3.67889
Epoch 44: Val Loss 3.21219
Epoch 45: Val Loss 2.86128
Epoch 46: Val Loss 2.58822
Epoch 47: Val Loss 2.35881
Epoch 48: Val Loss 2.16021
Epoch 49: Val Loss 1.97324
Epoch 50: Val Loss 1.80513
Epoch 51: Val Loss 1.65453
Epoch 52: Val Loss 1.52264
Epoch 53: Val Loss 1.41132
Epoch 54: Val Loss 1.31603
Epoch 55: Val Loss 1.23632
Epoch 56: Val Loss 1.16747
Epoch 57: Val Loss 1.10675
Epoch 58: Val Loss 1.05282
Epoch 59: Val Loss 1.00628
Epoch 60: Val Loss 0.96596
Epoch 61: Val Loss 0.93156
Epoch 62: Val Loss 0.90276
Epoch 63: Val Loss 0.87698
Epoch 64: Val Loss 0.85410
Epoch 65: Val Loss 0.83446
Epoch 66: Val Loss 0.81743
Epoch 67: Val Loss 0.80264
Epoch 68: Val Loss 0.78967
Epoch 69: Val Loss 0.77833
Epoch 70: Val Loss 0.76757
Epoch 71: Val Loss 0.75909
Epoch 72: Val Loss 0.75021
Epoch 73: Val Loss 0.74311
Epoch 74: Val Loss 0.73735
Epoch 75: Val Loss 0.73189
Epoch 76: Val Loss 0.72664
Epoch 77: Val Loss 0.72273
Epoch 78: Val Loss 0.71880
Epoch 79: Val Loss 0.71570
Epoch 80: Val Loss 0.71297
Epoch 81: Val Loss 0.71159
Epoch 82: Val Loss 0.70966
Epoch 83: Val Loss 0.70762
Epoch 84: Val Loss 0.70669
Epoch 85: Val Loss 0.70458
Epoch 86: Val Loss 0.70281
Epoch 87: Val Loss 0.70085
Epoch 88: Val Loss 0.69890
Epoch 89: Val Loss 0.69825
Epoch 90: Val Loss 0.69740
Epoch 91: Val Loss 0.69609
Epoch 92: Val Loss 0.69481
Epoch 93: Val Loss 0.69428
Epoch 94: Val Loss 0.69209
Epoch 95: Val Loss 0.69007
Epoch 96: Val Loss 0.68904
Epoch 97: Val Loss 0.68758
Epoch 98: Val Loss 0.68718
Epoch 99: Val Loss 0.68652
{'MSE - mean': 0.6174228303991144, 'MSE - std': 0.05850605896449952, 'R2 - mean': 0.05730923570542534, 'R2 - std': 0.07077992646938304} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.71660
Epoch 1: Val Loss 233.56306
Epoch 2: Val Loss 232.30229
Epoch 3: Val Loss 230.93175
Epoch 4: Val Loss 229.40967
Epoch 5: Val Loss 227.71405
Epoch 6: Val Loss 225.79459
Epoch 7: Val Loss 223.62726
Epoch 8: Val Loss 221.15637
Epoch 9: Val Loss 218.31232
Epoch 10: Val Loss 215.04872
Epoch 11: Val Loss 211.32497
Epoch 12: Val Loss 207.06744
Epoch 13: Val Loss 202.22159
Epoch 14: Val Loss 196.69341
Epoch 15: Val Loss 190.45570
Epoch 16: Val Loss 183.42268
Epoch 17: Val Loss 175.52890
Epoch 18: Val Loss 166.70926
Epoch 19: Val Loss 156.89438
Epoch 20: Val Loss 146.01909
Epoch 21: Val Loss 134.10760
Epoch 22: Val Loss 121.23196
Epoch 23: Val Loss 107.50158
Epoch 24: Val Loss 93.22896
Epoch 25: Val Loss 78.69148
Epoch 26: Val Loss 64.27939
Epoch 27: Val Loss 50.48885
Epoch 28: Val Loss 37.88271
Epoch 29: Val Loss 27.02324
Epoch 30: Val Loss 18.37490
Epoch 31: Val Loss 12.11914
Epoch 32: Val Loss 8.18458
Epoch 33: Val Loss 6.09602
Epoch 34: Val Loss 5.17083
Epoch 35: Val Loss 4.70407
Epoch 36: Val Loss 4.27003
Epoch 37: Val Loss 3.74872
Epoch 38: Val Loss 3.19818
Epoch 39: Val Loss 2.70886
Epoch 40: Val Loss 2.33225
Epoch 41: Val Loss 2.06689
Epoch 42: Val Loss 1.87054
Epoch 43: Val Loss 1.71180
Epoch 44: Val Loss 1.57093
Epoch 45: Val Loss 1.44325
Epoch 46: Val Loss 1.32413
Epoch 47: Val Loss 1.22382
Epoch 48: Val Loss 1.14278
Epoch 49: Val Loss 1.07574
Epoch 50: Val Loss 1.02263
Epoch 51: Val Loss 0.97970
Epoch 52: Val Loss 0.94370
Epoch 53: Val Loss 0.91359
Epoch 54: Val Loss 0.88898
Epoch 55: Val Loss 0.86801
Epoch 56: Val Loss 0.85043
Epoch 57: Val Loss 0.83386
Epoch 58: Val Loss 0.81977
Epoch 59: Val Loss 0.80761
Epoch 60: Val Loss 0.79674
Epoch 61: Val Loss 0.78738
Epoch 62: Val Loss 0.77880
Epoch 63: Val Loss 0.77101
Epoch 64: Val Loss 0.76488
Epoch 65: Val Loss 0.75936
Epoch 66: Val Loss 0.75453
Epoch 67: Val Loss 0.74868
Epoch 68: Val Loss 0.74476
Epoch 69: Val Loss 0.74092
Epoch 70: Val Loss 0.73797
Epoch 71: Val Loss 0.73513
Epoch 72: Val Loss 0.73277
Epoch 73: Val Loss 0.73001
Epoch 74: Val Loss 0.72774
Epoch 75: Val Loss 0.72577
Epoch 76: Val Loss 0.72370
Epoch 77: Val Loss 0.72142
Epoch 78: Val Loss 0.71993
Epoch 79: Val Loss 0.71766
Epoch 80: Val Loss 0.71522
Epoch 81: Val Loss 0.71338
Epoch 82: Val Loss 0.71134
Epoch 83: Val Loss 0.70935
Epoch 84: Val Loss 0.70753
Epoch 85: Val Loss 0.70615
Epoch 86: Val Loss 0.70422
Epoch 87: Val Loss 0.70450
Epoch 88: Val Loss 0.70307
Epoch 89: Val Loss 0.70109
Epoch 90: Val Loss 0.69997
Epoch 91: Val Loss 0.69907
Epoch 92: Val Loss 0.69814
Epoch 93: Val Loss 0.69694
Epoch 94: Val Loss 0.69545
Epoch 95: Val Loss 0.69398
Epoch 96: Val Loss 0.69226
Epoch 97: Val Loss 0.69113
Epoch 98: Val Loss 0.68998
Epoch 99: Val Loss 0.68906
{'MSE - mean': 0.6353328389856938, 'MSE - std': 0.059409800743498206, 'R2 - mean': 0.07326796240790656, 'R2 - std': 0.06724129212880961} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.58650
Epoch 1: Val Loss 232.48909
Epoch 2: Val Loss 231.34384
Epoch 3: Val Loss 230.15302
Epoch 4: Val Loss 228.88187
Epoch 5: Val Loss 227.53871
Epoch 6: Val Loss 226.10287
Epoch 7: Val Loss 224.55292
Epoch 8: Val Loss 222.84198
Epoch 9: Val Loss 220.91327
Epoch 10: Val Loss 218.77957
Epoch 11: Val Loss 216.40262
Epoch 12: Val Loss 213.74963
Epoch 13: Val Loss 210.77751
Epoch 14: Val Loss 207.46327
Epoch 15: Val Loss 203.72948
Epoch 16: Val Loss 199.57173
Epoch 17: Val Loss 194.94180
Epoch 18: Val Loss 189.80104
Epoch 19: Val Loss 184.13240
Epoch 20: Val Loss 177.89539
Epoch 21: Val Loss 171.05386
Epoch 22: Val Loss 163.58777
Epoch 23: Val Loss 155.48386
Epoch 24: Val Loss 146.77679
Epoch 25: Val Loss 137.49661
Epoch 26: Val Loss 127.69823
Epoch 27: Val Loss 117.43832
Epoch 28: Val Loss 106.81062
Epoch 29: Val Loss 95.95818
Epoch 30: Val Loss 85.01160
Epoch 31: Val Loss 74.14825
Epoch 32: Val Loss 63.51527
Epoch 33: Val Loss 53.31888
Epoch 34: Val Loss 43.73677
Epoch 35: Val Loss 34.97919
Epoch 36: Val Loss 27.20366
Epoch 37: Val Loss 20.53411
Epoch 38: Val Loss 15.06236
Epoch 39: Val Loss 10.79860
Epoch 40: Val Loss 7.69122
Epoch 41: Val Loss 5.60024
Epoch 42: Val Loss 4.34357
Epoch 43: Val Loss 3.66189
Epoch 44: Val Loss 3.32159
Epoch 45: Val Loss 3.13013
Epoch 46: Val Loss 2.96188
Epoch 47: Val Loss 2.76555
Epoch 48: Val Loss 2.55554
Epoch 49: Val Loss 2.35032
Epoch 50: Val Loss 2.16889
Epoch 51: Val Loss 2.01451
Epoch 52: Val Loss 1.88332
Epoch 53: Val Loss 1.76962
Epoch 54: Val Loss 1.66859
Epoch 55: Val Loss 1.57697
Epoch 56: Val Loss 1.49201
Epoch 57: Val Loss 1.41295
Epoch 58: Val Loss 1.33981
Epoch 59: Val Loss 1.27427
Epoch 60: Val Loss 1.21382
Epoch 61: Val Loss 1.15894
Epoch 62: Val Loss 1.10879
Epoch 63: Val Loss 1.06387
Epoch 64: Val Loss 1.02325
Epoch 65: Val Loss 0.98608
Epoch 66: Val Loss 0.95144
Epoch 67: Val Loss 0.92015
Epoch 68: Val Loss 0.89175
Epoch 69: Val Loss 0.86549
Epoch 70: Val Loss 0.84137
Epoch 71: Val Loss 0.81985
Epoch 72: Val Loss 0.80013
Epoch 73: Val Loss 0.78222
Epoch 74: Val Loss 0.76598
Epoch 75: Val Loss 0.75092
Epoch 76: Val Loss 0.73683
Epoch 77: Val Loss 0.72416
Epoch 78: Val Loss 0.71229
Epoch 79: Val Loss 0.70163
Epoch 80: Val Loss 0.69225
Epoch 81: Val Loss 0.68357
Epoch 82: Val Loss 0.67569
Epoch 83: Val Loss 0.66871
Epoch 84: Val Loss 0.66237
Epoch 85: Val Loss 0.65643
Epoch 86: Val Loss 0.65152
Epoch 87: Val Loss 0.64698
Epoch 88: Val Loss 0.64236
Epoch 89: Val Loss 0.63810
Epoch 90: Val Loss 0.63413
Epoch 91: Val Loss 0.63099
Epoch 92: Val Loss 0.62827
Epoch 93: Val Loss 0.62584
Epoch 94: Val Loss 0.62393
Epoch 95: Val Loss 0.62221
Epoch 96: Val Loss 0.62035
Epoch 97: Val Loss 0.61844
Epoch 98: Val Loss 0.61699
Epoch 99: Val Loss 0.61456
{'MSE - mean': 0.6311791118627574, 'MSE - std': 0.05378320681730496, 'R2 - mean': 0.054633654916119975, 'R2 - std': 0.07075353529340668} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 0.6311791118627574 and parameters: {'dim': 128, 'depth': 2, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 235.81944
Epoch 1: Val Loss 235.75212
Epoch 2: Val Loss 235.68509
Epoch 3: Val Loss 235.61819
Epoch 4: Val Loss 235.55199
Epoch 5: Val Loss 235.48592
Epoch 6: Val Loss 235.41975
Epoch 7: Val Loss 235.35327
Epoch 8: Val Loss 235.28674
Epoch 9: Val Loss 235.22040
Epoch 10: Val Loss 235.15427
Epoch 11: Val Loss 235.08794
Epoch 12: Val Loss 235.02180
Epoch 13: Val Loss 234.95581
Epoch 14: Val Loss 234.88951
Epoch 15: Val Loss 234.82335
Epoch 16: Val Loss 234.75630
Epoch 17: Val Loss 234.68881
Epoch 18: Val Loss 234.62061
Epoch 19: Val Loss 234.55270
Epoch 20: Val Loss 234.48407
Epoch 21: Val Loss 234.41406
Epoch 22: Val Loss 234.34431
Epoch 23: Val Loss 234.27441
Epoch 24: Val Loss 234.20265
Epoch 25: Val Loss 234.13074
Epoch 26: Val Loss 234.05769
Epoch 27: Val Loss 233.98314
Epoch 28: Val Loss 233.90703
Epoch 29: Val Loss 233.82942
Epoch 30: Val Loss 233.75034
Epoch 31: Val Loss 233.67014
Epoch 32: Val Loss 233.58791
Epoch 33: Val Loss 233.50307
Epoch 34: Val Loss 233.41615
Epoch 35: Val Loss 233.32729
Epoch 36: Val Loss 233.23485
Epoch 37: Val Loss 233.14038
Epoch 38: Val Loss 233.04559
Epoch 39: Val Loss 232.94861
Epoch 40: Val Loss 232.84898
Epoch 41: Val Loss 232.74626
Epoch 42: Val Loss 232.64113
Epoch 43: Val Loss 232.53462
Epoch 44: Val Loss 232.42497
Epoch 45: Val Loss 232.31128
Epoch 46: Val Loss 232.19534
Epoch 47: Val Loss 232.07613
Epoch 48: Val Loss 231.95309
Epoch 49: Val Loss 231.82855
Epoch 50: Val Loss 231.70116
Epoch 51: Val Loss 231.57149
Epoch 52: Val Loss 231.43901
Epoch 53: Val Loss 231.30267
Epoch 54: Val Loss 231.16010
Epoch 55: Val Loss 231.01158
Epoch 56: Val Loss 230.85999
Epoch 57: Val Loss 230.70412
Epoch 58: Val Loss 230.54457
Epoch 59: Val Loss 230.38148
Epoch 60: Val Loss 230.21478
Epoch 61: Val Loss 230.04506
Epoch 62: Val Loss 229.87210
Epoch 63: Val Loss 229.69545
Epoch 64: Val Loss 229.51407
Epoch 65: Val Loss 229.32648
Epoch 66: Val Loss 229.13402
Epoch 67: Val Loss 228.93736
Epoch 68: Val Loss 228.73672
Epoch 69: Val Loss 228.52966
Epoch 70: Val Loss 228.31873
Epoch 71: Val Loss 228.10240
Epoch 72: Val Loss 227.88206
Epoch 73: Val Loss 227.65762
Epoch 74: Val Loss 227.42674
Epoch 75: Val Loss 227.18942
Epoch 76: Val Loss 226.94864
Epoch 77: Val Loss 226.70209
Epoch 78: Val Loss 226.45340
Epoch 79: Val Loss 226.20120
Epoch 80: Val Loss 225.94423
Epoch 81: Val Loss 225.68349
Epoch 82: Val Loss 225.41635
Epoch 83: Val Loss 225.14485
Epoch 84: Val Loss 224.86894
Epoch 85: Val Loss 224.58688
Epoch 86: Val Loss 224.29906
Epoch 87: Val Loss 224.00418
Epoch 88: Val Loss 223.70410
Epoch 89: Val Loss 223.39709
Epoch 90: Val Loss 223.08360
Epoch 91: Val Loss 222.76343
Epoch 92: Val Loss 222.43736
Epoch 93: Val Loss 222.10396
Epoch 94: Val Loss 221.76398
Epoch 95: Val Loss 221.41850
Epoch 96: Val Loss 221.06598
Epoch 97: Val Loss 220.70808
Epoch 98: Val Loss 220.34480
Epoch 99: Val Loss 219.97575
{'MSE - mean': 219.97575469034203, 'MSE - std': 0.0, 'R2 - mean': -399.28314075705623, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 234.48219
Epoch 1: Val Loss 234.38585
Epoch 2: Val Loss 234.28860
Epoch 3: Val Loss 234.18999
Epoch 4: Val Loss 234.09074
Epoch 5: Val Loss 233.99034
Epoch 6: Val Loss 233.88936
Epoch 7: Val Loss 233.78760
Epoch 8: Val Loss 233.68488
Epoch 9: Val Loss 233.58022
Epoch 10: Val Loss 233.47464
Epoch 11: Val Loss 233.36800
Epoch 12: Val Loss 233.25938
Epoch 13: Val Loss 233.14865
Epoch 14: Val Loss 233.03619
Epoch 15: Val Loss 232.92281
Epoch 16: Val Loss 232.80795
Epoch 17: Val Loss 232.69040
Epoch 18: Val Loss 232.56955
Epoch 19: Val Loss 232.44679
Epoch 20: Val Loss 232.32112
Epoch 21: Val Loss 232.19231
Epoch 22: Val Loss 232.06113
Epoch 23: Val Loss 231.92696
Epoch 24: Val Loss 231.79045
Epoch 25: Val Loss 231.65105
Epoch 26: Val Loss 231.50975
Epoch 27: Val Loss 231.36578
Epoch 28: Val Loss 231.21910
Epoch 29: Val Loss 231.06924
Epoch 30: Val Loss 230.91615
Epoch 31: Val Loss 230.75961
Epoch 32: Val Loss 230.60075
Epoch 33: Val Loss 230.43925
Epoch 34: Val Loss 230.27461
Epoch 35: Val Loss 230.10707
Epoch 36: Val Loss 229.93637
Epoch 37: Val Loss 229.76268
Epoch 38: Val Loss 229.58606
Epoch 39: Val Loss 229.40555
Epoch 40: Val Loss 229.22156
Epoch 41: Val Loss 229.03377
Epoch 42: Val Loss 228.84215
Epoch 43: Val Loss 228.64775
Epoch 44: Val Loss 228.44870
Epoch 45: Val Loss 228.24615
Epoch 46: Val Loss 228.04054
Epoch 47: Val Loss 227.83022
Epoch 48: Val Loss 227.61629
Epoch 49: Val Loss 227.39827
Epoch 50: Val Loss 227.17615
Epoch 51: Val Loss 226.94977
Epoch 52: Val Loss 226.71799
Epoch 53: Val Loss 226.48163
Epoch 54: Val Loss 226.24075
Epoch 55: Val Loss 225.99516
Epoch 56: Val Loss 225.74544
Epoch 57: Val Loss 225.49094
Epoch 58: Val Loss 225.23146
Epoch 59: Val Loss 224.96754
Epoch 60: Val Loss 224.69934
Epoch 61: Val Loss 224.42615
Epoch 62: Val Loss 224.14667
Epoch 63: Val Loss 223.86221
Epoch 64: Val Loss 223.57088
Epoch 65: Val Loss 223.27412
Epoch 66: Val Loss 222.96994
Epoch 67: Val Loss 222.66034
Epoch 68: Val Loss 222.34476
Epoch 69: Val Loss 222.02411
Epoch 70: Val Loss 221.69913
Epoch 71: Val Loss 221.36684
Epoch 72: Val Loss 221.02925
Epoch 73: Val Loss 220.68723
Epoch 74: Val Loss 220.33705
Epoch 75: Val Loss 219.98137
Epoch 76: Val Loss 219.61775
Epoch 77: Val Loss 219.24869
Epoch 78: Val Loss 218.87169
Epoch 79: Val Loss 218.48654
Epoch 80: Val Loss 218.09506
Epoch 81: Val Loss 217.69435
Epoch 82: Val Loss 217.28841
Epoch 83: Val Loss 216.87640
Epoch 84: Val Loss 216.45758
Epoch 85: Val Loss 216.03166
Epoch 86: Val Loss 215.59869
Epoch 87: Val Loss 215.15974
Epoch 88: Val Loss 214.71251
Epoch 89: Val Loss 214.25655
Epoch 90: Val Loss 213.79408
Epoch 91: Val Loss 213.32446
Epoch 92: Val Loss 212.84470
Epoch 93: Val Loss 212.35709
Epoch 94: Val Loss 211.86177
Epoch 95: Val Loss 211.36061
Epoch 96: Val Loss 210.85123
Epoch 97: Val Loss 210.33202
Epoch 98: Val Loss 209.80519
Epoch 99: Val Loss 209.26973
{'MSE - mean': 214.62274242702404, 'MSE - std': 5.353012263318007, 'R2 - mean': -340.8368489682789, 'R2 - std': 58.44629178877736} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 233.12708
Epoch 1: Val Loss 232.98843
Epoch 2: Val Loss 232.85022
Epoch 3: Val Loss 232.71228
Epoch 4: Val Loss 232.57509
Epoch 5: Val Loss 232.43840
Epoch 6: Val Loss 232.30194
Epoch 7: Val Loss 232.16508
Epoch 8: Val Loss 232.02818
Epoch 9: Val Loss 231.89081
Epoch 10: Val Loss 231.75291
Epoch 11: Val Loss 231.61497
Epoch 12: Val Loss 231.47607
Epoch 13: Val Loss 231.33675
Epoch 14: Val Loss 231.19649
Epoch 15: Val Loss 231.05620
Epoch 16: Val Loss 230.91547
Epoch 17: Val Loss 230.77432
Epoch 18: Val Loss 230.63205
Epoch 19: Val Loss 230.48929
Epoch 20: Val Loss 230.34621
Epoch 21: Val Loss 230.20244
Epoch 22: Val Loss 230.05820
Epoch 23: Val Loss 229.91280
Epoch 24: Val Loss 229.76575
Epoch 25: Val Loss 229.61873
Epoch 26: Val Loss 229.47173
Epoch 27: Val Loss 229.32391
Epoch 28: Val Loss 229.17502
Epoch 29: Val Loss 229.02495
Epoch 30: Val Loss 228.87498
Epoch 31: Val Loss 228.72211
Epoch 32: Val Loss 228.56851
Epoch 33: Val Loss 228.41255
Epoch 34: Val Loss 228.25520
Epoch 35: Val Loss 228.09735
Epoch 36: Val Loss 227.93787
Epoch 37: Val Loss 227.77765
Epoch 38: Val Loss 227.61566
Epoch 39: Val Loss 227.45274
Epoch 40: Val Loss 227.28871
Epoch 41: Val Loss 227.12274
Epoch 42: Val Loss 226.95457
Epoch 43: Val Loss 226.78465
Epoch 44: Val Loss 226.61205
Epoch 45: Val Loss 226.43816
Epoch 46: Val Loss 226.26250
Epoch 47: Val Loss 226.08548
Epoch 48: Val Loss 225.90640
Epoch 49: Val Loss 225.72560
Epoch 50: Val Loss 225.54308
Epoch 51: Val Loss 225.35742
Epoch 52: Val Loss 225.17003
Epoch 53: Val Loss 224.97903
Epoch 54: Val Loss 224.78610
Epoch 55: Val Loss 224.59059
Epoch 56: Val Loss 224.39313
Epoch 57: Val Loss 224.19345
Epoch 58: Val Loss 223.99210
Epoch 59: Val Loss 223.78830
Epoch 60: Val Loss 223.58202
Epoch 61: Val Loss 223.37354
Epoch 62: Val Loss 223.16304
Epoch 63: Val Loss 222.94989
Epoch 64: Val Loss 222.73376
Epoch 65: Val Loss 222.51466
Epoch 66: Val Loss 222.29353
Epoch 67: Val Loss 222.06972
Epoch 68: Val Loss 221.84323
Epoch 69: Val Loss 221.61514
Epoch 70: Val Loss 221.38344
Epoch 71: Val Loss 221.14944
Epoch 72: Val Loss 220.91350
Epoch 73: Val Loss 220.67393
Epoch 74: Val Loss 220.43159
Epoch 75: Val Loss 220.18636
Epoch 76: Val Loss 219.93895
Epoch 77: Val Loss 219.68817
Epoch 78: Val Loss 219.43413
Epoch 79: Val Loss 219.17740
Epoch 80: Val Loss 218.91667
Epoch 81: Val Loss 218.65088
Epoch 82: Val Loss 218.38260
Epoch 83: Val Loss 218.11101
Epoch 84: Val Loss 217.83614
Epoch 85: Val Loss 217.55688
Epoch 86: Val Loss 217.27367
Epoch 87: Val Loss 216.98766
Epoch 88: Val Loss 216.69801
Epoch 89: Val Loss 216.40454
Epoch 90: Val Loss 216.10750
Epoch 91: Val Loss 215.80498
Epoch 92: Val Loss 215.49841
Epoch 93: Val Loss 215.18875
Epoch 94: Val Loss 214.87373
Epoch 95: Val Loss 214.55495
Epoch 96: Val Loss 214.22989
Epoch 97: Val Loss 213.90079
Epoch 98: Val Loss 213.56378
Epoch 99: Val Loss 213.22316
{'MSE - mean': 214.15621563578438, 'MSE - std': 4.420232424663213, 'R2 - mean': -330.05303615957513, 'R2 - std': 50.098841503161815} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.77287
Epoch 1: Val Loss 220.66321
Epoch 2: Val Loss 220.55226
Epoch 3: Val Loss 220.43948
Epoch 4: Val Loss 220.32504
Epoch 5: Val Loss 220.20940
Epoch 6: Val Loss 220.09209
Epoch 7: Val Loss 219.97269
Epoch 8: Val Loss 219.85219
Epoch 9: Val Loss 219.73097
Epoch 10: Val Loss 219.60783
Epoch 11: Val Loss 219.48323
Epoch 12: Val Loss 219.35594
Epoch 13: Val Loss 219.22534
Epoch 14: Val Loss 219.09283
Epoch 15: Val Loss 218.95746
Epoch 16: Val Loss 218.81952
Epoch 17: Val Loss 218.68010
Epoch 18: Val Loss 218.53786
Epoch 19: Val Loss 218.39258
Epoch 20: Val Loss 218.24380
Epoch 21: Val Loss 218.09126
Epoch 22: Val Loss 217.93709
Epoch 23: Val Loss 217.78044
Epoch 24: Val Loss 217.62000
Epoch 25: Val Loss 217.45650
Epoch 26: Val Loss 217.28795
Epoch 27: Val Loss 217.11639
Epoch 28: Val Loss 216.94061
Epoch 29: Val Loss 216.76100
Epoch 30: Val Loss 216.57744
Epoch 31: Val Loss 216.38942
Epoch 32: Val Loss 216.19684
Epoch 33: Val Loss 215.99915
Epoch 34: Val Loss 215.79703
Epoch 35: Val Loss 215.58966
Epoch 36: Val Loss 215.37723
Epoch 37: Val Loss 215.16103
Epoch 38: Val Loss 214.93994
Epoch 39: Val Loss 214.71277
Epoch 40: Val Loss 214.48061
Epoch 41: Val Loss 214.24358
Epoch 42: Val Loss 214.00060
Epoch 43: Val Loss 213.75217
Epoch 44: Val Loss 213.49840
Epoch 45: Val Loss 213.23914
Epoch 46: Val Loss 212.97302
Epoch 47: Val Loss 212.70247
Epoch 48: Val Loss 212.42590
Epoch 49: Val Loss 212.14441
Epoch 50: Val Loss 211.85689
Epoch 51: Val Loss 211.56348
Epoch 52: Val Loss 211.26530
Epoch 53: Val Loss 210.95978
Epoch 54: Val Loss 210.64688
Epoch 55: Val Loss 210.32877
Epoch 56: Val Loss 210.00349
Epoch 57: Val Loss 209.67152
Epoch 58: Val Loss 209.33101
Epoch 59: Val Loss 208.98238
Epoch 60: Val Loss 208.62617
Epoch 61: Val Loss 208.26361
Epoch 62: Val Loss 207.89290
Epoch 63: Val Loss 207.51402
Epoch 64: Val Loss 207.12698
Epoch 65: Val Loss 206.73021
Epoch 66: Val Loss 206.32381
Epoch 67: Val Loss 205.90999
Epoch 68: Val Loss 205.48895
Epoch 69: Val Loss 205.05537
Epoch 70: Val Loss 204.61351
Epoch 71: Val Loss 204.16238
Epoch 72: Val Loss 203.70322
Epoch 73: Val Loss 203.23404
Epoch 74: Val Loss 202.75604
Epoch 75: Val Loss 202.26723
Epoch 76: Val Loss 201.77150
Epoch 77: Val Loss 201.26724
Epoch 78: Val Loss 200.75288
Epoch 79: Val Loss 200.22778
Epoch 80: Val Loss 199.69131
Epoch 81: Val Loss 199.14490
Epoch 82: Val Loss 198.58980
Epoch 83: Val Loss 198.02075
Epoch 84: Val Loss 197.44234
Epoch 85: Val Loss 196.85361
Epoch 86: Val Loss 196.25475
Epoch 87: Val Loss 195.64629
Epoch 88: Val Loss 195.02716
Epoch 89: Val Loss 194.39600
Epoch 90: Val Loss 193.75204
Epoch 91: Val Loss 193.09798
Epoch 92: Val Loss 192.43385
Epoch 93: Val Loss 191.76025
Epoch 94: Val Loss 191.07414
Epoch 95: Val Loss 190.37749
Epoch 96: Val Loss 189.66891
Epoch 97: Val Loss 188.94971
Epoch 98: Val Loss 188.21840
Epoch 99: Val Loss 187.47543
{'MSE - mean': 207.48602259516568, 'MSE - std': 12.170795644220915, 'R2 - mean': -307.06802815484673, 'R2 - std': 58.88422725061} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.78949
Epoch 1: Val Loss 222.72243
Epoch 2: Val Loss 222.65634
Epoch 3: Val Loss 222.59155
Epoch 4: Val Loss 222.52753
Epoch 5: Val Loss 222.46423
Epoch 6: Val Loss 222.40199
Epoch 7: Val Loss 222.33990
Epoch 8: Val Loss 222.27934
Epoch 9: Val Loss 222.22174
Epoch 10: Val Loss 222.16536
Epoch 11: Val Loss 222.11005
Epoch 12: Val Loss 222.05542
Epoch 13: Val Loss 222.00105
Epoch 14: Val Loss 221.94765
Epoch 15: Val Loss 221.89532
Epoch 16: Val Loss 221.84357
Epoch 17: Val Loss 221.79160
Epoch 18: Val Loss 221.74049
Epoch 19: Val Loss 221.68956
Epoch 20: Val Loss 221.63925
Epoch 21: Val Loss 221.58878
Epoch 22: Val Loss 221.53845
Epoch 23: Val Loss 221.48817
Epoch 24: Val Loss 221.43729
Epoch 25: Val Loss 221.38593
Epoch 26: Val Loss 221.33492
Epoch 27: Val Loss 221.28410
Epoch 28: Val Loss 221.23291
Epoch 29: Val Loss 221.18141
Epoch 30: Val Loss 221.12906
Epoch 31: Val Loss 221.07660
Epoch 32: Val Loss 221.02327
Epoch 33: Val Loss 220.96931
Epoch 34: Val Loss 220.91434
Epoch 35: Val Loss 220.85843
Epoch 36: Val Loss 220.80156
Epoch 37: Val Loss 220.74365
Epoch 38: Val Loss 220.68356
Epoch 39: Val Loss 220.62195
Epoch 40: Val Loss 220.55800
Epoch 41: Val Loss 220.49194
Epoch 42: Val Loss 220.42471
Epoch 43: Val Loss 220.35521
Epoch 44: Val Loss 220.28430
Epoch 45: Val Loss 220.21022
Epoch 46: Val Loss 220.13365
Epoch 47: Val Loss 220.05556
Epoch 48: Val Loss 219.97560
Epoch 49: Val Loss 219.89255
Epoch 50: Val Loss 219.80722
Epoch 51: Val Loss 219.71838
Epoch 52: Val Loss 219.62744
Epoch 53: Val Loss 219.53299
Epoch 54: Val Loss 219.43643
Epoch 55: Val Loss 219.33562
Epoch 56: Val Loss 219.23187
Epoch 57: Val Loss 219.12469
Epoch 58: Val Loss 219.01602
Epoch 59: Val Loss 218.90359
Epoch 60: Val Loss 218.78851
Epoch 61: Val Loss 218.67050
Epoch 62: Val Loss 218.54951
Epoch 63: Val Loss 218.42516
Epoch 64: Val Loss 218.29596
Epoch 65: Val Loss 218.16133
Epoch 66: Val Loss 218.02139
Epoch 67: Val Loss 217.87587
Epoch 68: Val Loss 217.72595
Epoch 69: Val Loss 217.57173
Epoch 70: Val Loss 217.41240
Epoch 71: Val Loss 217.24771
Epoch 72: Val Loss 217.07584
Epoch 73: Val Loss 216.90019
Epoch 74: Val Loss 216.72038
Epoch 75: Val Loss 216.53290
Epoch 76: Val Loss 216.34053
Epoch 77: Val Loss 216.14157
Epoch 78: Val Loss 215.93709
Epoch 79: Val Loss 215.72719
Epoch 80: Val Loss 215.51167
Epoch 81: Val Loss 215.28922
Epoch 82: Val Loss 215.05943
Epoch 83: Val Loss 214.82417
Epoch 84: Val Loss 214.58322
Epoch 85: Val Loss 214.33607
Epoch 86: Val Loss 214.08215
Epoch 87: Val Loss 213.82301
Epoch 88: Val Loss 213.55835
Epoch 89: Val Loss 213.28871
Epoch 90: Val Loss 213.01161
Epoch 91: Val Loss 212.72745
Epoch 92: Val Loss 212.43629
Epoch 93: Val Loss 212.13705
Epoch 94: Val Loss 211.82782
Epoch 95: Val Loss 211.51134
Epoch 96: Val Loss 211.18686
Epoch 97: Val Loss 210.85362
Epoch 98: Val Loss 210.51131
Epoch 99: Val Loss 210.16199
{'MSE - mean': 208.02121632168556, 'MSE - std': 10.938388482328387, 'R2 - mean': -315.20953192247674, 'R2 - std': 55.12729006140718} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 208.02121632168556 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.99971
Epoch 1: Val Loss 232.14935
Epoch 2: Val Loss 231.29825
Epoch 3: Val Loss 230.40941
Epoch 4: Val Loss 229.48380
Epoch 5: Val Loss 228.48373
Epoch 6: Val Loss 227.37631
Epoch 7: Val Loss 226.14406
Epoch 8: Val Loss 224.78119
Epoch 9: Val Loss 223.26805
Epoch 10: Val Loss 221.56297
Epoch 11: Val Loss 219.62895
Epoch 12: Val Loss 217.42711
Epoch 13: Val Loss 214.93713
Epoch 14: Val Loss 212.10951
Epoch 15: Val Loss 208.90808
Epoch 16: Val Loss 205.28633
Epoch 17: Val Loss 201.18871
Epoch 18: Val Loss 196.58633
Epoch 19: Val Loss 191.40968
Epoch 20: Val Loss 185.57645
Epoch 21: Val Loss 179.04819
Epoch 22: Val Loss 171.78401
Epoch 23: Val Loss 163.76826
Epoch 24: Val Loss 154.96169
Epoch 25: Val Loss 145.35106
Epoch 26: Val Loss 134.96251
Epoch 27: Val Loss 123.83231
Epoch 28: Val Loss 112.08234
Epoch 29: Val Loss 99.88987
Epoch 30: Val Loss 87.43916
Epoch 31: Val Loss 74.96412
Epoch 32: Val Loss 62.73992
Epoch 33: Val Loss 51.10118
Epoch 34: Val Loss 40.41394
Epoch 35: Val Loss 30.96746
Epoch 36: Val Loss 23.04454
Epoch 37: Val Loss 16.75307
Epoch 38: Val Loss 12.13579
Epoch 39: Val Loss 8.99847
Epoch 40: Val Loss 7.08414
Epoch 41: Val Loss 5.99262
Epoch 42: Val Loss 5.35883
Epoch 43: Val Loss 4.93920
Epoch 44: Val Loss 4.58510
Epoch 45: Val Loss 4.24602
Epoch 46: Val Loss 3.92081
Epoch 47: Val Loss 3.62533
Epoch 48: Val Loss 3.36446
Epoch 49: Val Loss 3.14261
Epoch 50: Val Loss 2.95222
Epoch 51: Val Loss 2.78029
Epoch 52: Val Loss 2.62051
Epoch 53: Val Loss 2.46017
Epoch 54: Val Loss 2.31042
Epoch 55: Val Loss 2.16960
Epoch 56: Val Loss 2.04002
Epoch 57: Val Loss 1.92215
Epoch 58: Val Loss 1.81389
Epoch 59: Val Loss 1.71435
Epoch 60: Val Loss 1.62496
Epoch 61: Val Loss 1.54535
Epoch 62: Val Loss 1.46869
Epoch 63: Val Loss 1.40080
Epoch 64: Val Loss 1.33791
Epoch 65: Val Loss 1.28028
Epoch 66: Val Loss 1.22709
Epoch 67: Val Loss 1.17857
Epoch 68: Val Loss 1.13338
Epoch 69: Val Loss 1.09138
Epoch 70: Val Loss 1.05282
Epoch 71: Val Loss 1.01652
Epoch 72: Val Loss 0.98279
Epoch 73: Val Loss 0.95243
Epoch 74: Val Loss 0.92361
Epoch 75: Val Loss 0.89624
Epoch 76: Val Loss 0.87126
Epoch 77: Val Loss 0.84747
Epoch 78: Val Loss 0.82584
Epoch 79: Val Loss 0.80482
Epoch 80: Val Loss 0.78555
Epoch 81: Val Loss 0.76830
Epoch 82: Val Loss 0.75216
Epoch 83: Val Loss 0.73749
Epoch 84: Val Loss 0.72341
Epoch 85: Val Loss 0.71016
Epoch 86: Val Loss 0.69706
Epoch 87: Val Loss 0.68560
Epoch 88: Val Loss 0.67448
Epoch 89: Val Loss 0.66441
Epoch 90: Val Loss 0.65543
Epoch 91: Val Loss 0.64623
Epoch 92: Val Loss 0.63804
Epoch 93: Val Loss 0.63069
Epoch 94: Val Loss 0.62272
Epoch 95: Val Loss 0.61525
Epoch 96: Val Loss 0.60905
Epoch 97: Val Loss 0.60264
Epoch 98: Val Loss 0.59719
Epoch 99: Val Loss 0.59184
{'MSE - mean': 0.5918358406296538, 'MSE - std': 0.0, 'R2 - mean': -0.07694554535482867, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.46439
Epoch 1: Val Loss 222.50735
Epoch 2: Val Loss 221.53926
Epoch 3: Val Loss 220.54060
Epoch 4: Val Loss 219.48729
Epoch 5: Val Loss 218.36490
Epoch 6: Val Loss 217.16127
Epoch 7: Val Loss 215.83290
Epoch 8: Val Loss 214.35730
Epoch 9: Val Loss 212.70303
Epoch 10: Val Loss 210.83029
Epoch 11: Val Loss 208.70848
Epoch 12: Val Loss 206.31328
Epoch 13: Val Loss 203.59789
Epoch 14: Val Loss 200.52882
Epoch 15: Val Loss 197.04120
Epoch 16: Val Loss 193.08292
Epoch 17: Val Loss 188.55359
Epoch 18: Val Loss 183.36160
Epoch 19: Val Loss 177.42639
Epoch 20: Val Loss 170.67418
Epoch 21: Val Loss 162.98689
Epoch 22: Val Loss 154.30750
Epoch 23: Val Loss 144.60898
Epoch 24: Val Loss 133.86844
Epoch 25: Val Loss 122.11153
Epoch 26: Val Loss 109.50270
Epoch 27: Val Loss 96.24956
Epoch 28: Val Loss 82.66636
Epoch 29: Val Loss 69.05161
Epoch 30: Val Loss 55.75639
Epoch 31: Val Loss 43.21812
Epoch 32: Val Loss 31.94788
Epoch 33: Val Loss 22.38990
Epoch 34: Val Loss 14.84407
Epoch 35: Val Loss 9.43667
Epoch 36: Val Loss 5.98759
Epoch 37: Val Loss 4.12663
Epoch 38: Val Loss 3.30017
Epoch 39: Val Loss 2.98679
Epoch 40: Val Loss 2.82026
Epoch 41: Val Loss 2.62277
Epoch 42: Val Loss 2.37496
Epoch 43: Val Loss 2.13416
Epoch 44: Val Loss 1.92946
Epoch 45: Val Loss 1.77761
Epoch 46: Val Loss 1.66957
Epoch 47: Val Loss 1.58445
Epoch 48: Val Loss 1.51024
Epoch 49: Val Loss 1.43666
Epoch 50: Val Loss 1.36590
Epoch 51: Val Loss 1.29339
Epoch 52: Val Loss 1.22452
Epoch 53: Val Loss 1.16317
Epoch 54: Val Loss 1.10892
Epoch 55: Val Loss 1.06237
Epoch 56: Val Loss 1.02094
Epoch 57: Val Loss 0.98458
Epoch 58: Val Loss 0.95259
Epoch 59: Val Loss 0.92432
Epoch 60: Val Loss 0.89840
Epoch 61: Val Loss 0.87533
Epoch 62: Val Loss 0.85452
Epoch 63: Val Loss 0.83547
Epoch 64: Val Loss 0.81729
Epoch 65: Val Loss 0.80046
Epoch 66: Val Loss 0.78391
Epoch 67: Val Loss 0.76839
Epoch 68: Val Loss 0.75490
Epoch 69: Val Loss 0.74266
Epoch 70: Val Loss 0.73101
Epoch 71: Val Loss 0.72059
Epoch 72: Val Loss 0.70994
Epoch 73: Val Loss 0.70157
Epoch 74: Val Loss 0.69384
Epoch 75: Val Loss 0.68525
Epoch 76: Val Loss 0.67829
Epoch 77: Val Loss 0.67101
Epoch 78: Val Loss 0.66463
Epoch 79: Val Loss 0.65866
Epoch 80: Val Loss 0.65362
Epoch 81: Val Loss 0.64915
Epoch 82: Val Loss 0.64546
Epoch 83: Val Loss 0.64187
Epoch 84: Val Loss 0.63775
Epoch 85: Val Loss 0.63430
Epoch 86: Val Loss 0.62983
Epoch 87: Val Loss 0.62624
Epoch 88: Val Loss 0.62185
Epoch 89: Val Loss 0.61887
Epoch 90: Val Loss 0.61627
Epoch 91: Val Loss 0.61379
Epoch 92: Val Loss 0.61186
Epoch 93: Val Loss 0.60980
Epoch 94: Val Loss 0.60852
Epoch 95: Val Loss 0.60765
Epoch 96: Val Loss 0.60622
Epoch 97: Val Loss 0.60463
Epoch 98: Val Loss 0.60272
Epoch 99: Val Loss 0.60164
{'MSE - mean': 0.5967380595521485, 'MSE - std': 0.0049022189224947055, 'R2 - mean': 0.054160204855498106, 'R2 - std': 0.13110575021032678} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.95392
Epoch 1: Val Loss 235.92886
Epoch 2: Val Loss 234.99756
Epoch 3: Val Loss 234.12788
Epoch 4: Val Loss 233.30249
Epoch 5: Val Loss 232.48834
Epoch 6: Val Loss 231.65939
Epoch 7: Val Loss 230.80312
Epoch 8: Val Loss 229.91728
Epoch 9: Val Loss 228.98524
Epoch 10: Val Loss 227.98946
Epoch 11: Val Loss 226.91440
Epoch 12: Val Loss 225.73486
Epoch 13: Val Loss 224.44466
Epoch 14: Val Loss 223.03398
Epoch 15: Val Loss 221.46883
Epoch 16: Val Loss 219.72644
Epoch 17: Val Loss 217.78545
Epoch 18: Val Loss 215.60869
Epoch 19: Val Loss 213.15829
Epoch 20: Val Loss 210.39381
Epoch 21: Val Loss 207.28098
Epoch 22: Val Loss 203.80789
Epoch 23: Val Loss 199.93347
Epoch 24: Val Loss 195.60062
Epoch 25: Val Loss 190.81728
Epoch 26: Val Loss 185.53163
Epoch 27: Val Loss 179.73810
Epoch 28: Val Loss 173.40193
Epoch 29: Val Loss 166.51747
Epoch 30: Val Loss 159.12741
Epoch 31: Val Loss 151.23401
Epoch 32: Val Loss 142.89780
Epoch 33: Val Loss 134.12776
Epoch 34: Val Loss 125.01865
Epoch 35: Val Loss 115.61715
Epoch 36: Val Loss 106.00552
Epoch 37: Val Loss 96.29300
Epoch 38: Val Loss 86.58006
Epoch 39: Val Loss 76.99277
Epoch 40: Val Loss 67.66144
Epoch 41: Val Loss 58.72040
Epoch 42: Val Loss 50.29736
Epoch 43: Val Loss 42.52261
Epoch 44: Val Loss 35.48669
Epoch 45: Val Loss 29.26274
Epoch 46: Val Loss 23.94470
Epoch 47: Val Loss 19.51336
Epoch 48: Val Loss 15.91541
Epoch 49: Val Loss 13.09885
Epoch 50: Val Loss 11.00198
Epoch 51: Val Loss 9.44885
Epoch 52: Val Loss 8.32746
Epoch 53: Val Loss 7.51757
Epoch 54: Val Loss 6.92115
Epoch 55: Val Loss 6.47714
Epoch 56: Val Loss 6.12798
Epoch 57: Val Loss 5.83334
Epoch 58: Val Loss 5.58301
Epoch 59: Val Loss 5.35185
Epoch 60: Val Loss 5.14686
Epoch 61: Val Loss 4.95246
Epoch 62: Val Loss 4.77514
Epoch 63: Val Loss 4.60825
Epoch 64: Val Loss 4.44937
Epoch 65: Val Loss 4.30089
Epoch 66: Val Loss 4.16201
Epoch 67: Val Loss 4.02507
Epoch 68: Val Loss 3.88411
Epoch 69: Val Loss 3.74858
Epoch 70: Val Loss 3.61582
Epoch 71: Val Loss 3.48748
Epoch 72: Val Loss 3.36220
Epoch 73: Val Loss 3.24242
Epoch 74: Val Loss 3.13199
Epoch 75: Val Loss 3.02417
Epoch 76: Val Loss 2.91861
Epoch 77: Val Loss 2.81591
Epoch 78: Val Loss 2.71198
Epoch 79: Val Loss 2.61356
Epoch 80: Val Loss 2.52133
Epoch 81: Val Loss 2.43311
Epoch 82: Val Loss 2.34955
Epoch 83: Val Loss 2.26742
Epoch 84: Val Loss 2.18782
Epoch 85: Val Loss 2.11336
Epoch 86: Val Loss 2.04304
Epoch 87: Val Loss 1.97280
Epoch 88: Val Loss 1.90621
Epoch 89: Val Loss 1.84147
Epoch 90: Val Loss 1.77933
Epoch 91: Val Loss 1.71984
Epoch 92: Val Loss 1.66234
Epoch 93: Val Loss 1.60947
Epoch 94: Val Loss 1.55567
Epoch 95: Val Loss 1.50331
Epoch 96: Val Loss 1.45377
Epoch 97: Val Loss 1.40695
Epoch 98: Val Loss 1.36014
Epoch 99: Val Loss 1.31497
{'MSE - mean': 0.8361478849492551, 'MSE - std': 0.3386002808035684, 'R2 - mean': -0.2667685250187719, 'R2 - std': 0.46631496274109924} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 239.81360
Epoch 1: Val Loss 239.14964
Epoch 2: Val Loss 238.43770
Epoch 3: Val Loss 237.64914
Epoch 4: Val Loss 236.77542
Epoch 5: Val Loss 235.75389
Epoch 6: Val Loss 234.61392
Epoch 7: Val Loss 233.34714
Epoch 8: Val Loss 231.92828
Epoch 9: Val Loss 230.34914
Epoch 10: Val Loss 228.61591
Epoch 11: Val Loss 226.68980
Epoch 12: Val Loss 224.55923
Epoch 13: Val Loss 222.21660
Epoch 14: Val Loss 219.62024
Epoch 15: Val Loss 216.74123
Epoch 16: Val Loss 213.53699
Epoch 17: Val Loss 209.97328
Epoch 18: Val Loss 206.02251
Epoch 19: Val Loss 201.66786
Epoch 20: Val Loss 196.88788
Epoch 21: Val Loss 191.63905
Epoch 22: Val Loss 185.88039
Epoch 23: Val Loss 179.59653
Epoch 24: Val Loss 172.77129
Epoch 25: Val Loss 165.36238
Epoch 26: Val Loss 157.38771
Epoch 27: Val Loss 148.87917
Epoch 28: Val Loss 139.86682
Epoch 29: Val Loss 130.34850
Epoch 30: Val Loss 120.41045
Epoch 31: Val Loss 110.12676
Epoch 32: Val Loss 99.66468
Epoch 33: Val Loss 89.13614
Epoch 34: Val Loss 78.71869
Epoch 35: Val Loss 68.58020
Epoch 36: Val Loss 58.88290
Epoch 37: Val Loss 49.81065
Epoch 38: Val Loss 41.53613
Epoch 39: Val Loss 34.18082
Epoch 40: Val Loss 27.87014
Epoch 41: Val Loss 22.66119
Epoch 42: Val Loss 18.50131
Epoch 43: Val Loss 15.32690
Epoch 44: Val Loss 13.00138
Epoch 45: Val Loss 11.33285
Epoch 46: Val Loss 10.15535
Epoch 47: Val Loss 9.31568
Epoch 48: Val Loss 8.67503
Epoch 49: Val Loss 8.16315
Epoch 50: Val Loss 7.71250
Epoch 51: Val Loss 7.30243
Epoch 52: Val Loss 6.91676
Epoch 53: Val Loss 6.55615
Epoch 54: Val Loss 6.22376
Epoch 55: Val Loss 5.91842
Epoch 56: Val Loss 5.63258
Epoch 57: Val Loss 5.36241
Epoch 58: Val Loss 5.10866
Epoch 59: Val Loss 4.86789
Epoch 60: Val Loss 4.63683
Epoch 61: Val Loss 4.41951
Epoch 62: Val Loss 4.21009
Epoch 63: Val Loss 4.01314
Epoch 64: Val Loss 3.82036
Epoch 65: Val Loss 3.63626
Epoch 66: Val Loss 3.46456
Epoch 67: Val Loss 3.30014
Epoch 68: Val Loss 3.14597
Epoch 69: Val Loss 3.00252
Epoch 70: Val Loss 2.86404
Epoch 71: Val Loss 2.73544
Epoch 72: Val Loss 2.61392
Epoch 73: Val Loss 2.49896
Epoch 74: Val Loss 2.38967
Epoch 75: Val Loss 2.28690
Epoch 76: Val Loss 2.18911
Epoch 77: Val Loss 2.09560
Epoch 78: Val Loss 2.00868
Epoch 79: Val Loss 1.92607
Epoch 80: Val Loss 1.84755
Epoch 81: Val Loss 1.77254
Epoch 82: Val Loss 1.70228
Epoch 83: Val Loss 1.63416
Epoch 84: Val Loss 1.57091
Epoch 85: Val Loss 1.50953
Epoch 86: Val Loss 1.45236
Epoch 87: Val Loss 1.39890
Epoch 88: Val Loss 1.34689
Epoch 89: Val Loss 1.29848
Epoch 90: Val Loss 1.25231
Epoch 91: Val Loss 1.20973
Epoch 92: Val Loss 1.16996
Epoch 93: Val Loss 1.13268
Epoch 94: Val Loss 1.09799
Epoch 95: Val Loss 1.06639
Epoch 96: Val Loss 1.03620
Epoch 97: Val Loss 1.00889
Epoch 98: Val Loss 0.98252
Epoch 99: Val Loss 0.95775
{'MSE - mean': 0.8665491186351557, 'MSE - std': 0.2979266984495779, 'R2 - mean': -0.2554645951441069, 'R2 - std': 0.4043149389387048} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 236.49757
Epoch 1: Val Loss 235.55423
Epoch 2: Val Loss 234.58453
Epoch 3: Val Loss 233.56293
Epoch 4: Val Loss 232.48279
Epoch 5: Val Loss 231.31918
Epoch 6: Val Loss 230.06595
Epoch 7: Val Loss 228.69624
Epoch 8: Val Loss 227.20876
Epoch 9: Val Loss 225.57343
Epoch 10: Val Loss 223.77368
Epoch 11: Val Loss 221.78206
Epoch 12: Val Loss 219.56653
Epoch 13: Val Loss 217.08708
Epoch 14: Val Loss 214.31223
Epoch 15: Val Loss 211.19804
Epoch 16: Val Loss 207.70274
Epoch 17: Val Loss 203.79836
Epoch 18: Val Loss 199.41640
Epoch 19: Val Loss 194.52309
Epoch 20: Val Loss 189.06046
Epoch 21: Val Loss 182.96628
Epoch 22: Val Loss 176.18648
Epoch 23: Val Loss 168.73740
Epoch 24: Val Loss 160.57785
Epoch 25: Val Loss 151.68152
Epoch 26: Val Loss 142.09395
Epoch 27: Val Loss 131.82321
Epoch 28: Val Loss 120.95397
Epoch 29: Val Loss 109.56697
Epoch 30: Val Loss 97.79604
Epoch 31: Val Loss 85.76370
Epoch 32: Val Loss 73.71396
Epoch 33: Val Loss 61.94462
Epoch 34: Val Loss 50.79149
Epoch 35: Val Loss 40.53885
Epoch 36: Val Loss 31.53160
Epoch 37: Val Loss 24.01083
Epoch 38: Val Loss 18.09679
Epoch 39: Val Loss 13.82053
Epoch 40: Val Loss 10.98651
Epoch 41: Val Loss 9.26924
Epoch 42: Val Loss 8.24934
Epoch 43: Val Loss 7.58017
Epoch 44: Val Loss 7.01249
Epoch 45: Val Loss 6.44900
Epoch 46: Val Loss 5.87131
Epoch 47: Val Loss 5.31575
Epoch 48: Val Loss 4.82261
Epoch 49: Val Loss 4.39675
Epoch 50: Val Loss 4.02896
Epoch 51: Val Loss 3.70365
Epoch 52: Val Loss 3.41303
Epoch 53: Val Loss 3.14142
Epoch 54: Val Loss 2.88985
Epoch 55: Val Loss 2.65972
Epoch 56: Val Loss 2.45156
Epoch 57: Val Loss 2.26134
Epoch 58: Val Loss 2.09035
Epoch 59: Val Loss 1.93146
Epoch 60: Val Loss 1.78880
Epoch 61: Val Loss 1.66042
Epoch 62: Val Loss 1.54247
Epoch 63: Val Loss 1.43543
Epoch 64: Val Loss 1.33954
Epoch 65: Val Loss 1.25312
Epoch 66: Val Loss 1.17456
Epoch 67: Val Loss 1.10433
Epoch 68: Val Loss 1.04142
Epoch 69: Val Loss 0.98530
Epoch 70: Val Loss 0.93651
Epoch 71: Val Loss 0.89170
Epoch 72: Val Loss 0.85192
Epoch 73: Val Loss 0.81690
Epoch 74: Val Loss 0.78565
Epoch 75: Val Loss 0.75953
Epoch 76: Val Loss 0.73577
Epoch 77: Val Loss 0.71490
Epoch 78: Val Loss 0.69698
Epoch 79: Val Loss 0.68149
Epoch 80: Val Loss 0.66733
Epoch 81: Val Loss 0.65514
Epoch 82: Val Loss 0.64493
Epoch 83: Val Loss 0.63600
Epoch 84: Val Loss 0.62777
Epoch 85: Val Loss 0.62003
Epoch 86: Val Loss 0.61333
Epoch 87: Val Loss 0.60681
Epoch 88: Val Loss 0.60129
Epoch 89: Val Loss 0.59657
Epoch 90: Val Loss 0.59263
Epoch 91: Val Loss 0.59029
Epoch 92: Val Loss 0.58792
Epoch 93: Val Loss 0.58658
Epoch 94: Val Loss 0.58555
Epoch 95: Val Loss 0.58481
Epoch 96: Val Loss 0.58469
Epoch 97: Val Loss 0.58460
Epoch 98: Val Loss 0.58364
Epoch 99: Val Loss 0.58278
{'MSE - mean': 0.809794420298816, 'MSE - std': 0.2896422573663041, 'R2 - mean': -0.197801407987778, 'R2 - std': 0.37957427269417265} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 0.809794420298816 and parameters: {'dim': 128, 'depth': 3, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.08604
Epoch 1: Val Loss 218.97003
Epoch 2: Val Loss 218.85414
Epoch 3: Val Loss 218.73784
Epoch 4: Val Loss 218.62144
Epoch 5: Val Loss 218.50513
Epoch 6: Val Loss 218.38803
Epoch 7: Val Loss 218.26993
Epoch 8: Val Loss 218.15143
Epoch 9: Val Loss 218.03316
Epoch 10: Val Loss 217.91493
Epoch 11: Val Loss 217.79620
Epoch 12: Val Loss 217.67737
Epoch 13: Val Loss 217.55762
Epoch 14: Val Loss 217.43723
Epoch 15: Val Loss 217.31677
Epoch 16: Val Loss 217.19629
Epoch 17: Val Loss 217.07515
Epoch 18: Val Loss 216.95499
Epoch 19: Val Loss 216.83475
Epoch 20: Val Loss 216.71393
Epoch 21: Val Loss 216.59203
Epoch 22: Val Loss 216.46811
Epoch 23: Val Loss 216.34436
Epoch 24: Val Loss 216.22037
Epoch 25: Val Loss 216.09554
Epoch 26: Val Loss 215.96979
Epoch 27: Val Loss 215.84378
Epoch 28: Val Loss 215.71684
Epoch 29: Val Loss 215.58868
Epoch 30: Val Loss 215.45955
Epoch 31: Val Loss 215.32718
Epoch 32: Val Loss 215.19255
Epoch 33: Val Loss 215.05637
Epoch 34: Val Loss 214.91968
Epoch 35: Val Loss 214.78217
Epoch 36: Val Loss 214.64192
Epoch 37: Val Loss 214.50081
Epoch 38: Val Loss 214.35895
Epoch 39: Val Loss 214.21651
Epoch 40: Val Loss 214.07271
Epoch 41: Val Loss 213.92566
Epoch 42: Val Loss 213.77734
Epoch 43: Val Loss 213.62775
Epoch 44: Val Loss 213.47707
Epoch 45: Val Loss 213.32370
Epoch 46: Val Loss 213.16875
Epoch 47: Val Loss 213.01152
Epoch 48: Val Loss 212.85321
Epoch 49: Val Loss 212.69164
Epoch 50: Val Loss 212.52687
Epoch 51: Val Loss 212.35994
Epoch 52: Val Loss 212.19051
Epoch 53: Val Loss 212.01973
Epoch 54: Val Loss 211.84691
Epoch 55: Val Loss 211.67081
Epoch 56: Val Loss 211.49136
Epoch 57: Val Loss 211.30920
Epoch 58: Val Loss 211.12592
Epoch 59: Val Loss 210.94037
Epoch 60: Val Loss 210.74886
Epoch 61: Val Loss 210.55382
Epoch 62: Val Loss 210.35498
Epoch 63: Val Loss 210.15349
Epoch 64: Val Loss 209.94916
Epoch 65: Val Loss 209.74095
Epoch 66: Val Loss 209.52846
Epoch 67: Val Loss 209.31163
Epoch 68: Val Loss 209.08990
Epoch 69: Val Loss 208.86513
Epoch 70: Val Loss 208.63515
Epoch 71: Val Loss 208.40086
Epoch 72: Val Loss 208.16328
Epoch 73: Val Loss 207.92232
Epoch 74: Val Loss 207.67905
Epoch 75: Val Loss 207.43100
Epoch 76: Val Loss 207.17891
Epoch 77: Val Loss 206.92360
Epoch 78: Val Loss 206.66446
Epoch 79: Val Loss 206.40120
Epoch 80: Val Loss 206.13254
Epoch 81: Val Loss 205.85939
Epoch 82: Val Loss 205.58275
Epoch 83: Val Loss 205.30078
Epoch 84: Val Loss 205.01390
Epoch 85: Val Loss 204.72351
Epoch 86: Val Loss 204.42976
Epoch 87: Val Loss 204.13177
Epoch 88: Val Loss 203.83055
Epoch 89: Val Loss 203.52280
Epoch 90: Val Loss 203.20964
Epoch 91: Val Loss 202.89207
Epoch 92: Val Loss 202.56882
Epoch 93: Val Loss 202.24104
Epoch 94: Val Loss 201.90820
Epoch 95: Val Loss 201.56996
Epoch 96: Val Loss 201.22731
Epoch 97: Val Loss 200.87833
Epoch 98: Val Loss 200.52335
Epoch 99: Val Loss 200.16226
{'MSE - mean': 200.16226940243973, 'MSE - std': 0.0, 'R2 - mean': -363.2291486634746, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 224.75838
Epoch 1: Val Loss 224.67209
Epoch 2: Val Loss 224.58583
Epoch 3: Val Loss 224.49936
Epoch 4: Val Loss 224.41273
Epoch 5: Val Loss 224.32608
Epoch 6: Val Loss 224.23883
Epoch 7: Val Loss 224.15149
Epoch 8: Val Loss 224.06378
Epoch 9: Val Loss 223.97546
Epoch 10: Val Loss 223.88698
Epoch 11: Val Loss 223.79799
Epoch 12: Val Loss 223.70898
Epoch 13: Val Loss 223.62001
Epoch 14: Val Loss 223.53041
Epoch 15: Val Loss 223.43996
Epoch 16: Val Loss 223.34938
Epoch 17: Val Loss 223.25818
Epoch 18: Val Loss 223.16652
Epoch 19: Val Loss 223.07442
Epoch 20: Val Loss 222.98166
Epoch 21: Val Loss 222.88799
Epoch 22: Val Loss 222.79333
Epoch 23: Val Loss 222.69768
Epoch 24: Val Loss 222.60078
Epoch 25: Val Loss 222.50293
Epoch 26: Val Loss 222.40411
Epoch 27: Val Loss 222.30441
Epoch 28: Val Loss 222.20363
Epoch 29: Val Loss 222.10182
Epoch 30: Val Loss 221.99876
Epoch 31: Val Loss 221.89465
Epoch 32: Val Loss 221.78940
Epoch 33: Val Loss 221.68291
Epoch 34: Val Loss 221.57448
Epoch 35: Val Loss 221.46460
Epoch 36: Val Loss 221.35376
Epoch 37: Val Loss 221.24152
Epoch 38: Val Loss 221.12718
Epoch 39: Val Loss 221.01146
Epoch 40: Val Loss 220.89394
Epoch 41: Val Loss 220.77452
Epoch 42: Val Loss 220.65332
Epoch 43: Val Loss 220.53053
Epoch 44: Val Loss 220.40587
Epoch 45: Val Loss 220.27846
Epoch 46: Val Loss 220.14958
Epoch 47: Val Loss 220.01891
Epoch 48: Val Loss 219.88672
Epoch 49: Val Loss 219.75253
Epoch 50: Val Loss 219.61636
Epoch 51: Val Loss 219.47729
Epoch 52: Val Loss 219.33603
Epoch 53: Val Loss 219.19147
Epoch 54: Val Loss 219.04437
Epoch 55: Val Loss 218.89523
Epoch 56: Val Loss 218.74342
Epoch 57: Val Loss 218.58916
Epoch 58: Val Loss 218.43211
Epoch 59: Val Loss 218.27184
Epoch 60: Val Loss 218.10922
Epoch 61: Val Loss 217.94370
Epoch 62: Val Loss 217.77539
Epoch 63: Val Loss 217.60471
Epoch 64: Val Loss 217.43074
Epoch 65: Val Loss 217.25424
Epoch 66: Val Loss 217.07442
Epoch 67: Val Loss 216.89160
Epoch 68: Val Loss 216.70491
Epoch 69: Val Loss 216.51439
Epoch 70: Val Loss 216.32104
Epoch 71: Val Loss 216.12503
Epoch 72: Val Loss 215.92538
Epoch 73: Val Loss 215.72238
Epoch 74: Val Loss 215.51558
Epoch 75: Val Loss 215.30434
Epoch 76: Val Loss 215.08894
Epoch 77: Val Loss 214.87022
Epoch 78: Val Loss 214.64772
Epoch 79: Val Loss 214.42133
Epoch 80: Val Loss 214.19133
Epoch 81: Val Loss 213.95737
Epoch 82: Val Loss 213.72005
Epoch 83: Val Loss 213.47723
Epoch 84: Val Loss 213.23013
Epoch 85: Val Loss 212.97971
Epoch 86: Val Loss 212.72502
Epoch 87: Val Loss 212.46616
Epoch 88: Val Loss 212.20386
Epoch 89: Val Loss 211.93739
Epoch 90: Val Loss 211.66614
Epoch 91: Val Loss 211.38991
Epoch 92: Val Loss 211.10899
Epoch 93: Val Loss 210.82239
Epoch 94: Val Loss 210.52956
Epoch 95: Val Loss 210.23145
Epoch 96: Val Loss 209.92842
Epoch 97: Val Loss 209.62202
Epoch 98: Val Loss 209.30923
Epoch 99: Val Loss 208.99211
{'MSE - mean': 204.57719672234697, 'MSE - std': 4.41492731990725, 'R2 - mean': -322.62188748240345, 'R2 - std': 40.607261181071124} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 227.28362
Epoch 1: Val Loss 227.17633
Epoch 2: Val Loss 227.06854
Epoch 3: Val Loss 226.95992
Epoch 4: Val Loss 226.85023
Epoch 5: Val Loss 226.73961
Epoch 6: Val Loss 226.62839
Epoch 7: Val Loss 226.51575
Epoch 8: Val Loss 226.40308
Epoch 9: Val Loss 226.28940
Epoch 10: Val Loss 226.17416
Epoch 11: Val Loss 226.05835
Epoch 12: Val Loss 225.94055
Epoch 13: Val Loss 225.82121
Epoch 14: Val Loss 225.70058
Epoch 15: Val Loss 225.57877
Epoch 16: Val Loss 225.45573
Epoch 17: Val Loss 225.33144
Epoch 18: Val Loss 225.20590
Epoch 19: Val Loss 225.07977
Epoch 20: Val Loss 224.95242
Epoch 21: Val Loss 224.82404
Epoch 22: Val Loss 224.69478
Epoch 23: Val Loss 224.56392
Epoch 24: Val Loss 224.43098
Epoch 25: Val Loss 224.29683
Epoch 26: Val Loss 224.16164
Epoch 27: Val Loss 224.02448
Epoch 28: Val Loss 223.88593
Epoch 29: Val Loss 223.74513
Epoch 30: Val Loss 223.60330
Epoch 31: Val Loss 223.45868
Epoch 32: Val Loss 223.31340
Epoch 33: Val Loss 223.16667
Epoch 34: Val Loss 223.01779
Epoch 35: Val Loss 222.86624
Epoch 36: Val Loss 222.71223
Epoch 37: Val Loss 222.55638
Epoch 38: Val Loss 222.39816
Epoch 39: Val Loss 222.23790
Epoch 40: Val Loss 222.07536
Epoch 41: Val Loss 221.91074
Epoch 42: Val Loss 221.74422
Epoch 43: Val Loss 221.57506
Epoch 44: Val Loss 221.40271
Epoch 45: Val Loss 221.22815
Epoch 46: Val Loss 221.05186
Epoch 47: Val Loss 220.87318
Epoch 48: Val Loss 220.69214
Epoch 49: Val Loss 220.50735
Epoch 50: Val Loss 220.32060
Epoch 51: Val Loss 220.12987
Epoch 52: Val Loss 219.93695
Epoch 53: Val Loss 219.74030
Epoch 54: Val Loss 219.54131
Epoch 55: Val Loss 219.33907
Epoch 56: Val Loss 219.13409
Epoch 57: Val Loss 218.92612
Epoch 58: Val Loss 218.71489
Epoch 59: Val Loss 218.50121
Epoch 60: Val Loss 218.28479
Epoch 61: Val Loss 218.06509
Epoch 62: Val Loss 217.84265
Epoch 63: Val Loss 217.61743
Epoch 64: Val Loss 217.38948
Epoch 65: Val Loss 217.15846
Epoch 66: Val Loss 216.92448
Epoch 67: Val Loss 216.68863
Epoch 68: Val Loss 216.44942
Epoch 69: Val Loss 216.20682
Epoch 70: Val Loss 215.96146
Epoch 71: Val Loss 215.71269
Epoch 72: Val Loss 215.46074
Epoch 73: Val Loss 215.20375
Epoch 74: Val Loss 214.94244
Epoch 75: Val Loss 214.67554
Epoch 76: Val Loss 214.40540
Epoch 77: Val Loss 214.13025
Epoch 78: Val Loss 213.85030
Epoch 79: Val Loss 213.56602
Epoch 80: Val Loss 213.27672
Epoch 81: Val Loss 212.98265
Epoch 82: Val Loss 212.68350
Epoch 83: Val Loss 212.37950
Epoch 84: Val Loss 212.07028
Epoch 85: Val Loss 211.75735
Epoch 86: Val Loss 211.43889
Epoch 87: Val Loss 211.11436
Epoch 88: Val Loss 210.78485
Epoch 89: Val Loss 210.45110
Epoch 90: Val Loss 210.11093
Epoch 91: Val Loss 209.76610
Epoch 92: Val Loss 209.41707
Epoch 93: Val Loss 209.06247
Epoch 94: Val Loss 208.70326
Epoch 95: Val Loss 208.33798
Epoch 96: Val Loss 207.96881
Epoch 97: Val Loss 207.59233
Epoch 98: Val Loss 207.21252
Epoch 99: Val Loss 206.82635
{'MSE - mean': 205.3269198988742, 'MSE - std': 3.7574670337280542, 'R2 - mean': -314.8148255601381, 'R2 - std': 34.94567506098555} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 222.83063
Epoch 1: Val Loss 222.67491
Epoch 2: Val Loss 222.51949
Epoch 3: Val Loss 222.36354
Epoch 4: Val Loss 222.20776
Epoch 5: Val Loss 222.05164
Epoch 6: Val Loss 221.89557
Epoch 7: Val Loss 221.73831
Epoch 8: Val Loss 221.57973
Epoch 9: Val Loss 221.42055
Epoch 10: Val Loss 221.26138
Epoch 11: Val Loss 221.10172
Epoch 12: Val Loss 220.94215
Epoch 13: Val Loss 220.78236
Epoch 14: Val Loss 220.62207
Epoch 15: Val Loss 220.46184
Epoch 16: Val Loss 220.30081
Epoch 17: Val Loss 220.13925
Epoch 18: Val Loss 219.97672
Epoch 19: Val Loss 219.81328
Epoch 20: Val Loss 219.64961
Epoch 21: Val Loss 219.48546
Epoch 22: Val Loss 219.31909
Epoch 23: Val Loss 219.15231
Epoch 24: Val Loss 218.98451
Epoch 25: Val Loss 218.81613
Epoch 26: Val Loss 218.64700
Epoch 27: Val Loss 218.47636
Epoch 28: Val Loss 218.30492
Epoch 29: Val Loss 218.13332
Epoch 30: Val Loss 217.96136
Epoch 31: Val Loss 217.78937
Epoch 32: Val Loss 217.61629
Epoch 33: Val Loss 217.44278
Epoch 34: Val Loss 217.26837
Epoch 35: Val Loss 217.09264
Epoch 36: Val Loss 216.91521
Epoch 37: Val Loss 216.73651
Epoch 38: Val Loss 216.55623
Epoch 39: Val Loss 216.37608
Epoch 40: Val Loss 216.19446
Epoch 41: Val Loss 216.00987
Epoch 42: Val Loss 215.82451
Epoch 43: Val Loss 215.63817
Epoch 44: Val Loss 215.45012
Epoch 45: Val Loss 215.26109
Epoch 46: Val Loss 215.07140
Epoch 47: Val Loss 214.88060
Epoch 48: Val Loss 214.68828
Epoch 49: Val Loss 214.49533
Epoch 50: Val Loss 214.30096
Epoch 51: Val Loss 214.10501
Epoch 52: Val Loss 213.90703
Epoch 53: Val Loss 213.70686
Epoch 54: Val Loss 213.50562
Epoch 55: Val Loss 213.30231
Epoch 56: Val Loss 213.09773
Epoch 57: Val Loss 212.89078
Epoch 58: Val Loss 212.68294
Epoch 59: Val Loss 212.47438
Epoch 60: Val Loss 212.26378
Epoch 61: Val Loss 212.05168
Epoch 62: Val Loss 211.83882
Epoch 63: Val Loss 211.62451
Epoch 64: Val Loss 211.40828
Epoch 65: Val Loss 211.18985
Epoch 66: Val Loss 210.97020
Epoch 67: Val Loss 210.74840
Epoch 68: Val Loss 210.52361
Epoch 69: Val Loss 210.29697
Epoch 70: Val Loss 210.06799
Epoch 71: Val Loss 209.83778
Epoch 72: Val Loss 209.60501
Epoch 73: Val Loss 209.37015
Epoch 74: Val Loss 209.13274
Epoch 75: Val Loss 208.89410
Epoch 76: Val Loss 208.65346
Epoch 77: Val Loss 208.41048
Epoch 78: Val Loss 208.16545
Epoch 79: Val Loss 207.91806
Epoch 80: Val Loss 207.66792
Epoch 81: Val Loss 207.41602
Epoch 82: Val Loss 207.16269
Epoch 83: Val Loss 206.90620
Epoch 84: Val Loss 206.64795
Epoch 85: Val Loss 206.38742
Epoch 86: Val Loss 206.12408
Epoch 87: Val Loss 205.85809
Epoch 88: Val Loss 205.58983
Epoch 89: Val Loss 205.31895
Epoch 90: Val Loss 205.04324
Epoch 91: Val Loss 204.76442
Epoch 92: Val Loss 204.48195
Epoch 93: Val Loss 204.19708
Epoch 94: Val Loss 203.90961
Epoch 95: Val Loss 203.61961
Epoch 96: Val Loss 203.32695
Epoch 97: Val Loss 203.03117
Epoch 98: Val Loss 202.73137
Epoch 99: Val Loss 202.42795
{'MSE - mean': 204.60217695500393, 'MSE - std': 3.4877895415626896, 'R2 - mean': -300.4071123693483, 'R2 - std': 39.22558797857807} 
 

In get_device
()
On Device: cuda
Using dim 32 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.73233
Epoch 1: Val Loss 220.66188
Epoch 2: Val Loss 220.59018
Epoch 3: Val Loss 220.51825
Epoch 4: Val Loss 220.44531
Epoch 5: Val Loss 220.37134
Epoch 6: Val Loss 220.29691
Epoch 7: Val Loss 220.22183
Epoch 8: Val Loss 220.14612
Epoch 9: Val Loss 220.06918
Epoch 10: Val Loss 219.99092
Epoch 11: Val Loss 219.91116
Epoch 12: Val Loss 219.83005
Epoch 13: Val Loss 219.74748
Epoch 14: Val Loss 219.66335
Epoch 15: Val Loss 219.57767
Epoch 16: Val Loss 219.49046
Epoch 17: Val Loss 219.40244
Epoch 18: Val Loss 219.31323
Epoch 19: Val Loss 219.22247
Epoch 20: Val Loss 219.13002
Epoch 21: Val Loss 219.03619
Epoch 22: Val Loss 218.93982
Epoch 23: Val Loss 218.84229
Epoch 24: Val Loss 218.74345
Epoch 25: Val Loss 218.64333
Epoch 26: Val Loss 218.54114
Epoch 27: Val Loss 218.43739
Epoch 28: Val Loss 218.33217
Epoch 29: Val Loss 218.22540
Epoch 30: Val Loss 218.11778
Epoch 31: Val Loss 218.00845
Epoch 32: Val Loss 217.89763
Epoch 33: Val Loss 217.78506
Epoch 34: Val Loss 217.67020
Epoch 35: Val Loss 217.55318
Epoch 36: Val Loss 217.43446
Epoch 37: Val Loss 217.31325
Epoch 38: Val Loss 217.18964
Epoch 39: Val Loss 217.06436
Epoch 40: Val Loss 216.93620
Epoch 41: Val Loss 216.80611
Epoch 42: Val Loss 216.67336
Epoch 43: Val Loss 216.53815
Epoch 44: Val Loss 216.40094
Epoch 45: Val Loss 216.26280
Epoch 46: Val Loss 216.12219
Epoch 47: Val Loss 215.97969
Epoch 48: Val Loss 215.83495
Epoch 49: Val Loss 215.68726
Epoch 50: Val Loss 215.53711
Epoch 51: Val Loss 215.38542
Epoch 52: Val Loss 215.23122
Epoch 53: Val Loss 215.07420
Epoch 54: Val Loss 214.91463
Epoch 55: Val Loss 214.75264
Epoch 56: Val Loss 214.58774
Epoch 57: Val Loss 214.42020
Epoch 58: Val Loss 214.24944
Epoch 59: Val Loss 214.07672
Epoch 60: Val Loss 213.90118
Epoch 61: Val Loss 213.72224
Epoch 62: Val Loss 213.53970
Epoch 63: Val Loss 213.35536
Epoch 64: Val Loss 213.16878
Epoch 65: Val Loss 212.97839
Epoch 66: Val Loss 212.78558
Epoch 67: Val Loss 212.58946
Epoch 68: Val Loss 212.39021
Epoch 69: Val Loss 212.18686
Epoch 70: Val Loss 211.97926
Epoch 71: Val Loss 211.76892
Epoch 72: Val Loss 211.55431
Epoch 73: Val Loss 211.33698
Epoch 74: Val Loss 211.11488
Epoch 75: Val Loss 210.88910
Epoch 76: Val Loss 210.65872
Epoch 77: Val Loss 210.42329
Epoch 78: Val Loss 210.18524
Epoch 79: Val Loss 209.94138
Epoch 80: Val Loss 209.69380
Epoch 81: Val Loss 209.44131
Epoch 82: Val Loss 209.18506
Epoch 83: Val Loss 208.92404
Epoch 84: Val Loss 208.65820
Epoch 85: Val Loss 208.38911
Epoch 86: Val Loss 208.11540
Epoch 87: Val Loss 207.83542
Epoch 88: Val Loss 207.55081
Epoch 89: Val Loss 207.26070
Epoch 90: Val Loss 206.96729
Epoch 91: Val Loss 206.66898
Epoch 92: Val Loss 206.36623
Epoch 93: Val Loss 206.05766
Epoch 94: Val Loss 205.74480
Epoch 95: Val Loss 205.42586
Epoch 96: Val Loss 205.10284
Epoch 97: Val Loss 204.77681
Epoch 98: Val Loss 204.44493
Epoch 99: Val Loss 204.10811
{'MSE - mean': 204.50336664012, 'MSE - std': 3.1258270301309223, 'R2 - mean': -307.8714536841153, 'R2 - std': 38.12850596162252} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 204.50336664012 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 22 with value: 0.5915682181299382.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.56345
Epoch 1: Val Loss 218.47005
Epoch 2: Val Loss 217.38881
Epoch 3: Val Loss 216.29337
Epoch 4: Val Loss 215.15312
Epoch 5: Val Loss 213.92387
Epoch 6: Val Loss 212.59097
Epoch 7: Val Loss 211.11375
Epoch 8: Val Loss 209.45218
Epoch 9: Val Loss 207.54700
Epoch 10: Val Loss 205.33397
Epoch 11: Val Loss 202.78949
Epoch 12: Val Loss 199.83824
Epoch 13: Val Loss 196.41782
Epoch 14: Val Loss 192.50861
Epoch 15: Val Loss 188.06468
Epoch 16: Val Loss 183.04492
Epoch 17: Val Loss 177.30008
Epoch 18: Val Loss 170.82167
Epoch 19: Val Loss 163.59132
Epoch 20: Val Loss 155.55544
Epoch 21: Val Loss 146.72232
Epoch 22: Val Loss 137.09476
Epoch 23: Val Loss 126.70853
Epoch 24: Val Loss 115.61437
Epoch 25: Val Loss 103.98958
Epoch 26: Val Loss 91.96577
Epoch 27: Val Loss 79.78754
Epoch 28: Val Loss 67.70518
Epoch 29: Val Loss 56.03004
Epoch 30: Val Loss 45.11705
Epoch 31: Val Loss 35.29766
Epoch 32: Val Loss 26.89990
Epoch 33: Val Loss 20.13442
Epoch 34: Val Loss 15.04744
Epoch 35: Val Loss 11.48366
Epoch 36: Val Loss 9.18966
Epoch 37: Val Loss 7.79391
Epoch 38: Val Loss 6.91430
Epoch 39: Val Loss 6.26190
Epoch 40: Val Loss 5.68640
Epoch 41: Val Loss 5.13158
Epoch 42: Val Loss 4.61596
Epoch 43: Val Loss 4.14865
Epoch 44: Val Loss 3.74137
Epoch 45: Val Loss 3.38900
Epoch 46: Val Loss 3.08784
Epoch 47: Val Loss 2.82118
Epoch 48: Val Loss 2.57282
Epoch 49: Val Loss 2.34739
Epoch 50: Val Loss 2.14018
Epoch 51: Val Loss 1.95633
Epoch 52: Val Loss 1.79297
Epoch 53: Val Loss 1.64477
Epoch 54: Val Loss 1.51157
Epoch 55: Val Loss 1.39413
Epoch 56: Val Loss 1.29202
Epoch 57: Val Loss 1.20128
Epoch 58: Val Loss 1.12059
Epoch 59: Val Loss 1.04880
Epoch 60: Val Loss 0.98547
Epoch 61: Val Loss 0.92954
Epoch 62: Val Loss 0.87963
Epoch 63: Val Loss 0.83496
Epoch 64: Val Loss 0.79460
Epoch 65: Val Loss 0.76023
Epoch 66: Val Loss 0.72956
Epoch 67: Val Loss 0.70165
Epoch 68: Val Loss 0.67656
Epoch 69: Val Loss 0.65481
Epoch 70: Val Loss 0.63464
Epoch 71: Val Loss 0.61807
Epoch 72: Val Loss 0.60302
Epoch 73: Val Loss 0.59009
Epoch 74: Val Loss 0.57979
Epoch 75: Val Loss 0.57010
Epoch 76: Val Loss 0.56192
Epoch 77: Val Loss 0.55434
Epoch 78: Val Loss 0.54739
Epoch 79: Val Loss 0.54177
Epoch 80: Val Loss 0.53693
Epoch 81: Val Loss 0.53277
Epoch 82: Val Loss 0.53003
Epoch 83: Val Loss 0.52794
Epoch 84: Val Loss 0.52622
Epoch 85: Val Loss 0.52345
Epoch 86: Val Loss 0.52173
Epoch 87: Val Loss 0.52017
Epoch 88: Val Loss 0.51858
Epoch 89: Val Loss 0.51779
Epoch 90: Val Loss 0.51583
Epoch 91: Val Loss 0.51568
Epoch 92: Val Loss 0.51539
Epoch 93: Val Loss 0.51398
Epoch 94: Val Loss 0.51369
Epoch 95: Val Loss 0.51320
Epoch 96: Val Loss 0.51262
Epoch 97: Val Loss 0.51164
Epoch 98: Val Loss 0.50967
Epoch 99: Val Loss 0.50810
{'MSE - mean': 0.5081005111341773, 'MSE - std': 0.0, 'R2 - mean': 0.07542506807917937, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.74945
Epoch 1: Val Loss 218.96469
Epoch 2: Val Loss 218.14972
Epoch 3: Val Loss 217.28474
Epoch 4: Val Loss 216.35291
Epoch 5: Val Loss 215.30687
Epoch 6: Val Loss 214.12169
Epoch 7: Val Loss 212.75865
Epoch 8: Val Loss 211.12750
Epoch 9: Val Loss 209.19774
Epoch 10: Val Loss 206.90488
Epoch 11: Val Loss 204.25795
Epoch 12: Val Loss 201.22061
Epoch 13: Val Loss 197.71387
Epoch 14: Val Loss 193.67247
Epoch 15: Val Loss 189.02843
Epoch 16: Val Loss 183.73186
Epoch 17: Val Loss 177.72050
Epoch 18: Val Loss 170.92108
Epoch 19: Val Loss 163.25423
Epoch 20: Val Loss 154.69618
Epoch 21: Val Loss 145.22168
Epoch 22: Val Loss 134.79846
Epoch 23: Val Loss 123.43648
Epoch 24: Val Loss 111.22839
Epoch 25: Val Loss 98.32436
Epoch 26: Val Loss 84.93108
Epoch 27: Val Loss 71.33994
Epoch 28: Val Loss 57.94624
Epoch 29: Val Loss 45.24605
Epoch 30: Val Loss 33.73262
Epoch 31: Val Loss 23.91198
Epoch 32: Val Loss 16.11656
Epoch 33: Val Loss 10.54538
Epoch 34: Val Loss 7.08109
Epoch 35: Val Loss 5.25239
Epoch 36: Val Loss 4.42727
Epoch 37: Val Loss 4.04451
Epoch 38: Val Loss 3.71748
Epoch 39: Val Loss 3.32252
Epoch 40: Val Loss 2.90528
Epoch 41: Val Loss 2.53348
Epoch 42: Val Loss 2.23797
Epoch 43: Val Loss 2.02412
Epoch 44: Val Loss 1.86695
Epoch 45: Val Loss 1.73785
Epoch 46: Val Loss 1.61778
Epoch 47: Val Loss 1.49637
Epoch 48: Val Loss 1.37814
Epoch 49: Val Loss 1.26207
Epoch 50: Val Loss 1.15710
Epoch 51: Val Loss 1.06698
Epoch 52: Val Loss 0.99095
Epoch 53: Val Loss 0.92870
Epoch 54: Val Loss 0.87451
Epoch 55: Val Loss 0.82869
Epoch 56: Val Loss 0.79137
Epoch 57: Val Loss 0.75910
Epoch 58: Val Loss 0.73173
Epoch 59: Val Loss 0.70569
Epoch 60: Val Loss 0.68373
Epoch 61: Val Loss 0.66434
Epoch 62: Val Loss 0.64759
Epoch 63: Val Loss 0.63221
Epoch 64: Val Loss 0.61798
Epoch 65: Val Loss 0.60756
Epoch 66: Val Loss 0.59801
Epoch 67: Val Loss 0.59068
Epoch 68: Val Loss 0.58323
Epoch 69: Val Loss 0.57833
Epoch 70: Val Loss 0.57402
Epoch 71: Val Loss 0.56886
Epoch 72: Val Loss 0.56401
Epoch 73: Val Loss 0.56101
Epoch 74: Val Loss 0.55863
Epoch 75: Val Loss 0.55680
Epoch 76: Val Loss 0.55600
Epoch 77: Val Loss 0.55482
Epoch 78: Val Loss 0.55469
Epoch 79: Val Loss 0.55344
Epoch 80: Val Loss 0.55268
Epoch 81: Val Loss 0.55268
Epoch 82: Val Loss 0.55365
Epoch 83: Val Loss 0.55492
Epoch 84: Val Loss 0.55555
Epoch 85: Val Loss 0.55454
Epoch 86: Val Loss 0.55387
Epoch 87: Val Loss 0.55370
Epoch 88: Val Loss 0.55432
Epoch 89: Val Loss 0.55462
Epoch 90: Val Loss 0.55472
Epoch 91: Val Loss 0.55440
Epoch 92: Val Loss 0.55547
Epoch 93: Val Loss 0.55598
Epoch 94: Val Loss 0.55585
Epoch 95: Val Loss 0.55659
Epoch 96: Val Loss 0.55739
Epoch 97: Val Loss 0.55760
Epoch 98: Val Loss 0.55824
Epoch 99: Val Loss 0.55841
{'MSE - mean': 0.5303895482670868, 'MSE - std': 0.022289037132909473, 'R2 - mean': 0.16349718016313314, 'R2 - std': 0.08807211208395377} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.75600
Epoch 1: Val Loss 230.29874
Epoch 2: Val Loss 228.88304
Epoch 3: Val Loss 227.50101
Epoch 4: Val Loss 226.10974
Epoch 5: Val Loss 224.70393
Epoch 6: Val Loss 223.27156
Epoch 7: Val Loss 221.77350
Epoch 8: Val Loss 220.16345
Epoch 9: Val Loss 218.42261
Epoch 10: Val Loss 216.52704
Epoch 11: Val Loss 214.44656
Epoch 12: Val Loss 212.14835
Epoch 13: Val Loss 209.58516
Epoch 14: Val Loss 206.71326
Epoch 15: Val Loss 203.45688
Epoch 16: Val Loss 199.75482
Epoch 17: Val Loss 195.53888
Epoch 18: Val Loss 190.74162
Epoch 19: Val Loss 185.31935
Epoch 20: Val Loss 179.19778
Epoch 21: Val Loss 172.32050
Epoch 22: Val Loss 164.64703
Epoch 23: Val Loss 156.16142
Epoch 24: Val Loss 146.84215
Epoch 25: Val Loss 136.72481
Epoch 26: Val Loss 125.80095
Epoch 27: Val Loss 114.15417
Epoch 28: Val Loss 101.89375
Epoch 29: Val Loss 89.20348
Epoch 30: Val Loss 76.32590
Epoch 31: Val Loss 63.59888
Epoch 32: Val Loss 51.39653
Epoch 33: Val Loss 40.13358
Epoch 34: Val Loss 30.25050
Epoch 35: Val Loss 22.02246
Epoch 36: Val Loss 15.70307
Epoch 37: Val Loss 11.22583
Epoch 38: Val Loss 8.38177
Epoch 39: Val Loss 6.72340
Epoch 40: Val Loss 5.77705
Epoch 41: Val Loss 5.15687
Epoch 42: Val Loss 4.62387
Epoch 43: Val Loss 4.08518
Epoch 44: Val Loss 3.57259
Epoch 45: Val Loss 3.11609
Epoch 46: Val Loss 2.73385
Epoch 47: Val Loss 2.43127
Epoch 48: Val Loss 2.18309
Epoch 49: Val Loss 1.96641
Epoch 50: Val Loss 1.77594
Epoch 51: Val Loss 1.60805
Epoch 52: Val Loss 1.45522
Epoch 53: Val Loss 1.32348
Epoch 54: Val Loss 1.20829
Epoch 55: Val Loss 1.11341
Epoch 56: Val Loss 1.03544
Epoch 57: Val Loss 0.97180
Epoch 58: Val Loss 0.91939
Epoch 59: Val Loss 0.87688
Epoch 60: Val Loss 0.84197
Epoch 61: Val Loss 0.81202
Epoch 62: Val Loss 0.78836
Epoch 63: Val Loss 0.76772
Epoch 64: Val Loss 0.75115
Epoch 65: Val Loss 0.73767
Epoch 66: Val Loss 0.72674
Epoch 67: Val Loss 0.71845
Epoch 68: Val Loss 0.71111
Epoch 69: Val Loss 0.70514
Epoch 70: Val Loss 0.70042
Epoch 71: Val Loss 0.69668
Epoch 72: Val Loss 0.69399
Epoch 73: Val Loss 0.69097
Epoch 74: Val Loss 0.68888
Epoch 75: Val Loss 0.68685
Epoch 76: Val Loss 0.68522
Epoch 77: Val Loss 0.68420
Epoch 78: Val Loss 0.68282
Epoch 79: Val Loss 0.68161
Epoch 80: Val Loss 0.68019
Epoch 81: Val Loss 0.67913
Epoch 82: Val Loss 0.67806
Epoch 83: Val Loss 0.67702
Epoch 84: Val Loss 0.67661
Epoch 85: Val Loss 0.67678
Epoch 86: Val Loss 0.67654
Epoch 87: Val Loss 0.67681
Epoch 88: Val Loss 0.67724
Epoch 89: Val Loss 0.67737
Epoch 90: Val Loss 0.67716
Epoch 91: Val Loss 0.67716
Epoch 92: Val Loss 0.67687
Epoch 93: Val Loss 0.67631
Epoch 94: Val Loss 0.67596
Epoch 95: Val Loss 0.67516
Epoch 96: Val Loss 0.67423
Epoch 97: Val Loss 0.67364
Epoch 98: Val Loss 0.67362
Epoch 99: Val Loss 0.67336
{'MSE - mean': 0.5780475819967832, 'MSE - std': 0.06981244260280468, 'R2 - mean': 0.11654410543653067, 'R2 - std': 0.09787907711652599} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 230.88399
Epoch 1: Val Loss 230.30896
Epoch 2: Val Loss 229.69415
Epoch 3: Val Loss 229.01666
Epoch 4: Val Loss 228.23932
Epoch 5: Val Loss 227.32639
Epoch 6: Val Loss 226.25810
Epoch 7: Val Loss 224.96147
Epoch 8: Val Loss 223.42300
Epoch 9: Val Loss 221.61519
Epoch 10: Val Loss 219.49605
Epoch 11: Val Loss 217.00984
Epoch 12: Val Loss 214.14111
Epoch 13: Val Loss 210.84164
Epoch 14: Val Loss 207.07516
Epoch 15: Val Loss 202.75264
Epoch 16: Val Loss 197.75017
Epoch 17: Val Loss 192.03508
Epoch 18: Val Loss 185.50053
Epoch 19: Val Loss 178.05414
Epoch 20: Val Loss 169.57791
Epoch 21: Val Loss 160.08693
Epoch 22: Val Loss 149.56456
Epoch 23: Val Loss 138.03316
Epoch 24: Val Loss 125.56906
Epoch 25: Val Loss 112.25687
Epoch 26: Val Loss 98.30399
Epoch 27: Val Loss 83.96248
Epoch 28: Val Loss 69.62510
Epoch 29: Val Loss 55.75897
Epoch 30: Val Loss 42.84200
Epoch 31: Val Loss 31.41805
Epoch 32: Val Loss 21.90444
Epoch 33: Val Loss 14.68604
Epoch 34: Val Loss 9.76663
Epoch 35: Val Loss 6.86644
Epoch 36: Val Loss 5.41528
Epoch 37: Val Loss 4.75947
Epoch 38: Val Loss 4.36360
Epoch 39: Val Loss 3.97014
Epoch 40: Val Loss 3.52312
Epoch 41: Val Loss 3.09170
Epoch 42: Val Loss 2.72210
Epoch 43: Val Loss 2.44372
Epoch 44: Val Loss 2.23182
Epoch 45: Val Loss 2.06513
Epoch 46: Val Loss 1.91606
Epoch 47: Val Loss 1.76802
Epoch 48: Val Loss 1.62754
Epoch 49: Val Loss 1.49844
Epoch 50: Val Loss 1.38339
Epoch 51: Val Loss 1.28346
Epoch 52: Val Loss 1.19833
Epoch 53: Val Loss 1.12234
Epoch 54: Val Loss 1.05996
Epoch 55: Val Loss 1.00708
Epoch 56: Val Loss 0.96327
Epoch 57: Val Loss 0.92486
Epoch 58: Val Loss 0.89258
Epoch 59: Val Loss 0.86355
Epoch 60: Val Loss 0.83953
Epoch 61: Val Loss 0.81636
Epoch 62: Val Loss 0.79527
Epoch 63: Val Loss 0.77730
Epoch 64: Val Loss 0.76118
Epoch 65: Val Loss 0.74813
Epoch 66: Val Loss 0.73651
Epoch 67: Val Loss 0.72560
Epoch 68: Val Loss 0.71654
Epoch 69: Val Loss 0.71013
Epoch 70: Val Loss 0.70368
Epoch 71: Val Loss 0.69791
Epoch 72: Val Loss 0.69320
Epoch 73: Val Loss 0.68926
Epoch 74: Val Loss 0.68559
Epoch 75: Val Loss 0.68193
Epoch 76: Val Loss 0.67933
Epoch 77: Val Loss 0.67809
Epoch 78: Val Loss 0.67572
Epoch 79: Val Loss 0.67423
Epoch 80: Val Loss 0.67204
Epoch 81: Val Loss 0.67040
Epoch 82: Val Loss 0.66840
Epoch 83: Val Loss 0.66745
Epoch 84: Val Loss 0.66563
Epoch 85: Val Loss 0.66555
Epoch 86: Val Loss 0.66415
Epoch 87: Val Loss 0.66348
Epoch 88: Val Loss 0.66290
Epoch 89: Val Loss 0.66209
Epoch 90: Val Loss 0.66167
Epoch 91: Val Loss 0.66237
Epoch 92: Val Loss 0.66247
Epoch 93: Val Loss 0.66240
Epoch 94: Val Loss 0.66120
Epoch 95: Val Loss 0.66130
Epoch 96: Val Loss 0.66105
Epoch 97: Val Loss 0.66105
Epoch 98: Val Loss 0.66189
Epoch 99: Val Loss 0.66304
{'MSE - mean': 0.5987981393568412, 'MSE - std': 0.07033555110245883, 'R2 - mean': 0.12662633165106654, 'R2 - std': 0.08654587892678861} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 232.77289
Epoch 1: Val Loss 231.33636
Epoch 2: Val Loss 229.81650
Epoch 3: Val Loss 228.19695
Epoch 4: Val Loss 226.48505
Epoch 5: Val Loss 224.66174
Epoch 6: Val Loss 222.70053
Epoch 7: Val Loss 220.57961
Epoch 8: Val Loss 218.27597
Epoch 9: Val Loss 215.74472
Epoch 10: Val Loss 212.93626
Epoch 11: Val Loss 209.79802
Epoch 12: Val Loss 206.28879
Epoch 13: Val Loss 202.31891
Epoch 14: Val Loss 197.83980
Epoch 15: Val Loss 192.80553
Epoch 16: Val Loss 187.14149
Epoch 17: Val Loss 180.78052
Epoch 18: Val Loss 173.68680
Epoch 19: Val Loss 165.85004
Epoch 20: Val Loss 157.26430
Epoch 21: Val Loss 147.90340
Epoch 22: Val Loss 137.75439
Epoch 23: Val Loss 126.86297
Epoch 24: Val Loss 115.33939
Epoch 25: Val Loss 103.29173
Epoch 26: Val Loss 90.86985
Epoch 27: Val Loss 78.33792
Epoch 28: Val Loss 65.96458
Epoch 29: Val Loss 54.05100
Epoch 30: Val Loss 42.99866
Epoch 31: Val Loss 33.14918
Epoch 32: Val Loss 24.81007
Epoch 33: Val Loss 18.19036
Epoch 34: Val Loss 13.34072
Epoch 35: Val Loss 10.08363
Epoch 36: Val Loss 8.07178
Epoch 37: Val Loss 6.90176
Epoch 38: Val Loss 6.17447
Epoch 39: Val Loss 5.60499
Epoch 40: Val Loss 5.06458
Epoch 41: Val Loss 4.53402
Epoch 42: Val Loss 4.01789
Epoch 43: Val Loss 3.56199
Epoch 44: Val Loss 3.18525
Epoch 45: Val Loss 2.87643
Epoch 46: Val Loss 2.61840
Epoch 47: Val Loss 2.39372
Epoch 48: Val Loss 2.18811
Epoch 49: Val Loss 1.99958
Epoch 50: Val Loss 1.83023
Epoch 51: Val Loss 1.67496
Epoch 52: Val Loss 1.53700
Epoch 53: Val Loss 1.41648
Epoch 54: Val Loss 1.30965
Epoch 55: Val Loss 1.21596
Epoch 56: Val Loss 1.13437
Epoch 57: Val Loss 1.06367
Epoch 58: Val Loss 1.00225
Epoch 59: Val Loss 0.94868
Epoch 60: Val Loss 0.90206
Epoch 61: Val Loss 0.86167
Epoch 62: Val Loss 0.82636
Epoch 63: Val Loss 0.79625
Epoch 64: Val Loss 0.76894
Epoch 65: Val Loss 0.74452
Epoch 66: Val Loss 0.72458
Epoch 67: Val Loss 0.70764
Epoch 68: Val Loss 0.69281
Epoch 69: Val Loss 0.67959
Epoch 70: Val Loss 0.66951
Epoch 71: Val Loss 0.66074
Epoch 72: Val Loss 0.65264
Epoch 73: Val Loss 0.64507
Epoch 74: Val Loss 0.63910
Epoch 75: Val Loss 0.63448
Epoch 76: Val Loss 0.63063
Epoch 77: Val Loss 0.62749
Epoch 78: Val Loss 0.62521
Epoch 79: Val Loss 0.62352
Epoch 80: Val Loss 0.62314
Epoch 81: Val Loss 0.62151
Epoch 82: Val Loss 0.62030
Epoch 83: Val Loss 0.61869
Epoch 84: Val Loss 0.61779
Epoch 85: Val Loss 0.61690
Epoch 86: Val Loss 0.61581
Epoch 87: Val Loss 0.61549
Epoch 88: Val Loss 0.61503
Epoch 89: Val Loss 0.61516
Epoch 90: Val Loss 0.61633
Epoch 91: Val Loss 0.61614
Epoch 92: Val Loss 0.61577
Epoch 93: Val Loss 0.61576
Epoch 94: Val Loss 0.61548
Epoch 95: Val Loss 0.61521
Epoch 96: Val Loss 0.61606
Epoch 97: Val Loss 0.61625
Epoch 98: Val Loss 0.61747
Epoch 99: Val Loss 0.61692
{'MSE - mean': 0.6020449818248588, 'MSE - std': 0.06324428624909738, 'R2 - mean': 0.09716496665874053, 'R2 - std': 0.09728329474381764} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 0.6020449818248588 and parameters: {'dim': 128, 'depth': 3, 'heads': 8, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.4}. Best is trial 22 with value: 0.5915682181299382.
Best parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -3, 'dropout': 0.2}
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 219.56020
Epoch 1: Val Loss 218.91541
Epoch 2: Val Loss 218.23444
Epoch 3: Val Loss 217.49763
Epoch 4: Val Loss 216.68962
Epoch 5: Val Loss 215.77728
Epoch 6: Val Loss 214.77037
Epoch 7: Val Loss 213.66150
Epoch 8: Val Loss 212.40819
Epoch 9: Val Loss 210.99660
Epoch 10: Val Loss 209.39517
Epoch 11: Val Loss 207.56575
Epoch 12: Val Loss 205.47720
Epoch 13: Val Loss 203.06001
Epoch 14: Val Loss 200.23671
Epoch 15: Val Loss 196.97157
Epoch 16: Val Loss 193.16827
Epoch 17: Val Loss 188.78287
Epoch 18: Val Loss 183.74255
Epoch 19: Val Loss 177.92857
Epoch 20: Val Loss 171.21275
Epoch 21: Val Loss 163.58795
Epoch 22: Val Loss 155.02798
Epoch 23: Val Loss 145.42606
Epoch 24: Val Loss 134.81123
Epoch 25: Val Loss 123.32613
Epoch 26: Val Loss 111.01360
Epoch 27: Val Loss 98.04117
Epoch 28: Val Loss 84.67522
Epoch 29: Val Loss 71.26324
Epoch 30: Val Loss 58.25662
Epoch 31: Val Loss 46.04826
Epoch 32: Val Loss 35.20022
Epoch 33: Val Loss 26.11612
Epoch 34: Val Loss 19.09242
Epoch 35: Val Loss 14.14503
Epoch 36: Val Loss 10.98212
Epoch 37: Val Loss 9.16782
Epoch 38: Val Loss 8.11843
Epoch 39: Val Loss 7.36996
Epoch 40: Val Loss 6.64495
Epoch 41: Val Loss 5.90932
Epoch 42: Val Loss 5.21155
Epoch 43: Val Loss 4.61184
Epoch 44: Val Loss 4.12112
Epoch 45: Val Loss 3.71982
Epoch 46: Val Loss 3.36860
Epoch 47: Val Loss 3.05971
Epoch 48: Val Loss 2.77047
Epoch 49: Val Loss 2.49955
Epoch 50: Val Loss 2.25152
Epoch 51: Val Loss 2.02721
Epoch 52: Val Loss 1.83099
Epoch 53: Val Loss 1.65982
Epoch 54: Val Loss 1.51474
Epoch 55: Val Loss 1.38602
Epoch 56: Val Loss 1.27430
Epoch 57: Val Loss 1.17812
Epoch 58: Val Loss 1.09451
Epoch 59: Val Loss 1.02063
Epoch 60: Val Loss 0.95507
Epoch 61: Val Loss 0.89742
Epoch 62: Val Loss 0.84704
Epoch 63: Val Loss 0.80364
Epoch 64: Val Loss 0.76573
Epoch 65: Val Loss 0.73298
Epoch 66: Val Loss 0.70463
Epoch 67: Val Loss 0.68016
Epoch 68: Val Loss 0.66019
Epoch 69: Val Loss 0.64308
Epoch 70: Val Loss 0.62825
Epoch 71: Val Loss 0.61593
Epoch 72: Val Loss 0.60456
Epoch 73: Val Loss 0.59557
Epoch 74: Val Loss 0.58751
Epoch 75: Val Loss 0.57990
Epoch 76: Val Loss 0.57341
Epoch 77: Val Loss 0.56816
Epoch 78: Val Loss 0.56364
Epoch 79: Val Loss 0.55998
Epoch 80: Val Loss 0.55725
Epoch 81: Val Loss 0.55446
Epoch 82: Val Loss 0.55162
Epoch 83: Val Loss 0.54809
Epoch 84: Val Loss 0.54593
Epoch 85: Val Loss 0.54528
Epoch 86: Val Loss 0.54639
Epoch 87: Val Loss 0.54645
Epoch 88: Val Loss 0.54490
Epoch 89: Val Loss 0.54336
Epoch 90: Val Loss 0.54268
Epoch 91: Val Loss 0.54086
Epoch 92: Val Loss 0.54031
Epoch 93: Val Loss 0.53980
Epoch 94: Val Loss 0.53922
Epoch 95: Val Loss 0.53862
Epoch 96: Val Loss 0.53838
Epoch 97: Val Loss 0.53780
Epoch 98: Val Loss 0.53674
Epoch 99: Val Loss 0.53609
Saved Losses
{'MSE - mean': 0.5360904918834892, 'MSE - std': 0.0, 'R2 - mean': 0.024492557722136765, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.83252
Epoch 1: Val Loss 231.22652
Epoch 2: Val Loss 230.60390
Epoch 3: Val Loss 229.93507
Epoch 4: Val Loss 229.20535
Epoch 5: Val Loss 228.40526
Epoch 6: Val Loss 227.51266
Epoch 7: Val Loss 226.51439
Epoch 8: Val Loss 225.39191
Epoch 9: Val Loss 224.11636
Epoch 10: Val Loss 222.66470
Epoch 11: Val Loss 221.00134
Epoch 12: Val Loss 219.10216
Epoch 13: Val Loss 216.93344
Epoch 14: Val Loss 214.44034
Epoch 15: Val Loss 211.57295
Epoch 16: Val Loss 208.27779
Epoch 17: Val Loss 204.47769
Epoch 18: Val Loss 200.10239
Epoch 19: Val Loss 195.09398
Epoch 20: Val Loss 189.36679
Epoch 21: Val Loss 182.85989
Epoch 22: Val Loss 175.48123
Epoch 23: Val Loss 167.16794
Epoch 24: Val Loss 157.89246
Epoch 25: Val Loss 147.62845
Epoch 26: Val Loss 136.44810
Epoch 27: Val Loss 124.42736
Epoch 28: Val Loss 111.66696
Epoch 29: Val Loss 98.33485
Epoch 30: Val Loss 84.73230
Epoch 31: Val Loss 71.17714
Epoch 32: Val Loss 58.04381
Epoch 33: Val Loss 45.78665
Epoch 34: Val Loss 34.80082
Epoch 35: Val Loss 25.50246
Epoch 36: Val Loss 18.13914
Epoch 37: Val Loss 12.77315
Epoch 38: Val Loss 9.28890
Epoch 39: Val Loss 7.28098
Epoch 40: Val Loss 6.27464
Epoch 41: Val Loss 5.78799
Epoch 42: Val Loss 5.45637
Epoch 43: Val Loss 5.10490
Epoch 44: Val Loss 4.70886
Epoch 45: Val Loss 4.29785
Epoch 46: Val Loss 3.91228
Epoch 47: Val Loss 3.59305
Epoch 48: Val Loss 3.32769
Epoch 49: Val Loss 3.09582
Epoch 50: Val Loss 2.89138
Epoch 51: Val Loss 2.70140
Epoch 52: Val Loss 2.52386
Epoch 53: Val Loss 2.35652
Epoch 54: Val Loss 2.20022
Epoch 55: Val Loss 2.05524
Epoch 56: Val Loss 1.92148
Epoch 57: Val Loss 1.79677
Epoch 58: Val Loss 1.68407
Epoch 59: Val Loss 1.57939
Epoch 60: Val Loss 1.48644
Epoch 61: Val Loss 1.39936
Epoch 62: Val Loss 1.32077
Epoch 63: Val Loss 1.24836
Epoch 64: Val Loss 1.18110
Epoch 65: Val Loss 1.12121
Epoch 66: Val Loss 1.06578
Epoch 67: Val Loss 1.01545
Epoch 68: Val Loss 0.96922
Epoch 69: Val Loss 0.92840
Epoch 70: Val Loss 0.89062
Epoch 71: Val Loss 0.85717
Epoch 72: Val Loss 0.82566
Epoch 73: Val Loss 0.79736
Epoch 74: Val Loss 0.77175
Epoch 75: Val Loss 0.74884
Epoch 76: Val Loss 0.72790
Epoch 77: Val Loss 0.70931
Epoch 78: Val Loss 0.69295
Epoch 79: Val Loss 0.67804
Epoch 80: Val Loss 0.66561
Epoch 81: Val Loss 0.65376
Epoch 82: Val Loss 0.64319
Epoch 83: Val Loss 0.63420
Epoch 84: Val Loss 0.62598
Epoch 85: Val Loss 0.61901
Epoch 86: Val Loss 0.61281
Epoch 87: Val Loss 0.60618
Epoch 88: Val Loss 0.60094
Epoch 89: Val Loss 0.59651
Epoch 90: Val Loss 0.59256
Epoch 91: Val Loss 0.58885
Epoch 92: Val Loss 0.58475
Epoch 93: Val Loss 0.58148
Epoch 94: Val Loss 0.57925
Epoch 95: Val Loss 0.57774
Epoch 96: Val Loss 0.57684
Epoch 97: Val Loss 0.57601
Epoch 98: Val Loss 0.57498
Epoch 99: Val Loss 0.57453
Saved Losses
{'MSE - mean': 0.5553093095108717, 'MSE - std': 0.019218817627382367, 'R2 - mean': 0.12323673137983726, 'R2 - std': 0.0987441736577005} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.24741
Epoch 1: Val Loss 219.00398
Epoch 2: Val Loss 217.72513
Epoch 3: Val Loss 216.39281
Epoch 4: Val Loss 215.00784
Epoch 5: Val Loss 213.49579
Epoch 6: Val Loss 211.83553
Epoch 7: Val Loss 210.03073
Epoch 8: Val Loss 208.01057
Epoch 9: Val Loss 205.74554
Epoch 10: Val Loss 203.18468
Epoch 11: Val Loss 200.27664
Epoch 12: Val Loss 196.98125
Epoch 13: Val Loss 193.24358
Epoch 14: Val Loss 189.01164
Epoch 15: Val Loss 184.22327
Epoch 16: Val Loss 178.81799
Epoch 17: Val Loss 172.74881
Epoch 18: Val Loss 165.95511
Epoch 19: Val Loss 158.38737
Epoch 20: Val Loss 149.98566
Epoch 21: Val Loss 140.72041
Epoch 22: Val Loss 130.58084
Epoch 23: Val Loss 119.63367
Epoch 24: Val Loss 107.88992
Epoch 25: Val Loss 95.50505
Epoch 26: Val Loss 82.71136
Epoch 27: Val Loss 69.75508
Epoch 28: Val Loss 56.97988
Epoch 29: Val Loss 44.80352
Epoch 30: Val Loss 33.65077
Epoch 31: Val Loss 23.99739
Epoch 32: Val Loss 16.19724
Epoch 33: Val Loss 10.44351
Epoch 34: Val Loss 6.66386
Epoch 35: Val Loss 4.54333
Epoch 36: Val Loss 3.57979
Epoch 37: Val Loss 3.20207
Epoch 38: Val Loss 2.98694
Epoch 39: Val Loss 2.74743
Epoch 40: Val Loss 2.44064
Epoch 41: Val Loss 2.13073
Epoch 42: Val Loss 1.87593
Epoch 43: Val Loss 1.68898
Epoch 44: Val Loss 1.56054
Epoch 45: Val Loss 1.45846
Epoch 46: Val Loss 1.37238
Epoch 47: Val Loss 1.28820
Epoch 48: Val Loss 1.20808
Epoch 49: Val Loss 1.13504
Epoch 50: Val Loss 1.07039
Epoch 51: Val Loss 1.01762
Epoch 52: Val Loss 0.97297
Epoch 53: Val Loss 0.93727
Epoch 54: Val Loss 0.90842
Epoch 55: Val Loss 0.88291
Epoch 56: Val Loss 0.86153
Epoch 57: Val Loss 0.84320
Epoch 58: Val Loss 0.82653
Epoch 59: Val Loss 0.81155
Epoch 60: Val Loss 0.79832
Epoch 61: Val Loss 0.78628
Epoch 62: Val Loss 0.77615
Epoch 63: Val Loss 0.76717
Epoch 64: Val Loss 0.75918
Epoch 65: Val Loss 0.75243
Epoch 66: Val Loss 0.74637
Epoch 67: Val Loss 0.74045
Epoch 68: Val Loss 0.73656
Epoch 69: Val Loss 0.73222
Epoch 70: Val Loss 0.72796
Epoch 71: Val Loss 0.72461
Epoch 72: Val Loss 0.72078
Epoch 73: Val Loss 0.71801
Epoch 74: Val Loss 0.71541
Epoch 75: Val Loss 0.71304
Epoch 76: Val Loss 0.71117
Epoch 77: Val Loss 0.70989
Epoch 78: Val Loss 0.70800
Epoch 79: Val Loss 0.70656
Epoch 80: Val Loss 0.70467
Epoch 81: Val Loss 0.70227
Epoch 82: Val Loss 0.70021
Epoch 83: Val Loss 0.69867
Epoch 84: Val Loss 0.69659
Epoch 85: Val Loss 0.69419
Epoch 86: Val Loss 0.69292
Epoch 87: Val Loss 0.69151
Epoch 88: Val Loss 0.69070
Epoch 89: Val Loss 0.69006
Epoch 90: Val Loss 0.68896
Epoch 91: Val Loss 0.68792
Epoch 92: Val Loss 0.68745
Epoch 93: Val Loss 0.68623
Epoch 94: Val Loss 0.68521
Epoch 95: Val Loss 0.68401
Epoch 96: Val Loss 0.68343
Epoch 97: Val Loss 0.68268
Epoch 98: Val Loss 0.68166
Epoch 99: Val Loss 0.68109
Saved Losses
{'MSE - mean': 0.5972353348990904, 'MSE - std': 0.06133371973911948, 'R2 - mean': 0.0859669019142811, 'R2 - std': 0.09632421778042816} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 223.34908
Epoch 1: Val Loss 222.64607
Epoch 2: Val Loss 222.00349
Epoch 3: Val Loss 221.39149
Epoch 4: Val Loss 220.75845
Epoch 5: Val Loss 220.08058
Epoch 6: Val Loss 219.30391
Epoch 7: Val Loss 218.39774
Epoch 8: Val Loss 217.30258
Epoch 9: Val Loss 216.04535
Epoch 10: Val Loss 214.57448
Epoch 11: Val Loss 212.84268
Epoch 12: Val Loss 210.84999
Epoch 13: Val Loss 208.60254
Epoch 14: Val Loss 206.06464
Epoch 15: Val Loss 203.19160
Epoch 16: Val Loss 199.94522
Epoch 17: Val Loss 196.27632
Epoch 18: Val Loss 192.12933
Epoch 19: Val Loss 187.45219
Epoch 20: Val Loss 182.17543
Epoch 21: Val Loss 176.24222
Epoch 22: Val Loss 169.57156
Epoch 23: Val Loss 162.16922
Epoch 24: Val Loss 153.96884
Epoch 25: Val Loss 144.87329
Epoch 26: Val Loss 134.88130
Epoch 27: Val Loss 124.04980
Epoch 28: Val Loss 112.41781
Epoch 29: Val Loss 100.07446
Epoch 30: Val Loss 87.18911
Epoch 31: Val Loss 74.05594
Epoch 32: Val Loss 61.01252
Epoch 33: Val Loss 48.45777
Epoch 34: Val Loss 36.83259
Epoch 35: Val Loss 26.60081
Epoch 36: Val Loss 18.14298
Epoch 37: Val Loss 11.74219
Epoch 38: Val Loss 7.41688
Epoch 39: Val Loss 4.93344
Epoch 40: Val Loss 3.79815
Epoch 41: Val Loss 3.41494
Epoch 42: Val Loss 3.27233
Epoch 43: Val Loss 3.08681
Epoch 44: Val Loss 2.80524
Epoch 45: Val Loss 2.47366
Epoch 46: Val Loss 2.17419
Epoch 47: Val Loss 1.94268
Epoch 48: Val Loss 1.77922
Epoch 49: Val Loss 1.66119
Epoch 50: Val Loss 1.56469
Epoch 51: Val Loss 1.47644
Epoch 52: Val Loss 1.38561
Epoch 53: Val Loss 1.29973
Epoch 54: Val Loss 1.22091
Epoch 55: Val Loss 1.15173
Epoch 56: Val Loss 1.09147
Epoch 57: Val Loss 1.04052
Epoch 58: Val Loss 0.99635
Epoch 59: Val Loss 0.95810
Epoch 60: Val Loss 0.92481
Epoch 61: Val Loss 0.89667
Epoch 62: Val Loss 0.87204
Epoch 63: Val Loss 0.85039
Epoch 64: Val Loss 0.83044
Epoch 65: Val Loss 0.81255
Epoch 66: Val Loss 0.79727
Epoch 67: Val Loss 0.78167
Epoch 68: Val Loss 0.76672
Epoch 69: Val Loss 0.75292
Epoch 70: Val Loss 0.74133
Epoch 71: Val Loss 0.73143
Epoch 72: Val Loss 0.72174
Epoch 73: Val Loss 0.71260
Epoch 74: Val Loss 0.70571
Epoch 75: Val Loss 0.69943
Epoch 76: Val Loss 0.69364
Epoch 77: Val Loss 0.68784
Epoch 78: Val Loss 0.68215
Epoch 79: Val Loss 0.67576
Epoch 80: Val Loss 0.67052
Epoch 81: Val Loss 0.66683
Epoch 82: Val Loss 0.66287
Epoch 83: Val Loss 0.65873
Epoch 84: Val Loss 0.65511
Epoch 85: Val Loss 0.65065
Epoch 86: Val Loss 0.64706
Epoch 87: Val Loss 0.64404
Epoch 88: Val Loss 0.64117
Epoch 89: Val Loss 0.63826
Epoch 90: Val Loss 0.63526
Epoch 91: Val Loss 0.63289
Epoch 92: Val Loss 0.63153
Epoch 93: Val Loss 0.62894
Epoch 94: Val Loss 0.62736
Epoch 95: Val Loss 0.62606
Epoch 96: Val Loss 0.62399
Epoch 97: Val Loss 0.62211
Epoch 98: Val Loss 0.62047
Epoch 99: Val Loss 0.61875
Saved Losses
{'MSE - mean': 0.6026127685658949, 'MSE - std': 0.053926980828712154, 'R2 - mean': 0.11718268089577383, 'R2 - std': 0.0994084542205678} 
 

In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 231.69762
Epoch 1: Val Loss 230.67509
Epoch 2: Val Loss 229.60786
Epoch 3: Val Loss 228.48674
Epoch 4: Val Loss 227.30923
Epoch 5: Val Loss 226.05351
Epoch 6: Val Loss 224.71272
Epoch 7: Val Loss 223.26312
Epoch 8: Val Loss 221.66962
Epoch 9: Val Loss 219.91399
Epoch 10: Val Loss 217.95348
Epoch 11: Val Loss 215.77580
Epoch 12: Val Loss 213.33868
Epoch 13: Val Loss 210.58766
Epoch 14: Val Loss 207.49187
Epoch 15: Val Loss 203.98120
Epoch 16: Val Loss 199.98959
Epoch 17: Val Loss 195.47078
Epoch 18: Val Loss 190.33765
Epoch 19: Val Loss 184.50183
Epoch 20: Val Loss 177.93048
Epoch 21: Val Loss 170.49214
Epoch 22: Val Loss 162.14807
Epoch 23: Val Loss 152.87372
Epoch 24: Val Loss 142.65839
Epoch 25: Val Loss 131.53281
Epoch 26: Val Loss 119.54497
Epoch 27: Val Loss 106.83118
Epoch 28: Val Loss 93.50477
Epoch 29: Val Loss 79.88892
Epoch 30: Val Loss 66.32191
Epoch 31: Val Loss 53.19725
Epoch 32: Val Loss 40.95089
Epoch 33: Val Loss 30.00021
Epoch 34: Val Loss 20.78769
Epoch 35: Val Loss 13.61057
Epoch 36: Val Loss 8.54489
Epoch 37: Val Loss 5.41309
Epoch 38: Val Loss 3.81496
Epoch 39: Val Loss 3.15997
Epoch 40: Val Loss 2.92153
Epoch 41: Val Loss 2.76652
Epoch 42: Val Loss 2.53462
Epoch 43: Val Loss 2.25091
Epoch 44: Val Loss 1.95834
Epoch 45: Val Loss 1.71930
Epoch 46: Val Loss 1.54274
Epoch 47: Val Loss 1.42011
Epoch 48: Val Loss 1.32848
Epoch 49: Val Loss 1.25013
Epoch 50: Val Loss 1.17766
Epoch 51: Val Loss 1.10839
Epoch 52: Val Loss 1.04033
Epoch 53: Val Loss 0.97973
Epoch 54: Val Loss 0.92648
Epoch 55: Val Loss 0.88079
Epoch 56: Val Loss 0.84212
Epoch 57: Val Loss 0.80883
Epoch 58: Val Loss 0.78038
Epoch 59: Val Loss 0.75523
Epoch 60: Val Loss 0.73305
Epoch 61: Val Loss 0.71395
Epoch 62: Val Loss 0.69700
Epoch 63: Val Loss 0.68248
Epoch 64: Val Loss 0.66907
Epoch 65: Val Loss 0.65675
Epoch 66: Val Loss 0.64650
Epoch 67: Val Loss 0.63727
Epoch 68: Val Loss 0.62922
Epoch 69: Val Loss 0.62176
Epoch 70: Val Loss 0.61586
Epoch 71: Val Loss 0.61035
Epoch 72: Val Loss 0.60486
Epoch 73: Val Loss 0.60010
Epoch 74: Val Loss 0.59643
Epoch 75: Val Loss 0.59270
Epoch 76: Val Loss 0.58940
Epoch 77: Val Loss 0.58699
Epoch 78: Val Loss 0.58450
Epoch 79: Val Loss 0.58264
Epoch 80: Val Loss 0.58103
Epoch 81: Val Loss 0.57937
Epoch 82: Val Loss 0.57776
Epoch 83: Val Loss 0.57582
Epoch 84: Val Loss 0.57422
Epoch 85: Val Loss 0.57349
Epoch 86: Val Loss 0.57166
Epoch 87: Val Loss 0.57096
Epoch 88: Val Loss 0.57019
Epoch 89: Val Loss 0.56929
Epoch 90: Val Loss 0.56896
Epoch 91: Val Loss 0.56883
Epoch 92: Val Loss 0.56827
Epoch 93: Val Loss 0.56841
Epoch 94: Val Loss 0.56826
Epoch 95: Val Loss 0.56744
Epoch 96: Val Loss 0.56625
Epoch 97: Val Loss 0.56564
Epoch 98: Val Loss 0.56545
Epoch 99: Val Loss 0.56499
Saved Losses
{'MSE - mean': 0.595088430220736, 'MSE - std': 0.05052680555081574, 'R2 - mean': 0.1062193034263611, 'R2 - std': 0.09157737274330098} 
 

Saving model.....
Results After CV: {'MSE - mean': 0.595088430220736, 'MSE - std': 0.05052680555081574, 'R2 - mean': 0.1062193034263611, 'R2 - std': 0.09157737274330098}
Train time: 26.611985687800008
Inference time: 0.11078250259988635
Finished cross validation


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/moneyball.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/moneyball.yml', data_parallel=False, dataset='Moneyball', direction='minimize', dropna_idx=[9, 10, 12, 13], early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 8], num_classes=1, num_features=14, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Moneyball...
Dataset loaded! 

X b4 encoding : ['ARI' 'NL' 2012 688 81 0.3279999999999999 0.418 0.259 0 162] 

(1232, 10)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 8]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 8]
Cat Idx Part II: [0, 1, 8] 
ENDE 
 

X after Nominal Encoding: ['ARI' 'NL' 2012 688 81 0.3279999999999999 0.418 0.259 0 162] 
 

Scaling the data...
X after Scaling: ['ARI' 'NL' 1.5554755871677342 -0.2910721732671802 0.008362450087033452
 0.11120590052485849 0.6212382045299186 -0.0211383889172301 0
 0.13005495722996097] 
 

One Hot Encoding...
X after One Hot Encoding: [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.5554755871677342 -0.2910721732671802
 0.008362450087033452 0.11120590052485849 0.6212382045299186
 -0.0211383889172301 0.13005495722996097] 
 

args.num_features: 50
args.cat_idx: None
Cat Dims: []
New Shape: (1232, 50)
False 
 

Using an existing study with name 'TabTransformer_Moneyball' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516671.75000
Epoch 1: Val Loss 516669.78125
Epoch 2: Val Loss 516667.81250
Epoch 3: Val Loss 516665.81250
Epoch 4: Val Loss 516663.78125
Epoch 5: Val Loss 516661.71875
Epoch 6: Val Loss 516659.71875
Epoch 7: Val Loss 516657.62500
Epoch 8: Val Loss 516655.56250
Epoch 9: Val Loss 516653.53125
Epoch 10: Val Loss 516651.43750
Epoch 11: Val Loss 516649.34375
Epoch 12: Val Loss 516647.25000
Epoch 13: Val Loss 516645.12500
Epoch 14: Val Loss 516642.96875
Epoch 15: Val Loss 516640.81250
Epoch 16: Val Loss 516638.62500
Epoch 17: Val Loss 516636.40625
Epoch 18: Val Loss 516634.25000
Epoch 19: Val Loss 516632.03125
Epoch 20: Val Loss 516629.84375
Epoch 21: Val Loss 516627.53125
Epoch 22: Val Loss 516625.34375
Epoch 23: Val Loss 516623.00000
Epoch 24: Val Loss 516620.75000
Epoch 25: Val Loss 516618.43750
Epoch 26: Val Loss 516616.03125
Epoch 27: Val Loss 516613.71875
Epoch 28: Val Loss 516611.31250
Epoch 29: Val Loss 516608.93750
Epoch 30: Val Loss 516606.50000
Epoch 31: Val Loss 516604.09375
Epoch 32: Val Loss 516601.59375
Epoch 33: Val Loss 516599.06250
Epoch 34: Val Loss 516596.62500
Epoch 35: Val Loss 516594.03125
Epoch 36: Val Loss 516591.43750
Epoch 37: Val Loss 516588.84375
Epoch 38: Val Loss 516586.25000
Epoch 39: Val Loss 516583.65625
Epoch 40: Val Loss 516580.96875
Epoch 41: Val Loss 516578.28125
Epoch 42: Val Loss 516575.56250
Epoch 43: Val Loss 516572.84375
Epoch 44: Val Loss 516570.09375
Epoch 45: Val Loss 516567.28125
Epoch 46: Val Loss 516564.40625
Epoch 47: Val Loss 516561.59375
Epoch 48: Val Loss 516558.68750
Epoch 49: Val Loss 516555.75000
Epoch 50: Val Loss 516552.81250
Epoch 51: Val Loss 516549.87500
Epoch 52: Val Loss 516546.87500
Epoch 53: Val Loss 516543.84375
Epoch 54: Val Loss 516540.78125
Epoch 55: Val Loss 516537.68750
Epoch 56: Val Loss 516534.59375
Epoch 57: Val Loss 516531.40625
Epoch 58: Val Loss 516528.28125
Epoch 59: Val Loss 516525.03125
Epoch 60: Val Loss 516521.84375
Epoch 61: Val Loss 516518.59375
Epoch 62: Val Loss 516515.28125
Epoch 63: Val Loss 516512.00000
Epoch 64: Val Loss 516508.65625
Epoch 65: Val Loss 516505.28125
Epoch 66: Val Loss 516501.84375
Epoch 67: Val Loss 516498.43750
Epoch 68: Val Loss 516494.96875
Epoch 69: Val Loss 516491.43750
Epoch 70: Val Loss 516487.90625
Epoch 71: Val Loss 516484.34375
Epoch 72: Val Loss 516480.71875
Epoch 73: Val Loss 516477.15625
Epoch 74: Val Loss 516473.46875
Epoch 75: Val Loss 516469.78125
Epoch 76: Val Loss 516466.06250
Epoch 77: Val Loss 516462.25000
Epoch 78: Val Loss 516458.43750
Epoch 79: Val Loss 516454.59375
Epoch 80: Val Loss 516450.78125
Epoch 81: Val Loss 516446.84375
Epoch 82: Val Loss 516442.87500
Epoch 83: Val Loss 516438.90625
Epoch 84: Val Loss 516434.81250
Epoch 85: Val Loss 516430.71875
Epoch 86: Val Loss 516426.62500
Epoch 87: Val Loss 516422.46875
Epoch 88: Val Loss 516418.28125
Epoch 89: Val Loss 516414.06250
Epoch 90: Val Loss 516409.78125
Epoch 91: Val Loss 516405.53125
Epoch 92: Val Loss 516401.21875
Epoch 93: Val Loss 516396.81250
Epoch 94: Val Loss 516392.37500
Epoch 95: Val Loss 516387.96875
Epoch 96: Val Loss 516383.50000
Epoch 97: Val Loss 516379.03125
Epoch 98: Val Loss 516374.46875
Epoch 99: Val Loss 516369.90625
{'MSE - mean': 516369.8928633089, 'MSE - std': 0.0, 'R2 - mean': -59.038821287847774, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524538.93750
Epoch 1: Val Loss 524536.00000
Epoch 2: Val Loss 524533.06250
Epoch 3: Val Loss 524530.00000
Epoch 4: Val Loss 524527.00000
Epoch 5: Val Loss 524524.00000
Epoch 6: Val Loss 524520.87500
Epoch 7: Val Loss 524517.81250
Epoch 8: Val Loss 524514.75000
Epoch 9: Val Loss 524511.56250
Epoch 10: Val Loss 524508.50000
Epoch 11: Val Loss 524505.31250
Epoch 12: Val Loss 524502.18750
Epoch 13: Val Loss 524498.93750
Epoch 14: Val Loss 524495.75000
Epoch 15: Val Loss 524492.50000
Epoch 16: Val Loss 524489.25000
Epoch 17: Val Loss 524486.00000
Epoch 18: Val Loss 524482.62500
Epoch 19: Val Loss 524479.31250
Epoch 20: Val Loss 524475.93750
Epoch 21: Val Loss 524472.56250
Epoch 22: Val Loss 524469.06250
Epoch 23: Val Loss 524465.62500
Epoch 24: Val Loss 524462.18750
Epoch 25: Val Loss 524458.62500
Epoch 26: Val Loss 524455.12500
Epoch 27: Val Loss 524451.56250
Epoch 28: Val Loss 524447.93750
Epoch 29: Val Loss 524444.37500
Epoch 30: Val Loss 524440.75000
Epoch 31: Val Loss 524437.06250
Epoch 32: Val Loss 524433.31250
Epoch 33: Val Loss 524429.68750
Epoch 34: Val Loss 524425.93750
Epoch 35: Val Loss 524422.18750
Epoch 36: Val Loss 524418.50000
Epoch 37: Val Loss 524414.62500
Epoch 38: Val Loss 524410.81250
Epoch 39: Val Loss 524407.00000
Epoch 40: Val Loss 524403.12500
Epoch 41: Val Loss 524399.18750
Epoch 42: Val Loss 524395.25000
Epoch 43: Val Loss 524391.31250
Epoch 44: Val Loss 524387.37500
Epoch 45: Val Loss 524383.31250
Epoch 46: Val Loss 524379.31250
Epoch 47: Val Loss 524375.18750
Epoch 48: Val Loss 524371.12500
Epoch 49: Val Loss 524366.93750
Epoch 50: Val Loss 524362.81250
Epoch 51: Val Loss 524358.62500
Epoch 52: Val Loss 524354.43750
Epoch 53: Val Loss 524350.18750
Epoch 54: Val Loss 524345.87500
Epoch 55: Val Loss 524341.50000
Epoch 56: Val Loss 524337.25000
Epoch 57: Val Loss 524332.75000
Epoch 58: Val Loss 524328.31250
Epoch 59: Val Loss 524323.87500
Epoch 60: Val Loss 524319.37500
Epoch 61: Val Loss 524314.75000
Epoch 62: Val Loss 524310.18750
Epoch 63: Val Loss 524305.50000
Epoch 64: Val Loss 524300.81250
Epoch 65: Val Loss 524296.12500
Epoch 66: Val Loss 524291.37500
Epoch 67: Val Loss 524286.59375
Epoch 68: Val Loss 524281.65625
Epoch 69: Val Loss 524276.75000
Epoch 70: Val Loss 524271.81250
Epoch 71: Val Loss 524266.84375
Epoch 72: Val Loss 524261.78125
Epoch 73: Val Loss 524256.65625
Epoch 74: Val Loss 524251.53125
Epoch 75: Val Loss 524246.40625
Epoch 76: Val Loss 524241.21875
Epoch 77: Val Loss 524235.96875
Epoch 78: Val Loss 524230.75000
Epoch 79: Val Loss 524225.46875
Epoch 80: Val Loss 524220.00000
Epoch 81: Val Loss 524214.62500
Epoch 82: Val Loss 524209.15625
Epoch 83: Val Loss 524203.68750
Epoch 84: Val Loss 524198.12500
Epoch 85: Val Loss 524192.46875
Epoch 86: Val Loss 524186.78125
Epoch 87: Val Loss 524181.06250
Epoch 88: Val Loss 524175.25000
Epoch 89: Val Loss 524169.40625
Epoch 90: Val Loss 524163.50000
Epoch 91: Val Loss 524157.59375
Epoch 92: Val Loss 524151.56250
Epoch 93: Val Loss 524145.50000
Epoch 94: Val Loss 524139.31250
Epoch 95: Val Loss 524133.15625
Epoch 96: Val Loss 524126.93750
Epoch 97: Val Loss 524120.59375
Epoch 98: Val Loss 524114.25000
Epoch 99: Val Loss 524107.78125
{'MSE - mean': 520238.83127882663, 'MSE - std': 3868.9384155177395, 'R2 - mean': -63.31752312348074, 'R2 - std': 4.278701835632969} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518293.71875
Epoch 1: Val Loss 518290.31250
Epoch 2: Val Loss 518287.00000
Epoch 3: Val Loss 518283.65625
Epoch 4: Val Loss 518280.31250
Epoch 5: Val Loss 518277.00000
Epoch 6: Val Loss 518273.65625
Epoch 7: Val Loss 518270.31250
Epoch 8: Val Loss 518267.03125
Epoch 9: Val Loss 518263.78125
Epoch 10: Val Loss 518260.53125
Epoch 11: Val Loss 518257.28125
Epoch 12: Val Loss 518254.06250
Epoch 13: Val Loss 518250.81250
Epoch 14: Val Loss 518247.53125
Epoch 15: Val Loss 518244.28125
Epoch 16: Val Loss 518241.06250
Epoch 17: Val Loss 518237.78125
Epoch 18: Val Loss 518234.56250
Epoch 19: Val Loss 518231.37500
Epoch 20: Val Loss 518228.12500
Epoch 21: Val Loss 518224.96875
Epoch 22: Val Loss 518221.78125
Epoch 23: Val Loss 518218.43750
Epoch 24: Val Loss 518215.34375
Epoch 25: Val Loss 518212.09375
Epoch 26: Val Loss 518208.81250
Epoch 27: Val Loss 518205.62500
Epoch 28: Val Loss 518202.40625
Epoch 29: Val Loss 518199.15625
Epoch 30: Val Loss 518195.93750
Epoch 31: Val Loss 518192.71875
Epoch 32: Val Loss 518189.43750
Epoch 33: Val Loss 518186.18750
Epoch 34: Val Loss 518182.93750
Epoch 35: Val Loss 518179.65625
Epoch 36: Val Loss 518176.37500
Epoch 37: Val Loss 518173.03125
Epoch 38: Val Loss 518169.65625
Epoch 39: Val Loss 518166.28125
Epoch 40: Val Loss 518162.90625
Epoch 41: Val Loss 518159.56250
Epoch 42: Val Loss 518156.18750
Epoch 43: Val Loss 518152.75000
Epoch 44: Val Loss 518149.34375
Epoch 45: Val Loss 518145.90625
Epoch 46: Val Loss 518142.40625
Epoch 47: Val Loss 518138.90625
Epoch 48: Val Loss 518135.40625
Epoch 49: Val Loss 518131.81250
Epoch 50: Val Loss 518128.28125
Epoch 51: Val Loss 518124.68750
Epoch 52: Val Loss 518121.12500
Epoch 53: Val Loss 518117.50000
Epoch 54: Val Loss 518113.87500
Epoch 55: Val Loss 518110.15625
Epoch 56: Val Loss 518106.50000
Epoch 57: Val Loss 518102.75000
Epoch 58: Val Loss 518098.96875
Epoch 59: Val Loss 518095.21875
Epoch 60: Val Loss 518091.40625
Epoch 61: Val Loss 518087.62500
Epoch 62: Val Loss 518083.78125
Epoch 63: Val Loss 518079.87500
Epoch 64: Val Loss 518076.00000
Epoch 65: Val Loss 518072.09375
Epoch 66: Val Loss 518068.15625
Epoch 67: Val Loss 518064.21875
Epoch 68: Val Loss 518060.21875
Epoch 69: Val Loss 518056.21875
Epoch 70: Val Loss 518052.15625
Epoch 71: Val Loss 518048.09375
Epoch 72: Val Loss 518044.00000
Epoch 73: Val Loss 518039.87500
Epoch 74: Val Loss 518035.71875
Epoch 75: Val Loss 518031.59375
Epoch 76: Val Loss 518027.31250
Epoch 77: Val Loss 518023.09375
Epoch 78: Val Loss 518018.87500
Epoch 79: Val Loss 518014.59375
Epoch 80: Val Loss 518010.25000
Epoch 81: Val Loss 518005.90625
Epoch 82: Val Loss 518001.59375
Epoch 83: Val Loss 517997.21875
Epoch 84: Val Loss 517992.75000
Epoch 85: Val Loss 517988.34375
Epoch 86: Val Loss 517983.87500
Epoch 87: Val Loss 517979.43750
Epoch 88: Val Loss 517974.96875
Epoch 89: Val Loss 517970.50000
Epoch 90: Val Loss 517965.90625
Epoch 91: Val Loss 517961.37500
Epoch 92: Val Loss 517956.75000
Epoch 93: Val Loss 517952.18750
Epoch 94: Val Loss 517947.50000
Epoch 95: Val Loss 517942.87500
Epoch 96: Val Loss 517938.21875
Epoch 97: Val Loss 517933.56250
Epoch 98: Val Loss 517928.84375
Epoch 99: Val Loss 517924.15625
{'MSE - mean': 519467.2708469343, 'MSE - std': 3342.1151948284305, 'R2 - mean': -60.425119119568386, 'R2 - std': 5.3792993449453155} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520334.62500
Epoch 1: Val Loss 520330.53125
Epoch 2: Val Loss 520326.37500
Epoch 3: Val Loss 520322.21875
Epoch 4: Val Loss 520318.09375
Epoch 5: Val Loss 520313.93750
Epoch 6: Val Loss 520309.87500
Epoch 7: Val Loss 520305.62500
Epoch 8: Val Loss 520301.50000
Epoch 9: Val Loss 520297.34375
Epoch 10: Val Loss 520293.18750
Epoch 11: Val Loss 520289.03125
Epoch 12: Val Loss 520284.78125
Epoch 13: Val Loss 520280.62500
Epoch 14: Val Loss 520276.43750
Epoch 15: Val Loss 520272.25000
Epoch 16: Val Loss 520267.96875
Epoch 17: Val Loss 520263.71875
Epoch 18: Val Loss 520259.46875
Epoch 19: Val Loss 520255.25000
Epoch 20: Val Loss 520250.96875
Epoch 21: Val Loss 520246.71875
Epoch 22: Val Loss 520242.37500
Epoch 23: Val Loss 520238.09375
Epoch 24: Val Loss 520233.81250
Epoch 25: Val Loss 520229.43750
Epoch 26: Val Loss 520225.06250
Epoch 27: Val Loss 520220.71875
Epoch 28: Val Loss 520216.34375
Epoch 29: Val Loss 520211.90625
Epoch 30: Val Loss 520207.50000
Epoch 31: Val Loss 520203.03125
Epoch 32: Val Loss 520198.56250
Epoch 33: Val Loss 520194.06250
Epoch 34: Val Loss 520189.56250
Epoch 35: Val Loss 520185.03125
Epoch 36: Val Loss 520180.40625
Epoch 37: Val Loss 520175.87500
Epoch 38: Val Loss 520171.21875
Epoch 39: Val Loss 520166.62500
Epoch 40: Val Loss 520161.96875
Epoch 41: Val Loss 520157.28125
Epoch 42: Val Loss 520152.53125
Epoch 43: Val Loss 520147.78125
Epoch 44: Val Loss 520143.03125
Epoch 45: Val Loss 520138.34375
Epoch 46: Val Loss 520133.56250
Epoch 47: Val Loss 520128.71875
Epoch 48: Val Loss 520123.81250
Epoch 49: Val Loss 520118.93750
Epoch 50: Val Loss 520114.06250
Epoch 51: Val Loss 520109.09375
Epoch 52: Val Loss 520104.12500
Epoch 53: Val Loss 520099.12500
Epoch 54: Val Loss 520094.09375
Epoch 55: Val Loss 520089.03125
Epoch 56: Val Loss 520084.00000
Epoch 57: Val Loss 520078.81250
Epoch 58: Val Loss 520073.62500
Epoch 59: Val Loss 520068.46875
Epoch 60: Val Loss 520063.25000
Epoch 61: Val Loss 520058.00000
Epoch 62: Val Loss 520052.78125
Epoch 63: Val Loss 520047.46875
Epoch 64: Val Loss 520042.12500
Epoch 65: Val Loss 520036.71875
Epoch 66: Val Loss 520031.34375
Epoch 67: Val Loss 520025.87500
Epoch 68: Val Loss 520020.40625
Epoch 69: Val Loss 520014.84375
Epoch 70: Val Loss 520009.25000
Epoch 71: Val Loss 520003.65625
Epoch 72: Val Loss 519998.00000
Epoch 73: Val Loss 519992.37500
Epoch 74: Val Loss 519986.62500
Epoch 75: Val Loss 519980.81250
Epoch 76: Val Loss 519975.00000
Epoch 77: Val Loss 519969.15625
Epoch 78: Val Loss 519963.25000
Epoch 79: Val Loss 519957.34375
Epoch 80: Val Loss 519951.37500
Epoch 81: Val Loss 519945.28125
Epoch 82: Val Loss 519939.15625
Epoch 83: Val Loss 519933.09375
Epoch 84: Val Loss 519926.93750
Epoch 85: Val Loss 519920.75000
Epoch 86: Val Loss 519914.59375
Epoch 87: Val Loss 519908.28125
Epoch 88: Val Loss 519901.93750
Epoch 89: Val Loss 519895.56250
Epoch 90: Val Loss 519889.06250
Epoch 91: Val Loss 519882.65625
Epoch 92: Val Loss 519876.12500
Epoch 93: Val Loss 519869.56250
Epoch 94: Val Loss 519862.90625
Epoch 95: Val Loss 519856.25000
Epoch 96: Val Loss 519849.53125
Epoch 97: Val Loss 519842.81250
Epoch 98: Val Loss 519836.09375
Epoch 99: Val Loss 519829.18750
{'MSE - mean': 519557.75361987524, 'MSE - std': 2898.596535950556, 'R2 - mean': -60.715948725458645, 'R2 - std': 4.68576478932561} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518278.09375
Epoch 1: Val Loss 518276.34375
Epoch 2: Val Loss 518274.59375
Epoch 3: Val Loss 518272.81250
Epoch 4: Val Loss 518271.06250
Epoch 5: Val Loss 518269.28125
Epoch 6: Val Loss 518267.50000
Epoch 7: Val Loss 518265.71875
Epoch 8: Val Loss 518263.90625
Epoch 9: Val Loss 518262.12500
Epoch 10: Val Loss 518260.31250
Epoch 11: Val Loss 518258.43750
Epoch 12: Val Loss 518256.65625
Epoch 13: Val Loss 518254.81250
Epoch 14: Val Loss 518253.00000
Epoch 15: Val Loss 518251.09375
Epoch 16: Val Loss 518249.28125
Epoch 17: Val Loss 518247.40625
Epoch 18: Val Loss 518245.50000
Epoch 19: Val Loss 518243.62500
Epoch 20: Val Loss 518241.75000
Epoch 21: Val Loss 518239.84375
Epoch 22: Val Loss 518237.96875
Epoch 23: Val Loss 518236.09375
Epoch 24: Val Loss 518234.12500
Epoch 25: Val Loss 518232.25000
Epoch 26: Val Loss 518230.34375
Epoch 27: Val Loss 518228.43750
Epoch 28: Val Loss 518226.43750
Epoch 29: Val Loss 518224.50000
Epoch 30: Val Loss 518222.56250
Epoch 31: Val Loss 518220.56250
Epoch 32: Val Loss 518218.59375
Epoch 33: Val Loss 518216.62500
Epoch 34: Val Loss 518214.62500
Epoch 35: Val Loss 518212.62500
Epoch 36: Val Loss 518210.62500
Epoch 37: Val Loss 518208.59375
Epoch 38: Val Loss 518206.56250
Epoch 39: Val Loss 518204.50000
Epoch 40: Val Loss 518202.40625
Epoch 41: Val Loss 518200.37500
Epoch 42: Val Loss 518198.21875
Epoch 43: Val Loss 518196.15625
Epoch 44: Val Loss 518194.00000
Epoch 45: Val Loss 518191.84375
Epoch 46: Val Loss 518189.75000
Epoch 47: Val Loss 518187.56250
Epoch 48: Val Loss 518185.40625
Epoch 49: Val Loss 518183.18750
Epoch 50: Val Loss 518181.03125
Epoch 51: Val Loss 518178.78125
Epoch 52: Val Loss 518176.56250
Epoch 53: Val Loss 518174.28125
Epoch 54: Val Loss 518172.09375
Epoch 55: Val Loss 518169.84375
Epoch 56: Val Loss 518167.56250
Epoch 57: Val Loss 518165.25000
Epoch 58: Val Loss 518162.96875
Epoch 59: Val Loss 518160.68750
Epoch 60: Val Loss 518158.40625
Epoch 61: Val Loss 518156.09375
Epoch 62: Val Loss 518153.75000
Epoch 63: Val Loss 518151.40625
Epoch 64: Val Loss 518149.06250
Epoch 65: Val Loss 518146.68750
Epoch 66: Val Loss 518144.28125
Epoch 67: Val Loss 518141.90625
Epoch 68: Val Loss 518139.43750
Epoch 69: Val Loss 518137.00000
Epoch 70: Val Loss 518134.53125
Epoch 71: Val Loss 518132.00000
Epoch 72: Val Loss 518129.53125
Epoch 73: Val Loss 518127.00000
Epoch 74: Val Loss 518124.46875
Epoch 75: Val Loss 518121.93750
Epoch 76: Val Loss 518119.34375
Epoch 77: Val Loss 518116.68750
Epoch 78: Val Loss 518114.12500
Epoch 79: Val Loss 518111.46875
Epoch 80: Val Loss 518108.84375
Epoch 81: Val Loss 518106.25000
Epoch 82: Val Loss 518103.59375
Epoch 83: Val Loss 518100.93750
Epoch 84: Val Loss 518098.25000
Epoch 85: Val Loss 518095.53125
Epoch 86: Val Loss 518092.78125
Epoch 87: Val Loss 518090.00000
Epoch 88: Val Loss 518087.25000
Epoch 89: Val Loss 518084.46875
Epoch 90: Val Loss 518081.65625
Epoch 91: Val Loss 518078.78125
Epoch 92: Val Loss 518075.93750
Epoch 93: Val Loss 518073.09375
Epoch 94: Val Loss 518070.18750
Epoch 95: Val Loss 518067.25000
Epoch 96: Val Loss 518064.37500
Epoch 97: Val Loss 518061.46875
Epoch 98: Val Loss 518058.53125
Epoch 99: Val Loss 518055.56250
{'MSE - mean': 519257.31805584126, 'MSE - std': 2661.303367736466, 'R2 - mean': -61.35980201044522, 'R2 - std': 4.384438566003517} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 519257.31805584126 and parameters: {'dim': 32, 'depth': 1, 'heads': 8, 'weight_decay': -3, 'learning_rate': -5, 'dropout': 0.2}. Best is trial 2 with value: 519257.31805584126.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516748.84375
Epoch 1: Val Loss 516744.62500
Epoch 2: Val Loss 516740.40625
Epoch 3: Val Loss 516736.28125
Epoch 4: Val Loss 516732.21875
Epoch 5: Val Loss 516728.18750
Epoch 6: Val Loss 516724.09375
Epoch 7: Val Loss 516720.06250
Epoch 8: Val Loss 516716.15625
Epoch 9: Val Loss 516712.25000
Epoch 10: Val Loss 516708.34375
Epoch 11: Val Loss 516704.46875
Epoch 12: Val Loss 516700.65625
Epoch 13: Val Loss 516696.84375
Epoch 14: Val Loss 516693.03125
Epoch 15: Val Loss 516689.31250
Epoch 16: Val Loss 516685.62500
Epoch 17: Val Loss 516682.00000
Epoch 18: Val Loss 516678.37500
Epoch 19: Val Loss 516674.71875
Epoch 20: Val Loss 516671.25000
Epoch 21: Val Loss 516667.81250
Epoch 22: Val Loss 516664.28125
Epoch 23: Val Loss 516660.87500
Epoch 24: Val Loss 516657.37500
Epoch 25: Val Loss 516654.00000
Epoch 26: Val Loss 516650.62500
Epoch 27: Val Loss 516647.31250
Epoch 28: Val Loss 516643.96875
Epoch 29: Val Loss 516640.65625
Epoch 30: Val Loss 516637.34375
Epoch 31: Val Loss 516634.12500
Epoch 32: Val Loss 516630.87500
Epoch 33: Val Loss 516627.62500
Epoch 34: Val Loss 516624.40625
Epoch 35: Val Loss 516621.18750
Epoch 36: Val Loss 516618.00000
Epoch 37: Val Loss 516614.84375
Epoch 38: Val Loss 516611.75000
Epoch 39: Val Loss 516608.56250
Epoch 40: Val Loss 516605.43750
Epoch 41: Val Loss 516602.28125
Epoch 42: Val Loss 516599.12500
Epoch 43: Val Loss 516596.06250
Epoch 44: Val Loss 516592.93750
Epoch 45: Val Loss 516589.87500
Epoch 46: Val Loss 516586.84375
Epoch 47: Val Loss 516583.78125
Epoch 48: Val Loss 516580.68750
Epoch 49: Val Loss 516577.62500
Epoch 50: Val Loss 516574.56250
Epoch 51: Val Loss 516571.53125
Epoch 52: Val Loss 516568.50000
Epoch 53: Val Loss 516565.46875
Epoch 54: Val Loss 516562.46875
Epoch 55: Val Loss 516559.46875
Epoch 56: Val Loss 516556.43750
Epoch 57: Val Loss 516553.46875
Epoch 58: Val Loss 516550.50000
Epoch 59: Val Loss 516547.53125
Epoch 60: Val Loss 516544.62500
Epoch 61: Val Loss 516541.65625
Epoch 62: Val Loss 516538.68750
Epoch 63: Val Loss 516535.71875
Epoch 64: Val Loss 516532.78125
Epoch 65: Val Loss 516529.78125
Epoch 66: Val Loss 516526.87500
Epoch 67: Val Loss 516523.90625
Epoch 68: Val Loss 516521.00000
Epoch 69: Val Loss 516518.03125
Epoch 70: Val Loss 516515.06250
Epoch 71: Val Loss 516512.12500
Epoch 72: Val Loss 516509.15625
Epoch 73: Val Loss 516506.25000
Epoch 74: Val Loss 516503.34375
Epoch 75: Val Loss 516500.34375
Epoch 76: Val Loss 516497.43750
Epoch 77: Val Loss 516494.46875
Epoch 78: Val Loss 516491.50000
Epoch 79: Val Loss 516488.50000
Epoch 80: Val Loss 516485.53125
Epoch 81: Val Loss 516482.53125
Epoch 82: Val Loss 516479.56250
Epoch 83: Val Loss 516476.50000
Epoch 84: Val Loss 516473.53125
Epoch 85: Val Loss 516470.50000
Epoch 86: Val Loss 516467.50000
Epoch 87: Val Loss 516464.46875
Epoch 88: Val Loss 516461.46875
Epoch 89: Val Loss 516458.43750
Epoch 90: Val Loss 516455.40625
Epoch 91: Val Loss 516452.40625
Epoch 92: Val Loss 516449.31250
Epoch 93: Val Loss 516446.34375
Epoch 94: Val Loss 516443.28125
Epoch 95: Val Loss 516440.25000
Epoch 96: Val Loss 516437.21875
Epoch 97: Val Loss 516434.15625
Epoch 98: Val Loss 516431.09375
Epoch 99: Val Loss 516428.06250
{'MSE - mean': 516428.0269027258, 'MSE - std': 0.0, 'R2 - mean': -59.04558058820887, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525166.68750
Epoch 1: Val Loss 525164.56250
Epoch 2: Val Loss 525162.43750
Epoch 3: Val Loss 525160.31250
Epoch 4: Val Loss 525158.12500
Epoch 5: Val Loss 525156.00000
Epoch 6: Val Loss 525153.87500
Epoch 7: Val Loss 525151.75000
Epoch 8: Val Loss 525149.62500
Epoch 9: Val Loss 525147.50000
Epoch 10: Val Loss 525145.37500
Epoch 11: Val Loss 525143.25000
Epoch 12: Val Loss 525141.18750
Epoch 13: Val Loss 525139.06250
Epoch 14: Val Loss 525137.00000
Epoch 15: Val Loss 525134.87500
Epoch 16: Val Loss 525132.81250
Epoch 17: Val Loss 525130.75000
Epoch 18: Val Loss 525128.75000
Epoch 19: Val Loss 525126.56250
Epoch 20: Val Loss 525124.56250
Epoch 21: Val Loss 525122.43750
Epoch 22: Val Loss 525120.31250
Epoch 23: Val Loss 525118.25000
Epoch 24: Val Loss 525116.12500
Epoch 25: Val Loss 525114.06250
Epoch 26: Val Loss 525111.93750
Epoch 27: Val Loss 525109.75000
Epoch 28: Val Loss 525107.62500
Epoch 29: Val Loss 525105.50000
Epoch 30: Val Loss 525103.37500
Epoch 31: Val Loss 525101.25000
Epoch 32: Val Loss 525099.06250
Epoch 33: Val Loss 525096.93750
Epoch 34: Val Loss 525094.75000
Epoch 35: Val Loss 525092.62500
Epoch 36: Val Loss 525090.43750
Epoch 37: Val Loss 525088.25000
Epoch 38: Val Loss 525086.06250
Epoch 39: Val Loss 525083.81250
Epoch 40: Val Loss 525081.62500
Epoch 41: Val Loss 525079.43750
Epoch 42: Val Loss 525077.25000
Epoch 43: Val Loss 525074.93750
Epoch 44: Val Loss 525072.68750
Epoch 45: Val Loss 525070.43750
Epoch 46: Val Loss 525068.12500
Epoch 47: Val Loss 525065.81250
Epoch 48: Val Loss 525063.50000
Epoch 49: Val Loss 525061.18750
Epoch 50: Val Loss 525058.81250
Epoch 51: Val Loss 525056.56250
Epoch 52: Val Loss 525054.12500
Epoch 53: Val Loss 525051.75000
Epoch 54: Val Loss 525049.31250
Epoch 55: Val Loss 525047.00000
Epoch 56: Val Loss 525044.56250
Epoch 57: Val Loss 525042.12500
Epoch 58: Val Loss 525039.68750
Epoch 59: Val Loss 525037.25000
Epoch 60: Val Loss 525034.75000
Epoch 61: Val Loss 525032.31250
Epoch 62: Val Loss 525029.75000
Epoch 63: Val Loss 525027.12500
Epoch 64: Val Loss 525024.62500
Epoch 65: Val Loss 525022.00000
Epoch 66: Val Loss 525019.37500
Epoch 67: Val Loss 525016.75000
Epoch 68: Val Loss 525014.06250
Epoch 69: Val Loss 525011.37500
Epoch 70: Val Loss 525008.62500
Epoch 71: Val Loss 525005.81250
Epoch 72: Val Loss 525003.06250
Epoch 73: Val Loss 525000.25000
Epoch 74: Val Loss 524997.50000
Epoch 75: Val Loss 524994.62500
Epoch 76: Val Loss 524991.75000
Epoch 77: Val Loss 524988.81250
Epoch 78: Val Loss 524985.93750
Epoch 79: Val Loss 524983.00000
Epoch 80: Val Loss 524979.93750
Epoch 81: Val Loss 524977.00000
Epoch 82: Val Loss 524973.93750
Epoch 83: Val Loss 524970.87500
Epoch 84: Val Loss 524967.75000
Epoch 85: Val Loss 524964.62500
Epoch 86: Val Loss 524961.56250
Epoch 87: Val Loss 524958.31250
Epoch 88: Val Loss 524955.12500
Epoch 89: Val Loss 524952.00000
Epoch 90: Val Loss 524948.68750
Epoch 91: Val Loss 524945.50000
Epoch 92: Val Loss 524942.12500
Epoch 93: Val Loss 524938.81250
Epoch 94: Val Loss 524935.43750
Epoch 95: Val Loss 524932.12500
Epoch 96: Val Loss 524928.75000
Epoch 97: Val Loss 524925.31250
Epoch 98: Val Loss 524921.81250
Epoch 99: Val Loss 524918.37500
{'MSE - mean': 520673.17767533654, 'MSE - std': 4245.1507726107375, 'R2 - mean': -63.373946511819526, 'R2 - std': 4.328365923610651} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518586.06250
Epoch 1: Val Loss 518582.93750
Epoch 2: Val Loss 518579.87500
Epoch 3: Val Loss 518576.75000
Epoch 4: Val Loss 518573.65625
Epoch 5: Val Loss 518570.59375
Epoch 6: Val Loss 518567.46875
Epoch 7: Val Loss 518564.40625
Epoch 8: Val Loss 518561.34375
Epoch 9: Val Loss 518558.28125
Epoch 10: Val Loss 518555.25000
Epoch 11: Val Loss 518552.25000
Epoch 12: Val Loss 518549.18750
Epoch 13: Val Loss 518546.18750
Epoch 14: Val Loss 518543.18750
Epoch 15: Val Loss 518540.15625
Epoch 16: Val Loss 518537.21875
Epoch 17: Val Loss 518534.21875
Epoch 18: Val Loss 518531.18750
Epoch 19: Val Loss 518528.21875
Epoch 20: Val Loss 518525.21875
Epoch 21: Val Loss 518522.25000
Epoch 22: Val Loss 518519.25000
Epoch 23: Val Loss 518516.25000
Epoch 24: Val Loss 518513.21875
Epoch 25: Val Loss 518510.25000
Epoch 26: Val Loss 518507.25000
Epoch 27: Val Loss 518504.31250
Epoch 28: Val Loss 518501.25000
Epoch 29: Val Loss 518498.28125
Epoch 30: Val Loss 518495.25000
Epoch 31: Val Loss 518492.28125
Epoch 32: Val Loss 518489.28125
Epoch 33: Val Loss 518486.25000
Epoch 34: Val Loss 518483.21875
Epoch 35: Val Loss 518480.21875
Epoch 36: Val Loss 518477.15625
Epoch 37: Val Loss 518474.12500
Epoch 38: Val Loss 518471.03125
Epoch 39: Val Loss 518467.96875
Epoch 40: Val Loss 518464.96875
Epoch 41: Val Loss 518461.90625
Epoch 42: Val Loss 518458.84375
Epoch 43: Val Loss 518455.81250
Epoch 44: Val Loss 518452.78125
Epoch 45: Val Loss 518449.65625
Epoch 46: Val Loss 518446.68750
Epoch 47: Val Loss 518443.62500
Epoch 48: Val Loss 518440.56250
Epoch 49: Val Loss 518437.46875
Epoch 50: Val Loss 518434.40625
Epoch 51: Val Loss 518431.34375
Epoch 52: Val Loss 518428.34375
Epoch 53: Val Loss 518425.25000
Epoch 54: Val Loss 518422.15625
Epoch 55: Val Loss 518419.12500
Epoch 56: Val Loss 518416.00000
Epoch 57: Val Loss 518412.93750
Epoch 58: Val Loss 518409.87500
Epoch 59: Val Loss 518406.81250
Epoch 60: Val Loss 518403.65625
Epoch 61: Val Loss 518400.56250
Epoch 62: Val Loss 518397.43750
Epoch 63: Val Loss 518394.31250
Epoch 64: Val Loss 518391.15625
Epoch 65: Val Loss 518388.00000
Epoch 66: Val Loss 518384.78125
Epoch 67: Val Loss 518381.56250
Epoch 68: Val Loss 518378.37500
Epoch 69: Val Loss 518375.15625
Epoch 70: Val Loss 518371.93750
Epoch 71: Val Loss 518368.68750
Epoch 72: Val Loss 518365.40625
Epoch 73: Val Loss 518362.12500
Epoch 74: Val Loss 518358.84375
Epoch 75: Val Loss 518355.50000
Epoch 76: Val Loss 518352.18750
Epoch 77: Val Loss 518348.84375
Epoch 78: Val Loss 518345.46875
Epoch 79: Val Loss 518342.09375
Epoch 80: Val Loss 518338.68750
Epoch 81: Val Loss 518335.31250
Epoch 82: Val Loss 518331.87500
Epoch 83: Val Loss 518328.40625
Epoch 84: Val Loss 518324.96875
Epoch 85: Val Loss 518321.50000
Epoch 86: Val Loss 518317.96875
Epoch 87: Val Loss 518314.43750
Epoch 88: Val Loss 518310.87500
Epoch 89: Val Loss 518307.25000
Epoch 90: Val Loss 518303.71875
Epoch 91: Val Loss 518300.03125
Epoch 92: Val Loss 518296.43750
Epoch 93: Val Loss 518292.78125
Epoch 94: Val Loss 518289.06250
Epoch 95: Val Loss 518285.34375
Epoch 96: Val Loss 518281.59375
Epoch 97: Val Loss 518277.90625
Epoch 98: Val Loss 518274.06250
Epoch 99: Val Loss 518270.34375
{'MSE - mean': 519872.2415412978, 'MSE - std': 3646.532705152868, 'R2 - mean': -60.475132762109176, 'R2 - std': 5.412585028443942} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521353.75000
Epoch 1: Val Loss 521351.18750
Epoch 2: Val Loss 521348.65625
Epoch 3: Val Loss 521346.12500
Epoch 4: Val Loss 521343.65625
Epoch 5: Val Loss 521341.21875
Epoch 6: Val Loss 521338.71875
Epoch 7: Val Loss 521336.25000
Epoch 8: Val Loss 521333.81250
Epoch 9: Val Loss 521331.37500
Epoch 10: Val Loss 521328.96875
Epoch 11: Val Loss 521326.53125
Epoch 12: Val Loss 521324.12500
Epoch 13: Val Loss 521321.75000
Epoch 14: Val Loss 521319.37500
Epoch 15: Val Loss 521317.00000
Epoch 16: Val Loss 521314.62500
Epoch 17: Val Loss 521312.31250
Epoch 18: Val Loss 521309.96875
Epoch 19: Val Loss 521307.65625
Epoch 20: Val Loss 521305.31250
Epoch 21: Val Loss 521303.03125
Epoch 22: Val Loss 521300.68750
Epoch 23: Val Loss 521298.37500
Epoch 24: Val Loss 521296.12500
Epoch 25: Val Loss 521293.84375
Epoch 26: Val Loss 521291.56250
Epoch 27: Val Loss 521289.28125
Epoch 28: Val Loss 521287.00000
Epoch 29: Val Loss 521284.71875
Epoch 30: Val Loss 521282.43750
Epoch 31: Val Loss 521280.18750
Epoch 32: Val Loss 521277.93750
Epoch 33: Val Loss 521275.62500
Epoch 34: Val Loss 521273.43750
Epoch 35: Val Loss 521271.12500
Epoch 36: Val Loss 521268.90625
Epoch 37: Val Loss 521266.65625
Epoch 38: Val Loss 521264.37500
Epoch 39: Val Loss 521262.12500
Epoch 40: Val Loss 521259.84375
Epoch 41: Val Loss 521257.62500
Epoch 42: Val Loss 521255.43750
Epoch 43: Val Loss 521253.09375
Epoch 44: Val Loss 521250.90625
Epoch 45: Val Loss 521248.62500
Epoch 46: Val Loss 521246.40625
Epoch 47: Val Loss 521244.18750
Epoch 48: Val Loss 521241.87500
Epoch 49: Val Loss 521239.62500
Epoch 50: Val Loss 521237.40625
Epoch 51: Val Loss 521235.18750
Epoch 52: Val Loss 521232.96875
Epoch 53: Val Loss 521230.75000
Epoch 54: Val Loss 521228.50000
Epoch 55: Val Loss 521226.25000
Epoch 56: Val Loss 521224.03125
Epoch 57: Val Loss 521221.84375
Epoch 58: Val Loss 521219.56250
Epoch 59: Val Loss 521217.34375
Epoch 60: Val Loss 521215.12500
Epoch 61: Val Loss 521212.93750
Epoch 62: Val Loss 521210.71875
Epoch 63: Val Loss 521208.50000
Epoch 64: Val Loss 521206.28125
Epoch 65: Val Loss 521204.09375
Epoch 66: Val Loss 521201.84375
Epoch 67: Val Loss 521199.59375
Epoch 68: Val Loss 521197.37500
Epoch 69: Val Loss 521195.15625
Epoch 70: Val Loss 521192.90625
Epoch 71: Val Loss 521190.68750
Epoch 72: Val Loss 521188.50000
Epoch 73: Val Loss 521186.21875
Epoch 74: Val Loss 521184.00000
Epoch 75: Val Loss 521181.78125
Epoch 76: Val Loss 521179.50000
Epoch 77: Val Loss 521177.28125
Epoch 78: Val Loss 521175.00000
Epoch 79: Val Loss 521172.75000
Epoch 80: Val Loss 521170.53125
Epoch 81: Val Loss 521168.21875
Epoch 82: Val Loss 521165.96875
Epoch 83: Val Loss 521163.75000
Epoch 84: Val Loss 521161.40625
Epoch 85: Val Loss 521159.12500
Epoch 86: Val Loss 521156.84375
Epoch 87: Val Loss 521154.56250
Epoch 88: Val Loss 521152.25000
Epoch 89: Val Loss 521149.96875
Epoch 90: Val Loss 521147.56250
Epoch 91: Val Loss 521145.25000
Epoch 92: Val Loss 521142.87500
Epoch 93: Val Loss 521140.56250
Epoch 94: Val Loss 521138.15625
Epoch 95: Val Loss 521135.78125
Epoch 96: Val Loss 521133.37500
Epoch 97: Val Loss 521131.03125
Epoch 98: Val Loss 521128.62500
Epoch 99: Val Loss 521126.21875
{'MSE - mean': 520185.73835210985, 'MSE - std': 3204.331650825193, 'R2 - mean': -60.792500090216315, 'R2 - std': 4.719557561938538} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517942.75000
Epoch 1: Val Loss 517939.78125
Epoch 2: Val Loss 517936.81250
Epoch 3: Val Loss 517933.90625
Epoch 4: Val Loss 517931.00000
Epoch 5: Val Loss 517928.09375
Epoch 6: Val Loss 517925.18750
Epoch 7: Val Loss 517922.34375
Epoch 8: Val Loss 517919.46875
Epoch 9: Val Loss 517916.62500
Epoch 10: Val Loss 517913.81250
Epoch 11: Val Loss 517910.93750
Epoch 12: Val Loss 517908.12500
Epoch 13: Val Loss 517905.28125
Epoch 14: Val Loss 517902.43750
Epoch 15: Val Loss 517899.65625
Epoch 16: Val Loss 517896.84375
Epoch 17: Val Loss 517894.03125
Epoch 18: Val Loss 517891.25000
Epoch 19: Val Loss 517888.50000
Epoch 20: Val Loss 517885.78125
Epoch 21: Val Loss 517882.96875
Epoch 22: Val Loss 517880.18750
Epoch 23: Val Loss 517877.43750
Epoch 24: Val Loss 517874.65625
Epoch 25: Val Loss 517871.84375
Epoch 26: Val Loss 517869.06250
Epoch 27: Val Loss 517866.28125
Epoch 28: Val Loss 517863.46875
Epoch 29: Val Loss 517860.68750
Epoch 30: Val Loss 517857.93750
Epoch 31: Val Loss 517855.09375
Epoch 32: Val Loss 517852.34375
Epoch 33: Val Loss 517849.46875
Epoch 34: Val Loss 517846.68750
Epoch 35: Val Loss 517843.87500
Epoch 36: Val Loss 517841.06250
Epoch 37: Val Loss 517838.21875
Epoch 38: Val Loss 517835.37500
Epoch 39: Val Loss 517832.53125
Epoch 40: Val Loss 517829.71875
Epoch 41: Val Loss 517826.84375
Epoch 42: Val Loss 517824.00000
Epoch 43: Val Loss 517821.15625
Epoch 44: Val Loss 517818.25000
Epoch 45: Val Loss 517815.46875
Epoch 46: Val Loss 517812.53125
Epoch 47: Val Loss 517809.62500
Epoch 48: Val Loss 517806.78125
Epoch 49: Val Loss 517803.87500
Epoch 50: Val Loss 517801.03125
Epoch 51: Val Loss 517798.03125
Epoch 52: Val Loss 517795.09375
Epoch 53: Val Loss 517792.15625
Epoch 54: Val Loss 517789.21875
Epoch 55: Val Loss 517786.25000
Epoch 56: Val Loss 517783.25000
Epoch 57: Val Loss 517780.25000
Epoch 58: Val Loss 517777.28125
Epoch 59: Val Loss 517774.25000
Epoch 60: Val Loss 517771.21875
Epoch 61: Val Loss 517768.18750
Epoch 62: Val Loss 517765.12500
Epoch 63: Val Loss 517762.06250
Epoch 64: Val Loss 517758.93750
Epoch 65: Val Loss 517755.87500
Epoch 66: Val Loss 517752.75000
Epoch 67: Val Loss 517749.62500
Epoch 68: Val Loss 517746.40625
Epoch 69: Val Loss 517743.25000
Epoch 70: Val Loss 517740.00000
Epoch 71: Val Loss 517736.84375
Epoch 72: Val Loss 517733.59375
Epoch 73: Val Loss 517730.31250
Epoch 74: Val Loss 517727.03125
Epoch 75: Val Loss 517723.71875
Epoch 76: Val Loss 517720.37500
Epoch 77: Val Loss 517717.03125
Epoch 78: Val Loss 517713.62500
Epoch 79: Val Loss 517710.28125
Epoch 80: Val Loss 517706.81250
Epoch 81: Val Loss 517703.37500
Epoch 82: Val Loss 517699.84375
Epoch 83: Val Loss 517696.25000
Epoch 84: Val Loss 517692.71875
Epoch 85: Val Loss 517689.09375
Epoch 86: Val Loss 517685.46875
Epoch 87: Val Loss 517681.81250
Epoch 88: Val Loss 517678.09375
Epoch 89: Val Loss 517674.37500
Epoch 90: Val Loss 517670.62500
Epoch 91: Val Loss 517666.78125
Epoch 92: Val Loss 517662.90625
Epoch 93: Val Loss 517658.96875
Epoch 94: Val Loss 517655.03125
Epoch 95: Val Loss 517650.96875
Epoch 96: Val Loss 517646.93750
Epoch 97: Val Loss 517642.87500
Epoch 98: Val Loss 517638.71875
Epoch 99: Val Loss 517634.53125
{'MSE - mean': 519675.503052733, 'MSE - std': 3042.2940859990417, 'R2 - mean': -61.41048877860611, 'R2 - std': 4.398524632222941} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 519675.503052733 and parameters: {'dim': 64, 'depth': 3, 'heads': 8, 'weight_decay': -6, 'learning_rate': -5, 'dropout': 0.1}. Best is trial 2 with value: 519257.31805584126.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516384.46875
Epoch 1: Val Loss 515876.21875
Epoch 2: Val Loss 514795.75000
Epoch 3: Val Loss 512649.59375
Epoch 4: Val Loss 508656.34375
Epoch 5: Val Loss 501644.43750
Epoch 6: Val Loss 489938.00000
Epoch 7: Val Loss 471261.75000
Epoch 8: Val Loss 443095.90625
Epoch 9: Val Loss 402843.03125
Epoch 10: Val Loss 349096.56250
Epoch 11: Val Loss 283867.15625
Epoch 12: Val Loss 211924.35938
Epoch 13: Val Loss 143148.26562
Epoch 14: Val Loss 86977.62500
Epoch 15: Val Loss 50677.60156
Epoch 16: Val Loss 32196.47266
Epoch 17: Val Loss 23802.39453
Epoch 18: Val Loss 19509.96875
Epoch 19: Val Loss 16469.70898
Epoch 20: Val Loss 14207.04492
Epoch 21: Val Loss 12499.79980
Epoch 22: Val Loss 11151.68848
Epoch 23: Val Loss 10048.43848
Epoch 24: Val Loss 9141.28320
Epoch 25: Val Loss 8434.17188
Epoch 26: Val Loss 7830.47607
Epoch 27: Val Loss 7321.03223
Epoch 28: Val Loss 6921.10059
Epoch 29: Val Loss 6588.52637
Epoch 30: Val Loss 6279.88818
Epoch 31: Val Loss 6046.47705
Epoch 32: Val Loss 5836.06299
Epoch 33: Val Loss 5660.49805
Epoch 34: Val Loss 5509.71387
Epoch 35: Val Loss 5372.92432
Epoch 36: Val Loss 5253.76758
Epoch 37: Val Loss 5138.36572
Epoch 38: Val Loss 5036.56396
Epoch 39: Val Loss 4947.91699
Epoch 40: Val Loss 4869.83936
Epoch 41: Val Loss 4790.18311
Epoch 42: Val Loss 4716.50635
Epoch 43: Val Loss 4653.51221
Epoch 44: Val Loss 4598.09521
Epoch 45: Val Loss 4534.53027
Epoch 46: Val Loss 4490.18311
Epoch 47: Val Loss 4447.02197
Epoch 48: Val Loss 4394.84326
Epoch 49: Val Loss 4354.34521
Epoch 50: Val Loss 4315.76904
Epoch 51: Val Loss 4280.53760
Epoch 52: Val Loss 4249.14111
Epoch 53: Val Loss 4209.75586
Epoch 54: Val Loss 4181.62744
Epoch 55: Val Loss 4148.47412
Epoch 56: Val Loss 4118.84863
Epoch 57: Val Loss 4090.14038
Epoch 58: Val Loss 4064.04272
Epoch 59: Val Loss 4035.90747
Epoch 60: Val Loss 4015.76782
Epoch 61: Val Loss 3981.62109
Epoch 62: Val Loss 3964.82349
Epoch 63: Val Loss 3939.54517
Epoch 64: Val Loss 3909.96680
Epoch 65: Val Loss 3883.31787
Epoch 66: Val Loss 3857.82202
Epoch 67: Val Loss 3843.10571
Epoch 68: Val Loss 3809.89355
Epoch 69: Val Loss 3787.34839
Epoch 70: Val Loss 3767.48901
Epoch 71: Val Loss 3748.31201
Epoch 72: Val Loss 3715.80981
Epoch 73: Val Loss 3698.26099
Epoch 74: Val Loss 3670.53784
Epoch 75: Val Loss 3647.17676
Epoch 76: Val Loss 3625.77734
Epoch 77: Val Loss 3599.31982
Epoch 78: Val Loss 3576.94946
Epoch 79: Val Loss 3560.81274
Epoch 80: Val Loss 3534.07983
Epoch 81: Val Loss 3512.69897
Epoch 82: Val Loss 3500.32104
Epoch 83: Val Loss 3469.26978
Epoch 84: Val Loss 3453.44849
Epoch 85: Val Loss 3421.22070
Epoch 86: Val Loss 3401.61377
Epoch 87: Val Loss 3388.00171
Epoch 88: Val Loss 3358.93188
Epoch 89: Val Loss 3341.13770
Epoch 90: Val Loss 3315.02515
Epoch 91: Val Loss 3292.56934
Epoch 92: Val Loss 3290.29614
Epoch 93: Val Loss 3253.53809
Epoch 94: Val Loss 3240.65991
Epoch 95: Val Loss 3217.47217
Epoch 96: Val Loss 3204.02173
Epoch 97: Val Loss 3176.77734
Epoch 98: Val Loss 3159.89990
Epoch 99: Val Loss 3140.22290
{'MSE - mean': 3140.2230063798434, 'MSE - std': 0.0, 'R2 - mean': 0.6348832678090788, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524834.06250
Epoch 1: Val Loss 524299.75000
Epoch 2: Val Loss 523197.59375
Epoch 3: Val Loss 521026.21875
Epoch 4: Val Loss 516998.62500
Epoch 5: Val Loss 509885.15625
Epoch 6: Val Loss 498133.53125
Epoch 7: Val Loss 479777.62500
Epoch 8: Val Loss 452744.43750
Epoch 9: Val Loss 414989.46875
Epoch 10: Val Loss 365701.03125
Epoch 11: Val Loss 305352.84375
Epoch 12: Val Loss 237969.81250
Epoch 13: Val Loss 170054.07812
Epoch 14: Val Loss 110139.22656
Epoch 15: Val Loss 65584.38281
Epoch 16: Val Loss 39004.68359
Epoch 17: Val Loss 26214.03711
Epoch 18: Val Loss 20425.17383
Epoch 19: Val Loss 17141.67383
Epoch 20: Val Loss 14616.38867
Epoch 21: Val Loss 12638.41504
Epoch 22: Val Loss 11085.22070
Epoch 23: Val Loss 9861.98340
Epoch 24: Val Loss 8884.97754
Epoch 25: Val Loss 8095.93457
Epoch 26: Val Loss 7442.72705
Epoch 27: Val Loss 6898.78125
Epoch 28: Val Loss 6451.12012
Epoch 29: Val Loss 6097.54102
Epoch 30: Val Loss 5794.99414
Epoch 31: Val Loss 5541.29443
Epoch 32: Val Loss 5327.69287
Epoch 33: Val Loss 5109.56885
Epoch 34: Val Loss 4941.63281
Epoch 35: Val Loss 4799.63232
Epoch 36: Val Loss 4670.30322
Epoch 37: Val Loss 4542.12061
Epoch 38: Val Loss 4441.30322
Epoch 39: Val Loss 4346.12256
Epoch 40: Val Loss 4231.28467
Epoch 41: Val Loss 4145.44580
Epoch 42: Val Loss 4057.92480
Epoch 43: Val Loss 3979.10327
Epoch 44: Val Loss 3894.57544
Epoch 45: Val Loss 3830.89893
Epoch 46: Val Loss 3774.31885
Epoch 47: Val Loss 3694.93530
Epoch 48: Val Loss 3640.26660
Epoch 49: Val Loss 3581.68188
Epoch 50: Val Loss 3529.26025
Epoch 51: Val Loss 3466.61768
Epoch 52: Val Loss 3418.70679
Epoch 53: Val Loss 3368.35474
Epoch 54: Val Loss 3316.56128
Epoch 55: Val Loss 3299.10645
Epoch 56: Val Loss 3248.58765
Epoch 57: Val Loss 3195.12598
Epoch 58: Val Loss 3166.86133
Epoch 59: Val Loss 3134.51733
Epoch 60: Val Loss 3109.75415
Epoch 61: Val Loss 3058.95801
Epoch 62: Val Loss 3024.55811
Epoch 63: Val Loss 3001.23584
Epoch 64: Val Loss 2992.63818
Epoch 65: Val Loss 2948.44702
Epoch 66: Val Loss 2906.25000
Epoch 67: Val Loss 2893.26416
Epoch 68: Val Loss 2868.54004
Epoch 69: Val Loss 2875.90234
Epoch 70: Val Loss 2827.60693
Epoch 71: Val Loss 2797.66992
Epoch 72: Val Loss 2781.37109
Epoch 73: Val Loss 2767.58350
Epoch 74: Val Loss 2744.98633
Epoch 75: Val Loss 2702.59229
Epoch 76: Val Loss 2704.50903
Epoch 77: Val Loss 2668.08398
Epoch 78: Val Loss 2671.40259
Epoch 79: Val Loss 2635.85229
Epoch 80: Val Loss 2614.39844
Epoch 81: Val Loss 2607.98486
Epoch 82: Val Loss 2572.53979
Epoch 83: Val Loss 2567.15527
Epoch 84: Val Loss 2551.00269
Epoch 85: Val Loss 2524.36914
Epoch 86: Val Loss 2507.63477
Epoch 87: Val Loss 2494.72876
Epoch 88: Val Loss 2469.31836
Epoch 89: Val Loss 2451.01709
Epoch 90: Val Loss 2449.80469
Epoch 91: Val Loss 2431.49902
Epoch 92: Val Loss 2410.56836
Epoch 93: Val Loss 2393.21265
Epoch 94: Val Loss 2375.09277
Epoch 95: Val Loss 2359.07520
Epoch 96: Val Loss 2354.68628
Epoch 97: Val Loss 2321.96216
Epoch 98: Val Loss 2310.06226
Epoch 99: Val Loss 2302.81616
{'MSE - mean': 2721.519539527604, 'MSE - std': 418.703466852239, 'R2 - mean': 0.6667431530817634, 'R2 - std': 0.031859885272684574} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518483.31250
Epoch 1: Val Loss 518071.21875
Epoch 2: Val Loss 517239.25000
Epoch 3: Val Loss 515535.96875
Epoch 4: Val Loss 512143.75000
Epoch 5: Val Loss 505903.78125
Epoch 6: Val Loss 494976.96875
Epoch 7: Val Loss 477513.81250
Epoch 8: Val Loss 451314.78125
Epoch 9: Val Loss 414604.09375
Epoch 10: Val Loss 366795.78125
Epoch 11: Val Loss 308443.06250
Epoch 12: Val Loss 242680.14062
Epoch 13: Val Loss 175737.45312
Epoch 14: Val Loss 115558.03906
Epoch 15: Val Loss 69841.08594
Epoch 16: Val Loss 41242.69531
Epoch 17: Val Loss 26832.49805
Epoch 18: Val Loss 20048.70312
Epoch 19: Val Loss 16475.35938
Epoch 20: Val Loss 14059.51562
Epoch 21: Val Loss 12345.80762
Epoch 22: Val Loss 10881.76270
Epoch 23: Val Loss 9754.97070
Epoch 24: Val Loss 8885.62793
Epoch 25: Val Loss 8182.07373
Epoch 26: Val Loss 7630.12695
Epoch 27: Val Loss 7186.13379
Epoch 28: Val Loss 6831.05371
Epoch 29: Val Loss 6531.92432
Epoch 30: Val Loss 6267.69873
Epoch 31: Val Loss 6082.80664
Epoch 32: Val Loss 5869.28711
Epoch 33: Val Loss 5704.86035
Epoch 34: Val Loss 5524.25635
Epoch 35: Val Loss 5382.70801
Epoch 36: Val Loss 5288.39551
Epoch 37: Val Loss 5139.01416
Epoch 38: Val Loss 5001.84863
Epoch 39: Val Loss 4887.83643
Epoch 40: Val Loss 4775.24365
Epoch 41: Val Loss 4669.46680
Epoch 42: Val Loss 4571.40430
Epoch 43: Val Loss 4485.26465
Epoch 44: Val Loss 4377.96924
Epoch 45: Val Loss 4299.40723
Epoch 46: Val Loss 4226.16797
Epoch 47: Val Loss 4131.35498
Epoch 48: Val Loss 4069.58618
Epoch 49: Val Loss 4006.83691
Epoch 50: Val Loss 3918.75806
Epoch 51: Val Loss 3862.28857
Epoch 52: Val Loss 3808.10864
Epoch 53: Val Loss 3774.41064
Epoch 54: Val Loss 3695.50854
Epoch 55: Val Loss 3659.09106
Epoch 56: Val Loss 3614.71802
Epoch 57: Val Loss 3573.31226
Epoch 58: Val Loss 3523.95947
Epoch 59: Val Loss 3490.81152
Epoch 60: Val Loss 3452.48560
Epoch 61: Val Loss 3419.12427
Epoch 62: Val Loss 3396.04053
Epoch 63: Val Loss 3367.92114
Epoch 64: Val Loss 3325.44604
Epoch 65: Val Loss 3290.94434
Epoch 66: Val Loss 3263.09888
Epoch 67: Val Loss 3228.06958
Epoch 68: Val Loss 3214.20288
Epoch 69: Val Loss 3194.71021
Epoch 70: Val Loss 3156.71631
Epoch 71: Val Loss 3144.32373
Epoch 72: Val Loss 3114.42114
Epoch 73: Val Loss 3085.80127
Epoch 74: Val Loss 3075.98218
Epoch 75: Val Loss 3051.04858
Epoch 76: Val Loss 3017.85083
Epoch 77: Val Loss 2999.06079
Epoch 78: Val Loss 2975.66187
Epoch 79: Val Loss 2954.75781
Epoch 80: Val Loss 2938.37744
Epoch 81: Val Loss 2928.13818
Epoch 82: Val Loss 2892.47339
Epoch 83: Val Loss 2884.36743
Epoch 84: Val Loss 2864.38965
Epoch 85: Val Loss 2833.36157
Epoch 86: Val Loss 2824.55518
Epoch 87: Val Loss 2818.74634
Epoch 88: Val Loss 2779.19458
Epoch 89: Val Loss 2774.87646
Epoch 90: Val Loss 2744.77954
Epoch 91: Val Loss 2746.88306
Epoch 92: Val Loss 2719.89258
Epoch 93: Val Loss 2700.51025
Epoch 94: Val Loss 2704.41748
Epoch 95: Val Loss 2671.92529
Epoch 96: Val Loss 2657.25684
Epoch 97: Val Loss 2642.15088
Epoch 98: Val Loss 2624.78442
Epoch 99: Val Loss 2609.27075
{'MSE - mean': 2684.103302599524, 'MSE - std': 345.9407632448642, 'R2 - mean': 0.6843912510905849, 'R2 - std': 0.0360501352709022} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520619.59375
Epoch 1: Val Loss 520321.40625
Epoch 2: Val Loss 519840.25000
Epoch 3: Val Loss 518941.25000
Epoch 4: Val Loss 517205.90625
Epoch 5: Val Loss 514009.46875
Epoch 6: Val Loss 508487.40625
Epoch 7: Val Loss 499424.50000
Epoch 8: Val Loss 485643.56250
Epoch 9: Val Loss 465510.18750
Epoch 10: Val Loss 437739.37500
Epoch 11: Val Loss 401046.15625
Epoch 12: Val Loss 355032.81250
Epoch 13: Val Loss 300929.09375
Epoch 14: Val Loss 240870.40625
Epoch 15: Val Loss 179757.73438
Epoch 16: Val Loss 123267.35156
Epoch 17: Val Loss 77197.71094
Epoch 18: Val Loss 45285.60938
Epoch 19: Val Loss 27272.70312
Epoch 20: Val Loss 18676.34766
Epoch 21: Val Loss 14893.82227
Epoch 22: Val Loss 12943.20703
Epoch 23: Val Loss 11586.77441
Epoch 24: Val Loss 10506.25195
Epoch 25: Val Loss 9622.46094
Epoch 26: Val Loss 8897.33496
Epoch 27: Val Loss 8292.10547
Epoch 28: Val Loss 7780.58252
Epoch 29: Val Loss 7346.13330
Epoch 30: Val Loss 6980.90820
Epoch 31: Val Loss 6648.15869
Epoch 32: Val Loss 6362.83545
Epoch 33: Val Loss 6116.39600
Epoch 34: Val Loss 5892.58447
Epoch 35: Val Loss 5702.30322
Epoch 36: Val Loss 5520.38184
Epoch 37: Val Loss 5361.48145
Epoch 38: Val Loss 5213.99268
Epoch 39: Val Loss 5081.84961
Epoch 40: Val Loss 4957.06982
Epoch 41: Val Loss 4849.45410
Epoch 42: Val Loss 4738.02734
Epoch 43: Val Loss 4654.92480
Epoch 44: Val Loss 4559.43457
Epoch 45: Val Loss 4474.56201
Epoch 46: Val Loss 4392.24023
Epoch 47: Val Loss 4316.90674
Epoch 48: Val Loss 4242.74365
Epoch 49: Val Loss 4176.38330
Epoch 50: Val Loss 4104.57861
Epoch 51: Val Loss 4052.74585
Epoch 52: Val Loss 3990.83521
Epoch 53: Val Loss 3927.34546
Epoch 54: Val Loss 3871.81812
Epoch 55: Val Loss 3825.78809
Epoch 56: Val Loss 3755.26855
Epoch 57: Val Loss 3693.49536
Epoch 58: Val Loss 3647.43286
Epoch 59: Val Loss 3589.53442
Epoch 60: Val Loss 3548.07715
Epoch 61: Val Loss 3501.35254
Epoch 62: Val Loss 3444.19897
Epoch 63: Val Loss 3396.49976
Epoch 64: Val Loss 3354.08521
Epoch 65: Val Loss 3311.33423
Epoch 66: Val Loss 3253.75928
Epoch 67: Val Loss 3219.22803
Epoch 68: Val Loss 3177.99438
Epoch 69: Val Loss 3141.90894
Epoch 70: Val Loss 3106.13208
Epoch 71: Val Loss 3064.38672
Epoch 72: Val Loss 3021.35718
Epoch 73: Val Loss 2992.82080
Epoch 74: Val Loss 2965.61426
Epoch 75: Val Loss 2919.03491
Epoch 76: Val Loss 2886.58960
Epoch 77: Val Loss 2864.41992
Epoch 78: Val Loss 2831.30273
Epoch 79: Val Loss 2800.28809
Epoch 80: Val Loss 2778.11328
Epoch 81: Val Loss 2749.43677
Epoch 82: Val Loss 2727.16650
Epoch 83: Val Loss 2696.09595
Epoch 84: Val Loss 2685.76318
Epoch 85: Val Loss 2644.31250
Epoch 86: Val Loss 2645.32520
Epoch 87: Val Loss 2618.37305
Epoch 88: Val Loss 2598.73853
Epoch 89: Val Loss 2578.05371
Epoch 90: Val Loss 2562.75488
Epoch 91: Val Loss 2541.10254
Epoch 92: Val Loss 2534.78979
Epoch 93: Val Loss 2508.93164
Epoch 94: Val Loss 2484.61230
Epoch 95: Val Loss 2488.86279
Epoch 96: Val Loss 2457.79224
Epoch 97: Val Loss 2435.75000
Epoch 98: Val Loss 2427.96411
Epoch 99: Val Loss 2416.62939
{'MSE - mean': 2617.2348998875586, 'MSE - std': 321.2015077791169, 'R2 - mean': 0.6905517204791879, 'R2 - std': 0.032993383261670915} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517241.71875
Epoch 1: Val Loss 516858.18750
Epoch 2: Val Loss 516086.09375
Epoch 3: Val Loss 514455.93750
Epoch 4: Val Loss 511109.09375
Epoch 5: Val Loss 504626.12500
Epoch 6: Val Loss 492862.81250
Epoch 7: Val Loss 472714.31250
Epoch 8: Val Loss 440673.53125
Epoch 9: Val Loss 393529.09375
Epoch 10: Val Loss 330133.43750
Epoch 11: Val Loss 254255.31250
Epoch 12: Val Loss 174606.75000
Epoch 13: Val Loss 104897.11719
Epoch 14: Val Loss 57036.14844
Epoch 15: Val Loss 32623.53906
Epoch 16: Val Loss 22451.25000
Epoch 17: Val Loss 17472.75781
Epoch 18: Val Loss 14233.45312
Epoch 19: Val Loss 11879.59473
Epoch 20: Val Loss 10233.50195
Epoch 21: Val Loss 8964.42676
Epoch 22: Val Loss 7992.33203
Epoch 23: Val Loss 7288.52100
Epoch 24: Val Loss 6741.57861
Epoch 25: Val Loss 6336.65332
Epoch 26: Val Loss 6011.05029
Epoch 27: Val Loss 5754.30762
Epoch 28: Val Loss 5567.40332
Epoch 29: Val Loss 5395.71436
Epoch 30: Val Loss 5252.20996
Epoch 31: Val Loss 5120.60156
Epoch 32: Val Loss 5010.15918
Epoch 33: Val Loss 4919.57080
Epoch 34: Val Loss 4840.14697
Epoch 35: Val Loss 4753.91602
Epoch 36: Val Loss 4672.01562
Epoch 37: Val Loss 4602.96484
Epoch 38: Val Loss 4529.35742
Epoch 39: Val Loss 4477.06787
Epoch 40: Val Loss 4406.80176
Epoch 41: Val Loss 4344.66455
Epoch 42: Val Loss 4299.12793
Epoch 43: Val Loss 4230.16797
Epoch 44: Val Loss 4171.51562
Epoch 45: Val Loss 4136.36816
Epoch 46: Val Loss 4068.52026
Epoch 47: Val Loss 4030.13940
Epoch 48: Val Loss 3979.81323
Epoch 49: Val Loss 3925.90771
Epoch 50: Val Loss 3886.59277
Epoch 51: Val Loss 3841.59888
Epoch 52: Val Loss 3794.43286
Epoch 53: Val Loss 3754.03296
Epoch 54: Val Loss 3714.66260
Epoch 55: Val Loss 3680.20776
Epoch 56: Val Loss 3630.37280
Epoch 57: Val Loss 3600.49634
Epoch 58: Val Loss 3569.88452
Epoch 59: Val Loss 3523.24536
Epoch 60: Val Loss 3497.67456
Epoch 61: Val Loss 3463.92163
Epoch 62: Val Loss 3429.43335
Epoch 63: Val Loss 3389.28735
Epoch 64: Val Loss 3363.91699
Epoch 65: Val Loss 3326.01318
Epoch 66: Val Loss 3297.93140
Epoch 67: Val Loss 3266.21753
Epoch 68: Val Loss 3237.58594
Epoch 69: Val Loss 3211.80737
Epoch 70: Val Loss 3184.49219
Epoch 71: Val Loss 3158.63452
Epoch 72: Val Loss 3119.45264
Epoch 73: Val Loss 3089.63867
Epoch 74: Val Loss 3062.80908
Epoch 75: Val Loss 3050.96802
Epoch 76: Val Loss 3010.44043
Epoch 77: Val Loss 2982.40991
Epoch 78: Val Loss 2950.54102
Epoch 79: Val Loss 2929.57104
Epoch 80: Val Loss 2908.86938
Epoch 81: Val Loss 2879.12329
Epoch 82: Val Loss 2858.82812
Epoch 83: Val Loss 2838.35107
Epoch 84: Val Loss 2810.73462
Epoch 85: Val Loss 2789.34888
Epoch 86: Val Loss 2756.17627
Epoch 87: Val Loss 2729.10864
Epoch 88: Val Loss 2715.48364
Epoch 89: Val Loss 2694.35205
Epoch 90: Val Loss 2664.67480
Epoch 91: Val Loss 2642.89185
Epoch 92: Val Loss 2617.82617
Epoch 93: Val Loss 2604.85840
Epoch 94: Val Loss 2575.36401
Epoch 95: Val Loss 2554.64453
Epoch 96: Val Loss 2529.97144
Epoch 97: Val Loss 2509.32959
Epoch 98: Val Loss 2487.06274
Epoch 99: Val Loss 2473.94507
{'MSE - mean': 2588.5769697522965, 'MSE - std': 292.95295649338095, 'R2 - mean': 0.6904224832250874, 'R2 - std': 0.029511311057162384} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 2588.5769697522965 and parameters: {'dim': 64, 'depth': 2, 'heads': 8, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 4 with value: 2588.5769697522965.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516499.18750
Epoch 1: Val Loss 516496.25000
Epoch 2: Val Loss 516493.43750
Epoch 3: Val Loss 516490.40625
Epoch 4: Val Loss 516487.46875
Epoch 5: Val Loss 516484.46875
Epoch 6: Val Loss 516481.56250
Epoch 7: Val Loss 516478.50000
Epoch 8: Val Loss 516475.40625
Epoch 9: Val Loss 516472.37500
Epoch 10: Val Loss 516469.31250
Epoch 11: Val Loss 516466.25000
Epoch 12: Val Loss 516463.15625
Epoch 13: Val Loss 516460.06250
Epoch 14: Val Loss 516456.90625
Epoch 15: Val Loss 516453.78125
Epoch 16: Val Loss 516450.59375
Epoch 17: Val Loss 516447.40625
Epoch 18: Val Loss 516444.18750
Epoch 19: Val Loss 516441.00000
Epoch 20: Val Loss 516437.71875
Epoch 21: Val Loss 516434.50000
Epoch 22: Val Loss 516431.25000
Epoch 23: Val Loss 516427.93750
Epoch 24: Val Loss 516424.59375
Epoch 25: Val Loss 516421.25000
Epoch 26: Val Loss 516417.93750
Epoch 27: Val Loss 516414.46875
Epoch 28: Val Loss 516411.03125
Epoch 29: Val Loss 516407.59375
Epoch 30: Val Loss 516404.03125
Epoch 31: Val Loss 516400.50000
Epoch 32: Val Loss 516397.03125
Epoch 33: Val Loss 516393.43750
Epoch 34: Val Loss 516389.84375
Epoch 35: Val Loss 516386.21875
Epoch 36: Val Loss 516382.62500
Epoch 37: Val Loss 516378.93750
Epoch 38: Val Loss 516375.15625
Epoch 39: Val Loss 516371.50000
Epoch 40: Val Loss 516367.68750
Epoch 41: Val Loss 516363.87500
Epoch 42: Val Loss 516360.03125
Epoch 43: Val Loss 516356.09375
Epoch 44: Val Loss 516352.21875
Epoch 45: Val Loss 516348.25000
Epoch 46: Val Loss 516344.31250
Epoch 47: Val Loss 516340.34375
Epoch 48: Val Loss 516336.37500
Epoch 49: Val Loss 516332.21875
Epoch 50: Val Loss 516328.12500
Epoch 51: Val Loss 516323.96875
Epoch 52: Val Loss 516319.75000
Epoch 53: Val Loss 516315.53125
Epoch 54: Val Loss 516311.28125
Epoch 55: Val Loss 516307.00000
Epoch 56: Val Loss 516302.65625
Epoch 57: Val Loss 516298.21875
Epoch 58: Val Loss 516293.84375
Epoch 59: Val Loss 516289.37500
Epoch 60: Val Loss 516284.90625
Epoch 61: Val Loss 516280.37500
Epoch 62: Val Loss 516275.78125
Epoch 63: Val Loss 516271.15625
Epoch 64: Val Loss 516266.50000
Epoch 65: Val Loss 516261.78125
Epoch 66: Val Loss 516257.06250
Epoch 67: Val Loss 516252.21875
Epoch 68: Val Loss 516247.34375
Epoch 69: Val Loss 516242.43750
Epoch 70: Val Loss 516237.43750
Epoch 71: Val Loss 516232.46875
Epoch 72: Val Loss 516227.50000
Epoch 73: Val Loss 516222.37500
Epoch 74: Val Loss 516217.21875
Epoch 75: Val Loss 516212.00000
Epoch 76: Val Loss 516206.78125
Epoch 77: Val Loss 516201.46875
Epoch 78: Val Loss 516196.15625
Epoch 79: Val Loss 516190.75000
Epoch 80: Val Loss 516185.34375
Epoch 81: Val Loss 516179.78125
Epoch 82: Val Loss 516174.12500
Epoch 83: Val Loss 516168.56250
Epoch 84: Val Loss 516162.81250
Epoch 85: Val Loss 516157.09375
Epoch 86: Val Loss 516151.37500
Epoch 87: Val Loss 516145.56250
Epoch 88: Val Loss 516139.53125
Epoch 89: Val Loss 516133.56250
Epoch 90: Val Loss 516127.46875
Epoch 91: Val Loss 516121.40625
Epoch 92: Val Loss 516115.25000
Epoch 93: Val Loss 516108.96875
Epoch 94: Val Loss 516102.62500
Epoch 95: Val Loss 516096.25000
Epoch 96: Val Loss 516089.75000
Epoch 97: Val Loss 516083.28125
Epoch 98: Val Loss 516076.65625
Epoch 99: Val Loss 516069.96875
{'MSE - mean': 516069.9455595376, 'MSE - std': 0.0, 'R2 - mean': -59.00394612797538, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524774.06250
Epoch 1: Val Loss 524770.62500
Epoch 2: Val Loss 524767.18750
Epoch 3: Val Loss 524763.81250
Epoch 4: Val Loss 524760.37500
Epoch 5: Val Loss 524757.00000
Epoch 6: Val Loss 524753.56250
Epoch 7: Val Loss 524750.18750
Epoch 8: Val Loss 524746.81250
Epoch 9: Val Loss 524743.50000
Epoch 10: Val Loss 524740.18750
Epoch 11: Val Loss 524736.87500
Epoch 12: Val Loss 524733.56250
Epoch 13: Val Loss 524730.31250
Epoch 14: Val Loss 524727.00000
Epoch 15: Val Loss 524723.81250
Epoch 16: Val Loss 524720.68750
Epoch 17: Val Loss 524717.43750
Epoch 18: Val Loss 524714.25000
Epoch 19: Val Loss 524711.06250
Epoch 20: Val Loss 524707.87500
Epoch 21: Val Loss 524704.75000
Epoch 22: Val Loss 524701.56250
Epoch 23: Val Loss 524698.43750
Epoch 24: Val Loss 524695.31250
Epoch 25: Val Loss 524692.18750
Epoch 26: Val Loss 524689.12500
Epoch 27: Val Loss 524686.06250
Epoch 28: Val Loss 524682.93750
Epoch 29: Val Loss 524679.81250
Epoch 30: Val Loss 524676.75000
Epoch 31: Val Loss 524673.62500
Epoch 32: Val Loss 524670.50000
Epoch 33: Val Loss 524667.43750
Epoch 34: Val Loss 524664.37500
Epoch 35: Val Loss 524661.31250
Epoch 36: Val Loss 524658.25000
Epoch 37: Val Loss 524655.12500
Epoch 38: Val Loss 524652.06250
Epoch 39: Val Loss 524648.93750
Epoch 40: Val Loss 524645.93750
Epoch 41: Val Loss 524642.87500
Epoch 42: Val Loss 524639.75000
Epoch 43: Val Loss 524636.62500
Epoch 44: Val Loss 524633.56250
Epoch 45: Val Loss 524630.43750
Epoch 46: Val Loss 524627.31250
Epoch 47: Val Loss 524624.12500
Epoch 48: Val Loss 524621.06250
Epoch 49: Val Loss 524617.87500
Epoch 50: Val Loss 524614.75000
Epoch 51: Val Loss 524611.50000
Epoch 52: Val Loss 524608.31250
Epoch 53: Val Loss 524605.12500
Epoch 54: Val Loss 524601.93750
Epoch 55: Val Loss 524598.68750
Epoch 56: Val Loss 524595.43750
Epoch 57: Val Loss 524592.18750
Epoch 58: Val Loss 524588.93750
Epoch 59: Val Loss 524585.75000
Epoch 60: Val Loss 524582.43750
Epoch 61: Val Loss 524579.25000
Epoch 62: Val Loss 524575.93750
Epoch 63: Val Loss 524572.62500
Epoch 64: Val Loss 524569.37500
Epoch 65: Val Loss 524566.12500
Epoch 66: Val Loss 524562.87500
Epoch 67: Val Loss 524559.62500
Epoch 68: Val Loss 524556.37500
Epoch 69: Val Loss 524553.12500
Epoch 70: Val Loss 524549.93750
Epoch 71: Val Loss 524546.68750
Epoch 72: Val Loss 524543.50000
Epoch 73: Val Loss 524540.25000
Epoch 74: Val Loss 524537.00000
Epoch 75: Val Loss 524533.68750
Epoch 76: Val Loss 524530.56250
Epoch 77: Val Loss 524527.31250
Epoch 78: Val Loss 524524.12500
Epoch 79: Val Loss 524520.87500
Epoch 80: Val Loss 524517.62500
Epoch 81: Val Loss 524514.43750
Epoch 82: Val Loss 524511.18750
Epoch 83: Val Loss 524507.87500
Epoch 84: Val Loss 524504.68750
Epoch 85: Val Loss 524501.43750
Epoch 86: Val Loss 524498.18750
Epoch 87: Val Loss 524494.93750
Epoch 88: Val Loss 524491.75000
Epoch 89: Val Loss 524488.56250
Epoch 90: Val Loss 524485.31250
Epoch 91: Val Loss 524482.06250
Epoch 92: Val Loss 524478.81250
Epoch 93: Val Loss 524475.56250
Epoch 94: Val Loss 524472.25000
Epoch 95: Val Loss 524468.93750
Epoch 96: Val Loss 524465.68750
Epoch 97: Val Loss 524462.37500
Epoch 98: Val Loss 524459.06250
Epoch 99: Val Loss 524455.81250
{'MSE - mean': 520262.86739395483, 'MSE - std': 4192.921834417211, 'R2 - mean': -63.322860273883165, 'R2 - std': 4.318914145907783} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517923.18750
Epoch 1: Val Loss 517921.03125
Epoch 2: Val Loss 517918.78125
Epoch 3: Val Loss 517916.53125
Epoch 4: Val Loss 517914.31250
Epoch 5: Val Loss 517912.03125
Epoch 6: Val Loss 517909.78125
Epoch 7: Val Loss 517907.46875
Epoch 8: Val Loss 517905.15625
Epoch 9: Val Loss 517902.81250
Epoch 10: Val Loss 517900.43750
Epoch 11: Val Loss 517898.06250
Epoch 12: Val Loss 517895.65625
Epoch 13: Val Loss 517893.25000
Epoch 14: Val Loss 517890.78125
Epoch 15: Val Loss 517888.31250
Epoch 16: Val Loss 517885.81250
Epoch 17: Val Loss 517883.31250
Epoch 18: Val Loss 517880.75000
Epoch 19: Val Loss 517878.18750
Epoch 20: Val Loss 517875.62500
Epoch 21: Val Loss 517873.03125
Epoch 22: Val Loss 517870.40625
Epoch 23: Val Loss 517867.75000
Epoch 24: Val Loss 517865.09375
Epoch 25: Val Loss 517862.31250
Epoch 26: Val Loss 517859.56250
Epoch 27: Val Loss 517856.75000
Epoch 28: Val Loss 517853.96875
Epoch 29: Val Loss 517851.15625
Epoch 30: Val Loss 517848.31250
Epoch 31: Val Loss 517845.37500
Epoch 32: Val Loss 517842.50000
Epoch 33: Val Loss 517839.59375
Epoch 34: Val Loss 517836.59375
Epoch 35: Val Loss 517833.59375
Epoch 36: Val Loss 517830.62500
Epoch 37: Val Loss 517827.65625
Epoch 38: Val Loss 517824.53125
Epoch 39: Val Loss 517821.43750
Epoch 40: Val Loss 517818.31250
Epoch 41: Val Loss 517815.15625
Epoch 42: Val Loss 517812.00000
Epoch 43: Val Loss 517808.75000
Epoch 44: Val Loss 517805.53125
Epoch 45: Val Loss 517802.25000
Epoch 46: Val Loss 517798.93750
Epoch 47: Val Loss 517795.68750
Epoch 48: Val Loss 517792.31250
Epoch 49: Val Loss 517788.93750
Epoch 50: Val Loss 517785.53125
Epoch 51: Val Loss 517782.12500
Epoch 52: Val Loss 517778.65625
Epoch 53: Val Loss 517775.12500
Epoch 54: Val Loss 517771.68750
Epoch 55: Val Loss 517768.15625
Epoch 56: Val Loss 517764.59375
Epoch 57: Val Loss 517761.06250
Epoch 58: Val Loss 517757.43750
Epoch 59: Val Loss 517753.78125
Epoch 60: Val Loss 517750.15625
Epoch 61: Val Loss 517746.53125
Epoch 62: Val Loss 517742.81250
Epoch 63: Val Loss 517739.06250
Epoch 64: Val Loss 517735.25000
Epoch 65: Val Loss 517731.53125
Epoch 66: Val Loss 517727.71875
Epoch 67: Val Loss 517723.81250
Epoch 68: Val Loss 517719.90625
Epoch 69: Val Loss 517716.00000
Epoch 70: Val Loss 517712.12500
Epoch 71: Val Loss 517708.15625
Epoch 72: Val Loss 517704.09375
Epoch 73: Val Loss 517700.03125
Epoch 74: Val Loss 517695.96875
Epoch 75: Val Loss 517691.87500
Epoch 76: Val Loss 517687.78125
Epoch 77: Val Loss 517683.59375
Epoch 78: Val Loss 517679.37500
Epoch 79: Val Loss 517675.09375
Epoch 80: Val Loss 517670.81250
Epoch 81: Val Loss 517666.53125
Epoch 82: Val Loss 517662.18750
Epoch 83: Val Loss 517657.81250
Epoch 84: Val Loss 517653.40625
Epoch 85: Val Loss 517649.03125
Epoch 86: Val Loss 517644.56250
Epoch 87: Val Loss 517640.03125
Epoch 88: Val Loss 517635.50000
Epoch 89: Val Loss 517631.00000
Epoch 90: Val Loss 517626.37500
Epoch 91: Val Loss 517621.78125
Epoch 92: Val Loss 517617.09375
Epoch 93: Val Loss 517612.46875
Epoch 94: Val Loss 517607.71875
Epoch 95: Val Loss 517602.96875
Epoch 96: Val Loss 517598.12500
Epoch 97: Val Loss 517593.28125
Epoch 98: Val Loss 517588.40625
Epoch 99: Val Loss 517583.50000
{'MSE - mean': 519369.75572406914, 'MSE - std': 3649.0673579414442, 'R2 - mean': -60.41647976507367, 'R2 - std': 5.415666310848283} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520550.68750
Epoch 1: Val Loss 520547.03125
Epoch 2: Val Loss 520543.40625
Epoch 3: Val Loss 520539.81250
Epoch 4: Val Loss 520536.03125
Epoch 5: Val Loss 520532.40625
Epoch 6: Val Loss 520528.65625
Epoch 7: Val Loss 520524.90625
Epoch 8: Val Loss 520521.18750
Epoch 9: Val Loss 520517.37500
Epoch 10: Val Loss 520513.56250
Epoch 11: Val Loss 520509.75000
Epoch 12: Val Loss 520505.81250
Epoch 13: Val Loss 520501.90625
Epoch 14: Val Loss 520498.00000
Epoch 15: Val Loss 520494.03125
Epoch 16: Val Loss 520490.06250
Epoch 17: Val Loss 520486.09375
Epoch 18: Val Loss 520482.06250
Epoch 19: Val Loss 520478.03125
Epoch 20: Val Loss 520473.93750
Epoch 21: Val Loss 520469.78125
Epoch 22: Val Loss 520465.65625
Epoch 23: Val Loss 520461.46875
Epoch 24: Val Loss 520457.25000
Epoch 25: Val Loss 520453.06250
Epoch 26: Val Loss 520448.81250
Epoch 27: Val Loss 520444.53125
Epoch 28: Val Loss 520440.15625
Epoch 29: Val Loss 520435.84375
Epoch 30: Val Loss 520431.46875
Epoch 31: Val Loss 520427.00000
Epoch 32: Val Loss 520422.56250
Epoch 33: Val Loss 520418.06250
Epoch 34: Val Loss 520413.53125
Epoch 35: Val Loss 520409.03125
Epoch 36: Val Loss 520404.46875
Epoch 37: Val Loss 520399.90625
Epoch 38: Val Loss 520395.31250
Epoch 39: Val Loss 520390.68750
Epoch 40: Val Loss 520385.93750
Epoch 41: Val Loss 520381.18750
Epoch 42: Val Loss 520376.40625
Epoch 43: Val Loss 520371.62500
Epoch 44: Val Loss 520366.75000
Epoch 45: Val Loss 520361.90625
Epoch 46: Val Loss 520357.00000
Epoch 47: Val Loss 520352.12500
Epoch 48: Val Loss 520347.12500
Epoch 49: Val Loss 520342.15625
Epoch 50: Val Loss 520337.15625
Epoch 51: Val Loss 520332.00000
Epoch 52: Val Loss 520326.87500
Epoch 53: Val Loss 520321.75000
Epoch 54: Val Loss 520316.53125
Epoch 55: Val Loss 520311.34375
Epoch 56: Val Loss 520306.03125
Epoch 57: Val Loss 520300.71875
Epoch 58: Val Loss 520295.34375
Epoch 59: Val Loss 520289.93750
Epoch 60: Val Loss 520284.40625
Epoch 61: Val Loss 520279.00000
Epoch 62: Val Loss 520273.40625
Epoch 63: Val Loss 520267.81250
Epoch 64: Val Loss 520262.15625
Epoch 65: Val Loss 520256.50000
Epoch 66: Val Loss 520250.78125
Epoch 67: Val Loss 520245.00000
Epoch 68: Val Loss 520239.15625
Epoch 69: Val Loss 520233.28125
Epoch 70: Val Loss 520227.37500
Epoch 71: Val Loss 520221.34375
Epoch 72: Val Loss 520215.34375
Epoch 73: Val Loss 520209.21875
Epoch 74: Val Loss 520203.09375
Epoch 75: Val Loss 520196.84375
Epoch 76: Val Loss 520190.68750
Epoch 77: Val Loss 520184.46875
Epoch 78: Val Loss 520178.06250
Epoch 79: Val Loss 520171.65625
Epoch 80: Val Loss 520165.21875
Epoch 81: Val Loss 520158.75000
Epoch 82: Val Loss 520152.12500
Epoch 83: Val Loss 520145.46875
Epoch 84: Val Loss 520138.93750
Epoch 85: Val Loss 520132.15625
Epoch 86: Val Loss 520125.43750
Epoch 87: Val Loss 520118.59375
Epoch 88: Val Loss 520111.75000
Epoch 89: Val Loss 520104.78125
Epoch 90: Val Loss 520097.78125
Epoch 91: Val Loss 520090.65625
Epoch 92: Val Loss 520083.56250
Epoch 93: Val Loss 520076.40625
Epoch 94: Val Loss 520069.09375
Epoch 95: Val Loss 520061.78125
Epoch 96: Val Loss 520054.40625
Epoch 97: Val Loss 520046.93750
Epoch 98: Val Loss 520039.50000
Epoch 99: Val Loss 520031.84375
{'MSE - mean': 519535.27659592766, 'MSE - std': 3173.1626044307454, 'R2 - mean': -60.715568689111635, 'R2 - std': 4.718627315892765} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517816.09375
Epoch 1: Val Loss 517813.56250
Epoch 2: Val Loss 517810.96875
Epoch 3: Val Loss 517808.50000
Epoch 4: Val Loss 517805.96875
Epoch 5: Val Loss 517803.43750
Epoch 6: Val Loss 517801.00000
Epoch 7: Val Loss 517798.56250
Epoch 8: Val Loss 517796.09375
Epoch 9: Val Loss 517793.62500
Epoch 10: Val Loss 517791.18750
Epoch 11: Val Loss 517788.78125
Epoch 12: Val Loss 517786.34375
Epoch 13: Val Loss 517783.90625
Epoch 14: Val Loss 517781.50000
Epoch 15: Val Loss 517779.09375
Epoch 16: Val Loss 517776.68750
Epoch 17: Val Loss 517774.28125
Epoch 18: Val Loss 517771.87500
Epoch 19: Val Loss 517769.53125
Epoch 20: Val Loss 517767.12500
Epoch 21: Val Loss 517764.78125
Epoch 22: Val Loss 517762.37500
Epoch 23: Val Loss 517760.03125
Epoch 24: Val Loss 517757.62500
Epoch 25: Val Loss 517755.21875
Epoch 26: Val Loss 517752.84375
Epoch 27: Val Loss 517750.40625
Epoch 28: Val Loss 517748.03125
Epoch 29: Val Loss 517745.62500
Epoch 30: Val Loss 517743.21875
Epoch 31: Val Loss 517740.84375
Epoch 32: Val Loss 517738.43750
Epoch 33: Val Loss 517736.00000
Epoch 34: Val Loss 517733.59375
Epoch 35: Val Loss 517731.15625
Epoch 36: Val Loss 517728.68750
Epoch 37: Val Loss 517726.25000
Epoch 38: Val Loss 517723.81250
Epoch 39: Val Loss 517721.34375
Epoch 40: Val Loss 517718.87500
Epoch 41: Val Loss 517716.40625
Epoch 42: Val Loss 517713.87500
Epoch 43: Val Loss 517711.34375
Epoch 44: Val Loss 517708.78125
Epoch 45: Val Loss 517706.25000
Epoch 46: Val Loss 517703.71875
Epoch 47: Val Loss 517701.18750
Epoch 48: Val Loss 517698.65625
Epoch 49: Val Loss 517696.09375
Epoch 50: Val Loss 517693.50000
Epoch 51: Val Loss 517690.87500
Epoch 52: Val Loss 517688.31250
Epoch 53: Val Loss 517685.65625
Epoch 54: Val Loss 517683.03125
Epoch 55: Val Loss 517680.40625
Epoch 56: Val Loss 517677.78125
Epoch 57: Val Loss 517675.06250
Epoch 58: Val Loss 517672.37500
Epoch 59: Val Loss 517669.65625
Epoch 60: Val Loss 517666.96875
Epoch 61: Val Loss 517664.25000
Epoch 62: Val Loss 517661.50000
Epoch 63: Val Loss 517658.75000
Epoch 64: Val Loss 517655.96875
Epoch 65: Val Loss 517653.18750
Epoch 66: Val Loss 517650.40625
Epoch 67: Val Loss 517647.56250
Epoch 68: Val Loss 517644.71875
Epoch 69: Val Loss 517641.87500
Epoch 70: Val Loss 517638.93750
Epoch 71: Val Loss 517636.09375
Epoch 72: Val Loss 517633.15625
Epoch 73: Val Loss 517630.21875
Epoch 74: Val Loss 517627.25000
Epoch 75: Val Loss 517624.28125
Epoch 76: Val Loss 517621.25000
Epoch 77: Val Loss 517618.21875
Epoch 78: Val Loss 517615.09375
Epoch 79: Val Loss 517612.00000
Epoch 80: Val Loss 517608.90625
Epoch 81: Val Loss 517605.78125
Epoch 82: Val Loss 517602.56250
Epoch 83: Val Loss 517599.37500
Epoch 84: Val Loss 517596.12500
Epoch 85: Val Loss 517592.81250
Epoch 86: Val Loss 517589.43750
Epoch 87: Val Loss 517586.12500
Epoch 88: Val Loss 517582.62500
Epoch 89: Val Loss 517579.15625
Epoch 90: Val Loss 517575.59375
Epoch 91: Val Loss 517572.09375
Epoch 92: Val Loss 517568.43750
Epoch 93: Val Loss 517564.75000
Epoch 94: Val Loss 517561.09375
Epoch 95: Val Loss 517557.31250
Epoch 96: Val Loss 517553.46875
Epoch 97: Val Loss 517549.59375
Epoch 98: Val Loss 517545.75000
Epoch 99: Val Loss 517541.78125
{'MSE - mean': 519136.578288268, 'MSE - std': 2948.0519149853717, 'R2 - mean': -61.34661785269513, 'R2 - std': 4.405138724770113} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 519136.578288268 and parameters: {'dim': 256, 'depth': 3, 'heads': 8, 'weight_decay': -3, 'learning_rate': -5, 'dropout': 0.1}. Best is trial 4 with value: 2588.5769697522965.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516828.71875
Epoch 1: Val Loss 516803.28125
Epoch 2: Val Loss 516779.15625
Epoch 3: Val Loss 516755.96875
Epoch 4: Val Loss 516733.90625
Epoch 5: Val Loss 516712.87500
Epoch 6: Val Loss 516691.84375
Epoch 7: Val Loss 516670.46875
Epoch 8: Val Loss 516648.18750
Epoch 9: Val Loss 516624.81250
Epoch 10: Val Loss 516600.00000
Epoch 11: Val Loss 516573.09375
Epoch 12: Val Loss 516543.59375
Epoch 13: Val Loss 516510.78125
Epoch 14: Val Loss 516474.18750
Epoch 15: Val Loss 516433.37500
Epoch 16: Val Loss 516387.62500
Epoch 17: Val Loss 516337.53125
Epoch 18: Val Loss 516282.12500
Epoch 19: Val Loss 516220.46875
Epoch 20: Val Loss 516151.68750
Epoch 21: Val Loss 516075.87500
Epoch 22: Val Loss 515993.65625
Epoch 23: Val Loss 515904.21875
Epoch 24: Val Loss 515806.96875
Epoch 25: Val Loss 515700.87500
Epoch 26: Val Loss 515585.65625
Epoch 27: Val Loss 515461.68750
Epoch 28: Val Loss 515324.00000
Epoch 29: Val Loss 515175.71875
Epoch 30: Val Loss 515013.31250
Epoch 31: Val Loss 514835.50000
Epoch 32: Val Loss 514639.93750
Epoch 33: Val Loss 514430.53125
Epoch 34: Val Loss 514203.87500
Epoch 35: Val Loss 513954.96875
Epoch 36: Val Loss 513684.31250
Epoch 37: Val Loss 513393.65625
Epoch 38: Val Loss 513079.09375
Epoch 39: Val Loss 512736.21875
Epoch 40: Val Loss 512364.78125
Epoch 41: Val Loss 511966.00000
Epoch 42: Val Loss 511532.06250
Epoch 43: Val Loss 511071.12500
Epoch 44: Val Loss 510580.31250
Epoch 45: Val Loss 510051.31250
Epoch 46: Val Loss 509484.06250
Epoch 47: Val Loss 508874.43750
Epoch 48: Val Loss 508231.00000
Epoch 49: Val Loss 507552.59375
Epoch 50: Val Loss 506824.87500
Epoch 51: Val Loss 506053.34375
Epoch 52: Val Loss 505243.56250
Epoch 53: Val Loss 504388.78125
Epoch 54: Val Loss 503471.40625
Epoch 55: Val Loss 502501.53125
Epoch 56: Val Loss 501483.03125
Epoch 57: Val Loss 500410.43750
Epoch 58: Val Loss 499272.50000
Epoch 59: Val Loss 498091.90625
Epoch 60: Val Loss 496855.75000
Epoch 61: Val Loss 495561.09375
Epoch 62: Val Loss 494194.84375
Epoch 63: Val Loss 492760.18750
Epoch 64: Val Loss 491264.43750
Epoch 65: Val Loss 489706.03125
Epoch 66: Val Loss 488086.15625
Epoch 67: Val Loss 486394.34375
Epoch 68: Val Loss 484635.93750
Epoch 69: Val Loss 482813.09375
Epoch 70: Val Loss 480917.37500
Epoch 71: Val Loss 478948.62500
Epoch 72: Val Loss 476897.06250
Epoch 73: Val Loss 474771.84375
Epoch 74: Val Loss 472536.93750
Epoch 75: Val Loss 470236.93750
Epoch 76: Val Loss 467846.18750
Epoch 77: Val Loss 465381.90625
Epoch 78: Val Loss 462868.93750
Epoch 79: Val Loss 460284.15625
Epoch 80: Val Loss 457606.28125
Epoch 81: Val Loss 454848.71875
Epoch 82: Val Loss 451996.68750
Epoch 83: Val Loss 449045.37500
Epoch 84: Val Loss 446008.84375
Epoch 85: Val Loss 442911.96875
Epoch 86: Val Loss 439710.46875
Epoch 87: Val Loss 436459.37500
Epoch 88: Val Loss 433102.65625
Epoch 89: Val Loss 429701.84375
Epoch 90: Val Loss 426172.12500
Epoch 91: Val Loss 422585.00000
Epoch 92: Val Loss 418963.34375
Epoch 93: Val Loss 415197.43750
Epoch 94: Val Loss 411411.15625
Epoch 95: Val Loss 407491.84375
Epoch 96: Val Loss 403538.71875
Epoch 97: Val Loss 399528.68750
Epoch 98: Val Loss 395449.78125
Epoch 99: Val Loss 391244.31250
{'MSE - mean': 391244.2873978965, 'MSE - std': 0.0, 'R2 - mean': -44.49034747305027, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525413.25000
Epoch 1: Val Loss 525369.75000
Epoch 2: Val Loss 525328.12500
Epoch 3: Val Loss 525289.25000
Epoch 4: Val Loss 525251.81250
Epoch 5: Val Loss 525215.18750
Epoch 6: Val Loss 525178.06250
Epoch 7: Val Loss 525140.68750
Epoch 8: Val Loss 525101.87500
Epoch 9: Val Loss 525061.62500
Epoch 10: Val Loss 525019.06250
Epoch 11: Val Loss 524973.18750
Epoch 12: Val Loss 524922.81250
Epoch 13: Val Loss 524867.31250
Epoch 14: Val Loss 524807.31250
Epoch 15: Val Loss 524741.31250
Epoch 16: Val Loss 524670.56250
Epoch 17: Val Loss 524593.37500
Epoch 18: Val Loss 524509.37500
Epoch 19: Val Loss 524419.81250
Epoch 20: Val Loss 524320.81250
Epoch 21: Val Loss 524214.15625
Epoch 22: Val Loss 524098.00000
Epoch 23: Val Loss 523971.15625
Epoch 24: Val Loss 523831.53125
Epoch 25: Val Loss 523680.40625
Epoch 26: Val Loss 523512.59375
Epoch 27: Val Loss 523329.96875
Epoch 28: Val Loss 523130.96875
Epoch 29: Val Loss 522915.93750
Epoch 30: Val Loss 522680.12500
Epoch 31: Val Loss 522426.15625
Epoch 32: Val Loss 522151.03125
Epoch 33: Val Loss 521850.37500
Epoch 34: Val Loss 521529.09375
Epoch 35: Val Loss 521177.21875
Epoch 36: Val Loss 520805.15625
Epoch 37: Val Loss 520401.96875
Epoch 38: Val Loss 519975.68750
Epoch 39: Val Loss 519507.75000
Epoch 40: Val Loss 519016.25000
Epoch 41: Val Loss 518486.62500
Epoch 42: Val Loss 517922.40625
Epoch 43: Val Loss 517323.28125
Epoch 44: Val Loss 516689.12500
Epoch 45: Val Loss 516012.03125
Epoch 46: Val Loss 515299.84375
Epoch 47: Val Loss 514538.18750
Epoch 48: Val Loss 513728.06250
Epoch 49: Val Loss 512880.90625
Epoch 50: Val Loss 511968.15625
Epoch 51: Val Loss 511019.43750
Epoch 52: Val Loss 510025.15625
Epoch 53: Val Loss 508975.00000
Epoch 54: Val Loss 507881.34375
Epoch 55: Val Loss 506720.46875
Epoch 56: Val Loss 505514.00000
Epoch 57: Val Loss 504226.40625
Epoch 58: Val Loss 502886.71875
Epoch 59: Val Loss 501488.25000
Epoch 60: Val Loss 500018.31250
Epoch 61: Val Loss 498491.15625
Epoch 62: Val Loss 496895.50000
Epoch 63: Val Loss 495229.75000
Epoch 64: Val Loss 493500.43750
Epoch 65: Val Loss 491708.59375
Epoch 66: Val Loss 489835.93750
Epoch 67: Val Loss 487901.15625
Epoch 68: Val Loss 485913.68750
Epoch 69: Val Loss 483843.15625
Epoch 70: Val Loss 481726.93750
Epoch 71: Val Loss 479494.81250
Epoch 72: Val Loss 477221.59375
Epoch 73: Val Loss 474872.03125
Epoch 74: Val Loss 472446.12500
Epoch 75: Val Loss 469949.93750
Epoch 76: Val Loss 467355.59375
Epoch 77: Val Loss 464719.53125
Epoch 78: Val Loss 462005.28125
Epoch 79: Val Loss 459211.03125
Epoch 80: Val Loss 456347.50000
Epoch 81: Val Loss 453401.78125
Epoch 82: Val Loss 450364.43750
Epoch 83: Val Loss 447264.03125
Epoch 84: Val Loss 444131.56250
Epoch 85: Val Loss 440906.68750
Epoch 86: Val Loss 437571.18750
Epoch 87: Val Loss 434208.46875
Epoch 88: Val Loss 430765.81250
Epoch 89: Val Loss 427240.09375
Epoch 90: Val Loss 423625.65625
Epoch 91: Val Loss 419971.56250
Epoch 92: Val Loss 416272.28125
Epoch 93: Val Loss 412436.43750
Epoch 94: Val Loss 408589.21875
Epoch 95: Val Loss 404618.93750
Epoch 96: Val Loss 400623.34375
Epoch 97: Val Loss 396527.21875
Epoch 98: Val Loss 392393.15625
Epoch 99: Val Loss 388165.34375
{'MSE - mean': 389704.8152539027, 'MSE - std': 1539.4721439938294, 'R2 - mean': -47.147084123035285, 'R2 - std': 2.6567366499850102} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518557.00000
Epoch 1: Val Loss 518524.93750
Epoch 2: Val Loss 518493.28125
Epoch 3: Val Loss 518461.56250
Epoch 4: Val Loss 518429.71875
Epoch 5: Val Loss 518396.93750
Epoch 6: Val Loss 518362.96875
Epoch 7: Val Loss 518327.53125
Epoch 8: Val Loss 518289.81250
Epoch 9: Val Loss 518249.25000
Epoch 10: Val Loss 518204.90625
Epoch 11: Val Loss 518156.00000
Epoch 12: Val Loss 518102.09375
Epoch 13: Val Loss 518042.96875
Epoch 14: Val Loss 517978.03125
Epoch 15: Val Loss 517906.25000
Epoch 16: Val Loss 517827.09375
Epoch 17: Val Loss 517738.71875
Epoch 18: Val Loss 517640.15625
Epoch 19: Val Loss 517530.12500
Epoch 20: Val Loss 517406.81250
Epoch 21: Val Loss 517269.56250
Epoch 22: Val Loss 517118.03125
Epoch 23: Val Loss 516952.12500
Epoch 24: Val Loss 516770.68750
Epoch 25: Val Loss 516573.93750
Epoch 26: Val Loss 516358.87500
Epoch 27: Val Loss 516126.03125
Epoch 28: Val Loss 515875.62500
Epoch 29: Val Loss 515607.75000
Epoch 30: Val Loss 515321.28125
Epoch 31: Val Loss 515014.34375
Epoch 32: Val Loss 514686.21875
Epoch 33: Val Loss 514333.71875
Epoch 34: Val Loss 513957.53125
Epoch 35: Val Loss 513552.50000
Epoch 36: Val Loss 513121.40625
Epoch 37: Val Loss 512660.96875
Epoch 38: Val Loss 512165.03125
Epoch 39: Val Loss 511639.65625
Epoch 40: Val Loss 511072.81250
Epoch 41: Val Loss 510470.09375
Epoch 42: Val Loss 509825.46875
Epoch 43: Val Loss 509135.06250
Epoch 44: Val Loss 508398.87500
Epoch 45: Val Loss 507612.40625
Epoch 46: Val Loss 506777.34375
Epoch 47: Val Loss 505889.90625
Epoch 48: Val Loss 504943.75000
Epoch 49: Val Loss 503943.34375
Epoch 50: Val Loss 502885.21875
Epoch 51: Val Loss 501769.50000
Epoch 52: Val Loss 500586.96875
Epoch 53: Val Loss 499344.53125
Epoch 54: Val Loss 498031.34375
Epoch 55: Val Loss 496647.09375
Epoch 56: Val Loss 495194.68750
Epoch 57: Val Loss 493666.84375
Epoch 58: Val Loss 492083.90625
Epoch 59: Val Loss 490409.43750
Epoch 60: Val Loss 488653.06250
Epoch 61: Val Loss 486830.28125
Epoch 62: Val Loss 484912.18750
Epoch 63: Val Loss 482923.00000
Epoch 64: Val Loss 480839.84375
Epoch 65: Val Loss 478664.71875
Epoch 66: Val Loss 476393.68750
Epoch 67: Val Loss 474034.53125
Epoch 68: Val Loss 471588.00000
Epoch 69: Val Loss 469049.34375
Epoch 70: Val Loss 466397.18750
Epoch 71: Val Loss 463666.65625
Epoch 72: Val Loss 460857.03125
Epoch 73: Val Loss 457910.21875
Epoch 74: Val Loss 454892.87500
Epoch 75: Val Loss 451770.09375
Epoch 76: Val Loss 448541.28125
Epoch 77: Val Loss 445242.96875
Epoch 78: Val Loss 441841.87500
Epoch 79: Val Loss 438372.93750
Epoch 80: Val Loss 434795.68750
Epoch 81: Val Loss 431123.87500
Epoch 82: Val Loss 427360.21875
Epoch 83: Val Loss 423494.03125
Epoch 84: Val Loss 419539.93750
Epoch 85: Val Loss 415488.00000
Epoch 86: Val Loss 411358.09375
Epoch 87: Val Loss 407121.06250
Epoch 88: Val Loss 402835.68750
Epoch 89: Val Loss 398453.59375
Epoch 90: Val Loss 393977.68750
Epoch 91: Val Loss 389402.46875
Epoch 92: Val Loss 384768.00000
Epoch 93: Val Loss 380021.59375
Epoch 94: Val Loss 375203.31250
Epoch 95: Val Loss 370305.90625
Epoch 96: Val Loss 365353.15625
Epoch 97: Val Loss 360305.34375
Epoch 98: Val Loss 355205.00000
Epoch 99: Val Loss 350036.81250
{'MSE - mean': 376482.14289146225, 'MSE - std': 18741.881223532022, 'R2 - mean': -43.63281067313553, 'R2 - std': 5.422705552851175} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521268.03125
Epoch 1: Val Loss 521242.53125
Epoch 2: Val Loss 521216.71875
Epoch 3: Val Loss 521190.65625
Epoch 4: Val Loss 521164.09375
Epoch 5: Val Loss 521136.93750
Epoch 6: Val Loss 521108.96875
Epoch 7: Val Loss 521080.56250
Epoch 8: Val Loss 521051.15625
Epoch 9: Val Loss 521019.18750
Epoch 10: Val Loss 520984.90625
Epoch 11: Val Loss 520947.43750
Epoch 12: Val Loss 520905.53125
Epoch 13: Val Loss 520859.71875
Epoch 14: Val Loss 520808.37500
Epoch 15: Val Loss 520751.46875
Epoch 16: Val Loss 520690.18750
Epoch 17: Val Loss 520624.18750
Epoch 18: Val Loss 520552.37500
Epoch 19: Val Loss 520475.31250
Epoch 20: Val Loss 520392.50000
Epoch 21: Val Loss 520303.31250
Epoch 22: Val Loss 520207.81250
Epoch 23: Val Loss 520104.12500
Epoch 24: Val Loss 519994.96875
Epoch 25: Val Loss 519875.65625
Epoch 26: Val Loss 519748.75000
Epoch 27: Val Loss 519611.43750
Epoch 28: Val Loss 519462.65625
Epoch 29: Val Loss 519303.12500
Epoch 30: Val Loss 519130.37500
Epoch 31: Val Loss 518945.46875
Epoch 32: Val Loss 518745.46875
Epoch 33: Val Loss 518532.09375
Epoch 34: Val Loss 518301.06250
Epoch 35: Val Loss 518051.56250
Epoch 36: Val Loss 517775.78125
Epoch 37: Val Loss 517476.40625
Epoch 38: Val Loss 517153.59375
Epoch 39: Val Loss 516809.34375
Epoch 40: Val Loss 516443.18750
Epoch 41: Val Loss 516045.59375
Epoch 42: Val Loss 515623.96875
Epoch 43: Val Loss 515174.25000
Epoch 44: Val Loss 514694.15625
Epoch 45: Val Loss 514179.62500
Epoch 46: Val Loss 513634.43750
Epoch 47: Val Loss 513058.18750
Epoch 48: Val Loss 512448.00000
Epoch 49: Val Loss 511799.68750
Epoch 50: Val Loss 511120.15625
Epoch 51: Val Loss 510390.65625
Epoch 52: Val Loss 509626.31250
Epoch 53: Val Loss 508827.37500
Epoch 54: Val Loss 507979.87500
Epoch 55: Val Loss 507097.34375
Epoch 56: Val Loss 506165.81250
Epoch 57: Val Loss 505184.00000
Epoch 58: Val Loss 504156.53125
Epoch 59: Val Loss 503090.93750
Epoch 60: Val Loss 501959.84375
Epoch 61: Val Loss 500781.37500
Epoch 62: Val Loss 499552.90625
Epoch 63: Val Loss 498262.62500
Epoch 64: Val Loss 496925.87500
Epoch 65: Val Loss 495536.56250
Epoch 66: Val Loss 494077.15625
Epoch 67: Val Loss 492570.90625
Epoch 68: Val Loss 491002.37500
Epoch 69: Val Loss 489376.43750
Epoch 70: Val Loss 487688.59375
Epoch 71: Val Loss 485944.87500
Epoch 72: Val Loss 484155.93750
Epoch 73: Val Loss 482298.09375
Epoch 74: Val Loss 480348.87500
Epoch 75: Val Loss 478350.84375
Epoch 76: Val Loss 476293.84375
Epoch 77: Val Loss 474186.18750
Epoch 78: Val Loss 472004.34375
Epoch 79: Val Loss 469714.06250
Epoch 80: Val Loss 467372.65625
Epoch 81: Val Loss 464934.75000
Epoch 82: Val Loss 462442.53125
Epoch 83: Val Loss 459858.71875
Epoch 84: Val Loss 457202.18750
Epoch 85: Val Loss 454440.31250
Epoch 86: Val Loss 451655.78125
Epoch 87: Val Loss 448762.03125
Epoch 88: Val Loss 445799.00000
Epoch 89: Val Loss 442787.31250
Epoch 90: Val Loss 439693.03125
Epoch 91: Val Loss 436533.62500
Epoch 92: Val Loss 433320.50000
Epoch 93: Val Loss 430023.28125
Epoch 94: Val Loss 426640.56250
Epoch 95: Val Loss 423188.71875
Epoch 96: Val Loss 419692.53125
Epoch 97: Val Loss 416115.50000
Epoch 98: Val Loss 412423.71875
Epoch 99: Val Loss 408697.12500
{'MSE - mean': 384535.8920841158, 'MSE - std': 21401.687130276427, 'R2 - mean': -44.77658878257451, 'R2 - std': 5.096958576012526} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517174.40625
Epoch 1: Val Loss 517140.90625
Epoch 2: Val Loss 517103.40625
Epoch 3: Val Loss 517061.84375
Epoch 4: Val Loss 517015.53125
Epoch 5: Val Loss 516964.46875
Epoch 6: Val Loss 516907.81250
Epoch 7: Val Loss 516845.12500
Epoch 8: Val Loss 516776.46875
Epoch 9: Val Loss 516701.87500
Epoch 10: Val Loss 516619.81250
Epoch 11: Val Loss 516530.28125
Epoch 12: Val Loss 516433.15625
Epoch 13: Val Loss 516327.93750
Epoch 14: Val Loss 516212.09375
Epoch 15: Val Loss 516085.53125
Epoch 16: Val Loss 515947.09375
Epoch 17: Val Loss 515796.34375
Epoch 18: Val Loss 515632.31250
Epoch 19: Val Loss 515451.78125
Epoch 20: Val Loss 515253.96875
Epoch 21: Val Loss 515037.43750
Epoch 22: Val Loss 514800.21875
Epoch 23: Val Loss 514540.12500
Epoch 24: Val Loss 514253.46875
Epoch 25: Val Loss 513936.75000
Epoch 26: Val Loss 513581.18750
Epoch 27: Val Loss 513185.84375
Epoch 28: Val Loss 512740.25000
Epoch 29: Val Loss 512243.15625
Epoch 30: Val Loss 511698.31250
Epoch 31: Val Loss 511110.15625
Epoch 32: Val Loss 510478.68750
Epoch 33: Val Loss 509798.62500
Epoch 34: Val Loss 509069.84375
Epoch 35: Val Loss 508287.62500
Epoch 36: Val Loss 507460.21875
Epoch 37: Val Loss 506567.46875
Epoch 38: Val Loss 505622.68750
Epoch 39: Val Loss 504605.50000
Epoch 40: Val Loss 503531.81250
Epoch 41: Val Loss 502373.00000
Epoch 42: Val Loss 501154.93750
Epoch 43: Val Loss 499848.53125
Epoch 44: Val Loss 498448.75000
Epoch 45: Val Loss 496953.25000
Epoch 46: Val Loss 495384.50000
Epoch 47: Val Loss 493695.90625
Epoch 48: Val Loss 491915.00000
Epoch 49: Val Loss 490025.34375
Epoch 50: Val Loss 488037.56250
Epoch 51: Val Loss 485968.81250
Epoch 52: Val Loss 483747.03125
Epoch 53: Val Loss 481440.56250
Epoch 54: Val Loss 479029.56250
Epoch 55: Val Loss 476511.09375
Epoch 56: Val Loss 473885.03125
Epoch 57: Val Loss 471145.56250
Epoch 58: Val Loss 468257.31250
Epoch 59: Val Loss 465281.28125
Epoch 60: Val Loss 462172.31250
Epoch 61: Val Loss 458920.56250
Epoch 62: Val Loss 455571.62500
Epoch 63: Val Loss 452104.46875
Epoch 64: Val Loss 448491.56250
Epoch 65: Val Loss 444768.53125
Epoch 66: Val Loss 440937.28125
Epoch 67: Val Loss 436969.56250
Epoch 68: Val Loss 432892.53125
Epoch 69: Val Loss 428746.62500
Epoch 70: Val Loss 424412.34375
Epoch 71: Val Loss 419969.03125
Epoch 72: Val Loss 415444.12500
Epoch 73: Val Loss 410778.50000
Epoch 74: Val Loss 406054.46875
Epoch 75: Val Loss 401201.12500
Epoch 76: Val Loss 396241.46875
Epoch 77: Val Loss 391165.37500
Epoch 78: Val Loss 385996.00000
Epoch 79: Val Loss 380743.00000
Epoch 80: Val Loss 375398.71875
Epoch 81: Val Loss 369965.37500
Epoch 82: Val Loss 364403.15625
Epoch 83: Val Loss 358843.78125
Epoch 84: Val Loss 353150.56250
Epoch 85: Val Loss 347397.37500
Epoch 86: Val Loss 341596.96875
Epoch 87: Val Loss 335692.21875
Epoch 88: Val Loss 329694.81250
Epoch 89: Val Loss 323754.18750
Epoch 90: Val Loss 317715.53125
Epoch 91: Val Loss 311563.90625
Epoch 92: Val Loss 305436.90625
Epoch 93: Val Loss 299280.12500
Epoch 94: Val Loss 293040.90625
Epoch 95: Val Loss 286849.15625
Epoch 96: Val Loss 280565.03125
Epoch 97: Val Loss 274316.15625
Epoch 98: Val Loss 268022.34375
Epoch 99: Val Loss 261751.98438
{'MSE - mean': 359979.11429575586, 'MSE - std': 52712.11541007768, 'R2 - mean': -42.18308540817669, 'R2 - std': 6.905666397424771} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 359979.11429575586 and parameters: {'dim': 32, 'depth': 2, 'heads': 4, 'weight_decay': -6, 'learning_rate': -4, 'dropout': 0.2}. Best is trial 4 with value: 2588.5769697522965.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516747.59375
Epoch 1: Val Loss 516747.40625
Epoch 2: Val Loss 516747.15625
Epoch 3: Val Loss 516746.90625
Epoch 4: Val Loss 516746.68750
Epoch 5: Val Loss 516746.50000
Epoch 6: Val Loss 516746.25000
Epoch 7: Val Loss 516746.03125
Epoch 8: Val Loss 516745.81250
Epoch 9: Val Loss 516745.59375
Epoch 10: Val Loss 516745.37500
Epoch 11: Val Loss 516745.12500
Epoch 12: Val Loss 516744.87500
Epoch 13: Val Loss 516744.68750
Epoch 14: Val Loss 516744.46875
Epoch 15: Val Loss 516744.25000
Epoch 16: Val Loss 516743.96875
Epoch 17: Val Loss 516743.75000
Epoch 18: Val Loss 516743.56250
Epoch 19: Val Loss 516743.31250
Epoch 20: Val Loss 516743.12500
Epoch 21: Val Loss 516742.87500
Epoch 22: Val Loss 516742.68750
Epoch 23: Val Loss 516742.43750
Epoch 24: Val Loss 516742.21875
Epoch 25: Val Loss 516741.96875
Epoch 26: Val Loss 516741.75000
Epoch 27: Val Loss 516741.53125
Epoch 28: Val Loss 516741.25000
Epoch 29: Val Loss 516741.06250
Epoch 30: Val Loss 516740.81250
Epoch 31: Val Loss 516740.59375
Epoch 32: Val Loss 516740.37500
Epoch 33: Val Loss 516740.12500
Epoch 34: Val Loss 516739.90625
Epoch 35: Val Loss 516739.68750
Epoch 36: Val Loss 516739.43750
Epoch 37: Val Loss 516739.25000
Epoch 38: Val Loss 516739.00000
Epoch 39: Val Loss 516738.81250
Epoch 40: Val Loss 516738.53125
Epoch 41: Val Loss 516738.34375
Epoch 42: Val Loss 516738.12500
Epoch 43: Val Loss 516737.81250
Epoch 44: Val Loss 516737.62500
Epoch 45: Val Loss 516737.40625
Epoch 46: Val Loss 516737.15625
Epoch 47: Val Loss 516736.93750
Epoch 48: Val Loss 516736.68750
Epoch 49: Val Loss 516736.46875
Epoch 50: Val Loss 516736.25000
Epoch 51: Val Loss 516736.00000
Epoch 52: Val Loss 516735.75000
Epoch 53: Val Loss 516735.56250
Epoch 54: Val Loss 516735.31250
Epoch 55: Val Loss 516735.06250
Epoch 56: Val Loss 516734.84375
Epoch 57: Val Loss 516734.65625
Epoch 58: Val Loss 516734.40625
Epoch 59: Val Loss 516734.15625
Epoch 60: Val Loss 516733.93750
Epoch 61: Val Loss 516733.68750
Epoch 62: Val Loss 516733.46875
Epoch 63: Val Loss 516733.21875
Epoch 64: Val Loss 516733.03125
Epoch 65: Val Loss 516732.75000
Epoch 66: Val Loss 516732.50000
Epoch 67: Val Loss 516732.28125
Epoch 68: Val Loss 516732.06250
Epoch 69: Val Loss 516731.84375
Epoch 70: Val Loss 516731.56250
Epoch 71: Val Loss 516731.34375
Epoch 72: Val Loss 516731.12500
Epoch 73: Val Loss 516730.90625
Epoch 74: Val Loss 516730.62500
Epoch 75: Val Loss 516730.43750
Epoch 76: Val Loss 516730.18750
Epoch 77: Val Loss 516729.93750
Epoch 78: Val Loss 516729.71875
Epoch 79: Val Loss 516729.46875
Epoch 80: Val Loss 516729.25000
Epoch 81: Val Loss 516729.03125
Epoch 82: Val Loss 516728.75000
Epoch 83: Val Loss 516728.53125
Epoch 84: Val Loss 516728.28125
Epoch 85: Val Loss 516728.03125
Epoch 86: Val Loss 516727.81250
Epoch 87: Val Loss 516727.56250
Epoch 88: Val Loss 516727.34375
Epoch 89: Val Loss 516727.09375
Epoch 90: Val Loss 516726.84375
Epoch 91: Val Loss 516726.62500
Epoch 92: Val Loss 516726.40625
Epoch 93: Val Loss 516726.12500
Epoch 94: Val Loss 516725.90625
Epoch 95: Val Loss 516725.65625
Epoch 96: Val Loss 516725.37500
Epoch 97: Val Loss 516725.15625
Epoch 98: Val Loss 516724.93750
Epoch 99: Val Loss 516724.75000
{'MSE - mean': 516724.6976791863, 'MSE - std': 0.0, 'R2 - mean': -59.08007478311723, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524781.43750
Epoch 1: Val Loss 524781.06250
Epoch 2: Val Loss 524780.62500
Epoch 3: Val Loss 524780.31250
Epoch 4: Val Loss 524779.87500
Epoch 5: Val Loss 524779.56250
Epoch 6: Val Loss 524779.12500
Epoch 7: Val Loss 524778.81250
Epoch 8: Val Loss 524778.37500
Epoch 9: Val Loss 524778.00000
Epoch 10: Val Loss 524777.68750
Epoch 11: Val Loss 524777.31250
Epoch 12: Val Loss 524776.87500
Epoch 13: Val Loss 524776.56250
Epoch 14: Val Loss 524776.18750
Epoch 15: Val Loss 524775.81250
Epoch 16: Val Loss 524775.37500
Epoch 17: Val Loss 524775.06250
Epoch 18: Val Loss 524774.68750
Epoch 19: Val Loss 524774.31250
Epoch 20: Val Loss 524773.93750
Epoch 21: Val Loss 524773.56250
Epoch 22: Val Loss 524773.18750
Epoch 23: Val Loss 524772.81250
Epoch 24: Val Loss 524772.43750
Epoch 25: Val Loss 524772.12500
Epoch 26: Val Loss 524771.68750
Epoch 27: Val Loss 524771.31250
Epoch 28: Val Loss 524770.93750
Epoch 29: Val Loss 524770.56250
Epoch 30: Val Loss 524770.18750
Epoch 31: Val Loss 524769.81250
Epoch 32: Val Loss 524769.50000
Epoch 33: Val Loss 524769.06250
Epoch 34: Val Loss 524768.75000
Epoch 35: Val Loss 524768.37500
Epoch 36: Val Loss 524767.93750
Epoch 37: Val Loss 524767.56250
Epoch 38: Val Loss 524767.25000
Epoch 39: Val Loss 524766.87500
Epoch 40: Val Loss 524766.50000
Epoch 41: Val Loss 524766.06250
Epoch 42: Val Loss 524765.75000
Epoch 43: Val Loss 524765.37500
Epoch 44: Val Loss 524764.93750
Epoch 45: Val Loss 524764.62500
Epoch 46: Val Loss 524764.25000
Epoch 47: Val Loss 524763.87500
Epoch 48: Val Loss 524763.50000
Epoch 49: Val Loss 524763.12500
Epoch 50: Val Loss 524762.75000
Epoch 51: Val Loss 524762.37500
Epoch 52: Val Loss 524762.00000
Epoch 53: Val Loss 524761.62500
Epoch 54: Val Loss 524761.25000
Epoch 55: Val Loss 524760.87500
Epoch 56: Val Loss 524760.50000
Epoch 57: Val Loss 524760.12500
Epoch 58: Val Loss 524759.81250
Epoch 59: Val Loss 524759.37500
Epoch 60: Val Loss 524759.00000
Epoch 61: Val Loss 524758.62500
Epoch 62: Val Loss 524758.31250
Epoch 63: Val Loss 524757.93750
Epoch 64: Val Loss 524757.50000
Epoch 65: Val Loss 524757.18750
Epoch 66: Val Loss 524756.75000
Epoch 67: Val Loss 524756.43750
Epoch 68: Val Loss 524756.06250
Epoch 69: Val Loss 524755.68750
Epoch 70: Val Loss 524755.31250
Epoch 71: Val Loss 524754.93750
Epoch 72: Val Loss 524754.56250
Epoch 73: Val Loss 524754.18750
Epoch 74: Val Loss 524753.81250
Epoch 75: Val Loss 524753.37500
Epoch 76: Val Loss 524753.06250
Epoch 77: Val Loss 524752.68750
Epoch 78: Val Loss 524752.31250
Epoch 79: Val Loss 524751.93750
Epoch 80: Val Loss 524751.56250
Epoch 81: Val Loss 524751.18750
Epoch 82: Val Loss 524750.81250
Epoch 83: Val Loss 524750.43750
Epoch 84: Val Loss 524750.06250
Epoch 85: Val Loss 524749.68750
Epoch 86: Val Loss 524749.31250
Epoch 87: Val Loss 524748.87500
Epoch 88: Val Loss 524748.56250
Epoch 89: Val Loss 524748.18750
Epoch 90: Val Loss 524747.81250
Epoch 91: Val Loss 524747.50000
Epoch 92: Val Loss 524747.06250
Epoch 93: Val Loss 524746.75000
Epoch 94: Val Loss 524746.37500
Epoch 95: Val Loss 524746.00000
Epoch 96: Val Loss 524745.62500
Epoch 97: Val Loss 524745.18750
Epoch 98: Val Loss 524744.87500
Epoch 99: Val Loss 524744.50000
{'MSE - mean': 520734.58182537416, 'MSE - std': 4009.884146187833, 'R2 - mean': -63.379815882943646, 'R2 - std': 4.2997410998264165} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518626.03125
Epoch 1: Val Loss 518625.75000
Epoch 2: Val Loss 518625.56250
Epoch 3: Val Loss 518625.34375
Epoch 4: Val Loss 518625.12500
Epoch 5: Val Loss 518624.93750
Epoch 6: Val Loss 518624.68750
Epoch 7: Val Loss 518624.46875
Epoch 8: Val Loss 518624.25000
Epoch 9: Val Loss 518624.03125
Epoch 10: Val Loss 518623.84375
Epoch 11: Val Loss 518623.59375
Epoch 12: Val Loss 518623.40625
Epoch 13: Val Loss 518623.15625
Epoch 14: Val Loss 518622.96875
Epoch 15: Val Loss 518622.75000
Epoch 16: Val Loss 518622.56250
Epoch 17: Val Loss 518622.28125
Epoch 18: Val Loss 518622.09375
Epoch 19: Val Loss 518621.90625
Epoch 20: Val Loss 518621.71875
Epoch 21: Val Loss 518621.46875
Epoch 22: Val Loss 518621.25000
Epoch 23: Val Loss 518621.06250
Epoch 24: Val Loss 518620.84375
Epoch 25: Val Loss 518620.62500
Epoch 26: Val Loss 518620.37500
Epoch 27: Val Loss 518620.21875
Epoch 28: Val Loss 518619.96875
Epoch 29: Val Loss 518619.75000
Epoch 30: Val Loss 518619.56250
Epoch 31: Val Loss 518619.34375
Epoch 32: Val Loss 518619.12500
Epoch 33: Val Loss 518618.90625
Epoch 34: Val Loss 518618.71875
Epoch 35: Val Loss 518618.43750
Epoch 36: Val Loss 518618.25000
Epoch 37: Val Loss 518618.06250
Epoch 38: Val Loss 518617.81250
Epoch 39: Val Loss 518617.59375
Epoch 40: Val Loss 518617.40625
Epoch 41: Val Loss 518617.18750
Epoch 42: Val Loss 518616.96875
Epoch 43: Val Loss 518616.75000
Epoch 44: Val Loss 518616.53125
Epoch 45: Val Loss 518616.31250
Epoch 46: Val Loss 518616.12500
Epoch 47: Val Loss 518615.84375
Epoch 48: Val Loss 518615.65625
Epoch 49: Val Loss 518615.46875
Epoch 50: Val Loss 518615.18750
Epoch 51: Val Loss 518615.00000
Epoch 52: Val Loss 518614.78125
Epoch 53: Val Loss 518614.59375
Epoch 54: Val Loss 518614.34375
Epoch 55: Val Loss 518614.15625
Epoch 56: Val Loss 518613.90625
Epoch 57: Val Loss 518613.71875
Epoch 58: Val Loss 518613.50000
Epoch 59: Val Loss 518613.28125
Epoch 60: Val Loss 518613.06250
Epoch 61: Val Loss 518612.81250
Epoch 62: Val Loss 518612.59375
Epoch 63: Val Loss 518612.37500
Epoch 64: Val Loss 518612.18750
Epoch 65: Val Loss 518611.93750
Epoch 66: Val Loss 518611.71875
Epoch 67: Val Loss 518611.53125
Epoch 68: Val Loss 518611.31250
Epoch 69: Val Loss 518611.06250
Epoch 70: Val Loss 518610.87500
Epoch 71: Val Loss 518610.65625
Epoch 72: Val Loss 518610.40625
Epoch 73: Val Loss 518610.18750
Epoch 74: Val Loss 518610.00000
Epoch 75: Val Loss 518609.78125
Epoch 76: Val Loss 518609.53125
Epoch 77: Val Loss 518609.31250
Epoch 78: Val Loss 518609.12500
Epoch 79: Val Loss 518608.90625
Epoch 80: Val Loss 518608.62500
Epoch 81: Val Loss 518608.46875
Epoch 82: Val Loss 518608.25000
Epoch 83: Val Loss 518608.00000
Epoch 84: Val Loss 518607.78125
Epoch 85: Val Loss 518607.59375
Epoch 86: Val Loss 518607.34375
Epoch 87: Val Loss 518607.12500
Epoch 88: Val Loss 518606.93750
Epoch 89: Val Loss 518606.68750
Epoch 90: Val Loss 518606.50000
Epoch 91: Val Loss 518606.28125
Epoch 92: Val Loss 518606.03125
Epoch 93: Val Loss 518605.81250
Epoch 94: Val Loss 518605.56250
Epoch 95: Val Loss 518605.37500
Epoch 96: Val Loss 518605.15625
Epoch 97: Val Loss 518604.90625
Epoch 98: Val Loss 518604.68750
Epoch 99: Val Loss 518604.46875
{'MSE - mean': 520024.5506167407, 'MSE - std': 3424.578204445896, 'R2 - mean': -60.491010414661616, 'R2 - std': 5.386610847045661} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521342.96875
Epoch 1: Val Loss 521342.59375
Epoch 2: Val Loss 521342.21875
Epoch 3: Val Loss 521341.87500
Epoch 4: Val Loss 521341.46875
Epoch 5: Val Loss 521341.12500
Epoch 6: Val Loss 521340.75000
Epoch 7: Val Loss 521340.34375
Epoch 8: Val Loss 521339.96875
Epoch 9: Val Loss 521339.62500
Epoch 10: Val Loss 521339.25000
Epoch 11: Val Loss 521338.84375
Epoch 12: Val Loss 521338.50000
Epoch 13: Val Loss 521338.12500
Epoch 14: Val Loss 521337.75000
Epoch 15: Val Loss 521337.37500
Epoch 16: Val Loss 521337.03125
Epoch 17: Val Loss 521336.62500
Epoch 18: Val Loss 521336.31250
Epoch 19: Val Loss 521335.90625
Epoch 20: Val Loss 521335.53125
Epoch 21: Val Loss 521335.12500
Epoch 22: Val Loss 521334.81250
Epoch 23: Val Loss 521334.37500
Epoch 24: Val Loss 521334.03125
Epoch 25: Val Loss 521333.65625
Epoch 26: Val Loss 521333.31250
Epoch 27: Val Loss 521332.93750
Epoch 28: Val Loss 521332.56250
Epoch 29: Val Loss 521332.21875
Epoch 30: Val Loss 521331.78125
Epoch 31: Val Loss 521331.43750
Epoch 32: Val Loss 521331.09375
Epoch 33: Val Loss 521330.71875
Epoch 34: Val Loss 521330.31250
Epoch 35: Val Loss 521329.96875
Epoch 36: Val Loss 521329.59375
Epoch 37: Val Loss 521329.21875
Epoch 38: Val Loss 521328.81250
Epoch 39: Val Loss 521328.46875
Epoch 40: Val Loss 521328.12500
Epoch 41: Val Loss 521327.71875
Epoch 42: Val Loss 521327.37500
Epoch 43: Val Loss 521327.00000
Epoch 44: Val Loss 521326.62500
Epoch 45: Val Loss 521326.25000
Epoch 46: Val Loss 521325.87500
Epoch 47: Val Loss 521325.50000
Epoch 48: Val Loss 521325.15625
Epoch 49: Val Loss 521324.78125
Epoch 50: Val Loss 521324.40625
Epoch 51: Val Loss 521324.00000
Epoch 52: Val Loss 521323.68750
Epoch 53: Val Loss 521323.25000
Epoch 54: Val Loss 521322.90625
Epoch 55: Val Loss 521322.59375
Epoch 56: Val Loss 521322.18750
Epoch 57: Val Loss 521321.81250
Epoch 58: Val Loss 521321.46875
Epoch 59: Val Loss 521321.09375
Epoch 60: Val Loss 521320.68750
Epoch 61: Val Loss 521320.34375
Epoch 62: Val Loss 521319.96875
Epoch 63: Val Loss 521319.62500
Epoch 64: Val Loss 521319.25000
Epoch 65: Val Loss 521318.90625
Epoch 66: Val Loss 521318.53125
Epoch 67: Val Loss 521318.12500
Epoch 68: Val Loss 521317.78125
Epoch 69: Val Loss 521317.43750
Epoch 70: Val Loss 521317.06250
Epoch 71: Val Loss 521316.65625
Epoch 72: Val Loss 521316.34375
Epoch 73: Val Loss 521315.93750
Epoch 74: Val Loss 521315.59375
Epoch 75: Val Loss 521315.18750
Epoch 76: Val Loss 521314.84375
Epoch 77: Val Loss 521314.43750
Epoch 78: Val Loss 521314.09375
Epoch 79: Val Loss 521313.75000
Epoch 80: Val Loss 521313.37500
Epoch 81: Val Loss 521313.00000
Epoch 82: Val Loss 521312.65625
Epoch 83: Val Loss 521312.31250
Epoch 84: Val Loss 521311.90625
Epoch 85: Val Loss 521311.53125
Epoch 86: Val Loss 521311.15625
Epoch 87: Val Loss 521310.81250
Epoch 88: Val Loss 521310.40625
Epoch 89: Val Loss 521310.09375
Epoch 90: Val Loss 521309.78125
Epoch 91: Val Loss 521309.37500
Epoch 92: Val Loss 521308.96875
Epoch 93: Val Loss 521308.65625
Epoch 94: Val Loss 521308.28125
Epoch 95: Val Loss 521307.90625
Epoch 96: Val Loss 521307.56250
Epoch 97: Val Loss 521307.15625
Epoch 98: Val Loss 521306.84375
Epoch 99: Val Loss 521306.43750
{'MSE - mean': 520345.02709083015, 'MSE - std': 3017.2698619959147, 'R2 - mean': -60.80983327107775, 'R2 - std': 4.697512783851953} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517921.25000
Epoch 1: Val Loss 517920.81250
Epoch 2: Val Loss 517920.40625
Epoch 3: Val Loss 517919.96875
Epoch 4: Val Loss 517919.53125
Epoch 5: Val Loss 517919.12500
Epoch 6: Val Loss 517918.68750
Epoch 7: Val Loss 517918.21875
Epoch 8: Val Loss 517917.84375
Epoch 9: Val Loss 517917.40625
Epoch 10: Val Loss 517917.00000
Epoch 11: Val Loss 517916.53125
Epoch 12: Val Loss 517916.15625
Epoch 13: Val Loss 517915.68750
Epoch 14: Val Loss 517915.25000
Epoch 15: Val Loss 517914.87500
Epoch 16: Val Loss 517914.40625
Epoch 17: Val Loss 517914.00000
Epoch 18: Val Loss 517913.53125
Epoch 19: Val Loss 517913.12500
Epoch 20: Val Loss 517912.68750
Epoch 21: Val Loss 517912.31250
Epoch 22: Val Loss 517911.87500
Epoch 23: Val Loss 517911.46875
Epoch 24: Val Loss 517911.03125
Epoch 25: Val Loss 517910.62500
Epoch 26: Val Loss 517910.15625
Epoch 27: Val Loss 517909.78125
Epoch 28: Val Loss 517909.37500
Epoch 29: Val Loss 517908.93750
Epoch 30: Val Loss 517908.50000
Epoch 31: Val Loss 517908.12500
Epoch 32: Val Loss 517907.68750
Epoch 33: Val Loss 517907.21875
Epoch 34: Val Loss 517906.84375
Epoch 35: Val Loss 517906.37500
Epoch 36: Val Loss 517906.00000
Epoch 37: Val Loss 517905.56250
Epoch 38: Val Loss 517905.15625
Epoch 39: Val Loss 517904.75000
Epoch 40: Val Loss 517904.34375
Epoch 41: Val Loss 517903.90625
Epoch 42: Val Loss 517903.50000
Epoch 43: Val Loss 517903.06250
Epoch 44: Val Loss 517902.65625
Epoch 45: Val Loss 517902.28125
Epoch 46: Val Loss 517901.84375
Epoch 47: Val Loss 517901.40625
Epoch 48: Val Loss 517901.00000
Epoch 49: Val Loss 517900.59375
Epoch 50: Val Loss 517900.18750
Epoch 51: Val Loss 517899.75000
Epoch 52: Val Loss 517899.37500
Epoch 53: Val Loss 517898.90625
Epoch 54: Val Loss 517898.53125
Epoch 55: Val Loss 517898.12500
Epoch 56: Val Loss 517897.65625
Epoch 57: Val Loss 517897.28125
Epoch 58: Val Loss 517896.90625
Epoch 59: Val Loss 517896.43750
Epoch 60: Val Loss 517896.03125
Epoch 61: Val Loss 517895.59375
Epoch 62: Val Loss 517895.18750
Epoch 63: Val Loss 517894.78125
Epoch 64: Val Loss 517894.40625
Epoch 65: Val Loss 517893.96875
Epoch 66: Val Loss 517893.56250
Epoch 67: Val Loss 517893.15625
Epoch 68: Val Loss 517892.71875
Epoch 69: Val Loss 517892.31250
Epoch 70: Val Loss 517891.93750
Epoch 71: Val Loss 517891.50000
Epoch 72: Val Loss 517891.09375
Epoch 73: Val Loss 517890.68750
Epoch 74: Val Loss 517890.25000
Epoch 75: Val Loss 517889.90625
Epoch 76: Val Loss 517889.43750
Epoch 77: Val Loss 517889.03125
Epoch 78: Val Loss 517888.62500
Epoch 79: Val Loss 517888.25000
Epoch 80: Val Loss 517887.84375
Epoch 81: Val Loss 517887.46875
Epoch 82: Val Loss 517887.00000
Epoch 83: Val Loss 517886.62500
Epoch 84: Val Loss 517886.21875
Epoch 85: Val Loss 517885.81250
Epoch 86: Val Loss 517885.37500
Epoch 87: Val Loss 517885.00000
Epoch 88: Val Loss 517884.59375
Epoch 89: Val Loss 517884.18750
Epoch 90: Val Loss 517883.78125
Epoch 91: Val Loss 517883.40625
Epoch 92: Val Loss 517882.96875
Epoch 93: Val Loss 517882.56250
Epoch 94: Val Loss 517882.15625
Epoch 95: Val Loss 517881.75000
Epoch 96: Val Loss 517881.31250
Epoch 97: Val Loss 517880.96875
Epoch 98: Val Loss 517880.50000
Epoch 99: Val Loss 517880.15625
{'MSE - mean': 519852.05131860205, 'MSE - std': 2873.192368099035, 'R2 - mean': -61.43051188440908, 'R2 - std': 4.3811264359465385} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 519852.05131860205 and parameters: {'dim': 64, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 4 with value: 2588.5769697522965.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516371.15625
Epoch 1: Val Loss 515995.53125
Epoch 2: Val Loss 515224.21875
Epoch 3: Val Loss 513557.65625
Epoch 4: Val Loss 510320.28125
Epoch 5: Val Loss 504463.00000
Epoch 6: Val Loss 494494.96875
Epoch 7: Val Loss 478571.71875
Epoch 8: Val Loss 454417.21875
Epoch 9: Val Loss 420016.12500
Epoch 10: Val Loss 374305.87500
Epoch 11: Val Loss 317912.18750
Epoch 12: Val Loss 253548.76562
Epoch 13: Val Loss 187003.62500
Epoch 14: Val Loss 126277.96094
Epoch 15: Val Loss 78719.43750
Epoch 16: Val Loss 48077.34375
Epoch 17: Val Loss 31610.44336
Epoch 18: Val Loss 23530.91797
Epoch 19: Val Loss 19259.19922
Epoch 20: Val Loss 16468.99219
Epoch 21: Val Loss 14345.03027
Epoch 22: Val Loss 12787.57520
Epoch 23: Val Loss 11456.48047
Epoch 24: Val Loss 10386.98438
Epoch 25: Val Loss 9483.15430
Epoch 26: Val Loss 8723.66602
Epoch 27: Val Loss 8072.32178
Epoch 28: Val Loss 7510.72461
Epoch 29: Val Loss 7039.51953
Epoch 30: Val Loss 6621.47314
Epoch 31: Val Loss 6269.66016
Epoch 32: Val Loss 5966.16504
Epoch 33: Val Loss 5717.85645
Epoch 34: Val Loss 5482.95996
Epoch 35: Val Loss 5283.39307
Epoch 36: Val Loss 5130.77295
Epoch 37: Val Loss 4959.12695
Epoch 38: Val Loss 4835.56982
Epoch 39: Val Loss 4712.87988
Epoch 40: Val Loss 4616.00000
Epoch 41: Val Loss 4515.24658
Epoch 42: Val Loss 4428.76953
Epoch 43: Val Loss 4350.00928
Epoch 44: Val Loss 4293.77588
Epoch 45: Val Loss 4231.26221
Epoch 46: Val Loss 4174.40186
Epoch 47: Val Loss 4102.38477
Epoch 48: Val Loss 4056.08813
Epoch 49: Val Loss 4019.04761
Epoch 50: Val Loss 3973.14233
Epoch 51: Val Loss 3921.06982
Epoch 52: Val Loss 3886.29688
Epoch 53: Val Loss 3851.61694
Epoch 54: Val Loss 3803.43335
Epoch 55: Val Loss 3770.00000
Epoch 56: Val Loss 3734.63037
Epoch 57: Val Loss 3699.98730
Epoch 58: Val Loss 3660.83057
Epoch 59: Val Loss 3626.68579
Epoch 60: Val Loss 3608.31812
Epoch 61: Val Loss 3582.49658
Epoch 62: Val Loss 3561.27539
Epoch 63: Val Loss 3513.73047
Epoch 64: Val Loss 3484.42212
Epoch 65: Val Loss 3470.85767
Epoch 66: Val Loss 3437.15967
Epoch 67: Val Loss 3398.77686
Epoch 68: Val Loss 3376.47925
Epoch 69: Val Loss 3367.93701
Epoch 70: Val Loss 3332.76050
Epoch 71: Val Loss 3312.58057
Epoch 72: Val Loss 3276.07129
Epoch 73: Val Loss 3260.96875
Epoch 74: Val Loss 3251.20923
Epoch 75: Val Loss 3217.77832
Epoch 76: Val Loss 3184.09888
Epoch 77: Val Loss 3174.69385
Epoch 78: Val Loss 3166.46167
Epoch 79: Val Loss 3127.01196
Epoch 80: Val Loss 3108.35791
Epoch 81: Val Loss 3087.14136
Epoch 82: Val Loss 3060.61133
Epoch 83: Val Loss 3048.73535
Epoch 84: Val Loss 3014.97437
Epoch 85: Val Loss 3006.15063
Epoch 86: Val Loss 2991.14233
Epoch 87: Val Loss 2967.77783
Epoch 88: Val Loss 2947.38916
Epoch 89: Val Loss 2925.27734
Epoch 90: Val Loss 2918.66284
Epoch 91: Val Loss 2887.38770
Epoch 92: Val Loss 2870.50464
Epoch 93: Val Loss 2851.90991
Epoch 94: Val Loss 2842.47339
Epoch 95: Val Loss 2809.12061
Epoch 96: Val Loss 2808.75195
Epoch 97: Val Loss 2787.70410
Epoch 98: Val Loss 2751.28662
Epoch 99: Val Loss 2749.55933
{'MSE - mean': 2749.559310437188, 'MSE - std': 0.0, 'R2 - mean': 0.6803061093583634, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525005.18750
Epoch 1: Val Loss 524552.81250
Epoch 2: Val Loss 523616.06250
Epoch 3: Val Loss 521678.90625
Epoch 4: Val Loss 517955.56250
Epoch 5: Val Loss 511328.65625
Epoch 6: Val Loss 500319.53125
Epoch 7: Val Loss 483034.87500
Epoch 8: Val Loss 457074.59375
Epoch 9: Val Loss 420002.40625
Epoch 10: Val Loss 370649.87500
Epoch 11: Val Loss 310051.62500
Epoch 12: Val Loss 240448.92188
Epoch 13: Val Loss 169497.32812
Epoch 14: Val Loss 106939.50000
Epoch 15: Val Loss 61643.55469
Epoch 16: Val Loss 36260.18750
Epoch 17: Val Loss 25004.77734
Epoch 18: Val Loss 19945.10938
Epoch 19: Val Loss 16760.05469
Epoch 20: Val Loss 14436.19238
Epoch 21: Val Loss 12647.54492
Epoch 22: Val Loss 11187.44141
Epoch 23: Val Loss 9995.25098
Epoch 24: Val Loss 9009.78125
Epoch 25: Val Loss 8187.32422
Epoch 26: Val Loss 7495.81689
Epoch 27: Val Loss 6893.02051
Epoch 28: Val Loss 6401.94629
Epoch 29: Val Loss 5976.70850
Epoch 30: Val Loss 5614.63965
Epoch 31: Val Loss 5314.80371
Epoch 32: Val Loss 5046.23682
Epoch 33: Val Loss 4831.77051
Epoch 34: Val Loss 4635.94385
Epoch 35: Val Loss 4474.56689
Epoch 36: Val Loss 4327.33594
Epoch 37: Val Loss 4201.64502
Epoch 38: Val Loss 4097.42188
Epoch 39: Val Loss 4004.28125
Epoch 40: Val Loss 3915.91772
Epoch 41: Val Loss 3837.60840
Epoch 42: Val Loss 3769.66113
Epoch 43: Val Loss 3707.20093
Epoch 44: Val Loss 3653.49536
Epoch 45: Val Loss 3603.67310
Epoch 46: Val Loss 3558.17773
Epoch 47: Val Loss 3510.22754
Epoch 48: Val Loss 3473.82910
Epoch 49: Val Loss 3443.12158
Epoch 50: Val Loss 3404.84521
Epoch 51: Val Loss 3363.13867
Epoch 52: Val Loss 3333.62720
Epoch 53: Val Loss 3305.04370
Epoch 54: Val Loss 3271.91821
Epoch 55: Val Loss 3246.99609
Epoch 56: Val Loss 3223.06396
Epoch 57: Val Loss 3196.16235
Epoch 58: Val Loss 3177.18726
Epoch 59: Val Loss 3147.78882
Epoch 60: Val Loss 3123.06226
Epoch 61: Val Loss 3104.12915
Epoch 62: Val Loss 3081.14087
Epoch 63: Val Loss 3053.83765
Epoch 64: Val Loss 3042.24341
Epoch 65: Val Loss 3016.96167
Epoch 66: Val Loss 2993.39209
Epoch 67: Val Loss 2965.85620
Epoch 68: Val Loss 2978.64941
Epoch 69: Val Loss 2928.42969
Epoch 70: Val Loss 2915.84155
Epoch 71: Val Loss 2899.50879
Epoch 72: Val Loss 2880.76270
Epoch 73: Val Loss 2867.78052
Epoch 74: Val Loss 2849.02100
Epoch 75: Val Loss 2830.10156
Epoch 76: Val Loss 2808.68311
Epoch 77: Val Loss 2810.33301
Epoch 78: Val Loss 2785.98022
Epoch 79: Val Loss 2760.64502
Epoch 80: Val Loss 2738.14380
Epoch 81: Val Loss 2729.96484
Epoch 82: Val Loss 2706.64209
Epoch 83: Val Loss 2701.63257
Epoch 84: Val Loss 2688.24951
Epoch 85: Val Loss 2652.40723
Epoch 86: Val Loss 2649.51367
Epoch 87: Val Loss 2634.74292
Epoch 88: Val Loss 2609.41455
Epoch 89: Val Loss 2598.08472
Epoch 90: Val Loss 2586.26025
Epoch 91: Val Loss 2575.56128
Epoch 92: Val Loss 2554.40479
Epoch 93: Val Loss 2540.77881
Epoch 94: Val Loss 2530.83057
Epoch 95: Val Loss 2506.37451
Epoch 96: Val Loss 2495.01221
Epoch 97: Val Loss 2474.82959
Epoch 98: Val Loss 2469.34448
Epoch 99: Val Loss 2463.65332
{'MSE - mean': 2606.6062712731023, 'MSE - std': 142.95303916408557, 'R2 - mean': 0.6789292366946138, 'R2 - std': 0.0013768726637496953} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517541.65625
Epoch 1: Val Loss 516986.37500
Epoch 2: Val Loss 515893.25000
Epoch 3: Val Loss 513491.43750
Epoch 4: Val Loss 508545.28125
Epoch 5: Val Loss 499375.43750
Epoch 6: Val Loss 483828.21875
Epoch 7: Val Loss 459345.93750
Epoch 8: Val Loss 423709.12500
Epoch 9: Val Loss 374582.31250
Epoch 10: Val Loss 313305.09375
Epoch 11: Val Loss 242470.07812
Epoch 12: Val Loss 169889.29688
Epoch 13: Val Loss 105776.36719
Epoch 14: Val Loss 59055.16016
Epoch 15: Val Loss 32694.97852
Epoch 16: Val Loss 20935.27539
Epoch 17: Val Loss 15785.23340
Epoch 18: Val Loss 12837.68066
Epoch 19: Val Loss 10972.15918
Epoch 20: Val Loss 9588.27441
Epoch 21: Val Loss 8607.57422
Epoch 22: Val Loss 7791.81592
Epoch 23: Val Loss 7200.10840
Epoch 24: Val Loss 6770.37695
Epoch 25: Val Loss 6389.62891
Epoch 26: Val Loss 6087.84521
Epoch 27: Val Loss 5873.52295
Epoch 28: Val Loss 5698.82031
Epoch 29: Val Loss 5523.45508
Epoch 30: Val Loss 5377.92236
Epoch 31: Val Loss 5234.87744
Epoch 32: Val Loss 5088.59961
Epoch 33: Val Loss 5014.01416
Epoch 34: Val Loss 4918.73535
Epoch 35: Val Loss 4798.94189
Epoch 36: Val Loss 4711.78613
Epoch 37: Val Loss 4624.13379
Epoch 38: Val Loss 4570.69434
Epoch 39: Val Loss 4487.00098
Epoch 40: Val Loss 4409.68652
Epoch 41: Val Loss 4355.69922
Epoch 42: Val Loss 4285.13330
Epoch 43: Val Loss 4226.72314
Epoch 44: Val Loss 4174.04785
Epoch 45: Val Loss 4105.43848
Epoch 46: Val Loss 4026.82788
Epoch 47: Val Loss 3992.60083
Epoch 48: Val Loss 3947.34595
Epoch 49: Val Loss 3910.80322
Epoch 50: Val Loss 3859.01562
Epoch 51: Val Loss 3803.65845
Epoch 52: Val Loss 3746.35571
Epoch 53: Val Loss 3729.90576
Epoch 54: Val Loss 3683.70630
Epoch 55: Val Loss 3642.75269
Epoch 56: Val Loss 3593.87329
Epoch 57: Val Loss 3552.63647
Epoch 58: Val Loss 3551.10303
Epoch 59: Val Loss 3467.96509
Epoch 60: Val Loss 3455.56128
Epoch 61: Val Loss 3428.88647
Epoch 62: Val Loss 3360.29858
Epoch 63: Val Loss 3336.92700
Epoch 64: Val Loss 3333.72388
Epoch 65: Val Loss 3270.03369
Epoch 66: Val Loss 3244.63281
Epoch 67: Val Loss 3216.72778
Epoch 68: Val Loss 3224.22803
Epoch 69: Val Loss 3143.17139
Epoch 70: Val Loss 3153.51465
Epoch 71: Val Loss 3087.12695
Epoch 72: Val Loss 3094.63037
Epoch 73: Val Loss 3067.12231
Epoch 74: Val Loss 3023.25757
Epoch 75: Val Loss 2997.94946
Epoch 76: Val Loss 2976.92065
Epoch 77: Val Loss 2972.66089
Epoch 78: Val Loss 2907.06860
Epoch 79: Val Loss 2923.50000
Epoch 80: Val Loss 2897.59644
Epoch 81: Val Loss 2845.30273
Epoch 82: Val Loss 2841.54272
Epoch 83: Val Loss 2802.62378
Epoch 84: Val Loss 2801.17212
Epoch 85: Val Loss 2778.31128
Epoch 86: Val Loss 2743.73047
Epoch 87: Val Loss 2721.14185
Epoch 88: Val Loss 2716.00293
Epoch 89: Val Loss 2690.02271
Epoch 90: Val Loss 2655.90649
Epoch 91: Val Loss 2651.03711
Epoch 92: Val Loss 2612.89404
Epoch 93: Val Loss 2627.67090
Epoch 94: Val Loss 2588.92188
Epoch 95: Val Loss 2562.06763
Epoch 96: Val Loss 2571.65942
Epoch 97: Val Loss 2525.47974
Epoch 98: Val Loss 2505.92578
Epoch 99: Val Loss 2496.01196
{'MSE - mean': 2569.7414462932416, 'MSE - std': 127.83483450305668, 'R2 - mean': 0.6965710923496653, 'R2 - std': 0.024974667047406265} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521026.25000
Epoch 1: Val Loss 520601.46875
Epoch 2: Val Loss 519790.12500
Epoch 3: Val Loss 518160.53125
Epoch 4: Val Loss 514992.15625
Epoch 5: Val Loss 509241.62500
Epoch 6: Val Loss 499381.28125
Epoch 7: Val Loss 483685.93750
Epoch 8: Val Loss 459916.65625
Epoch 9: Val Loss 425996.43750
Epoch 10: Val Loss 381167.09375
Epoch 11: Val Loss 324931.56250
Epoch 12: Val Loss 260717.15625
Epoch 13: Val Loss 193136.56250
Epoch 14: Val Loss 130872.75781
Epoch 15: Val Loss 81170.17969
Epoch 16: Val Loss 48434.02344
Epoch 17: Val Loss 31188.62695
Epoch 18: Val Loss 23313.16992
Epoch 19: Val Loss 19332.57617
Epoch 20: Val Loss 16731.55273
Epoch 21: Val Loss 14751.61328
Epoch 22: Val Loss 13113.84668
Epoch 23: Val Loss 11782.02637
Epoch 24: Val Loss 10692.44238
Epoch 25: Val Loss 9774.66602
Epoch 26: Val Loss 8998.36328
Epoch 27: Val Loss 8348.30664
Epoch 28: Val Loss 7765.35645
Epoch 29: Val Loss 7265.26758
Epoch 30: Val Loss 6829.39014
Epoch 31: Val Loss 6470.25293
Epoch 32: Val Loss 6141.50684
Epoch 33: Val Loss 5858.50586
Epoch 34: Val Loss 5612.05859
Epoch 35: Val Loss 5396.89014
Epoch 36: Val Loss 5194.90869
Epoch 37: Val Loss 5008.23242
Epoch 38: Val Loss 4843.00537
Epoch 39: Val Loss 4691.09131
Epoch 40: Val Loss 4554.42920
Epoch 41: Val Loss 4423.18848
Epoch 42: Val Loss 4302.62793
Epoch 43: Val Loss 4195.66260
Epoch 44: Val Loss 4088.85913
Epoch 45: Val Loss 3990.55225
Epoch 46: Val Loss 3903.62695
Epoch 47: Val Loss 3817.06348
Epoch 48: Val Loss 3734.12598
Epoch 49: Val Loss 3656.89209
Epoch 50: Val Loss 3589.38159
Epoch 51: Val Loss 3515.37427
Epoch 52: Val Loss 3453.85767
Epoch 53: Val Loss 3389.36304
Epoch 54: Val Loss 3338.35547
Epoch 55: Val Loss 3280.63623
Epoch 56: Val Loss 3229.83472
Epoch 57: Val Loss 3184.98779
Epoch 58: Val Loss 3137.29712
Epoch 59: Val Loss 3095.48364
Epoch 60: Val Loss 3057.91992
Epoch 61: Val Loss 3020.69702
Epoch 62: Val Loss 2979.63354
Epoch 63: Val Loss 2940.74634
Epoch 64: Val Loss 2909.42212
Epoch 65: Val Loss 2872.64624
Epoch 66: Val Loss 2839.42310
Epoch 67: Val Loss 2819.65039
Epoch 68: Val Loss 2793.53296
Epoch 69: Val Loss 2751.76611
Epoch 70: Val Loss 2730.00171
Epoch 71: Val Loss 2707.46680
Epoch 72: Val Loss 2679.06641
Epoch 73: Val Loss 2650.42017
Epoch 74: Val Loss 2628.23364
Epoch 75: Val Loss 2602.73828
Epoch 76: Val Loss 2585.31396
Epoch 77: Val Loss 2567.38550
Epoch 78: Val Loss 2538.92969
Epoch 79: Val Loss 2517.27002
Epoch 80: Val Loss 2505.96899
Epoch 81: Val Loss 2482.05786
Epoch 82: Val Loss 2460.30078
Epoch 83: Val Loss 2443.93384
Epoch 84: Val Loss 2423.78149
Epoch 85: Val Loss 2410.36841
Epoch 86: Val Loss 2390.41187
Epoch 87: Val Loss 2369.58521
Epoch 88: Val Loss 2347.97705
Epoch 89: Val Loss 2336.79858
Epoch 90: Val Loss 2315.25952
Epoch 91: Val Loss 2298.96436
Epoch 92: Val Loss 2282.21753
Epoch 93: Val Loss 2264.83984
Epoch 94: Val Loss 2247.38208
Epoch 95: Val Loss 2234.07861
Epoch 96: Val Loss 2214.17505
Epoch 97: Val Loss 2210.29199
Epoch 98: Val Loss 2190.78979
Epoch 99: Val Loss 2169.28760
{'MSE - mean': 2469.6279518502834, 'MSE - std': 205.7290548101034, 'R2 - mean': 0.7071317214901904, 'R2 - std': 0.028326333315975144} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517527.31250
Epoch 1: Val Loss 517305.06250
Epoch 2: Val Loss 516921.18750
Epoch 3: Val Loss 516162.78125
Epoch 4: Val Loss 514660.46875
Epoch 5: Val Loss 511902.81250
Epoch 6: Val Loss 507143.84375
Epoch 7: Val Loss 499326.31250
Epoch 8: Val Loss 487015.34375
Epoch 9: Val Loss 468757.96875
Epoch 10: Val Loss 443178.68750
Epoch 11: Val Loss 409075.81250
Epoch 12: Val Loss 366273.28125
Epoch 13: Val Loss 315607.93750
Epoch 14: Val Loss 259081.82812
Epoch 15: Val Loss 200574.67188
Epoch 16: Val Loss 144935.67188
Epoch 17: Val Loss 98018.42969
Epoch 18: Val Loss 63252.40234
Epoch 19: Val Loss 41010.85156
Epoch 20: Val Loss 28644.83203
Epoch 21: Val Loss 22109.42188
Epoch 22: Val Loss 18408.87891
Epoch 23: Val Loss 16114.50977
Epoch 24: Val Loss 14366.17871
Epoch 25: Val Loss 12935.76855
Epoch 26: Val Loss 11777.03418
Epoch 27: Val Loss 10847.03320
Epoch 28: Val Loss 10002.17090
Epoch 29: Val Loss 9313.12305
Epoch 30: Val Loss 8716.33594
Epoch 31: Val Loss 8181.40527
Epoch 32: Val Loss 7714.85547
Epoch 33: Val Loss 7305.44531
Epoch 34: Val Loss 6953.64111
Epoch 35: Val Loss 6613.82666
Epoch 36: Val Loss 6344.65039
Epoch 37: Val Loss 6092.95020
Epoch 38: Val Loss 5851.02051
Epoch 39: Val Loss 5641.14209
Epoch 40: Val Loss 5429.02783
Epoch 41: Val Loss 5262.61816
Epoch 42: Val Loss 5088.22656
Epoch 43: Val Loss 4929.19580
Epoch 44: Val Loss 4804.17480
Epoch 45: Val Loss 4698.78467
Epoch 46: Val Loss 4564.72705
Epoch 47: Val Loss 4467.49902
Epoch 48: Val Loss 4360.88281
Epoch 49: Val Loss 4273.99463
Epoch 50: Val Loss 4175.57715
Epoch 51: Val Loss 4081.45776
Epoch 52: Val Loss 3998.07495
Epoch 53: Val Loss 3923.73389
Epoch 54: Val Loss 3852.31982
Epoch 55: Val Loss 3783.83081
Epoch 56: Val Loss 3714.06714
Epoch 57: Val Loss 3642.59570
Epoch 58: Val Loss 3610.75879
Epoch 59: Val Loss 3529.94897
Epoch 60: Val Loss 3469.45190
Epoch 61: Val Loss 3414.37720
Epoch 62: Val Loss 3362.57446
Epoch 63: Val Loss 3308.09229
Epoch 64: Val Loss 3269.23633
Epoch 65: Val Loss 3218.90405
Epoch 66: Val Loss 3166.11646
Epoch 67: Val Loss 3129.86670
Epoch 68: Val Loss 3094.58740
Epoch 69: Val Loss 3040.15576
Epoch 70: Val Loss 3002.91162
Epoch 71: Val Loss 2960.93994
Epoch 72: Val Loss 2938.89771
Epoch 73: Val Loss 2884.07520
Epoch 74: Val Loss 2845.87329
Epoch 75: Val Loss 2814.22607
Epoch 76: Val Loss 2794.63330
Epoch 77: Val Loss 2749.39453
Epoch 78: Val Loss 2706.50952
Epoch 79: Val Loss 2681.84546
Epoch 80: Val Loss 2661.38403
Epoch 81: Val Loss 2625.55176
Epoch 82: Val Loss 2591.80518
Epoch 83: Val Loss 2574.24097
Epoch 84: Val Loss 2552.04272
Epoch 85: Val Loss 2517.87378
Epoch 86: Val Loss 2492.98486
Epoch 87: Val Loss 2458.93066
Epoch 88: Val Loss 2443.61938
Epoch 89: Val Loss 2414.84766
Epoch 90: Val Loss 2396.85693
Epoch 91: Val Loss 2374.60400
Epoch 92: Val Loss 2356.97754
Epoch 93: Val Loss 2328.20776
Epoch 94: Val Loss 2308.94458
Epoch 95: Val Loss 2303.73877
Epoch 96: Val Loss 2256.88623
Epoch 97: Val Loss 2254.87451
Epoch 98: Val Loss 2236.30176
Epoch 99: Val Loss 2208.04858
{'MSE - mean': 2417.3120980283456, 'MSE - std': 211.67746559074328, 'R2 - mean': 0.7103521977364663, 'R2 - std': 0.026141744324231463} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 2417.3120980283456 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516582.28125
Epoch 1: Val Loss 516154.21875
Epoch 2: Val Loss 515300.09375
Epoch 3: Val Loss 513618.71875
Epoch 4: Val Loss 510420.12500
Epoch 5: Val Loss 504666.31250
Epoch 6: Val Loss 494951.00000
Epoch 7: Val Loss 479658.18750
Epoch 8: Val Loss 456934.21875
Epoch 9: Val Loss 425325.96875
Epoch 10: Val Loss 383550.90625
Epoch 11: Val Loss 331719.15625
Epoch 12: Val Loss 272248.46875
Epoch 13: Val Loss 208292.35938
Epoch 14: Val Loss 146918.23438
Epoch 15: Val Loss 95239.01562
Epoch 16: Val Loss 58365.05469
Epoch 17: Val Loss 36552.86328
Epoch 18: Val Loss 25761.34766
Epoch 19: Val Loss 20234.34570
Epoch 20: Val Loss 16975.14844
Epoch 21: Val Loss 14685.43164
Epoch 22: Val Loss 12993.46191
Epoch 23: Val Loss 11626.13965
Epoch 24: Val Loss 10505.24414
Epoch 25: Val Loss 9602.49219
Epoch 26: Val Loss 8887.67773
Epoch 27: Val Loss 8272.58203
Epoch 28: Val Loss 7768.77344
Epoch 29: Val Loss 7321.64600
Epoch 30: Val Loss 6964.23486
Epoch 31: Val Loss 6673.01318
Epoch 32: Val Loss 6420.00732
Epoch 33: Val Loss 6193.18018
Epoch 34: Val Loss 5999.34521
Epoch 35: Val Loss 5821.96875
Epoch 36: Val Loss 5671.53906
Epoch 37: Val Loss 5536.50244
Epoch 38: Val Loss 5414.60693
Epoch 39: Val Loss 5297.54932
Epoch 40: Val Loss 5188.34619
Epoch 41: Val Loss 5095.78223
Epoch 42: Val Loss 5001.42383
Epoch 43: Val Loss 4920.52051
Epoch 44: Val Loss 4833.12402
Epoch 45: Val Loss 4759.95361
Epoch 46: Val Loss 4694.02686
Epoch 47: Val Loss 4618.94141
Epoch 48: Val Loss 4545.54297
Epoch 49: Val Loss 4486.45752
Epoch 50: Val Loss 4425.52930
Epoch 51: Val Loss 4355.33936
Epoch 52: Val Loss 4297.86279
Epoch 53: Val Loss 4251.13965
Epoch 54: Val Loss 4199.72070
Epoch 55: Val Loss 4148.09229
Epoch 56: Val Loss 4097.02832
Epoch 57: Val Loss 4055.71143
Epoch 58: Val Loss 4012.25977
Epoch 59: Val Loss 3959.83862
Epoch 60: Val Loss 3921.51270
Epoch 61: Val Loss 3881.60181
Epoch 62: Val Loss 3840.10034
Epoch 63: Val Loss 3810.33008
Epoch 64: Val Loss 3771.61938
Epoch 65: Val Loss 3735.90405
Epoch 66: Val Loss 3705.79053
Epoch 67: Val Loss 3680.29419
Epoch 68: Val Loss 3652.57397
Epoch 69: Val Loss 3619.66455
Epoch 70: Val Loss 3590.32812
Epoch 71: Val Loss 3565.12012
Epoch 72: Val Loss 3536.57324
Epoch 73: Val Loss 3510.17896
Epoch 74: Val Loss 3486.42847
Epoch 75: Val Loss 3460.45654
Epoch 76: Val Loss 3432.70898
Epoch 77: Val Loss 3417.58203
Epoch 78: Val Loss 3393.31885
Epoch 79: Val Loss 3368.62500
Epoch 80: Val Loss 3348.82690
Epoch 81: Val Loss 3327.28760
Epoch 82: Val Loss 3313.71118
Epoch 83: Val Loss 3285.17236
Epoch 84: Val Loss 3260.89087
Epoch 85: Val Loss 3258.00610
Epoch 86: Val Loss 3225.94263
Epoch 87: Val Loss 3200.16504
Epoch 88: Val Loss 3193.73071
Epoch 89: Val Loss 3166.64209
Epoch 90: Val Loss 3150.39282
Epoch 91: Val Loss 3137.44946
Epoch 92: Val Loss 3114.42773
Epoch 93: Val Loss 3102.31592
Epoch 94: Val Loss 3078.60864
Epoch 95: Val Loss 3075.80249
Epoch 96: Val Loss 3050.62524
Epoch 97: Val Loss 3026.36401
Epoch 98: Val Loss 3019.10913
Epoch 99: Val Loss 3003.25586
{'MSE - mean': 3003.2558357264083, 'MSE - std': 0.0, 'R2 - mean': 0.6508085717332961, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524135.84375
Epoch 1: Val Loss 523054.40625
Epoch 2: Val Loss 521123.37500
Epoch 3: Val Loss 517594.28125
Epoch 4: Val Loss 511206.59375
Epoch 5: Val Loss 500183.12500
Epoch 6: Val Loss 482536.18750
Epoch 7: Val Loss 455778.56250
Epoch 8: Val Loss 417818.03125
Epoch 9: Val Loss 367616.03125
Epoch 10: Val Loss 305637.84375
Epoch 11: Val Loss 235125.65625
Epoch 12: Val Loss 164860.89062
Epoch 13: Val Loss 103566.85938
Epoch 14: Val Loss 59513.46875
Epoch 15: Val Loss 34690.34766
Epoch 16: Val Loss 23518.64062
Epoch 17: Val Loss 18397.13867
Epoch 18: Val Loss 15295.12793
Epoch 19: Val Loss 12928.85059
Epoch 20: Val Loss 11189.31836
Epoch 21: Val Loss 9801.82617
Epoch 22: Val Loss 8687.63770
Epoch 23: Val Loss 7841.67041
Epoch 24: Val Loss 7129.67285
Epoch 25: Val Loss 6567.51514
Epoch 26: Val Loss 6110.06348
Epoch 27: Val Loss 5756.67529
Epoch 28: Val Loss 5456.63867
Epoch 29: Val Loss 5233.89160
Epoch 30: Val Loss 5016.30371
Epoch 31: Val Loss 4863.71338
Epoch 32: Val Loss 4718.76318
Epoch 33: Val Loss 4620.70605
Epoch 34: Val Loss 4558.63477
Epoch 35: Val Loss 4466.36328
Epoch 36: Val Loss 4391.24512
Epoch 37: Val Loss 4335.29297
Epoch 38: Val Loss 4304.68164
Epoch 39: Val Loss 4271.22070
Epoch 40: Val Loss 4221.12549
Epoch 41: Val Loss 4178.28320
Epoch 42: Val Loss 4178.71875
Epoch 43: Val Loss 4126.93506
Epoch 44: Val Loss 4106.67822
Epoch 45: Val Loss 4083.10596
Epoch 46: Val Loss 4062.89087
Epoch 47: Val Loss 4038.18652
Epoch 48: Val Loss 4001.10010
Epoch 49: Val Loss 3989.46680
Epoch 50: Val Loss 3961.82373
Epoch 51: Val Loss 3932.75366
Epoch 52: Val Loss 3936.48804
Epoch 53: Val Loss 3885.86792
Epoch 54: Val Loss 3883.67773
Epoch 55: Val Loss 3840.09131
Epoch 56: Val Loss 3839.96118
Epoch 57: Val Loss 3817.85645
Epoch 58: Val Loss 3782.98071
Epoch 59: Val Loss 3765.55981
Epoch 60: Val Loss 3734.67261
Epoch 61: Val Loss 3771.35474
Epoch 62: Val Loss 3718.60132
Epoch 63: Val Loss 3701.80347
Epoch 64: Val Loss 3657.66870
Epoch 65: Val Loss 3672.53198
Epoch 66: Val Loss 3673.68164
Epoch 67: Val Loss 3653.66650
Epoch 68: Val Loss 3593.44214
Epoch 69: Val Loss 3606.56567
Epoch 70: Val Loss 3584.28076
Epoch 71: Val Loss 3544.96021
Epoch 72: Val Loss 3542.27612
Epoch 73: Val Loss 3522.24463
Epoch 74: Val Loss 3510.93530
Epoch 75: Val Loss 3475.02393
Epoch 76: Val Loss 3478.18335
Epoch 77: Val Loss 3442.32568
Epoch 78: Val Loss 3427.47925
Epoch 79: Val Loss 3436.43018
Epoch 80: Val Loss 3387.34595
Epoch 81: Val Loss 3374.01929
Epoch 82: Val Loss 3363.50391
Epoch 83: Val Loss 3356.81250
Epoch 84: Val Loss 3307.28882
Epoch 85: Val Loss 3290.69092
Epoch 86: Val Loss 3314.89673
Epoch 87: Val Loss 3247.34766
Epoch 88: Val Loss 3277.66357
Epoch 89: Val Loss 3229.99146
Epoch 90: Val Loss 3240.51318
Epoch 91: Val Loss 3189.24902
Epoch 92: Val Loss 3192.08521
Epoch 93: Val Loss 3173.85425
Epoch 94: Val Loss 3162.26929
Epoch 95: Val Loss 3130.22241
Epoch 96: Val Loss 3136.24146
Epoch 97: Val Loss 3100.73584
Epoch 98: Val Loss 3071.20239
Epoch 99: Val Loss 3051.07690
{'MSE - mean': 3027.166357469281, 'MSE - std': 23.91052174287279, 'R2 - mean': 0.6257389039776854, 'R2 - std': 0.025069667755610703} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518047.90625
Epoch 1: Val Loss 517594.37500
Epoch 2: Val Loss 516806.00000
Epoch 3: Val Loss 515328.37500
Epoch 4: Val Loss 512583.18750
Epoch 5: Val Loss 507648.62500
Epoch 6: Val Loss 499182.96875
Epoch 7: Val Loss 485433.34375
Epoch 8: Val Loss 464176.12500
Epoch 9: Val Loss 432968.90625
Epoch 10: Val Loss 390710.56250
Epoch 11: Val Loss 336425.56250
Epoch 12: Val Loss 273224.50000
Epoch 13: Val Loss 205253.14062
Epoch 14: Val Loss 140492.06250
Epoch 15: Val Loss 87724.21875
Epoch 16: Val Loss 51837.57031
Epoch 17: Val Loss 32135.94727
Epoch 18: Val Loss 22757.60938
Epoch 19: Val Loss 17929.12109
Epoch 20: Val Loss 15015.90820
Epoch 21: Val Loss 12916.72168
Epoch 22: Val Loss 11414.09277
Epoch 23: Val Loss 10108.40625
Epoch 24: Val Loss 9090.64355
Epoch 25: Val Loss 8278.12402
Epoch 26: Val Loss 7625.83398
Epoch 27: Val Loss 7085.78418
Epoch 28: Val Loss 6643.73340
Epoch 29: Val Loss 6274.20508
Epoch 30: Val Loss 5965.45703
Epoch 31: Val Loss 5717.94580
Epoch 32: Val Loss 5512.59814
Epoch 33: Val Loss 5309.36670
Epoch 34: Val Loss 5147.28760
Epoch 35: Val Loss 5024.74609
Epoch 36: Val Loss 4905.76123
Epoch 37: Val Loss 4801.41650
Epoch 38: Val Loss 4712.73193
Epoch 39: Val Loss 4609.16895
Epoch 40: Val Loss 4556.85645
Epoch 41: Val Loss 4454.85742
Epoch 42: Val Loss 4359.61475
Epoch 43: Val Loss 4296.04248
Epoch 44: Val Loss 4244.94971
Epoch 45: Val Loss 4185.53369
Epoch 46: Val Loss 4115.82617
Epoch 47: Val Loss 4099.31689
Epoch 48: Val Loss 4011.93091
Epoch 49: Val Loss 3964.04468
Epoch 50: Val Loss 3941.56445
Epoch 51: Val Loss 3869.21729
Epoch 52: Val Loss 3856.63867
Epoch 53: Val Loss 3775.72290
Epoch 54: Val Loss 3741.84619
Epoch 55: Val Loss 3701.57202
Epoch 56: Val Loss 3660.17700
Epoch 57: Val Loss 3622.74219
Epoch 58: Val Loss 3585.65771
Epoch 59: Val Loss 3588.76929
Epoch 60: Val Loss 3543.67163
Epoch 61: Val Loss 3487.82275
Epoch 62: Val Loss 3459.84326
Epoch 63: Val Loss 3455.58813
Epoch 64: Val Loss 3394.17603
Epoch 65: Val Loss 3396.60107
Epoch 66: Val Loss 3368.46851
Epoch 67: Val Loss 3335.42358
Epoch 68: Val Loss 3305.01050
Epoch 69: Val Loss 3286.57983
Epoch 70: Val Loss 3255.73193
Epoch 71: Val Loss 3241.55981
Epoch 72: Val Loss 3199.87329
Epoch 73: Val Loss 3198.47827
Epoch 74: Val Loss 3162.58276
Epoch 75: Val Loss 3157.08667
Epoch 76: Val Loss 3106.72705
Epoch 77: Val Loss 3091.82031
Epoch 78: Val Loss 3093.14209
Epoch 79: Val Loss 3044.65381
Epoch 80: Val Loss 3032.07910
Epoch 81: Val Loss 3006.36523
Epoch 82: Val Loss 2980.95923
Epoch 83: Val Loss 2971.70044
Epoch 84: Val Loss 2938.30957
Epoch 85: Val Loss 2934.03955
Epoch 86: Val Loss 2907.04004
Epoch 87: Val Loss 2899.90918
Epoch 88: Val Loss 2868.59131
Epoch 89: Val Loss 2856.89380
Epoch 90: Val Loss 2834.92114
Epoch 91: Val Loss 2812.05688
Epoch 92: Val Loss 2800.21484
Epoch 93: Val Loss 2778.74438
Epoch 94: Val Loss 2773.26050
Epoch 95: Val Loss 2748.79810
Epoch 96: Val Loss 2713.41772
Epoch 97: Val Loss 2714.14453
Epoch 98: Val Loss 2700.93433
Epoch 99: Val Loss 2663.47266
{'MSE - mean': 2905.935230827055, 'MSE - std': 172.55467008235766, 'R2 - mean': 0.6551141158514778, 'R2 - std': 0.04631196720263773} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520754.12500
Epoch 1: Val Loss 520239.90625
Epoch 2: Val Loss 519280.50000
Epoch 3: Val Loss 517405.50000
Epoch 4: Val Loss 513820.71875
Epoch 5: Val Loss 507289.93750
Epoch 6: Val Loss 495835.87500
Epoch 7: Val Loss 477042.90625
Epoch 8: Val Loss 448108.59375
Epoch 9: Val Loss 406445.96875
Epoch 10: Val Loss 350856.46875
Epoch 11: Val Loss 283118.46875
Epoch 12: Val Loss 207243.46875
Epoch 13: Val Loss 133264.85938
Epoch 14: Val Loss 72479.70312
Epoch 15: Val Loss 33484.44531
Epoch 16: Val Loss 15960.40137
Epoch 17: Val Loss 10571.97559
Epoch 18: Val Loss 8927.66211
Epoch 19: Val Loss 7984.33496
Epoch 20: Val Loss 7288.00781
Epoch 21: Val Loss 6800.58105
Epoch 22: Val Loss 6459.48438
Epoch 23: Val Loss 6213.45996
Epoch 24: Val Loss 6019.27393
Epoch 25: Val Loss 5869.37891
Epoch 26: Val Loss 5749.22461
Epoch 27: Val Loss 5627.99072
Epoch 28: Val Loss 5534.56299
Epoch 29: Val Loss 5440.16455
Epoch 30: Val Loss 5353.55762
Epoch 31: Val Loss 5280.63818
Epoch 32: Val Loss 5196.29102
Epoch 33: Val Loss 5132.36035
Epoch 34: Val Loss 5053.39014
Epoch 35: Val Loss 4991.47705
Epoch 36: Val Loss 4953.35938
Epoch 37: Val Loss 4886.38135
Epoch 38: Val Loss 4824.72754
Epoch 39: Val Loss 4775.73389
Epoch 40: Val Loss 4707.37695
Epoch 41: Val Loss 4643.52344
Epoch 42: Val Loss 4584.84131
Epoch 43: Val Loss 4560.38379
Epoch 44: Val Loss 4493.91455
Epoch 45: Val Loss 4436.65186
Epoch 46: Val Loss 4419.31299
Epoch 47: Val Loss 4372.92139
Epoch 48: Val Loss 4334.73975
Epoch 49: Val Loss 4270.78125
Epoch 50: Val Loss 4226.51807
Epoch 51: Val Loss 4180.86084
Epoch 52: Val Loss 4167.69287
Epoch 53: Val Loss 4105.46924
Epoch 54: Val Loss 4062.75903
Epoch 55: Val Loss 4033.32739
Epoch 56: Val Loss 3994.40381
Epoch 57: Val Loss 3958.12817
Epoch 58: Val Loss 3900.64014
Epoch 59: Val Loss 3875.46875
Epoch 60: Val Loss 3859.11963
Epoch 61: Val Loss 3821.13477
Epoch 62: Val Loss 3786.47388
Epoch 63: Val Loss 3730.76440
Epoch 64: Val Loss 3725.95410
Epoch 65: Val Loss 3701.63403
Epoch 66: Val Loss 3645.78223
Epoch 67: Val Loss 3631.52637
Epoch 68: Val Loss 3586.69531
Epoch 69: Val Loss 3577.94775
Epoch 70: Val Loss 3550.79761
Epoch 71: Val Loss 3511.04321
Epoch 72: Val Loss 3493.16968
Epoch 73: Val Loss 3448.10132
Epoch 74: Val Loss 3427.26099
Epoch 75: Val Loss 3429.21777
Epoch 76: Val Loss 3365.09741
Epoch 77: Val Loss 3344.97095
Epoch 78: Val Loss 3319.34888
Epoch 79: Val Loss 3306.63159
Epoch 80: Val Loss 3294.05322
Epoch 81: Val Loss 3236.36621
Epoch 82: Val Loss 3239.54028
Epoch 83: Val Loss 3202.36597
Epoch 84: Val Loss 3182.60815
Epoch 85: Val Loss 3148.78491
Epoch 86: Val Loss 3134.62866
Epoch 87: Val Loss 3112.96021
Epoch 88: Val Loss 3070.53027
Epoch 89: Val Loss 3069.89331
Epoch 90: Val Loss 3039.56665
Epoch 91: Val Loss 3023.04053
Epoch 92: Val Loss 3005.17261
Epoch 93: Val Loss 2987.65869
Epoch 94: Val Loss 2947.82520
Epoch 95: Val Loss 2957.65063
Epoch 96: Val Loss 2908.62842
Epoch 97: Val Loss 2872.14844
Epoch 98: Val Loss 2872.00659
Epoch 99: Val Loss 2844.08984
{'MSE - mean': 2890.4739309040747, 'MSE - std': 151.81729483609075, 'R2 - mean': 0.6557271069220816, 'R2 - std': 0.040121390850661756} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517162.65625
Epoch 1: Val Loss 516438.40625
Epoch 2: Val Loss 515162.84375
Epoch 3: Val Loss 512880.50000
Epoch 4: Val Loss 508736.56250
Epoch 5: Val Loss 501569.59375
Epoch 6: Val Loss 489702.59375
Epoch 7: Val Loss 470714.84375
Epoch 8: Val Loss 442276.31250
Epoch 9: Val Loss 402153.21875
Epoch 10: Val Loss 349720.43750
Epoch 11: Val Loss 285750.15625
Epoch 12: Val Loss 216094.43750
Epoch 13: Val Loss 148575.07812
Epoch 14: Val Loss 93182.43750
Epoch 15: Val Loss 56233.32812
Epoch 16: Val Loss 36458.35547
Epoch 17: Val Loss 27063.68359
Epoch 18: Val Loss 22066.35547
Epoch 19: Val Loss 18710.70312
Epoch 20: Val Loss 16270.82324
Epoch 21: Val Loss 14337.70898
Epoch 22: Val Loss 12772.22070
Epoch 23: Val Loss 11435.65039
Epoch 24: Val Loss 10378.31543
Epoch 25: Val Loss 9445.53223
Epoch 26: Val Loss 8676.82422
Epoch 27: Val Loss 8070.78369
Epoch 28: Val Loss 7561.09619
Epoch 29: Val Loss 7075.74658
Epoch 30: Val Loss 6697.03613
Epoch 31: Val Loss 6365.12354
Epoch 32: Val Loss 6064.79883
Epoch 33: Val Loss 5844.00146
Epoch 34: Val Loss 5611.23193
Epoch 35: Val Loss 5430.64209
Epoch 36: Val Loss 5229.06299
Epoch 37: Val Loss 5080.95508
Epoch 38: Val Loss 4941.72607
Epoch 39: Val Loss 4806.75830
Epoch 40: Val Loss 4679.20166
Epoch 41: Val Loss 4558.85596
Epoch 42: Val Loss 4436.57520
Epoch 43: Val Loss 4342.74951
Epoch 44: Val Loss 4263.84668
Epoch 45: Val Loss 4172.12549
Epoch 46: Val Loss 4084.79785
Epoch 47: Val Loss 4018.14160
Epoch 48: Val Loss 3942.26099
Epoch 49: Val Loss 3865.86108
Epoch 50: Val Loss 3805.71631
Epoch 51: Val Loss 3740.00244
Epoch 52: Val Loss 3688.81128
Epoch 53: Val Loss 3624.40210
Epoch 54: Val Loss 3583.46802
Epoch 55: Val Loss 3543.91748
Epoch 56: Val Loss 3495.20605
Epoch 57: Val Loss 3445.07397
Epoch 58: Val Loss 3399.73169
Epoch 59: Val Loss 3367.53882
Epoch 60: Val Loss 3329.35352
Epoch 61: Val Loss 3289.66406
Epoch 62: Val Loss 3263.50220
Epoch 63: Val Loss 3211.82397
Epoch 64: Val Loss 3182.12622
Epoch 65: Val Loss 3153.59180
Epoch 66: Val Loss 3125.44531
Epoch 67: Val Loss 3089.84839
Epoch 68: Val Loss 3066.49292
Epoch 69: Val Loss 3024.28491
Epoch 70: Val Loss 2995.21118
Epoch 71: Val Loss 2972.33667
Epoch 72: Val Loss 2940.41284
Epoch 73: Val Loss 2921.34595
Epoch 74: Val Loss 2891.11523
Epoch 75: Val Loss 2867.87134
Epoch 76: Val Loss 2842.08008
Epoch 77: Val Loss 2811.08008
Epoch 78: Val Loss 2789.59033
Epoch 79: Val Loss 2772.89380
Epoch 80: Val Loss 2734.31641
Epoch 81: Val Loss 2728.70801
Epoch 82: Val Loss 2693.69409
Epoch 83: Val Loss 2667.70068
Epoch 84: Val Loss 2660.21729
Epoch 85: Val Loss 2620.34814
Epoch 86: Val Loss 2600.06958
Epoch 87: Val Loss 2597.40430
Epoch 88: Val Loss 2562.04395
Epoch 89: Val Loss 2531.12158
Epoch 90: Val Loss 2524.75854
Epoch 91: Val Loss 2498.97095
Epoch 92: Val Loss 2472.63257
Epoch 93: Val Loss 2457.01416
Epoch 94: Val Loss 2448.63110
Epoch 95: Val Loss 2414.02539
Epoch 96: Val Loss 2393.99951
Epoch 97: Val Loss 2387.23560
Epoch 98: Val Loss 2357.75562
Epoch 99: Val Loss 2341.98828
{'MSE - mean': 2780.7767921144537, 'MSE - std': 258.0167472153627, 'R2 - mean': 0.6658707992349109, 'R2 - std': 0.04122327956364784} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 2780.7767921144537 and parameters: {'dim': 128, 'depth': 1, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516629.31250
Epoch 1: Val Loss 516126.37500
Epoch 2: Val Loss 515111.90625
Epoch 3: Val Loss 513126.87500
Epoch 4: Val Loss 509320.31250
Epoch 5: Val Loss 502356.62500
Epoch 6: Val Loss 490673.06250
Epoch 7: Val Loss 472061.28125
Epoch 8: Val Loss 443814.81250
Epoch 9: Val Loss 403197.56250
Epoch 10: Val Loss 348476.71875
Epoch 11: Val Loss 280751.71875
Epoch 12: Val Loss 204765.46875
Epoch 13: Val Loss 130371.43750
Epoch 14: Val Loss 68663.66406
Epoch 15: Val Loss 29555.41797
Epoch 16: Val Loss 12425.11328
Epoch 17: Val Loss 8304.28809
Epoch 18: Val Loss 7705.64795
Epoch 19: Val Loss 7493.23096
Epoch 20: Val Loss 7278.11963
Epoch 21: Val Loss 7103.19873
Epoch 22: Val Loss 6937.81250
Epoch 23: Val Loss 6797.70850
Epoch 24: Val Loss 6661.18750
Epoch 25: Val Loss 6520.30273
Epoch 26: Val Loss 6387.06836
Epoch 27: Val Loss 6271.13135
Epoch 28: Val Loss 6162.89404
Epoch 29: Val Loss 6049.38232
Epoch 30: Val Loss 5942.53271
Epoch 31: Val Loss 5841.84961
Epoch 32: Val Loss 5752.32861
Epoch 33: Val Loss 5653.00098
Epoch 34: Val Loss 5568.18115
Epoch 35: Val Loss 5487.74707
Epoch 36: Val Loss 5411.63867
Epoch 37: Val Loss 5345.52344
Epoch 38: Val Loss 5284.47607
Epoch 39: Val Loss 5224.76025
Epoch 40: Val Loss 5161.38672
Epoch 41: Val Loss 5098.90723
Epoch 42: Val Loss 5053.35938
Epoch 43: Val Loss 5007.66406
Epoch 44: Val Loss 4957.25293
Epoch 45: Val Loss 4917.79053
Epoch 46: Val Loss 4881.00732
Epoch 47: Val Loss 4839.63623
Epoch 48: Val Loss 4795.95752
Epoch 49: Val Loss 4755.78760
Epoch 50: Val Loss 4726.52637
Epoch 51: Val Loss 4686.54443
Epoch 52: Val Loss 4651.78027
Epoch 53: Val Loss 4617.55176
Epoch 54: Val Loss 4578.48682
Epoch 55: Val Loss 4561.73535
Epoch 56: Val Loss 4519.65381
Epoch 57: Val Loss 4479.06201
Epoch 58: Val Loss 4453.59277
Epoch 59: Val Loss 4437.28857
Epoch 60: Val Loss 4390.65479
Epoch 61: Val Loss 4370.42578
Epoch 62: Val Loss 4340.08691
Epoch 63: Val Loss 4306.60352
Epoch 64: Val Loss 4277.00244
Epoch 65: Val Loss 4253.66895
Epoch 66: Val Loss 4223.64893
Epoch 67: Val Loss 4195.90381
Epoch 68: Val Loss 4164.03516
Epoch 69: Val Loss 4133.33936
Epoch 70: Val Loss 4119.78418
Epoch 71: Val Loss 4086.87256
Epoch 72: Val Loss 4043.21436
Epoch 73: Val Loss 4031.44824
Epoch 74: Val Loss 3999.98291
Epoch 75: Val Loss 3966.41406
Epoch 76: Val Loss 3931.61914
Epoch 77: Val Loss 3910.39038
Epoch 78: Val Loss 3915.99805
Epoch 79: Val Loss 3863.81836
Epoch 80: Val Loss 3833.49902
Epoch 81: Val Loss 3808.87793
Epoch 82: Val Loss 3783.91699
Epoch 83: Val Loss 3756.69141
Epoch 84: Val Loss 3733.02246
Epoch 85: Val Loss 3694.98462
Epoch 86: Val Loss 3684.85645
Epoch 87: Val Loss 3661.87451
Epoch 88: Val Loss 3621.12769
Epoch 89: Val Loss 3601.24194
Epoch 90: Val Loss 3577.77539
Epoch 91: Val Loss 3563.02783
Epoch 92: Val Loss 3525.28613
Epoch 93: Val Loss 3510.77832
Epoch 94: Val Loss 3477.86841
Epoch 95: Val Loss 3446.04468
Epoch 96: Val Loss 3430.46924
Epoch 97: Val Loss 3404.77905
Epoch 98: Val Loss 3394.20752
Epoch 99: Val Loss 3340.54370
{'MSE - mean': 3340.543712808008, 'MSE - std': 0.0, 'R2 - mean': 0.6115917876904272, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524499.31250
Epoch 1: Val Loss 524000.03125
Epoch 2: Val Loss 522971.96875
Epoch 3: Val Loss 520844.84375
Epoch 4: Val Loss 516776.75000
Epoch 5: Val Loss 509482.65625
Epoch 6: Val Loss 497241.96875
Epoch 7: Val Loss 477728.43750
Epoch 8: Val Loss 448627.50000
Epoch 9: Val Loss 407841.46875
Epoch 10: Val Loss 355021.34375
Epoch 11: Val Loss 290979.75000
Epoch 12: Val Loss 220302.62500
Epoch 13: Val Loss 150566.54688
Epoch 14: Val Loss 91535.85938
Epoch 15: Val Loss 50101.17578
Epoch 16: Val Loss 27507.13867
Epoch 17: Val Loss 17785.91211
Epoch 18: Val Loss 13629.59570
Epoch 19: Val Loss 11123.20898
Epoch 20: Val Loss 9330.53906
Epoch 21: Val Loss 8024.14160
Epoch 22: Val Loss 7085.38623
Epoch 23: Val Loss 6381.94385
Epoch 24: Val Loss 5871.71533
Epoch 25: Val Loss 5513.41650
Epoch 26: Val Loss 5246.42139
Epoch 27: Val Loss 5042.30762
Epoch 28: Val Loss 4901.50000
Epoch 29: Val Loss 4768.48340
Epoch 30: Val Loss 4679.57715
Epoch 31: Val Loss 4593.45068
Epoch 32: Val Loss 4526.24805
Epoch 33: Val Loss 4453.22900
Epoch 34: Val Loss 4400.60254
Epoch 35: Val Loss 4341.25537
Epoch 36: Val Loss 4304.69434
Epoch 37: Val Loss 4248.81299
Epoch 38: Val Loss 4193.40967
Epoch 39: Val Loss 4152.21924
Epoch 40: Val Loss 4108.16260
Epoch 41: Val Loss 4071.28442
Epoch 42: Val Loss 4027.26318
Epoch 43: Val Loss 3974.90552
Epoch 44: Val Loss 3947.84839
Epoch 45: Val Loss 3902.52588
Epoch 46: Val Loss 3879.64233
Epoch 47: Val Loss 3827.75708
Epoch 48: Val Loss 3811.18213
Epoch 49: Val Loss 3774.10889
Epoch 50: Val Loss 3756.95312
Epoch 51: Val Loss 3725.01123
Epoch 52: Val Loss 3689.17407
Epoch 53: Val Loss 3646.37207
Epoch 54: Val Loss 3638.71875
Epoch 55: Val Loss 3614.99390
Epoch 56: Val Loss 3587.75464
Epoch 57: Val Loss 3561.59106
Epoch 58: Val Loss 3539.47681
Epoch 59: Val Loss 3511.27954
Epoch 60: Val Loss 3506.39746
Epoch 61: Val Loss 3469.58960
Epoch 62: Val Loss 3439.78345
Epoch 63: Val Loss 3408.54150
Epoch 64: Val Loss 3415.31885
Epoch 65: Val Loss 3392.87207
Epoch 66: Val Loss 3373.36694
Epoch 67: Val Loss 3356.68335
Epoch 68: Val Loss 3348.10229
Epoch 69: Val Loss 3322.98901
Epoch 70: Val Loss 3300.89185
Epoch 71: Val Loss 3290.20459
Epoch 72: Val Loss 3277.98193
Epoch 73: Val Loss 3244.21924
Epoch 74: Val Loss 3238.64648
Epoch 75: Val Loss 3221.75903
Epoch 76: Val Loss 3213.78027
Epoch 77: Val Loss 3208.28149
Epoch 78: Val Loss 3164.01660
Epoch 79: Val Loss 3160.29736
Epoch 80: Val Loss 3144.77173
Epoch 81: Val Loss 3133.76318
Epoch 82: Val Loss 3116.48975
Epoch 83: Val Loss 3099.93774
Epoch 84: Val Loss 3089.95703
Epoch 85: Val Loss 3068.90918
Epoch 86: Val Loss 3045.10742
Epoch 87: Val Loss 3042.42090
Epoch 88: Val Loss 3027.28760
Epoch 89: Val Loss 3013.22192
Epoch 90: Val Loss 3001.48315
Epoch 91: Val Loss 2976.47510
Epoch 92: Val Loss 2980.42456
Epoch 93: Val Loss 2959.12866
Epoch 94: Val Loss 2941.43970
Epoch 95: Val Loss 2917.04053
Epoch 96: Val Loss 2909.75757
Epoch 97: Val Loss 2900.02490
Epoch 98: Val Loss 2892.06250
Epoch 99: Val Loss 2868.67896
{'MSE - mean': 3104.6113105997338, 'MSE - std': 235.9324022082742, 'R2 - mean': 0.6180668091855392, 'R2 - std': 0.006475021495112088} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518225.93750
Epoch 1: Val Loss 518027.75000
Epoch 2: Val Loss 517652.96875
Epoch 3: Val Loss 516875.62500
Epoch 4: Val Loss 515192.84375
Epoch 5: Val Loss 511622.75000
Epoch 6: Val Loss 504800.90625
Epoch 7: Val Loss 492751.18750
Epoch 8: Val Loss 473042.53125
Epoch 9: Val Loss 443328.03125
Epoch 10: Val Loss 401485.34375
Epoch 11: Val Loss 346704.96875
Epoch 12: Val Loss 280576.50000
Epoch 13: Val Loss 207465.07812
Epoch 14: Val Loss 135902.28125
Epoch 15: Val Loss 76260.14062
Epoch 16: Val Loss 36609.90234
Epoch 17: Val Loss 17178.91406
Epoch 18: Val Loss 10869.81250
Epoch 19: Val Loss 9284.21094
Epoch 20: Val Loss 8680.66406
Epoch 21: Val Loss 8299.50391
Epoch 22: Val Loss 8001.61670
Epoch 23: Val Loss 7749.50537
Epoch 24: Val Loss 7487.56055
Epoch 25: Val Loss 7273.76416
Epoch 26: Val Loss 7105.93408
Epoch 27: Val Loss 6908.83691
Epoch 28: Val Loss 6711.74609
Epoch 29: Val Loss 6548.86963
Epoch 30: Val Loss 6442.67578
Epoch 31: Val Loss 6318.33203
Epoch 32: Val Loss 6147.47119
Epoch 33: Val Loss 6051.64062
Epoch 34: Val Loss 5923.63086
Epoch 35: Val Loss 5832.59863
Epoch 36: Val Loss 5709.98486
Epoch 37: Val Loss 5604.82666
Epoch 38: Val Loss 5505.78662
Epoch 39: Val Loss 5397.42871
Epoch 40: Val Loss 5318.43066
Epoch 41: Val Loss 5213.22754
Epoch 42: Val Loss 5153.68799
Epoch 43: Val Loss 5049.43799
Epoch 44: Val Loss 5020.01953
Epoch 45: Val Loss 4932.82715
Epoch 46: Val Loss 4818.19189
Epoch 47: Val Loss 4743.38525
Epoch 48: Val Loss 4683.77441
Epoch 49: Val Loss 4646.47363
Epoch 50: Val Loss 4583.90625
Epoch 51: Val Loss 4526.39258
Epoch 52: Val Loss 4463.29932
Epoch 53: Val Loss 4421.17871
Epoch 54: Val Loss 4350.54980
Epoch 55: Val Loss 4319.02441
Epoch 56: Val Loss 4246.42383
Epoch 57: Val Loss 4221.84180
Epoch 58: Val Loss 4162.00195
Epoch 59: Val Loss 4111.23438
Epoch 60: Val Loss 4075.81348
Epoch 61: Val Loss 4023.18311
Epoch 62: Val Loss 4017.68530
Epoch 63: Val Loss 3975.00635
Epoch 64: Val Loss 3925.86133
Epoch 65: Val Loss 3886.13599
Epoch 66: Val Loss 3867.10107
Epoch 67: Val Loss 3811.92920
Epoch 68: Val Loss 3789.34912
Epoch 69: Val Loss 3755.83472
Epoch 70: Val Loss 3719.29150
Epoch 71: Val Loss 3688.82666
Epoch 72: Val Loss 3664.28198
Epoch 73: Val Loss 3625.44556
Epoch 74: Val Loss 3590.62280
Epoch 75: Val Loss 3581.02393
Epoch 76: Val Loss 3555.54004
Epoch 77: Val Loss 3496.95312
Epoch 78: Val Loss 3504.43237
Epoch 79: Val Loss 3448.80347
Epoch 80: Val Loss 3423.63818
Epoch 81: Val Loss 3403.76807
Epoch 82: Val Loss 3385.01465
Epoch 83: Val Loss 3361.91089
Epoch 84: Val Loss 3337.33276
Epoch 85: Val Loss 3302.70728
Epoch 86: Val Loss 3284.55640
Epoch 87: Val Loss 3248.91406
Epoch 88: Val Loss 3241.54199
Epoch 89: Val Loss 3200.39209
Epoch 90: Val Loss 3200.31689
Epoch 91: Val Loss 3170.31250
Epoch 92: Val Loss 3135.44897
Epoch 93: Val Loss 3138.88770
Epoch 94: Val Loss 3091.37817
Epoch 95: Val Loss 3073.06616
Epoch 96: Val Loss 3044.99316
Epoch 97: Val Loss 3025.89282
Epoch 98: Val Loss 3003.59082
Epoch 99: Val Loss 2999.14819
{'MSE - mean': 3069.457067435285, 'MSE - std': 198.94984434803752, 'R2 - mean': 0.6379789031867132, 'R2 - std': 0.028651938455619662} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520571.62500
Epoch 1: Val Loss 520199.34375
Epoch 2: Val Loss 519481.93750
Epoch 3: Val Loss 518074.12500
Epoch 4: Val Loss 515411.53125
Epoch 5: Val Loss 510534.37500
Epoch 6: Val Loss 502004.43750
Epoch 7: Val Loss 488110.65625
Epoch 8: Val Loss 466352.06250
Epoch 9: Val Loss 435013.84375
Epoch 10: Val Loss 392161.87500
Epoch 11: Val Loss 338030.37500
Epoch 12: Val Loss 274002.84375
Epoch 13: Val Loss 205742.84375
Epoch 14: Val Loss 140044.00000
Epoch 15: Val Loss 85891.20312
Epoch 16: Val Loss 49434.63281
Epoch 17: Val Loss 30367.60742
Epoch 18: Val Loss 22376.79688
Epoch 19: Val Loss 18705.57031
Epoch 20: Val Loss 16281.13379
Epoch 21: Val Loss 14397.56445
Epoch 22: Val Loss 12892.83008
Epoch 23: Val Loss 11676.44922
Epoch 24: Val Loss 10664.47363
Epoch 25: Val Loss 9858.00586
Epoch 26: Val Loss 9145.93262
Epoch 27: Val Loss 8517.74023
Epoch 28: Val Loss 8007.73633
Epoch 29: Val Loss 7555.33496
Epoch 30: Val Loss 7160.22852
Epoch 31: Val Loss 6804.11670
Epoch 32: Val Loss 6500.82666
Epoch 33: Val Loss 6222.23975
Epoch 34: Val Loss 5994.63574
Epoch 35: Val Loss 5781.53027
Epoch 36: Val Loss 5592.53125
Epoch 37: Val Loss 5407.64014
Epoch 38: Val Loss 5222.82910
Epoch 39: Val Loss 5072.62451
Epoch 40: Val Loss 4928.37695
Epoch 41: Val Loss 4796.31543
Epoch 42: Val Loss 4674.30371
Epoch 43: Val Loss 4555.99756
Epoch 44: Val Loss 4447.60352
Epoch 45: Val Loss 4353.19678
Epoch 46: Val Loss 4253.88135
Epoch 47: Val Loss 4132.81445
Epoch 48: Val Loss 4056.36572
Epoch 49: Val Loss 3980.02637
Epoch 50: Val Loss 3912.61426
Epoch 51: Val Loss 3835.52466
Epoch 52: Val Loss 3791.77026
Epoch 53: Val Loss 3711.71338
Epoch 54: Val Loss 3652.91187
Epoch 55: Val Loss 3595.25659
Epoch 56: Val Loss 3539.09082
Epoch 57: Val Loss 3495.88477
Epoch 58: Val Loss 3459.27832
Epoch 59: Val Loss 3405.80127
Epoch 60: Val Loss 3360.38794
Epoch 61: Val Loss 3316.50854
Epoch 62: Val Loss 3307.48096
Epoch 63: Val Loss 3242.26050
Epoch 64: Val Loss 3222.09863
Epoch 65: Val Loss 3177.26367
Epoch 66: Val Loss 3153.25146
Epoch 67: Val Loss 3122.34033
Epoch 68: Val Loss 3092.61768
Epoch 69: Val Loss 3052.96777
Epoch 70: Val Loss 3035.53467
Epoch 71: Val Loss 3009.59937
Epoch 72: Val Loss 2988.12109
Epoch 73: Val Loss 2967.49121
Epoch 74: Val Loss 2934.11157
Epoch 75: Val Loss 2911.61646
Epoch 76: Val Loss 2879.33789
Epoch 77: Val Loss 2879.45215
Epoch 78: Val Loss 2828.89160
Epoch 79: Val Loss 2814.32495
Epoch 80: Val Loss 2817.60815
Epoch 81: Val Loss 2770.32251
Epoch 82: Val Loss 2751.20435
Epoch 83: Val Loss 2729.29761
Epoch 84: Val Loss 2720.60303
Epoch 85: Val Loss 2707.07153
Epoch 86: Val Loss 2669.51343
Epoch 87: Val Loss 2649.33472
Epoch 88: Val Loss 2660.69849
Epoch 89: Val Loss 2612.02979
Epoch 90: Val Loss 2605.66650
Epoch 91: Val Loss 2577.23413
Epoch 92: Val Loss 2580.19434
Epoch 93: Val Loss 2550.11768
Epoch 94: Val Loss 2539.82104
Epoch 95: Val Loss 2523.15527
Epoch 96: Val Loss 2497.03540
Epoch 97: Val Loss 2491.39111
Epoch 98: Val Loss 2460.04663
Epoch 99: Val Loss 2451.30835
{'MSE - mean': 2914.919901416008, 'MSE - std': 318.3252856252386, 'R2 - mean': 0.6546986136268124, 'R2 - std': 0.0381358929953232} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517779.31250
Epoch 1: Val Loss 517303.90625
Epoch 2: Val Loss 516333.43750
Epoch 3: Val Loss 514315.53125
Epoch 4: Val Loss 510309.78125
Epoch 5: Val Loss 502873.25000
Epoch 6: Val Loss 489895.03125
Epoch 7: Val Loss 468963.37500
Epoch 8: Val Loss 437410.87500
Epoch 9: Val Loss 393876.81250
Epoch 10: Val Loss 337272.68750
Epoch 11: Val Loss 270181.37500
Epoch 12: Val Loss 198377.73438
Epoch 13: Val Loss 131228.03125
Epoch 14: Val Loss 78026.63281
Epoch 15: Val Loss 44805.32031
Epoch 16: Val Loss 28510.98242
Epoch 17: Val Loss 21431.71484
Epoch 18: Val Loss 17594.82812
Epoch 19: Val Loss 15161.31738
Epoch 20: Val Loss 13444.31250
Epoch 21: Val Loss 12101.41602
Epoch 22: Val Loss 11097.21289
Epoch 23: Val Loss 10214.60742
Epoch 24: Val Loss 9530.12988
Epoch 25: Val Loss 9030.23633
Epoch 26: Val Loss 8560.05273
Epoch 27: Val Loss 8167.67090
Epoch 28: Val Loss 7798.03613
Epoch 29: Val Loss 7531.29297
Epoch 30: Val Loss 7238.92871
Epoch 31: Val Loss 6990.59229
Epoch 32: Val Loss 6749.91943
Epoch 33: Val Loss 6525.95264
Epoch 34: Val Loss 6305.25391
Epoch 35: Val Loss 6119.98096
Epoch 36: Val Loss 5942.28516
Epoch 37: Val Loss 5753.94824
Epoch 38: Val Loss 5590.53467
Epoch 39: Val Loss 5419.00732
Epoch 40: Val Loss 5260.99414
Epoch 41: Val Loss 5134.80957
Epoch 42: Val Loss 5004.57617
Epoch 43: Val Loss 4873.87598
Epoch 44: Val Loss 4750.24512
Epoch 45: Val Loss 4630.87598
Epoch 46: Val Loss 4520.25781
Epoch 47: Val Loss 4428.29102
Epoch 48: Val Loss 4336.85107
Epoch 49: Val Loss 4245.42529
Epoch 50: Val Loss 4151.40967
Epoch 51: Val Loss 4075.34155
Epoch 52: Val Loss 4004.06738
Epoch 53: Val Loss 3951.58813
Epoch 54: Val Loss 3878.97510
Epoch 55: Val Loss 3817.33130
Epoch 56: Val Loss 3755.27686
Epoch 57: Val Loss 3698.45410
Epoch 58: Val Loss 3654.05981
Epoch 59: Val Loss 3596.24023
Epoch 60: Val Loss 3550.71875
Epoch 61: Val Loss 3499.16040
Epoch 62: Val Loss 3455.90063
Epoch 63: Val Loss 3412.74097
Epoch 64: Val Loss 3372.44482
Epoch 65: Val Loss 3327.66699
Epoch 66: Val Loss 3299.19702
Epoch 67: Val Loss 3251.94019
Epoch 68: Val Loss 3207.70312
Epoch 69: Val Loss 3185.00659
Epoch 70: Val Loss 3150.66040
Epoch 71: Val Loss 3105.35352
Epoch 72: Val Loss 3082.61987
Epoch 73: Val Loss 3033.25000
Epoch 74: Val Loss 3017.76660
Epoch 75: Val Loss 2980.96655
Epoch 76: Val Loss 2946.14014
Epoch 77: Val Loss 2919.67725
Epoch 78: Val Loss 2903.58740
Epoch 79: Val Loss 2857.59839
Epoch 80: Val Loss 2835.65161
Epoch 81: Val Loss 2811.55127
Epoch 82: Val Loss 2791.15649
Epoch 83: Val Loss 2760.85010
Epoch 84: Val Loss 2737.00708
Epoch 85: Val Loss 2713.60449
Epoch 86: Val Loss 2684.79517
Epoch 87: Val Loss 2659.33838
Epoch 88: Val Loss 2643.00366
Epoch 89: Val Loss 2613.69849
Epoch 90: Val Loss 2594.55420
Epoch 91: Val Loss 2571.27588
Epoch 92: Val Loss 2550.72949
Epoch 93: Val Loss 2524.77222
Epoch 94: Val Loss 2502.59570
Epoch 95: Val Loss 2495.03833
Epoch 96: Val Loss 2464.88403
Epoch 97: Val Loss 2446.13330
Epoch 98: Val Loss 2430.25293
Epoch 99: Val Loss 2405.11084
{'MSE - mean': 2812.958137372625, 'MSE - std': 350.2136423414513, 'R2 - mean': 0.6634655892504949, 'R2 - std': 0.03835252943399655} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 2812.958137372625 and parameters: {'dim': 256, 'depth': 3, 'heads': 2, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.2}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516688.90625
Epoch 1: Val Loss 516688.59375
Epoch 2: Val Loss 516688.25000
Epoch 3: Val Loss 516687.93750
Epoch 4: Val Loss 516687.62500
Epoch 5: Val Loss 516687.31250
Epoch 6: Val Loss 516686.96875
Epoch 7: Val Loss 516686.65625
Epoch 8: Val Loss 516686.34375
Epoch 9: Val Loss 516686.00000
Epoch 10: Val Loss 516685.68750
Epoch 11: Val Loss 516685.34375
Epoch 12: Val Loss 516685.03125
Epoch 13: Val Loss 516684.71875
Epoch 14: Val Loss 516684.37500
Epoch 15: Val Loss 516684.06250
Epoch 16: Val Loss 516683.78125
Epoch 17: Val Loss 516683.40625
Epoch 18: Val Loss 516683.09375
Epoch 19: Val Loss 516682.78125
Epoch 20: Val Loss 516682.43750
Epoch 21: Val Loss 516682.12500
Epoch 22: Val Loss 516681.78125
Epoch 23: Val Loss 516681.46875
Epoch 24: Val Loss 516681.15625
Epoch 25: Val Loss 516680.81250
Epoch 26: Val Loss 516680.50000
Epoch 27: Val Loss 516680.18750
Epoch 28: Val Loss 516679.84375
Epoch 29: Val Loss 516679.53125
Epoch 30: Val Loss 516679.21875
Epoch 31: Val Loss 516678.87500
Epoch 32: Val Loss 516678.56250
Epoch 33: Val Loss 516678.25000
Epoch 34: Val Loss 516677.90625
Epoch 35: Val Loss 516677.59375
Epoch 36: Val Loss 516677.25000
Epoch 37: Val Loss 516676.93750
Epoch 38: Val Loss 516676.62500
Epoch 39: Val Loss 516676.31250
Epoch 40: Val Loss 516675.96875
Epoch 41: Val Loss 516675.62500
Epoch 42: Val Loss 516675.31250
Epoch 43: Val Loss 516675.00000
Epoch 44: Val Loss 516674.65625
Epoch 45: Val Loss 516674.34375
Epoch 46: Val Loss 516674.03125
Epoch 47: Val Loss 516673.68750
Epoch 48: Val Loss 516673.37500
Epoch 49: Val Loss 516673.06250
Epoch 50: Val Loss 516672.71875
Epoch 51: Val Loss 516672.43750
Epoch 52: Val Loss 516672.12500
Epoch 53: Val Loss 516671.75000
Epoch 54: Val Loss 516671.43750
Epoch 55: Val Loss 516671.09375
Epoch 56: Val Loss 516670.78125
Epoch 57: Val Loss 516670.43750
Epoch 58: Val Loss 516670.12500
Epoch 59: Val Loss 516669.78125
Epoch 60: Val Loss 516669.46875
Epoch 61: Val Loss 516669.09375
Epoch 62: Val Loss 516668.78125
Epoch 63: Val Loss 516668.46875
Epoch 64: Val Loss 516668.12500
Epoch 65: Val Loss 516667.84375
Epoch 66: Val Loss 516667.50000
Epoch 67: Val Loss 516667.15625
Epoch 68: Val Loss 516666.87500
Epoch 69: Val Loss 516666.53125
Epoch 70: Val Loss 516666.21875
Epoch 71: Val Loss 516665.84375
Epoch 72: Val Loss 516665.56250
Epoch 73: Val Loss 516665.21875
Epoch 74: Val Loss 516664.87500
Epoch 75: Val Loss 516664.56250
Epoch 76: Val Loss 516664.25000
Epoch 77: Val Loss 516663.90625
Epoch 78: Val Loss 516663.59375
Epoch 79: Val Loss 516663.25000
Epoch 80: Val Loss 516662.90625
Epoch 81: Val Loss 516662.56250
Epoch 82: Val Loss 516662.25000
Epoch 83: Val Loss 516661.90625
Epoch 84: Val Loss 516661.65625
Epoch 85: Val Loss 516661.28125
Epoch 86: Val Loss 516660.93750
Epoch 87: Val Loss 516660.62500
Epoch 88: Val Loss 516660.28125
Epoch 89: Val Loss 516659.96875
Epoch 90: Val Loss 516659.62500
Epoch 91: Val Loss 516659.31250
Epoch 92: Val Loss 516659.00000
Epoch 93: Val Loss 516658.65625
Epoch 94: Val Loss 516658.34375
Epoch 95: Val Loss 516658.00000
Epoch 96: Val Loss 516657.65625
Epoch 97: Val Loss 516657.37500
Epoch 98: Val Loss 516657.06250
Epoch 99: Val Loss 516656.71875
{'MSE - mean': 516656.68545739184, 'MSE - std': 0.0, 'R2 - mean': -59.072166937043825, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524798.37500
Epoch 1: Val Loss 524798.06250
Epoch 2: Val Loss 524797.75000
Epoch 3: Val Loss 524797.43750
Epoch 4: Val Loss 524797.12500
Epoch 5: Val Loss 524796.75000
Epoch 6: Val Loss 524796.37500
Epoch 7: Val Loss 524796.12500
Epoch 8: Val Loss 524795.75000
Epoch 9: Val Loss 524795.43750
Epoch 10: Val Loss 524795.06250
Epoch 11: Val Loss 524794.75000
Epoch 12: Val Loss 524794.43750
Epoch 13: Val Loss 524794.12500
Epoch 14: Val Loss 524793.75000
Epoch 15: Val Loss 524793.50000
Epoch 16: Val Loss 524793.12500
Epoch 17: Val Loss 524792.75000
Epoch 18: Val Loss 524792.50000
Epoch 19: Val Loss 524792.12500
Epoch 20: Val Loss 524791.81250
Epoch 21: Val Loss 524791.43750
Epoch 22: Val Loss 524791.18750
Epoch 23: Val Loss 524790.81250
Epoch 24: Val Loss 524790.50000
Epoch 25: Val Loss 524790.18750
Epoch 26: Val Loss 524789.75000
Epoch 27: Val Loss 524789.43750
Epoch 28: Val Loss 524789.12500
Epoch 29: Val Loss 524788.81250
Epoch 30: Val Loss 524788.50000
Epoch 31: Val Loss 524788.18750
Epoch 32: Val Loss 524787.75000
Epoch 33: Val Loss 524787.43750
Epoch 34: Val Loss 524787.12500
Epoch 35: Val Loss 524786.81250
Epoch 36: Val Loss 524786.43750
Epoch 37: Val Loss 524786.12500
Epoch 38: Val Loss 524785.81250
Epoch 39: Val Loss 524785.43750
Epoch 40: Val Loss 524785.06250
Epoch 41: Val Loss 524784.81250
Epoch 42: Val Loss 524784.43750
Epoch 43: Val Loss 524784.12500
Epoch 44: Val Loss 524783.75000
Epoch 45: Val Loss 524783.43750
Epoch 46: Val Loss 524783.12500
Epoch 47: Val Loss 524782.75000
Epoch 48: Val Loss 524782.43750
Epoch 49: Val Loss 524782.12500
Epoch 50: Val Loss 524781.75000
Epoch 51: Val Loss 524781.43750
Epoch 52: Val Loss 524781.12500
Epoch 53: Val Loss 524780.75000
Epoch 54: Val Loss 524780.43750
Epoch 55: Val Loss 524780.06250
Epoch 56: Val Loss 524779.75000
Epoch 57: Val Loss 524779.37500
Epoch 58: Val Loss 524779.00000
Epoch 59: Val Loss 524778.68750
Epoch 60: Val Loss 524778.37500
Epoch 61: Val Loss 524778.06250
Epoch 62: Val Loss 524777.68750
Epoch 63: Val Loss 524777.37500
Epoch 64: Val Loss 524777.00000
Epoch 65: Val Loss 524776.68750
Epoch 66: Val Loss 524776.31250
Epoch 67: Val Loss 524776.00000
Epoch 68: Val Loss 524775.68750
Epoch 69: Val Loss 524775.31250
Epoch 70: Val Loss 524774.93750
Epoch 71: Val Loss 524774.62500
Epoch 72: Val Loss 524774.31250
Epoch 73: Val Loss 524774.00000
Epoch 74: Val Loss 524773.62500
Epoch 75: Val Loss 524773.25000
Epoch 76: Val Loss 524772.93750
Epoch 77: Val Loss 524772.62500
Epoch 78: Val Loss 524772.25000
Epoch 79: Val Loss 524771.87500
Epoch 80: Val Loss 524771.56250
Epoch 81: Val Loss 524771.25000
Epoch 82: Val Loss 524770.87500
Epoch 83: Val Loss 524770.56250
Epoch 84: Val Loss 524770.18750
Epoch 85: Val Loss 524769.81250
Epoch 86: Val Loss 524769.50000
Epoch 87: Val Loss 524769.12500
Epoch 88: Val Loss 524768.81250
Epoch 89: Val Loss 524768.43750
Epoch 90: Val Loss 524768.12500
Epoch 91: Val Loss 524767.75000
Epoch 92: Val Loss 524767.43750
Epoch 93: Val Loss 524767.06250
Epoch 94: Val Loss 524766.75000
Epoch 95: Val Loss 524766.37500
Epoch 96: Val Loss 524766.06250
Epoch 97: Val Loss 524765.68750
Epoch 98: Val Loss 524765.37500
Epoch 99: Val Loss 524765.06250
{'MSE - mean': 520710.8463896406, 'MSE - std': 4054.1609322487784, 'R2 - mean': -63.377206205470074, 'R2 - std': 4.3050392684262455} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518410.40625
Epoch 1: Val Loss 518409.96875
Epoch 2: Val Loss 518409.56250
Epoch 3: Val Loss 518409.15625
Epoch 4: Val Loss 518408.71875
Epoch 5: Val Loss 518408.31250
Epoch 6: Val Loss 518407.84375
Epoch 7: Val Loss 518407.46875
Epoch 8: Val Loss 518407.06250
Epoch 9: Val Loss 518406.62500
Epoch 10: Val Loss 518406.18750
Epoch 11: Val Loss 518405.78125
Epoch 12: Val Loss 518405.37500
Epoch 13: Val Loss 518404.93750
Epoch 14: Val Loss 518404.50000
Epoch 15: Val Loss 518404.12500
Epoch 16: Val Loss 518403.68750
Epoch 17: Val Loss 518403.31250
Epoch 18: Val Loss 518402.81250
Epoch 19: Val Loss 518402.43750
Epoch 20: Val Loss 518402.00000
Epoch 21: Val Loss 518401.59375
Epoch 22: Val Loss 518401.21875
Epoch 23: Val Loss 518400.75000
Epoch 24: Val Loss 518400.34375
Epoch 25: Val Loss 518399.90625
Epoch 26: Val Loss 518399.50000
Epoch 27: Val Loss 518399.12500
Epoch 28: Val Loss 518398.68750
Epoch 29: Val Loss 518398.31250
Epoch 30: Val Loss 518397.84375
Epoch 31: Val Loss 518397.40625
Epoch 32: Val Loss 518397.06250
Epoch 33: Val Loss 518396.59375
Epoch 34: Val Loss 518396.21875
Epoch 35: Val Loss 518395.78125
Epoch 36: Val Loss 518395.37500
Epoch 37: Val Loss 518394.96875
Epoch 38: Val Loss 518394.56250
Epoch 39: Val Loss 518394.12500
Epoch 40: Val Loss 518393.75000
Epoch 41: Val Loss 518393.34375
Epoch 42: Val Loss 518392.90625
Epoch 43: Val Loss 518392.50000
Epoch 44: Val Loss 518392.12500
Epoch 45: Val Loss 518391.68750
Epoch 46: Val Loss 518391.25000
Epoch 47: Val Loss 518390.87500
Epoch 48: Val Loss 518390.40625
Epoch 49: Val Loss 518390.06250
Epoch 50: Val Loss 518389.65625
Epoch 51: Val Loss 518389.25000
Epoch 52: Val Loss 518388.78125
Epoch 53: Val Loss 518388.43750
Epoch 54: Val Loss 518388.00000
Epoch 55: Val Loss 518387.62500
Epoch 56: Val Loss 518387.21875
Epoch 57: Val Loss 518386.78125
Epoch 58: Val Loss 518386.37500
Epoch 59: Val Loss 518386.00000
Epoch 60: Val Loss 518385.59375
Epoch 61: Val Loss 518385.21875
Epoch 62: Val Loss 518384.75000
Epoch 63: Val Loss 518384.40625
Epoch 64: Val Loss 518383.96875
Epoch 65: Val Loss 518383.59375
Epoch 66: Val Loss 518383.18750
Epoch 67: Val Loss 518382.78125
Epoch 68: Val Loss 518382.40625
Epoch 69: Val Loss 518381.96875
Epoch 70: Val Loss 518381.59375
Epoch 71: Val Loss 518381.18750
Epoch 72: Val Loss 518380.78125
Epoch 73: Val Loss 518380.40625
Epoch 74: Val Loss 518379.96875
Epoch 75: Val Loss 518379.59375
Epoch 76: Val Loss 518379.15625
Epoch 77: Val Loss 518378.81250
Epoch 78: Val Loss 518378.40625
Epoch 79: Val Loss 518378.00000
Epoch 80: Val Loss 518377.59375
Epoch 81: Val Loss 518377.21875
Epoch 82: Val Loss 518376.78125
Epoch 83: Val Loss 518376.43750
Epoch 84: Val Loss 518376.00000
Epoch 85: Val Loss 518375.59375
Epoch 86: Val Loss 518375.21875
Epoch 87: Val Loss 518374.84375
Epoch 88: Val Loss 518374.40625
Epoch 89: Val Loss 518374.03125
Epoch 90: Val Loss 518373.62500
Epoch 91: Val Loss 518373.21875
Epoch 92: Val Loss 518372.84375
Epoch 93: Val Loss 518372.46875
Epoch 94: Val Loss 518372.09375
Epoch 95: Val Loss 518371.68750
Epoch 96: Val Loss 518371.25000
Epoch 97: Val Loss 518370.87500
Epoch 98: Val Loss 518370.43750
Epoch 99: Val Loss 518370.06250
{'MSE - mean': 519930.6001913945, 'MSE - std': 3489.276262512996, 'R2 - mean': -60.48087750967682, 'R2 - std': 5.397500831861493} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520649.15625
Epoch 1: Val Loss 520648.75000
Epoch 2: Val Loss 520648.37500
Epoch 3: Val Loss 520648.00000
Epoch 4: Val Loss 520647.65625
Epoch 5: Val Loss 520647.21875
Epoch 6: Val Loss 520646.87500
Epoch 7: Val Loss 520646.50000
Epoch 8: Val Loss 520646.09375
Epoch 9: Val Loss 520645.71875
Epoch 10: Val Loss 520645.34375
Epoch 11: Val Loss 520644.96875
Epoch 12: Val Loss 520644.56250
Epoch 13: Val Loss 520644.18750
Epoch 14: Val Loss 520643.78125
Epoch 15: Val Loss 520643.43750
Epoch 16: Val Loss 520643.03125
Epoch 17: Val Loss 520642.68750
Epoch 18: Val Loss 520642.28125
Epoch 19: Val Loss 520641.93750
Epoch 20: Val Loss 520641.50000
Epoch 21: Val Loss 520641.15625
Epoch 22: Val Loss 520640.75000
Epoch 23: Val Loss 520640.37500
Epoch 24: Val Loss 520639.96875
Epoch 25: Val Loss 520639.59375
Epoch 26: Val Loss 520639.18750
Epoch 27: Val Loss 520638.84375
Epoch 28: Val Loss 520638.50000
Epoch 29: Val Loss 520638.09375
Epoch 30: Val Loss 520637.71875
Epoch 31: Val Loss 520637.25000
Epoch 32: Val Loss 520636.90625
Epoch 33: Val Loss 520636.53125
Epoch 34: Val Loss 520636.15625
Epoch 35: Val Loss 520635.75000
Epoch 36: Val Loss 520635.37500
Epoch 37: Val Loss 520634.96875
Epoch 38: Val Loss 520634.59375
Epoch 39: Val Loss 520634.18750
Epoch 40: Val Loss 520633.81250
Epoch 41: Val Loss 520633.43750
Epoch 42: Val Loss 520633.03125
Epoch 43: Val Loss 520632.68750
Epoch 44: Val Loss 520632.28125
Epoch 45: Val Loss 520631.84375
Epoch 46: Val Loss 520631.50000
Epoch 47: Val Loss 520631.09375
Epoch 48: Val Loss 520630.68750
Epoch 49: Val Loss 520630.28125
Epoch 50: Val Loss 520629.93750
Epoch 51: Val Loss 520629.50000
Epoch 52: Val Loss 520629.18750
Epoch 53: Val Loss 520628.71875
Epoch 54: Val Loss 520628.37500
Epoch 55: Val Loss 520627.96875
Epoch 56: Val Loss 520627.56250
Epoch 57: Val Loss 520627.21875
Epoch 58: Val Loss 520626.78125
Epoch 59: Val Loss 520626.37500
Epoch 60: Val Loss 520626.00000
Epoch 61: Val Loss 520625.62500
Epoch 62: Val Loss 520625.25000
Epoch 63: Val Loss 520624.81250
Epoch 64: Val Loss 520624.43750
Epoch 65: Val Loss 520624.09375
Epoch 66: Val Loss 520623.65625
Epoch 67: Val Loss 520623.25000
Epoch 68: Val Loss 520622.84375
Epoch 69: Val Loss 520622.50000
Epoch 70: Val Loss 520622.03125
Epoch 71: Val Loss 520621.65625
Epoch 72: Val Loss 520621.25000
Epoch 73: Val Loss 520620.93750
Epoch 74: Val Loss 520620.50000
Epoch 75: Val Loss 520620.09375
Epoch 76: Val Loss 520619.68750
Epoch 77: Val Loss 520619.31250
Epoch 78: Val Loss 520618.90625
Epoch 79: Val Loss 520618.50000
Epoch 80: Val Loss 520618.09375
Epoch 81: Val Loss 520617.65625
Epoch 82: Val Loss 520617.28125
Epoch 83: Val Loss 520616.90625
Epoch 84: Val Loss 520616.50000
Epoch 85: Val Loss 520616.12500
Epoch 86: Val Loss 520615.71875
Epoch 87: Val Loss 520615.31250
Epoch 88: Val Loss 520614.87500
Epoch 89: Val Loss 520614.50000
Epoch 90: Val Loss 520614.09375
Epoch 91: Val Loss 520613.71875
Epoch 92: Val Loss 520613.31250
Epoch 93: Val Loss 520612.90625
Epoch 94: Val Loss 520612.46875
Epoch 95: Val Loss 520612.09375
Epoch 96: Val Loss 520611.68750
Epoch 97: Val Loss 520611.31250
Epoch 98: Val Loss 520610.90625
Epoch 99: Val Loss 520610.50000
{'MSE - mean': 520100.57687472185, 'MSE - std': 3036.1098210913633, 'R2 - mean': -60.78128517448194, 'R2 - std': 4.703243106376717} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517544.50000
Epoch 1: Val Loss 517544.25000
Epoch 2: Val Loss 517543.93750
Epoch 3: Val Loss 517543.65625
Epoch 4: Val Loss 517543.40625
Epoch 5: Val Loss 517543.12500
Epoch 6: Val Loss 517542.84375
Epoch 7: Val Loss 517542.56250
Epoch 8: Val Loss 517542.28125
Epoch 9: Val Loss 517542.00000
Epoch 10: Val Loss 517541.78125
Epoch 11: Val Loss 517541.43750
Epoch 12: Val Loss 517541.12500
Epoch 13: Val Loss 517540.93750
Epoch 14: Val Loss 517540.59375
Epoch 15: Val Loss 517540.37500
Epoch 16: Val Loss 517540.00000
Epoch 17: Val Loss 517539.75000
Epoch 18: Val Loss 517539.50000
Epoch 19: Val Loss 517539.18750
Epoch 20: Val Loss 517538.90625
Epoch 21: Val Loss 517538.65625
Epoch 22: Val Loss 517538.37500
Epoch 23: Val Loss 517538.06250
Epoch 24: Val Loss 517537.81250
Epoch 25: Val Loss 517537.50000
Epoch 26: Val Loss 517537.21875
Epoch 27: Val Loss 517536.96875
Epoch 28: Val Loss 517536.62500
Epoch 29: Val Loss 517536.34375
Epoch 30: Val Loss 517536.12500
Epoch 31: Val Loss 517535.81250
Epoch 32: Val Loss 517535.53125
Epoch 33: Val Loss 517535.25000
Epoch 34: Val Loss 517534.96875
Epoch 35: Val Loss 517534.68750
Epoch 36: Val Loss 517534.37500
Epoch 37: Val Loss 517534.09375
Epoch 38: Val Loss 517533.84375
Epoch 39: Val Loss 517533.56250
Epoch 40: Val Loss 517533.28125
Epoch 41: Val Loss 517533.00000
Epoch 42: Val Loss 517532.68750
Epoch 43: Val Loss 517532.40625
Epoch 44: Val Loss 517532.12500
Epoch 45: Val Loss 517531.81250
Epoch 46: Val Loss 517531.56250
Epoch 47: Val Loss 517531.31250
Epoch 48: Val Loss 517531.03125
Epoch 49: Val Loss 517530.71875
Epoch 50: Val Loss 517530.37500
Epoch 51: Val Loss 517530.15625
Epoch 52: Val Loss 517529.87500
Epoch 53: Val Loss 517529.56250
Epoch 54: Val Loss 517529.28125
Epoch 55: Val Loss 517529.03125
Epoch 56: Val Loss 517528.65625
Epoch 57: Val Loss 517528.43750
Epoch 58: Val Loss 517528.12500
Epoch 59: Val Loss 517527.90625
Epoch 60: Val Loss 517527.56250
Epoch 61: Val Loss 517527.25000
Epoch 62: Val Loss 517527.00000
Epoch 63: Val Loss 517526.68750
Epoch 64: Val Loss 517526.40625
Epoch 65: Val Loss 517526.09375
Epoch 66: Val Loss 517525.84375
Epoch 67: Val Loss 517525.50000
Epoch 68: Val Loss 517525.28125
Epoch 69: Val Loss 517524.93750
Epoch 70: Val Loss 517524.65625
Epoch 71: Val Loss 517524.37500
Epoch 72: Val Loss 517524.09375
Epoch 73: Val Loss 517523.78125
Epoch 74: Val Loss 517523.50000
Epoch 75: Val Loss 517523.21875
Epoch 76: Val Loss 517522.96875
Epoch 77: Val Loss 517522.65625
Epoch 78: Val Loss 517522.34375
Epoch 79: Val Loss 517522.06250
Epoch 80: Val Loss 517521.75000
Epoch 81: Val Loss 517521.46875
Epoch 82: Val Loss 517521.15625
Epoch 83: Val Loss 517520.90625
Epoch 84: Val Loss 517520.56250
Epoch 85: Val Loss 517520.34375
Epoch 86: Val Loss 517520.03125
Epoch 87: Val Loss 517519.71875
Epoch 88: Val Loss 517519.43750
Epoch 89: Val Loss 517519.12500
Epoch 90: Val Loss 517518.87500
Epoch 91: Val Loss 517518.56250
Epoch 92: Val Loss 517518.21875
Epoch 93: Val Loss 517517.96875
Epoch 94: Val Loss 517517.65625
Epoch 95: Val Loss 517517.37500
Epoch 96: Val Loss 517517.06250
Epoch 97: Val Loss 517516.78125
Epoch 98: Val Loss 517516.50000
Epoch 99: Val Loss 517516.21875
{'MSE - mean': 519583.70319063065, 'MSE - std': 2905.6847554072497, 'R2 - mean': -61.39854986567359, 'R2 - std': 4.384114433966319} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 519583.70319063065 and parameters: {'dim': 256, 'depth': 1, 'heads': 8, 'weight_decay': -4, 'learning_rate': -6, 'dropout': 0.3}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516621.50000
Epoch 1: Val Loss 516594.84375
Epoch 2: Val Loss 516567.84375
Epoch 3: Val Loss 516540.31250
Epoch 4: Val Loss 516512.06250
Epoch 5: Val Loss 516482.40625
Epoch 6: Val Loss 516451.15625
Epoch 7: Val Loss 516417.90625
Epoch 8: Val Loss 516382.65625
Epoch 9: Val Loss 516344.12500
Epoch 10: Val Loss 516302.06250
Epoch 11: Val Loss 516255.68750
Epoch 12: Val Loss 516205.03125
Epoch 13: Val Loss 516148.90625
Epoch 14: Val Loss 516087.78125
Epoch 15: Val Loss 516019.90625
Epoch 16: Val Loss 515944.78125
Epoch 17: Val Loss 515862.43750
Epoch 18: Val Loss 515771.00000
Epoch 19: Val Loss 515669.65625
Epoch 20: Val Loss 515554.87500
Epoch 21: Val Loss 515425.93750
Epoch 22: Val Loss 515282.00000
Epoch 23: Val Loss 515126.03125
Epoch 24: Val Loss 514952.75000
Epoch 25: Val Loss 514765.06250
Epoch 26: Val Loss 514560.84375
Epoch 27: Val Loss 514339.37500
Epoch 28: Val Loss 514100.06250
Epoch 29: Val Loss 513840.40625
Epoch 30: Val Loss 513559.15625
Epoch 31: Val Loss 513254.40625
Epoch 32: Val Loss 512925.03125
Epoch 33: Val Loss 512574.06250
Epoch 34: Val Loss 512192.50000
Epoch 35: Val Loss 511782.93750
Epoch 36: Val Loss 511340.78125
Epoch 37: Val Loss 510866.12500
Epoch 38: Val Loss 510361.78125
Epoch 39: Val Loss 509821.62500
Epoch 40: Val Loss 509242.12500
Epoch 41: Val Loss 508628.34375
Epoch 42: Val Loss 507974.96875
Epoch 43: Val Loss 507281.71875
Epoch 44: Val Loss 506539.96875
Epoch 45: Val Loss 505755.84375
Epoch 46: Val Loss 504926.12500
Epoch 47: Val Loss 504057.06250
Epoch 48: Val Loss 503139.00000
Epoch 49: Val Loss 502160.59375
Epoch 50: Val Loss 501137.78125
Epoch 51: Val Loss 500049.43750
Epoch 52: Val Loss 498912.75000
Epoch 53: Val Loss 497713.65625
Epoch 54: Val Loss 496449.50000
Epoch 55: Val Loss 495126.43750
Epoch 56: Val Loss 493739.06250
Epoch 57: Val Loss 492298.71875
Epoch 58: Val Loss 490783.34375
Epoch 59: Val Loss 489206.96875
Epoch 60: Val Loss 487564.25000
Epoch 61: Val Loss 485841.18750
Epoch 62: Val Loss 484039.71875
Epoch 63: Val Loss 482177.62500
Epoch 64: Val Loss 480241.68750
Epoch 65: Val Loss 478262.62500
Epoch 66: Val Loss 476163.78125
Epoch 67: Val Loss 474008.87500
Epoch 68: Val Loss 471776.87500
Epoch 69: Val Loss 469437.31250
Epoch 70: Val Loss 467033.12500
Epoch 71: Val Loss 464543.03125
Epoch 72: Val Loss 461987.84375
Epoch 73: Val Loss 459354.90625
Epoch 74: Val Loss 456653.68750
Epoch 75: Val Loss 453849.40625
Epoch 76: Val Loss 450955.90625
Epoch 77: Val Loss 447987.25000
Epoch 78: Val Loss 444952.25000
Epoch 79: Val Loss 441811.25000
Epoch 80: Val Loss 438591.50000
Epoch 81: Val Loss 435282.31250
Epoch 82: Val Loss 431921.25000
Epoch 83: Val Loss 428438.50000
Epoch 84: Val Loss 424940.06250
Epoch 85: Val Loss 421320.18750
Epoch 86: Val Loss 417623.37500
Epoch 87: Val Loss 413834.84375
Epoch 88: Val Loss 409985.12500
Epoch 89: Val Loss 406046.40625
Epoch 90: Val Loss 402093.59375
Epoch 91: Val Loss 397991.00000
Epoch 92: Val Loss 393847.65625
Epoch 93: Val Loss 389655.00000
Epoch 94: Val Loss 385373.81250
Epoch 95: Val Loss 381065.65625
Epoch 96: Val Loss 376649.00000
Epoch 97: Val Loss 372120.56250
Epoch 98: Val Loss 367657.93750
Epoch 99: Val Loss 363036.84375
{'MSE - mean': 363036.81344066106, 'MSE - std': 0.0, 'R2 - mean': -41.21063749904451, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524871.18750
Epoch 1: Val Loss 524838.87500
Epoch 2: Val Loss 524806.25000
Epoch 3: Val Loss 524772.75000
Epoch 4: Val Loss 524737.87500
Epoch 5: Val Loss 524701.62500
Epoch 6: Val Loss 524663.31250
Epoch 7: Val Loss 524622.62500
Epoch 8: Val Loss 524579.06250
Epoch 9: Val Loss 524533.00000
Epoch 10: Val Loss 524483.68750
Epoch 11: Val Loss 524430.18750
Epoch 12: Val Loss 524372.43750
Epoch 13: Val Loss 524309.81250
Epoch 14: Val Loss 524241.21875
Epoch 15: Val Loss 524166.03125
Epoch 16: Val Loss 524083.84375
Epoch 17: Val Loss 523993.28125
Epoch 18: Val Loss 523893.53125
Epoch 19: Val Loss 523786.78125
Epoch 20: Val Loss 523669.09375
Epoch 21: Val Loss 523542.62500
Epoch 22: Val Loss 523402.18750
Epoch 23: Val Loss 523251.28125
Epoch 24: Val Loss 523086.25000
Epoch 25: Val Loss 522906.93750
Epoch 26: Val Loss 522710.93750
Epoch 27: Val Loss 522495.93750
Epoch 28: Val Loss 522262.78125
Epoch 29: Val Loss 522007.65625
Epoch 30: Val Loss 521734.75000
Epoch 31: Val Loss 521434.78125
Epoch 32: Val Loss 521114.96875
Epoch 33: Val Loss 520764.43750
Epoch 34: Val Loss 520388.37500
Epoch 35: Val Loss 519980.09375
Epoch 36: Val Loss 519544.46875
Epoch 37: Val Loss 519080.18750
Epoch 38: Val Loss 518576.28125
Epoch 39: Val Loss 518031.53125
Epoch 40: Val Loss 517461.53125
Epoch 41: Val Loss 516840.03125
Epoch 42: Val Loss 516185.37500
Epoch 43: Val Loss 515488.40625
Epoch 44: Val Loss 514744.31250
Epoch 45: Val Loss 513969.59375
Epoch 46: Val Loss 513147.56250
Epoch 47: Val Loss 512274.28125
Epoch 48: Val Loss 511365.15625
Epoch 49: Val Loss 510393.65625
Epoch 50: Val Loss 509382.68750
Epoch 51: Val Loss 508318.46875
Epoch 52: Val Loss 507217.18750
Epoch 53: Val Loss 506071.40625
Epoch 54: Val Loss 504850.09375
Epoch 55: Val Loss 503577.21875
Epoch 56: Val Loss 502264.00000
Epoch 57: Val Loss 500880.59375
Epoch 58: Val Loss 499439.43750
Epoch 59: Val Loss 497945.09375
Epoch 60: Val Loss 496398.59375
Epoch 61: Val Loss 494790.34375
Epoch 62: Val Loss 493118.34375
Epoch 63: Val Loss 491376.00000
Epoch 64: Val Loss 489580.43750
Epoch 65: Val Loss 487701.68750
Epoch 66: Val Loss 485767.37500
Epoch 67: Val Loss 483754.90625
Epoch 68: Val Loss 481698.06250
Epoch 69: Val Loss 479543.53125
Epoch 70: Val Loss 477331.06250
Epoch 71: Val Loss 475024.53125
Epoch 72: Val Loss 472663.34375
Epoch 73: Val Loss 470228.31250
Epoch 74: Val Loss 467699.71875
Epoch 75: Val Loss 465130.12500
Epoch 76: Val Loss 462498.50000
Epoch 77: Val Loss 459746.96875
Epoch 78: Val Loss 456955.12500
Epoch 79: Val Loss 454100.28125
Epoch 80: Val Loss 451171.12500
Epoch 81: Val Loss 448165.90625
Epoch 82: Val Loss 445065.59375
Epoch 83: Val Loss 441902.59375
Epoch 84: Val Loss 438690.65625
Epoch 85: Val Loss 435412.12500
Epoch 86: Val Loss 432059.59375
Epoch 87: Val Loss 428644.50000
Epoch 88: Val Loss 425173.78125
Epoch 89: Val Loss 421601.62500
Epoch 90: Val Loss 417955.62500
Epoch 91: Val Loss 414271.81250
Epoch 92: Val Loss 410495.15625
Epoch 93: Val Loss 406668.06250
Epoch 94: Val Loss 402801.90625
Epoch 95: Val Loss 398805.12500
Epoch 96: Val Loss 394798.71875
Epoch 97: Val Loss 390717.75000
Epoch 98: Val Loss 386566.28125
Epoch 99: Val Loss 382386.06250
{'MSE - mean': 372711.4295821108, 'MSE - std': 9674.616141449718, 'R2 - mean': -45.12902640126778, 'R2 - std': 3.918388902223274} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518163.37500
Epoch 1: Val Loss 518126.21875
Epoch 2: Val Loss 518089.50000
Epoch 3: Val Loss 518053.18750
Epoch 4: Val Loss 518016.40625
Epoch 5: Val Loss 517979.65625
Epoch 6: Val Loss 517942.15625
Epoch 7: Val Loss 517903.25000
Epoch 8: Val Loss 517861.84375
Epoch 9: Val Loss 517817.37500
Epoch 10: Val Loss 517769.75000
Epoch 11: Val Loss 517718.28125
Epoch 12: Val Loss 517663.31250
Epoch 13: Val Loss 517602.84375
Epoch 14: Val Loss 517537.93750
Epoch 15: Val Loss 517467.31250
Epoch 16: Val Loss 517390.75000
Epoch 17: Val Loss 517306.43750
Epoch 18: Val Loss 517212.37500
Epoch 19: Val Loss 517110.43750
Epoch 20: Val Loss 516998.50000
Epoch 21: Val Loss 516878.56250
Epoch 22: Val Loss 516745.37500
Epoch 23: Val Loss 516602.06250
Epoch 24: Val Loss 516444.90625
Epoch 25: Val Loss 516271.12500
Epoch 26: Val Loss 516077.59375
Epoch 27: Val Loss 515861.50000
Epoch 28: Val Loss 515623.96875
Epoch 29: Val Loss 515356.68750
Epoch 30: Val Loss 515063.12500
Epoch 31: Val Loss 514740.00000
Epoch 32: Val Loss 514387.00000
Epoch 33: Val Loss 514004.56250
Epoch 34: Val Loss 513587.09375
Epoch 35: Val Loss 513133.06250
Epoch 36: Val Loss 512638.90625
Epoch 37: Val Loss 512103.87500
Epoch 38: Val Loss 511528.40625
Epoch 39: Val Loss 510901.96875
Epoch 40: Val Loss 510229.71875
Epoch 41: Val Loss 509507.68750
Epoch 42: Val Loss 508738.18750
Epoch 43: Val Loss 507920.56250
Epoch 44: Val Loss 507035.71875
Epoch 45: Val Loss 506091.78125
Epoch 46: Val Loss 505108.37500
Epoch 47: Val Loss 504049.34375
Epoch 48: Val Loss 502918.09375
Epoch 49: Val Loss 501735.53125
Epoch 50: Val Loss 500478.62500
Epoch 51: Val Loss 499167.68750
Epoch 52: Val Loss 497752.37500
Epoch 53: Val Loss 496277.50000
Epoch 54: Val Loss 494741.62500
Epoch 55: Val Loss 493103.40625
Epoch 56: Val Loss 491410.06250
Epoch 57: Val Loss 489628.34375
Epoch 58: Val Loss 487769.21875
Epoch 59: Val Loss 485814.62500
Epoch 60: Val Loss 483769.53125
Epoch 61: Val Loss 481633.78125
Epoch 62: Val Loss 479413.43750
Epoch 63: Val Loss 477086.00000
Epoch 64: Val Loss 474675.93750
Epoch 65: Val Loss 472168.15625
Epoch 66: Val Loss 469553.93750
Epoch 67: Val Loss 466865.21875
Epoch 68: Val Loss 464095.56250
Epoch 69: Val Loss 461218.81250
Epoch 70: Val Loss 458253.15625
Epoch 71: Val Loss 455175.06250
Epoch 72: Val Loss 452051.75000
Epoch 73: Val Loss 448776.71875
Epoch 74: Val Loss 445437.12500
Epoch 75: Val Loss 442006.03125
Epoch 76: Val Loss 438514.53125
Epoch 77: Val Loss 434847.46875
Epoch 78: Val Loss 431095.96875
Epoch 79: Val Loss 427315.68750
Epoch 80: Val Loss 423445.78125
Epoch 81: Val Loss 419453.18750
Epoch 82: Val Loss 415445.62500
Epoch 83: Val Loss 411325.50000
Epoch 84: Val Loss 407108.31250
Epoch 85: Val Loss 402787.75000
Epoch 86: Val Loss 398333.75000
Epoch 87: Val Loss 393884.34375
Epoch 88: Val Loss 389294.65625
Epoch 89: Val Loss 384602.18750
Epoch 90: Val Loss 379911.43750
Epoch 91: Val Loss 375127.56250
Epoch 92: Val Loss 370243.56250
Epoch 93: Val Loss 365317.25000
Epoch 94: Val Loss 360310.21875
Epoch 95: Val Loss 355271.68750
Epoch 96: Val Loss 350148.68750
Epoch 97: Val Loss 344931.18750
Epoch 98: Val Loss 339709.43750
Epoch 99: Val Loss 334411.62500
{'MSE - mean': 359944.84473762615, 'MSE - std': 19707.10977048084, 'R2 - mean': -41.727906018748314, 'R2 - std': 5.776771368058728} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521315.93750
Epoch 1: Val Loss 521293.50000
Epoch 2: Val Loss 521271.96875
Epoch 3: Val Loss 521251.12500
Epoch 4: Val Loss 521230.28125
Epoch 5: Val Loss 521209.09375
Epoch 6: Val Loss 521186.90625
Epoch 7: Val Loss 521162.90625
Epoch 8: Val Loss 521137.09375
Epoch 9: Val Loss 521109.06250
Epoch 10: Val Loss 521078.21875
Epoch 11: Val Loss 521044.31250
Epoch 12: Val Loss 521006.25000
Epoch 13: Val Loss 520964.93750
Epoch 14: Val Loss 520919.78125
Epoch 15: Val Loss 520871.34375
Epoch 16: Val Loss 520818.87500
Epoch 17: Val Loss 520761.84375
Epoch 18: Val Loss 520700.50000
Epoch 19: Val Loss 520633.65625
Epoch 20: Val Loss 520561.12500
Epoch 21: Val Loss 520482.65625
Epoch 22: Val Loss 520398.00000
Epoch 23: Val Loss 520305.18750
Epoch 24: Val Loss 520204.46875
Epoch 25: Val Loss 520094.40625
Epoch 26: Val Loss 519974.34375
Epoch 27: Val Loss 519844.09375
Epoch 28: Val Loss 519700.46875
Epoch 29: Val Loss 519544.59375
Epoch 30: Val Loss 519377.09375
Epoch 31: Val Loss 519194.81250
Epoch 32: Val Loss 518996.78125
Epoch 33: Val Loss 518785.34375
Epoch 34: Val Loss 518556.03125
Epoch 35: Val Loss 518310.81250
Epoch 36: Val Loss 518045.43750
Epoch 37: Val Loss 517760.75000
Epoch 38: Val Loss 517454.12500
Epoch 39: Val Loss 517126.40625
Epoch 40: Val Loss 516770.59375
Epoch 41: Val Loss 516391.46875
Epoch 42: Val Loss 515986.34375
Epoch 43: Val Loss 515554.12500
Epoch 44: Val Loss 515091.31250
Epoch 45: Val Loss 514598.28125
Epoch 46: Val Loss 514075.53125
Epoch 47: Val Loss 513518.71875
Epoch 48: Val Loss 512931.68750
Epoch 49: Val Loss 512305.62500
Epoch 50: Val Loss 511641.09375
Epoch 51: Val Loss 510940.46875
Epoch 52: Val Loss 510202.31250
Epoch 53: Val Loss 509422.81250
Epoch 54: Val Loss 508603.21875
Epoch 55: Val Loss 507748.53125
Epoch 56: Val Loss 506836.53125
Epoch 57: Val Loss 505892.21875
Epoch 58: Val Loss 504903.62500
Epoch 59: Val Loss 503860.03125
Epoch 60: Val Loss 502762.90625
Epoch 61: Val Loss 501612.09375
Epoch 62: Val Loss 500416.21875
Epoch 63: Val Loss 499174.12500
Epoch 64: Val Loss 497861.12500
Epoch 65: Val Loss 496489.34375
Epoch 66: Val Loss 495084.25000
Epoch 67: Val Loss 493611.81250
Epoch 68: Val Loss 492077.43750
Epoch 69: Val Loss 490501.46875
Epoch 70: Val Loss 488857.46875
Epoch 71: Val Loss 487132.40625
Epoch 72: Val Loss 485379.12500
Epoch 73: Val Loss 483564.68750
Epoch 74: Val Loss 481687.34375
Epoch 75: Val Loss 479738.46875
Epoch 76: Val Loss 477730.65625
Epoch 77: Val Loss 475661.90625
Epoch 78: Val Loss 473538.28125
Epoch 79: Val Loss 471342.18750
Epoch 80: Val Loss 469089.03125
Epoch 81: Val Loss 466768.75000
Epoch 82: Val Loss 464385.81250
Epoch 83: Val Loss 461959.46875
Epoch 84: Val Loss 459449.81250
Epoch 85: Val Loss 456867.25000
Epoch 86: Val Loss 454223.78125
Epoch 87: Val Loss 451520.46875
Epoch 88: Val Loss 448750.81250
Epoch 89: Val Loss 445921.34375
Epoch 90: Val Loss 443023.65625
Epoch 91: Val Loss 440057.15625
Epoch 92: Val Loss 437046.96875
Epoch 93: Val Loss 433955.06250
Epoch 94: Val Loss 430802.03125
Epoch 95: Val Loss 427579.87500
Epoch 96: Val Loss 424302.96875
Epoch 97: Val Loss 420975.28125
Epoch 98: Val Loss 417581.56250
Epoch 99: Val Loss 414134.56250
{'MSE - mean': 373492.274258923, 'MSE - std': 29015.102473244064, 'R2 - mean': -43.511579348188654, 'R2 - std': 5.879862857039073} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517973.56250
Epoch 1: Val Loss 517937.81250
Epoch 2: Val Loss 517901.65625
Epoch 3: Val Loss 517864.37500
Epoch 4: Val Loss 517825.78125
Epoch 5: Val Loss 517785.25000
Epoch 6: Val Loss 517742.40625
Epoch 7: Val Loss 517696.81250
Epoch 8: Val Loss 517647.46875
Epoch 9: Val Loss 517594.50000
Epoch 10: Val Loss 517536.68750
Epoch 11: Val Loss 517474.00000
Epoch 12: Val Loss 517404.93750
Epoch 13: Val Loss 517329.71875
Epoch 14: Val Loss 517247.18750
Epoch 15: Val Loss 517157.75000
Epoch 16: Val Loss 517060.40625
Epoch 17: Val Loss 516955.25000
Epoch 18: Val Loss 516841.84375
Epoch 19: Val Loss 516720.03125
Epoch 20: Val Loss 516587.93750
Epoch 21: Val Loss 516445.18750
Epoch 22: Val Loss 516294.09375
Epoch 23: Val Loss 516129.21875
Epoch 24: Val Loss 515952.46875
Epoch 25: Val Loss 515763.87500
Epoch 26: Val Loss 515562.37500
Epoch 27: Val Loss 515343.65625
Epoch 28: Val Loss 515109.56250
Epoch 29: Val Loss 514858.81250
Epoch 30: Val Loss 514586.87500
Epoch 31: Val Loss 514301.31250
Epoch 32: Val Loss 513994.90625
Epoch 33: Val Loss 513669.03125
Epoch 34: Val Loss 513319.15625
Epoch 35: Val Loss 512949.00000
Epoch 36: Val Loss 512549.03125
Epoch 37: Val Loss 512127.84375
Epoch 38: Val Loss 511680.12500
Epoch 39: Val Loss 511204.93750
Epoch 40: Val Loss 510708.93750
Epoch 41: Val Loss 510182.28125
Epoch 42: Val Loss 509624.12500
Epoch 43: Val Loss 509033.59375
Epoch 44: Val Loss 508408.96875
Epoch 45: Val Loss 507755.93750
Epoch 46: Val Loss 507067.75000
Epoch 47: Val Loss 506349.12500
Epoch 48: Val Loss 505592.75000
Epoch 49: Val Loss 504804.53125
Epoch 50: Val Loss 503977.59375
Epoch 51: Val Loss 503108.59375
Epoch 52: Val Loss 502210.12500
Epoch 53: Val Loss 501261.84375
Epoch 54: Val Loss 500286.56250
Epoch 55: Val Loss 499255.34375
Epoch 56: Val Loss 498186.78125
Epoch 57: Val Loss 497094.93750
Epoch 58: Val Loss 495946.90625
Epoch 59: Val Loss 494767.53125
Epoch 60: Val Loss 493538.37500
Epoch 61: Val Loss 492267.00000
Epoch 62: Val Loss 490933.56250
Epoch 63: Val Loss 489565.18750
Epoch 64: Val Loss 488159.78125
Epoch 65: Val Loss 486706.37500
Epoch 66: Val Loss 485201.21875
Epoch 67: Val Loss 483669.96875
Epoch 68: Val Loss 482067.53125
Epoch 69: Val Loss 480422.56250
Epoch 70: Val Loss 478734.06250
Epoch 71: Val Loss 476982.50000
Epoch 72: Val Loss 475176.09375
Epoch 73: Val Loss 473338.90625
Epoch 74: Val Loss 471454.81250
Epoch 75: Val Loss 469513.40625
Epoch 76: Val Loss 467502.34375
Epoch 77: Val Loss 465452.00000
Epoch 78: Val Loss 463333.59375
Epoch 79: Val Loss 461164.65625
Epoch 80: Val Loss 458961.25000
Epoch 81: Val Loss 456696.00000
Epoch 82: Val Loss 454375.71875
Epoch 83: Val Loss 452011.43750
Epoch 84: Val Loss 449587.87500
Epoch 85: Val Loss 447081.15625
Epoch 86: Val Loss 444568.90625
Epoch 87: Val Loss 442002.37500
Epoch 88: Val Loss 439361.68750
Epoch 89: Val Loss 436651.81250
Epoch 90: Val Loss 433915.06250
Epoch 91: Val Loss 431161.03125
Epoch 92: Val Loss 428295.34375
Epoch 93: Val Loss 425405.43750
Epoch 94: Val Loss 422490.84375
Epoch 95: Val Loss 419494.50000
Epoch 96: Val Loss 416431.84375
Epoch 97: Val Loss 413343.71875
Epoch 98: Val Loss 410202.56250
Epoch 99: Val Loss 407010.93750
{'MSE - mean': 380196.0100854259, 'MSE - std': 29210.63562047966, 'R2 - mean': -44.812548632365115, 'R2 - std': 5.867564578899206} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 380196.0100854259 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516174.06250
Epoch 1: Val Loss 515353.34375
Epoch 2: Val Loss 513664.53125
Epoch 3: Val Loss 510205.50000
Epoch 4: Val Loss 503624.06250
Epoch 5: Val Loss 491717.65625
Epoch 6: Val Loss 472078.71875
Epoch 7: Val Loss 441620.40625
Epoch 8: Val Loss 397919.87500
Epoch 9: Val Loss 339446.18750
Epoch 10: Val Loss 269294.68750
Epoch 11: Val Loss 193741.09375
Epoch 12: Val Loss 123704.96875
Epoch 13: Val Loss 71038.46094
Epoch 14: Val Loss 40209.03516
Epoch 15: Val Loss 26105.88672
Epoch 16: Val Loss 19575.82227
Epoch 17: Val Loss 15493.60059
Epoch 18: Val Loss 12817.44531
Epoch 19: Val Loss 10891.55469
Epoch 20: Val Loss 9469.01270
Epoch 21: Val Loss 8367.42285
Epoch 22: Val Loss 7589.21387
Epoch 23: Val Loss 7003.53320
Epoch 24: Val Loss 6524.39941
Epoch 25: Val Loss 6122.85059
Epoch 26: Val Loss 5851.06592
Epoch 27: Val Loss 5583.89502
Epoch 28: Val Loss 5413.95654
Epoch 29: Val Loss 5254.00732
Epoch 30: Val Loss 5136.58105
Epoch 31: Val Loss 5012.98193
Epoch 32: Val Loss 4904.34082
Epoch 33: Val Loss 4813.06201
Epoch 34: Val Loss 4756.09131
Epoch 35: Val Loss 4663.75635
Epoch 36: Val Loss 4611.44385
Epoch 37: Val Loss 4529.06055
Epoch 38: Val Loss 4484.11768
Epoch 39: Val Loss 4427.47705
Epoch 40: Val Loss 4381.52148
Epoch 41: Val Loss 4329.98975
Epoch 42: Val Loss 4291.03320
Epoch 43: Val Loss 4226.76904
Epoch 44: Val Loss 4168.17334
Epoch 45: Val Loss 4134.88379
Epoch 46: Val Loss 4083.56348
Epoch 47: Val Loss 4052.26636
Epoch 48: Val Loss 4013.05835
Epoch 49: Val Loss 3991.36523
Epoch 50: Val Loss 3945.47363
Epoch 51: Val Loss 3908.29565
Epoch 52: Val Loss 3852.49609
Epoch 53: Val Loss 3828.11963
Epoch 54: Val Loss 3809.34668
Epoch 55: Val Loss 3758.14185
Epoch 56: Val Loss 3733.03418
Epoch 57: Val Loss 3695.66162
Epoch 58: Val Loss 3665.47119
Epoch 59: Val Loss 3641.05811
Epoch 60: Val Loss 3595.63452
Epoch 61: Val Loss 3592.54321
Epoch 62: Val Loss 3552.53784
Epoch 63: Val Loss 3516.12915
Epoch 64: Val Loss 3490.26929
Epoch 65: Val Loss 3460.28101
Epoch 66: Val Loss 3432.16357
Epoch 67: Val Loss 3413.22363
Epoch 68: Val Loss 3382.14648
Epoch 69: Val Loss 3361.43701
Epoch 70: Val Loss 3319.96436
Epoch 71: Val Loss 3304.17163
Epoch 72: Val Loss 3292.23901
Epoch 73: Val Loss 3242.79102
Epoch 74: Val Loss 3258.65015
Epoch 75: Val Loss 3206.35718
Epoch 76: Val Loss 3184.97827
Epoch 77: Val Loss 3175.87744
Epoch 78: Val Loss 3137.37207
Epoch 79: Val Loss 3130.07739
Epoch 80: Val Loss 3095.06738
Epoch 81: Val Loss 3076.27295
Epoch 82: Val Loss 3055.89355
Epoch 83: Val Loss 3021.14233
Epoch 84: Val Loss 3005.55664
Epoch 85: Val Loss 2977.88159
Epoch 86: Val Loss 2977.34253
Epoch 87: Val Loss 2931.41797
Epoch 88: Val Loss 2935.02856
Epoch 89: Val Loss 2903.28491
Epoch 90: Val Loss 2877.02319
Epoch 91: Val Loss 2865.44800
Epoch 92: Val Loss 2866.24292
Epoch 93: Val Loss 2805.12720
Epoch 94: Val Loss 2805.95923
Epoch 95: Val Loss 2797.21558
Epoch 96: Val Loss 2749.76685
Epoch 97: Val Loss 2759.37720
Epoch 98: Val Loss 2709.32886
Epoch 99: Val Loss 2730.76489
{'MSE - mean': 2709.3285799421187, 'MSE - std': 0.0, 'R2 - mean': 0.684983774868797, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524957.56250
Epoch 1: Val Loss 524851.93750
Epoch 2: Val Loss 524622.06250
Epoch 3: Val Loss 524121.09375
Epoch 4: Val Loss 523039.68750
Epoch 5: Val Loss 520772.56250
Epoch 6: Val Loss 516355.21875
Epoch 7: Val Loss 508486.12500
Epoch 8: Val Loss 495385.21875
Epoch 9: Val Loss 474987.87500
Epoch 10: Val Loss 445240.18750
Epoch 11: Val Loss 404484.37500
Epoch 12: Val Loss 352314.96875
Epoch 13: Val Loss 290853.71875
Epoch 14: Val Loss 223488.76562
Epoch 15: Val Loss 156687.28125
Epoch 16: Val Loss 98739.01562
Epoch 17: Val Loss 55917.73047
Epoch 18: Val Loss 30603.22656
Epoch 19: Val Loss 18431.93750
Epoch 20: Val Loss 13524.62207
Epoch 21: Val Loss 11161.05078
Epoch 22: Val Loss 9633.42285
Epoch 23: Val Loss 8492.95996
Epoch 24: Val Loss 7580.70850
Epoch 25: Val Loss 6869.19141
Epoch 26: Val Loss 6309.66797
Epoch 27: Val Loss 5854.32715
Epoch 28: Val Loss 5495.41016
Epoch 29: Val Loss 5208.91211
Epoch 30: Val Loss 4968.25000
Epoch 31: Val Loss 4775.13770
Epoch 32: Val Loss 4610.20850
Epoch 33: Val Loss 4469.62158
Epoch 34: Val Loss 4362.02051
Epoch 35: Val Loss 4260.40039
Epoch 36: Val Loss 4172.38281
Epoch 37: Val Loss 4094.03076
Epoch 38: Val Loss 4028.22583
Epoch 39: Val Loss 3965.46387
Epoch 40: Val Loss 3908.90845
Epoch 41: Val Loss 3856.28442
Epoch 42: Val Loss 3804.79102
Epoch 43: Val Loss 3758.14941
Epoch 44: Val Loss 3715.49707
Epoch 45: Val Loss 3675.79565
Epoch 46: Val Loss 3631.86450
Epoch 47: Val Loss 3594.08154
Epoch 48: Val Loss 3557.38330
Epoch 49: Val Loss 3516.98242
Epoch 50: Val Loss 3484.70752
Epoch 51: Val Loss 3448.15674
Epoch 52: Val Loss 3414.91602
Epoch 53: Val Loss 3380.85205
Epoch 54: Val Loss 3348.04639
Epoch 55: Val Loss 3318.44653
Epoch 56: Val Loss 3288.84009
Epoch 57: Val Loss 3256.05835
Epoch 58: Val Loss 3233.48608
Epoch 59: Val Loss 3202.47168
Epoch 60: Val Loss 3184.54565
Epoch 61: Val Loss 3150.84790
Epoch 62: Val Loss 3123.30176
Epoch 63: Val Loss 3094.69482
Epoch 64: Val Loss 3072.58301
Epoch 65: Val Loss 3051.40698
Epoch 66: Val Loss 3033.90576
Epoch 67: Val Loss 3008.52148
Epoch 68: Val Loss 2986.58008
Epoch 69: Val Loss 2962.23462
Epoch 70: Val Loss 2940.11401
Epoch 71: Val Loss 2928.34839
Epoch 72: Val Loss 2902.99414
Epoch 73: Val Loss 2884.55469
Epoch 74: Val Loss 2866.02344
Epoch 75: Val Loss 2847.39087
Epoch 76: Val Loss 2832.74146
Epoch 77: Val Loss 2815.72974
Epoch 78: Val Loss 2785.00586
Epoch 79: Val Loss 2773.85767
Epoch 80: Val Loss 2759.21069
Epoch 81: Val Loss 2744.24292
Epoch 82: Val Loss 2721.29297
Epoch 83: Val Loss 2711.56079
Epoch 84: Val Loss 2698.77173
Epoch 85: Val Loss 2675.54785
Epoch 86: Val Loss 2665.59424
Epoch 87: Val Loss 2647.24341
Epoch 88: Val Loss 2634.00317
Epoch 89: Val Loss 2621.63770
Epoch 90: Val Loss 2605.99683
Epoch 91: Val Loss 2589.22021
Epoch 92: Val Loss 2579.53345
Epoch 93: Val Loss 2576.89990
Epoch 94: Val Loss 2563.78955
Epoch 95: Val Loss 2531.10620
Epoch 96: Val Loss 2531.66211
Epoch 97: Val Loss 2514.21411
Epoch 98: Val Loss 2499.00342
Epoch 99: Val Loss 2484.34619
{'MSE - mean': 2596.8374272858014, 'MSE - std': 112.49115265631735, 'R2 - mean': 0.6799138970048673, 'R2 - std': 0.0050698778639297615} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517909.53125
Epoch 1: Val Loss 517662.34375
Epoch 2: Val Loss 517203.59375
Epoch 3: Val Loss 516295.18750
Epoch 4: Val Loss 514465.46875
Epoch 5: Val Loss 510960.84375
Epoch 6: Val Loss 504708.71875
Epoch 7: Val Loss 494112.25000
Epoch 8: Val Loss 477497.40625
Epoch 9: Val Loss 452570.78125
Epoch 10: Val Loss 418151.59375
Epoch 11: Val Loss 373140.06250
Epoch 12: Val Loss 317621.15625
Epoch 13: Val Loss 254297.18750
Epoch 14: Val Loss 188526.82812
Epoch 15: Val Loss 127307.39062
Epoch 16: Val Loss 78352.35156
Epoch 17: Val Loss 45427.75391
Epoch 18: Val Loss 27357.40234
Epoch 19: Val Loss 18855.32031
Epoch 20: Val Loss 14664.89062
Epoch 21: Val Loss 12285.97070
Epoch 22: Val Loss 10574.65820
Epoch 23: Val Loss 9318.72266
Epoch 24: Val Loss 8306.89258
Epoch 25: Val Loss 7564.49805
Epoch 26: Val Loss 6911.14795
Epoch 27: Val Loss 6429.83838
Epoch 28: Val Loss 6015.41455
Epoch 29: Val Loss 5751.14062
Epoch 30: Val Loss 5493.00879
Epoch 31: Val Loss 5265.60059
Epoch 32: Val Loss 5087.94678
Epoch 33: Val Loss 4943.42139
Epoch 34: Val Loss 4824.89160
Epoch 35: Val Loss 4724.04980
Epoch 36: Val Loss 4598.11133
Epoch 37: Val Loss 4512.65918
Epoch 38: Val Loss 4419.42822
Epoch 39: Val Loss 4408.10938
Epoch 40: Val Loss 4312.63281
Epoch 41: Val Loss 4201.87158
Epoch 42: Val Loss 4190.96289
Epoch 43: Val Loss 4094.20581
Epoch 44: Val Loss 4062.87598
Epoch 45: Val Loss 3997.35327
Epoch 46: Val Loss 3938.88306
Epoch 47: Val Loss 3920.38916
Epoch 48: Val Loss 3831.63525
Epoch 49: Val Loss 3815.19922
Epoch 50: Val Loss 3755.76929
Epoch 51: Val Loss 3743.84888
Epoch 52: Val Loss 3667.75098
Epoch 53: Val Loss 3653.76367
Epoch 54: Val Loss 3598.82715
Epoch 55: Val Loss 3561.93726
Epoch 56: Val Loss 3526.14648
Epoch 57: Val Loss 3502.31494
Epoch 58: Val Loss 3446.12988
Epoch 59: Val Loss 3444.09277
Epoch 60: Val Loss 3378.44800
Epoch 61: Val Loss 3364.27100
Epoch 62: Val Loss 3300.99219
Epoch 63: Val Loss 3311.54346
Epoch 64: Val Loss 3239.44604
Epoch 65: Val Loss 3223.29297
Epoch 66: Val Loss 3230.86914
Epoch 67: Val Loss 3176.87524
Epoch 68: Val Loss 3124.62280
Epoch 69: Val Loss 3107.34399
Epoch 70: Val Loss 3110.27026
Epoch 71: Val Loss 3057.05469
Epoch 72: Val Loss 3034.95532
Epoch 73: Val Loss 3006.78125
Epoch 74: Val Loss 2984.43994
Epoch 75: Val Loss 2966.61816
Epoch 76: Val Loss 2896.02295
Epoch 77: Val Loss 2916.18677
Epoch 78: Val Loss 2899.98730
Epoch 79: Val Loss 2876.05469
Epoch 80: Val Loss 2835.99512
Epoch 81: Val Loss 2809.83179
Epoch 82: Val Loss 2787.62427
Epoch 83: Val Loss 2775.68018
Epoch 84: Val Loss 2769.87939
Epoch 85: Val Loss 2725.50659
Epoch 86: Val Loss 2699.07544
Epoch 87: Val Loss 2676.42334
Epoch 88: Val Loss 2664.20508
Epoch 89: Val Loss 2655.56860
Epoch 90: Val Loss 2639.14331
Epoch 91: Val Loss 2584.91650
Epoch 92: Val Loss 2620.61157
Epoch 93: Val Loss 2565.48877
Epoch 94: Val Loss 2542.72388
Epoch 95: Val Loss 2561.01978
Epoch 96: Val Loss 2517.32764
Epoch 97: Val Loss 2505.38354
Epoch 98: Val Loss 2478.18652
Epoch 99: Val Loss 2463.40137
{'MSE - mean': 2552.358771133233, 'MSE - std': 111.32328892423212, 'R2 - mean': 0.6983953028592372, 'R2 - std': 0.02646243562255728} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520481.21875
Epoch 1: Val Loss 519991.25000
Epoch 2: Val Loss 518929.62500
Epoch 3: Val Loss 516599.59375
Epoch 4: Val Loss 511892.34375
Epoch 5: Val Loss 503165.50000
Epoch 6: Val Loss 488214.34375
Epoch 7: Val Loss 464491.15625
Epoch 8: Val Loss 429444.50000
Epoch 9: Val Loss 381338.31250
Epoch 10: Val Loss 319727.59375
Epoch 11: Val Loss 248159.64062
Epoch 12: Val Loss 172768.96875
Epoch 13: Val Loss 104431.96094
Epoch 14: Val Loss 54061.35156
Epoch 15: Val Loss 25686.30469
Epoch 16: Val Loss 14269.15820
Epoch 17: Val Loss 10455.60938
Epoch 18: Val Loss 8759.38770
Epoch 19: Val Loss 7711.40137
Epoch 20: Val Loss 7011.46631
Epoch 21: Val Loss 6511.62598
Epoch 22: Val Loss 6149.37256
Epoch 23: Val Loss 5860.36572
Epoch 24: Val Loss 5627.80469
Epoch 25: Val Loss 5444.95215
Epoch 26: Val Loss 5301.41455
Epoch 27: Val Loss 5164.98535
Epoch 28: Val Loss 5048.40625
Epoch 29: Val Loss 4940.94531
Epoch 30: Val Loss 4847.28125
Epoch 31: Val Loss 4757.99072
Epoch 32: Val Loss 4680.66797
Epoch 33: Val Loss 4592.52832
Epoch 34: Val Loss 4518.43994
Epoch 35: Val Loss 4445.01123
Epoch 36: Val Loss 4380.43945
Epoch 37: Val Loss 4316.26221
Epoch 38: Val Loss 4252.38721
Epoch 39: Val Loss 4193.76953
Epoch 40: Val Loss 4137.41846
Epoch 41: Val Loss 4080.71631
Epoch 42: Val Loss 4028.24658
Epoch 43: Val Loss 3975.37646
Epoch 44: Val Loss 3925.71851
Epoch 45: Val Loss 3876.07349
Epoch 46: Val Loss 3826.24365
Epoch 47: Val Loss 3778.63818
Epoch 48: Val Loss 3731.12451
Epoch 49: Val Loss 3687.16797
Epoch 50: Val Loss 3645.35767
Epoch 51: Val Loss 3601.34644
Epoch 52: Val Loss 3560.72998
Epoch 53: Val Loss 3522.93677
Epoch 54: Val Loss 3482.18506
Epoch 55: Val Loss 3445.08325
Epoch 56: Val Loss 3411.90332
Epoch 57: Val Loss 3371.28491
Epoch 58: Val Loss 3334.03540
Epoch 59: Val Loss 3303.97754
Epoch 60: Val Loss 3267.56177
Epoch 61: Val Loss 3233.62891
Epoch 62: Val Loss 3202.64526
Epoch 63: Val Loss 3166.36328
Epoch 64: Val Loss 3132.95776
Epoch 65: Val Loss 3104.34058
Epoch 66: Val Loss 3077.92163
Epoch 67: Val Loss 3043.48779
Epoch 68: Val Loss 3017.42236
Epoch 69: Val Loss 2987.96802
Epoch 70: Val Loss 2956.61694
Epoch 71: Val Loss 2932.32349
Epoch 72: Val Loss 2902.94458
Epoch 73: Val Loss 2873.72510
Epoch 74: Val Loss 2850.16528
Epoch 75: Val Loss 2818.94849
Epoch 76: Val Loss 2797.04858
Epoch 77: Val Loss 2764.41406
Epoch 78: Val Loss 2742.66748
Epoch 79: Val Loss 2722.32129
Epoch 80: Val Loss 2693.11426
Epoch 81: Val Loss 2668.16016
Epoch 82: Val Loss 2648.19971
Epoch 83: Val Loss 2613.98730
Epoch 84: Val Loss 2596.52368
Epoch 85: Val Loss 2572.96118
Epoch 86: Val Loss 2550.56006
Epoch 87: Val Loss 2526.69263
Epoch 88: Val Loss 2501.42676
Epoch 89: Val Loss 2487.13550
Epoch 90: Val Loss 2456.18726
Epoch 91: Val Loss 2440.17725
Epoch 92: Val Loss 2416.77661
Epoch 93: Val Loss 2401.39331
Epoch 94: Val Loss 2378.48096
Epoch 95: Val Loss 2351.85645
Epoch 96: Val Loss 2338.74341
Epoch 97: Val Loss 2309.46704
Epoch 98: Val Loss 2295.73560
Epoch 99: Val Loss 2266.74927
{'MSE - mean': 2480.9564238744506, 'MSE - std': 156.81052762768303, 'R2 - mean': 0.7055662288252382, 'R2 - std': 0.026066490211344} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517378.18750
Epoch 1: Val Loss 517036.28125
Epoch 2: Val Loss 516359.93750
Epoch 3: Val Loss 514962.56250
Epoch 4: Val Loss 512149.00000
Epoch 5: Val Loss 506871.81250
Epoch 6: Val Loss 497696.03125
Epoch 7: Val Loss 482668.50000
Epoch 8: Val Loss 459850.31250
Epoch 9: Val Loss 427303.46875
Epoch 10: Val Loss 383901.93750
Epoch 11: Val Loss 329797.71875
Epoch 12: Val Loss 268102.37500
Epoch 13: Val Loss 202786.14062
Epoch 14: Val Loss 141407.04688
Epoch 15: Val Loss 91595.28906
Epoch 16: Val Loss 57746.14062
Epoch 17: Val Loss 38313.64062
Epoch 18: Val Loss 28472.02734
Epoch 19: Val Loss 23141.41406
Epoch 20: Val Loss 19700.67773
Epoch 21: Val Loss 17142.34570
Epoch 22: Val Loss 15278.11133
Epoch 23: Val Loss 13688.00977
Epoch 24: Val Loss 12364.24316
Epoch 25: Val Loss 11214.06641
Epoch 26: Val Loss 10220.14941
Epoch 27: Val Loss 9406.74707
Epoch 28: Val Loss 8711.10352
Epoch 29: Val Loss 8126.23047
Epoch 30: Val Loss 7591.26221
Epoch 31: Val Loss 7117.06006
Epoch 32: Val Loss 6651.26270
Epoch 33: Val Loss 6316.23047
Epoch 34: Val Loss 5987.09521
Epoch 35: Val Loss 5718.95605
Epoch 36: Val Loss 5434.16064
Epoch 37: Val Loss 5201.40479
Epoch 38: Val Loss 5039.28760
Epoch 39: Val Loss 4848.53027
Epoch 40: Val Loss 4677.57129
Epoch 41: Val Loss 4532.44336
Epoch 42: Val Loss 4388.31201
Epoch 43: Val Loss 4290.22656
Epoch 44: Val Loss 4130.39600
Epoch 45: Val Loss 4033.17871
Epoch 46: Val Loss 3945.34204
Epoch 47: Val Loss 3863.08130
Epoch 48: Val Loss 3798.09277
Epoch 49: Val Loss 3717.42676
Epoch 50: Val Loss 3622.67920
Epoch 51: Val Loss 3574.42212
Epoch 52: Val Loss 3513.39160
Epoch 53: Val Loss 3454.14502
Epoch 54: Val Loss 3412.89868
Epoch 55: Val Loss 3344.58887
Epoch 56: Val Loss 3288.84131
Epoch 57: Val Loss 3245.92310
Epoch 58: Val Loss 3215.46899
Epoch 59: Val Loss 3167.78857
Epoch 60: Val Loss 3140.98706
Epoch 61: Val Loss 3087.87329
Epoch 62: Val Loss 3035.95459
Epoch 63: Val Loss 3008.70776
Epoch 64: Val Loss 2965.85767
Epoch 65: Val Loss 2956.14746
Epoch 66: Val Loss 2900.98608
Epoch 67: Val Loss 2883.12500
Epoch 68: Val Loss 2849.75757
Epoch 69: Val Loss 2819.48828
Epoch 70: Val Loss 2795.79517
Epoch 71: Val Loss 2767.99634
Epoch 72: Val Loss 2727.13599
Epoch 73: Val Loss 2695.30957
Epoch 74: Val Loss 2698.24048
Epoch 75: Val Loss 2649.58643
Epoch 76: Val Loss 2633.86841
Epoch 77: Val Loss 2596.69604
Epoch 78: Val Loss 2584.31787
Epoch 79: Val Loss 2569.15259
Epoch 80: Val Loss 2540.73120
Epoch 81: Val Loss 2509.36279
Epoch 82: Val Loss 2501.41064
Epoch 83: Val Loss 2470.49341
Epoch 84: Val Loss 2456.97681
Epoch 85: Val Loss 2433.60010
Epoch 86: Val Loss 2412.69458
Epoch 87: Val Loss 2385.44702
Epoch 88: Val Loss 2371.38135
Epoch 89: Val Loss 2340.81201
Epoch 90: Val Loss 2315.49683
Epoch 91: Val Loss 2314.26172
Epoch 92: Val Loss 2299.48828
Epoch 93: Val Loss 2258.19287
Epoch 94: Val Loss 2250.58374
Epoch 95: Val Loss 2239.00854
Epoch 96: Val Loss 2211.96069
Epoch 97: Val Loss 2197.11475
Epoch 98: Val Loss 2174.89185
Epoch 99: Val Loss 2165.46387
{'MSE - mean': 2417.8579079535752, 'MSE - std': 188.6725314018065, 'R2 - mean': 0.710167355341192, 'R2 - std': 0.025064935475953756} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 2417.8579079535752 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516569.43750
Epoch 1: Val Loss 516542.53125
Epoch 2: Val Loss 516514.15625
Epoch 3: Val Loss 516484.09375
Epoch 4: Val Loss 516451.78125
Epoch 5: Val Loss 516417.40625
Epoch 6: Val Loss 516380.46875
Epoch 7: Val Loss 516340.56250
Epoch 8: Val Loss 516297.65625
Epoch 9: Val Loss 516251.21875
Epoch 10: Val Loss 516200.87500
Epoch 11: Val Loss 516146.25000
Epoch 12: Val Loss 516087.34375
Epoch 13: Val Loss 516023.75000
Epoch 14: Val Loss 515955.12500
Epoch 15: Val Loss 515880.46875
Epoch 16: Val Loss 515799.84375
Epoch 17: Val Loss 515711.84375
Epoch 18: Val Loss 515616.06250
Epoch 19: Val Loss 515510.25000
Epoch 20: Val Loss 515393.50000
Epoch 21: Val Loss 515265.37500
Epoch 22: Val Loss 515122.50000
Epoch 23: Val Loss 514966.56250
Epoch 24: Val Loss 514795.18750
Epoch 25: Val Loss 514608.65625
Epoch 26: Val Loss 514406.37500
Epoch 27: Val Loss 514180.96875
Epoch 28: Val Loss 513940.09375
Epoch 29: Val Loss 513675.31250
Epoch 30: Val Loss 513387.59375
Epoch 31: Val Loss 513075.12500
Epoch 32: Val Loss 512738.87500
Epoch 33: Val Loss 512373.15625
Epoch 34: Val Loss 511980.12500
Epoch 35: Val Loss 511554.15625
Epoch 36: Val Loss 511099.15625
Epoch 37: Val Loss 510616.87500
Epoch 38: Val Loss 510094.96875
Epoch 39: Val Loss 509538.53125
Epoch 40: Val Loss 508950.43750
Epoch 41: Val Loss 508316.12500
Epoch 42: Val Loss 507646.15625
Epoch 43: Val Loss 506941.78125
Epoch 44: Val Loss 506189.15625
Epoch 45: Val Loss 505390.59375
Epoch 46: Val Loss 504543.62500
Epoch 47: Val Loss 503642.96875
Epoch 48: Val Loss 502705.12500
Epoch 49: Val Loss 501713.68750
Epoch 50: Val Loss 500666.96875
Epoch 51: Val Loss 499555.25000
Epoch 52: Val Loss 498399.59375
Epoch 53: Val Loss 497165.93750
Epoch 54: Val Loss 495898.90625
Epoch 55: Val Loss 494555.40625
Epoch 56: Val Loss 493147.71875
Epoch 57: Val Loss 491676.53125
Epoch 58: Val Loss 490140.43750
Epoch 59: Val Loss 488532.81250
Epoch 60: Val Loss 486862.21875
Epoch 61: Val Loss 485114.90625
Epoch 62: Val Loss 483318.28125
Epoch 63: Val Loss 481429.59375
Epoch 64: Val Loss 479464.65625
Epoch 65: Val Loss 477416.40625
Epoch 66: Val Loss 475312.40625
Epoch 67: Val Loss 473123.78125
Epoch 68: Val Loss 470864.59375
Epoch 69: Val Loss 468533.34375
Epoch 70: Val Loss 466101.71875
Epoch 71: Val Loss 463590.62500
Epoch 72: Val Loss 461013.50000
Epoch 73: Val Loss 458335.59375
Epoch 74: Val Loss 455560.62500
Epoch 75: Val Loss 452735.84375
Epoch 76: Val Loss 449784.65625
Epoch 77: Val Loss 446766.53125
Epoch 78: Val Loss 443646.59375
Epoch 79: Val Loss 440481.43750
Epoch 80: Val Loss 437227.40625
Epoch 81: Val Loss 433861.96875
Epoch 82: Val Loss 430427.81250
Epoch 83: Val Loss 426924.78125
Epoch 84: Val Loss 423339.15625
Epoch 85: Val Loss 419658.31250
Epoch 86: Val Loss 415927.06250
Epoch 87: Val Loss 412109.68750
Epoch 88: Val Loss 408219.62500
Epoch 89: Val Loss 404254.31250
Epoch 90: Val Loss 400200.18750
Epoch 91: Val Loss 396079.93750
Epoch 92: Val Loss 391912.62500
Epoch 93: Val Loss 387732.90625
Epoch 94: Val Loss 383417.46875
Epoch 95: Val Loss 379048.25000
Epoch 96: Val Loss 374553.40625
Epoch 97: Val Loss 370071.75000
Epoch 98: Val Loss 365556.15625
Epoch 99: Val Loss 360911.06250
{'MSE - mean': 360911.06867692596, 'MSE - std': 0.0, 'R2 - mean': -40.963475122350225, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524550.81250
Epoch 1: Val Loss 524512.12500
Epoch 2: Val Loss 524473.06250
Epoch 3: Val Loss 524432.75000
Epoch 4: Val Loss 524390.87500
Epoch 5: Val Loss 524346.93750
Epoch 6: Val Loss 524300.43750
Epoch 7: Val Loss 524251.28125
Epoch 8: Val Loss 524199.12500
Epoch 9: Val Loss 524143.25000
Epoch 10: Val Loss 524083.81250
Epoch 11: Val Loss 524019.28125
Epoch 12: Val Loss 523951.15625
Epoch 13: Val Loss 523876.62500
Epoch 14: Val Loss 523798.50000
Epoch 15: Val Loss 523711.31250
Epoch 16: Val Loss 523616.78125
Epoch 17: Val Loss 523513.93750
Epoch 18: Val Loss 523402.06250
Epoch 19: Val Loss 523283.56250
Epoch 20: Val Loss 523154.03125
Epoch 21: Val Loss 523011.50000
Epoch 22: Val Loss 522860.00000
Epoch 23: Val Loss 522696.21875
Epoch 24: Val Loss 522520.06250
Epoch 25: Val Loss 522327.40625
Epoch 26: Val Loss 522125.71875
Epoch 27: Val Loss 521907.15625
Epoch 28: Val Loss 521671.00000
Epoch 29: Val Loss 521419.46875
Epoch 30: Val Loss 521148.96875
Epoch 31: Val Loss 520853.93750
Epoch 32: Val Loss 520541.75000
Epoch 33: Val Loss 520206.81250
Epoch 34: Val Loss 519852.78125
Epoch 35: Val Loss 519469.31250
Epoch 36: Val Loss 519059.50000
Epoch 37: Val Loss 518624.78125
Epoch 38: Val Loss 518154.31250
Epoch 39: Val Loss 517656.71875
Epoch 40: Val Loss 517132.43750
Epoch 41: Val Loss 516569.34375
Epoch 42: Val Loss 515978.46875
Epoch 43: Val Loss 515351.40625
Epoch 44: Val Loss 514686.65625
Epoch 45: Val Loss 513985.68750
Epoch 46: Val Loss 513250.31250
Epoch 47: Val Loss 512477.75000
Epoch 48: Val Loss 511672.56250
Epoch 49: Val Loss 510807.28125
Epoch 50: Val Loss 509910.15625
Epoch 51: Val Loss 508974.75000
Epoch 52: Val Loss 507999.78125
Epoch 53: Val Loss 506976.06250
Epoch 54: Val Loss 505908.75000
Epoch 55: Val Loss 504775.34375
Epoch 56: Val Loss 503614.84375
Epoch 57: Val Loss 502407.81250
Epoch 58: Val Loss 501148.90625
Epoch 59: Val Loss 499841.06250
Epoch 60: Val Loss 498467.34375
Epoch 61: Val Loss 497042.59375
Epoch 62: Val Loss 495565.59375
Epoch 63: Val Loss 494043.96875
Epoch 64: Val Loss 492488.56250
Epoch 65: Val Loss 490827.21875
Epoch 66: Val Loss 489157.03125
Epoch 67: Val Loss 487391.12500
Epoch 68: Val Loss 485594.06250
Epoch 69: Val Loss 483716.53125
Epoch 70: Val Loss 481829.56250
Epoch 71: Val Loss 479878.09375
Epoch 72: Val Loss 477826.59375
Epoch 73: Val Loss 475761.37500
Epoch 74: Val Loss 473614.46875
Epoch 75: Val Loss 471408.81250
Epoch 76: Val Loss 469139.75000
Epoch 77: Val Loss 466820.21875
Epoch 78: Val Loss 464469.06250
Epoch 79: Val Loss 462043.25000
Epoch 80: Val Loss 459551.00000
Epoch 81: Val Loss 456995.43750
Epoch 82: Val Loss 454342.21875
Epoch 83: Val Loss 451679.03125
Epoch 84: Val Loss 448997.43750
Epoch 85: Val Loss 446204.71875
Epoch 86: Val Loss 443359.43750
Epoch 87: Val Loss 440461.43750
Epoch 88: Val Loss 437488.90625
Epoch 89: Val Loss 434475.06250
Epoch 90: Val Loss 431402.50000
Epoch 91: Val Loss 428297.28125
Epoch 92: Val Loss 425119.21875
Epoch 93: Val Loss 421866.37500
Epoch 94: Val Loss 418574.65625
Epoch 95: Val Loss 415208.93750
Epoch 96: Val Loss 411773.34375
Epoch 97: Val Loss 408322.68750
Epoch 98: Val Loss 404826.18750
Epoch 99: Val Loss 401258.18750
{'MSE - mean': 381084.62573791045, 'MSE - std': 20173.557060984487, 'R2 - mean': -46.240455868299634, 'R2 - std': 5.276980745949409} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517998.56250
Epoch 1: Val Loss 517962.53125
Epoch 2: Val Loss 517924.62500
Epoch 3: Val Loss 517885.81250
Epoch 4: Val Loss 517844.78125
Epoch 5: Val Loss 517801.75000
Epoch 6: Val Loss 517757.15625
Epoch 7: Val Loss 517710.15625
Epoch 8: Val Loss 517659.75000
Epoch 9: Val Loss 517606.21875
Epoch 10: Val Loss 517549.21875
Epoch 11: Val Loss 517488.65625
Epoch 12: Val Loss 517424.18750
Epoch 13: Val Loss 517354.43750
Epoch 14: Val Loss 517280.25000
Epoch 15: Val Loss 517200.56250
Epoch 16: Val Loss 517114.43750
Epoch 17: Val Loss 517022.28125
Epoch 18: Val Loss 516922.78125
Epoch 19: Val Loss 516815.59375
Epoch 20: Val Loss 516698.06250
Epoch 21: Val Loss 516572.65625
Epoch 22: Val Loss 516436.09375
Epoch 23: Val Loss 516285.96875
Epoch 24: Val Loss 516119.43750
Epoch 25: Val Loss 515939.15625
Epoch 26: Val Loss 515740.21875
Epoch 27: Val Loss 515518.87500
Epoch 28: Val Loss 515277.90625
Epoch 29: Val Loss 515015.68750
Epoch 30: Val Loss 514726.09375
Epoch 31: Val Loss 514411.87500
Epoch 32: Val Loss 514072.09375
Epoch 33: Val Loss 513704.96875
Epoch 34: Val Loss 513300.00000
Epoch 35: Val Loss 512867.12500
Epoch 36: Val Loss 512395.50000
Epoch 37: Val Loss 511889.56250
Epoch 38: Val Loss 511342.03125
Epoch 39: Val Loss 510766.21875
Epoch 40: Val Loss 510130.96875
Epoch 41: Val Loss 509470.34375
Epoch 42: Val Loss 508759.09375
Epoch 43: Val Loss 508004.31250
Epoch 44: Val Loss 507190.15625
Epoch 45: Val Loss 506328.43750
Epoch 46: Val Loss 505423.78125
Epoch 47: Val Loss 504459.09375
Epoch 48: Val Loss 503445.96875
Epoch 49: Val Loss 502381.43750
Epoch 50: Val Loss 501250.25000
Epoch 51: Val Loss 500054.50000
Epoch 52: Val Loss 498809.09375
Epoch 53: Val Loss 497486.34375
Epoch 54: Val Loss 496122.93750
Epoch 55: Val Loss 494690.25000
Epoch 56: Val Loss 493199.93750
Epoch 57: Val Loss 491631.06250
Epoch 58: Val Loss 489994.50000
Epoch 59: Val Loss 488303.93750
Epoch 60: Val Loss 486533.25000
Epoch 61: Val Loss 484695.46875
Epoch 62: Val Loss 482797.84375
Epoch 63: Val Loss 480815.53125
Epoch 64: Val Loss 478770.37500
Epoch 65: Val Loss 476650.06250
Epoch 66: Val Loss 474461.96875
Epoch 67: Val Loss 472150.25000
Epoch 68: Val Loss 469776.43750
Epoch 69: Val Loss 467334.75000
Epoch 70: Val Loss 464809.03125
Epoch 71: Val Loss 462177.15625
Epoch 72: Val Loss 459478.21875
Epoch 73: Val Loss 456693.34375
Epoch 74: Val Loss 453799.71875
Epoch 75: Val Loss 450853.56250
Epoch 76: Val Loss 447798.21875
Epoch 77: Val Loss 444649.34375
Epoch 78: Val Loss 441425.46875
Epoch 79: Val Loss 438111.34375
Epoch 80: Val Loss 434725.25000
Epoch 81: Val Loss 431239.12500
Epoch 82: Val Loss 427639.59375
Epoch 83: Val Loss 423989.65625
Epoch 84: Val Loss 420288.65625
Epoch 85: Val Loss 416468.28125
Epoch 86: Val Loss 412587.37500
Epoch 87: Val Loss 408639.43750
Epoch 88: Val Loss 404586.25000
Epoch 89: Val Loss 400495.34375
Epoch 90: Val Loss 396306.09375
Epoch 91: Val Loss 392066.65625
Epoch 92: Val Loss 387737.53125
Epoch 93: Val Loss 383345.46875
Epoch 94: Val Loss 378892.68750
Epoch 95: Val Loss 374366.71875
Epoch 96: Val Loss 369785.46875
Epoch 97: Val Loss 365125.96875
Epoch 98: Val Loss 360415.37500
Epoch 99: Val Loss 355712.15625
{'MSE - mean': 372627.13754633657, 'MSE - std': 20356.157529105356, 'R2 - mean': -43.23162556164363, 'R2 - std': 6.055614762949376} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520752.18750
Epoch 1: Val Loss 520700.28125
Epoch 2: Val Loss 520642.84375
Epoch 3: Val Loss 520578.65625
Epoch 4: Val Loss 520507.15625
Epoch 5: Val Loss 520428.59375
Epoch 6: Val Loss 520342.62500
Epoch 7: Val Loss 520245.75000
Epoch 8: Val Loss 520140.21875
Epoch 9: Val Loss 520023.25000
Epoch 10: Val Loss 519895.00000
Epoch 11: Val Loss 519754.59375
Epoch 12: Val Loss 519594.81250
Epoch 13: Val Loss 519415.93750
Epoch 14: Val Loss 519218.75000
Epoch 15: Val Loss 518998.18750
Epoch 16: Val Loss 518753.93750
Epoch 17: Val Loss 518483.93750
Epoch 18: Val Loss 518185.96875
Epoch 19: Val Loss 517859.93750
Epoch 20: Val Loss 517510.09375
Epoch 21: Val Loss 517128.81250
Epoch 22: Val Loss 516712.46875
Epoch 23: Val Loss 516267.56250
Epoch 24: Val Loss 515776.43750
Epoch 25: Val Loss 515253.18750
Epoch 26: Val Loss 514691.21875
Epoch 27: Val Loss 514084.65625
Epoch 28: Val Loss 513439.65625
Epoch 29: Val Loss 512738.90625
Epoch 30: Val Loss 511993.09375
Epoch 31: Val Loss 511201.84375
Epoch 32: Val Loss 510348.15625
Epoch 33: Val Loss 509454.68750
Epoch 34: Val Loss 508491.37500
Epoch 35: Val Loss 507470.68750
Epoch 36: Val Loss 506388.43750
Epoch 37: Val Loss 505253.56250
Epoch 38: Val Loss 504054.09375
Epoch 39: Val Loss 502779.90625
Epoch 40: Val Loss 501441.81250
Epoch 41: Val Loss 500018.59375
Epoch 42: Val Loss 498534.09375
Epoch 43: Val Loss 496983.84375
Epoch 44: Val Loss 495357.75000
Epoch 45: Val Loss 493625.09375
Epoch 46: Val Loss 491857.25000
Epoch 47: Val Loss 489949.62500
Epoch 48: Val Loss 488005.18750
Epoch 49: Val Loss 485948.93750
Epoch 50: Val Loss 483820.75000
Epoch 51: Val Loss 481603.87500
Epoch 52: Val Loss 479320.31250
Epoch 53: Val Loss 476914.96875
Epoch 54: Val Loss 474454.90625
Epoch 55: Val Loss 471859.31250
Epoch 56: Val Loss 469184.18750
Epoch 57: Val Loss 466410.21875
Epoch 58: Val Loss 463560.93750
Epoch 59: Val Loss 460605.65625
Epoch 60: Val Loss 457534.37500
Epoch 61: Val Loss 454397.25000
Epoch 62: Val Loss 451123.31250
Epoch 63: Val Loss 447773.65625
Epoch 64: Val Loss 444354.71875
Epoch 65: Val Loss 440838.43750
Epoch 66: Val Loss 437177.68750
Epoch 67: Val Loss 433421.00000
Epoch 68: Val Loss 429566.62500
Epoch 69: Val Loss 425681.03125
Epoch 70: Val Loss 421620.84375
Epoch 71: Val Loss 417511.53125
Epoch 72: Val Loss 413321.87500
Epoch 73: Val Loss 409009.87500
Epoch 74: Val Loss 404666.40625
Epoch 75: Val Loss 400205.75000
Epoch 76: Val Loss 395671.06250
Epoch 77: Val Loss 391083.75000
Epoch 78: Val Loss 386342.09375
Epoch 79: Val Loss 381547.31250
Epoch 80: Val Loss 376667.46875
Epoch 81: Val Loss 371740.15625
Epoch 82: Val Loss 366659.37500
Epoch 83: Val Loss 361579.75000
Epoch 84: Val Loss 356453.03125
Epoch 85: Val Loss 351195.50000
Epoch 86: Val Loss 345913.68750
Epoch 87: Val Loss 340597.96875
Epoch 88: Val Loss 335235.84375
Epoch 89: Val Loss 329722.84375
Epoch 90: Val Loss 324236.87500
Epoch 91: Val Loss 318678.87500
Epoch 92: Val Loss 313050.25000
Epoch 93: Val Loss 307443.15625
Epoch 94: Val Loss 301719.21875
Epoch 95: Val Loss 296040.78125
Epoch 96: Val Loss 290331.81250
Epoch 97: Val Loss 284549.06250
Epoch 98: Val Loss 278807.25000
Epoch 99: Val Loss 273013.78125
{'MSE - mean': 347723.8065899491, 'MSE - std': 46597.291124940035, 'R2 - mean': -40.39156647797662, 'R2 - std': 7.1903170593581205} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517542.21875
Epoch 1: Val Loss 517520.31250
Epoch 2: Val Loss 517498.37500
Epoch 3: Val Loss 517476.00000
Epoch 4: Val Loss 517453.15625
Epoch 5: Val Loss 517429.46875
Epoch 6: Val Loss 517404.78125
Epoch 7: Val Loss 517378.25000
Epoch 8: Val Loss 517350.03125
Epoch 9: Val Loss 517319.59375
Epoch 10: Val Loss 517286.50000
Epoch 11: Val Loss 517250.00000
Epoch 12: Val Loss 517209.18750
Epoch 13: Val Loss 517164.34375
Epoch 14: Val Loss 517114.37500
Epoch 15: Val Loss 517058.31250
Epoch 16: Val Loss 516994.84375
Epoch 17: Val Loss 516924.00000
Epoch 18: Val Loss 516844.78125
Epoch 19: Val Loss 516756.31250
Epoch 20: Val Loss 516657.93750
Epoch 21: Val Loss 516548.03125
Epoch 22: Val Loss 516424.18750
Epoch 23: Val Loss 516283.87500
Epoch 24: Val Loss 516125.96875
Epoch 25: Val Loss 515946.96875
Epoch 26: Val Loss 515746.96875
Epoch 27: Val Loss 515526.03125
Epoch 28: Val Loss 515280.46875
Epoch 29: Val Loss 515007.25000
Epoch 30: Val Loss 514701.68750
Epoch 31: Val Loss 514360.59375
Epoch 32: Val Loss 513988.40625
Epoch 33: Val Loss 513579.18750
Epoch 34: Val Loss 513136.56250
Epoch 35: Val Loss 512650.59375
Epoch 36: Val Loss 512120.75000
Epoch 37: Val Loss 511549.00000
Epoch 38: Val Loss 510922.21875
Epoch 39: Val Loss 510251.53125
Epoch 40: Val Loss 509523.90625
Epoch 41: Val Loss 508744.59375
Epoch 42: Val Loss 507917.71875
Epoch 43: Val Loss 507010.96875
Epoch 44: Val Loss 506045.03125
Epoch 45: Val Loss 505023.34375
Epoch 46: Val Loss 503925.46875
Epoch 47: Val Loss 502758.81250
Epoch 48: Val Loss 501519.34375
Epoch 49: Val Loss 500183.53125
Epoch 50: Val Loss 498778.00000
Epoch 51: Val Loss 497284.09375
Epoch 52: Val Loss 495697.03125
Epoch 53: Val Loss 494022.93750
Epoch 54: Val Loss 492226.06250
Epoch 55: Val Loss 490373.12500
Epoch 56: Val Loss 488381.84375
Epoch 57: Val Loss 486314.96875
Epoch 58: Val Loss 484128.06250
Epoch 59: Val Loss 481855.56250
Epoch 60: Val Loss 479461.40625
Epoch 61: Val Loss 476955.81250
Epoch 62: Val Loss 474365.90625
Epoch 63: Val Loss 471665.37500
Epoch 64: Val Loss 468842.18750
Epoch 65: Val Loss 465912.59375
Epoch 66: Val Loss 462869.40625
Epoch 67: Val Loss 459695.00000
Epoch 68: Val Loss 456446.75000
Epoch 69: Val Loss 453082.00000
Epoch 70: Val Loss 449624.50000
Epoch 71: Val Loss 446053.78125
Epoch 72: Val Loss 442338.12500
Epoch 73: Val Loss 438545.28125
Epoch 74: Val Loss 434668.75000
Epoch 75: Val Loss 430677.18750
Epoch 76: Val Loss 426536.62500
Epoch 77: Val Loss 422373.71875
Epoch 78: Val Loss 418094.43750
Epoch 79: Val Loss 413705.25000
Epoch 80: Val Loss 409236.03125
Epoch 81: Val Loss 404606.43750
Epoch 82: Val Loss 399938.59375
Epoch 83: Val Loss 395168.93750
Epoch 84: Val Loss 390270.71875
Epoch 85: Val Loss 385280.68750
Epoch 86: Val Loss 380206.09375
Epoch 87: Val Loss 375113.12500
Epoch 88: Val Loss 369892.28125
Epoch 89: Val Loss 364562.31250
Epoch 90: Val Loss 359255.84375
Epoch 91: Val Loss 353800.12500
Epoch 92: Val Loss 348285.90625
Epoch 93: Val Loss 342739.43750
Epoch 94: Val Loss 337098.06250
Epoch 95: Val Loss 331437.96875
Epoch 96: Val Loss 325687.18750
Epoch 97: Val Loss 319937.09375
Epoch 98: Val Loss 314151.21875
Epoch 99: Val Loss 308333.62500
{'MSE - mean': 339845.7715779604, 'MSE - std': 44556.70291601943, 'R2 - mean': -39.84281411630063, 'R2 - std': 6.524189156443658} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 339845.7715779604 and parameters: {'dim': 128, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 515819.31250
Epoch 1: Val Loss 515283.37500
Epoch 2: Val Loss 514265.46875
Epoch 3: Val Loss 512313.34375
Epoch 4: Val Loss 508679.28125
Epoch 5: Val Loss 502208.59375
Epoch 6: Val Loss 491121.50000
Epoch 7: Val Loss 472929.81250
Epoch 8: Val Loss 444872.87500
Epoch 9: Val Loss 404101.59375
Epoch 10: Val Loss 349235.31250
Epoch 11: Val Loss 281621.84375
Epoch 12: Val Loss 207154.54688
Epoch 13: Val Loss 134938.85938
Epoch 14: Val Loss 76404.13281
Epoch 15: Val Loss 39254.46875
Epoch 16: Val Loss 21968.78125
Epoch 17: Val Loss 15691.28516
Epoch 18: Val Loss 12939.99707
Epoch 19: Val Loss 11264.57910
Epoch 20: Val Loss 10114.16504
Epoch 21: Val Loss 9294.32617
Epoch 22: Val Loss 8681.19531
Epoch 23: Val Loss 8211.64355
Epoch 24: Val Loss 7834.43555
Epoch 25: Val Loss 7540.15381
Epoch 26: Val Loss 7293.02734
Epoch 27: Val Loss 7075.27051
Epoch 28: Val Loss 6877.88525
Epoch 29: Val Loss 6701.37793
Epoch 30: Val Loss 6544.10352
Epoch 31: Val Loss 6396.85059
Epoch 32: Val Loss 6249.45410
Epoch 33: Val Loss 6122.32422
Epoch 34: Val Loss 5987.75000
Epoch 35: Val Loss 5882.38184
Epoch 36: Val Loss 5779.76270
Epoch 37: Val Loss 5675.81396
Epoch 38: Val Loss 5582.58594
Epoch 39: Val Loss 5502.06982
Epoch 40: Val Loss 5415.81006
Epoch 41: Val Loss 5332.92432
Epoch 42: Val Loss 5253.66650
Epoch 43: Val Loss 5180.88965
Epoch 44: Val Loss 5121.74512
Epoch 45: Val Loss 5054.19580
Epoch 46: Val Loss 4991.32324
Epoch 47: Val Loss 4943.23926
Epoch 48: Val Loss 4894.02246
Epoch 49: Val Loss 4827.15479
Epoch 50: Val Loss 4780.89697
Epoch 51: Val Loss 4740.22900
Epoch 52: Val Loss 4698.80615
Epoch 53: Val Loss 4648.77832
Epoch 54: Val Loss 4607.30029
Epoch 55: Val Loss 4571.18506
Epoch 56: Val Loss 4548.44531
Epoch 57: Val Loss 4492.55615
Epoch 58: Val Loss 4455.98535
Epoch 59: Val Loss 4413.45752
Epoch 60: Val Loss 4393.19629
Epoch 61: Val Loss 4350.20508
Epoch 62: Val Loss 4320.22266
Epoch 63: Val Loss 4288.92676
Epoch 64: Val Loss 4257.09326
Epoch 65: Val Loss 4225.69092
Epoch 66: Val Loss 4194.58936
Epoch 67: Val Loss 4169.45117
Epoch 68: Val Loss 4143.28613
Epoch 69: Val Loss 4107.49463
Epoch 70: Val Loss 4086.74927
Epoch 71: Val Loss 4050.02515
Epoch 72: Val Loss 4032.30518
Epoch 73: Val Loss 4002.88306
Epoch 74: Val Loss 3983.90625
Epoch 75: Val Loss 3960.98193
Epoch 76: Val Loss 3917.21631
Epoch 77: Val Loss 3894.18579
Epoch 78: Val Loss 3879.86963
Epoch 79: Val Loss 3839.16650
Epoch 80: Val Loss 3819.59521
Epoch 81: Val Loss 3801.45459
Epoch 82: Val Loss 3775.55322
Epoch 83: Val Loss 3747.52100
Epoch 84: Val Loss 3718.15039
Epoch 85: Val Loss 3704.88086
Epoch 86: Val Loss 3676.28687
Epoch 87: Val Loss 3654.94751
Epoch 88: Val Loss 3621.74854
Epoch 89: Val Loss 3601.85645
Epoch 90: Val Loss 3575.80078
Epoch 91: Val Loss 3556.14062
Epoch 92: Val Loss 3540.25757
Epoch 93: Val Loss 3522.69604
Epoch 94: Val Loss 3488.32446
Epoch 95: Val Loss 3463.77588
Epoch 96: Val Loss 3456.02832
Epoch 97: Val Loss 3424.66162
Epoch 98: Val Loss 3402.27979
Epoch 99: Val Loss 3380.54761
{'MSE - mean': 3380.54775700314, 'MSE - std': 0.0, 'R2 - mean': 0.606940479212885, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524688.12500
Epoch 1: Val Loss 524178.65625
Epoch 2: Val Loss 523313.90625
Epoch 3: Val Loss 521782.90625
Epoch 4: Val Loss 519031.00000
Epoch 5: Val Loss 514206.21875
Epoch 6: Val Loss 506221.53125
Epoch 7: Val Loss 493622.62500
Epoch 8: Val Loss 474806.62500
Epoch 9: Val Loss 448363.59375
Epoch 10: Val Loss 413202.18750
Epoch 11: Val Loss 368436.21875
Epoch 12: Val Loss 314962.46875
Epoch 13: Val Loss 255035.28125
Epoch 14: Val Loss 192755.34375
Epoch 15: Val Loss 133727.43750
Epoch 16: Val Loss 83927.15625
Epoch 17: Val Loss 48100.11328
Epoch 18: Val Loss 26766.42578
Epoch 19: Val Loss 16720.87305
Epoch 20: Val Loss 12667.89258
Epoch 21: Val Loss 10869.23828
Epoch 22: Val Loss 9728.02637
Epoch 23: Val Loss 8928.53223
Epoch 24: Val Loss 8271.15625
Epoch 25: Val Loss 7781.35352
Epoch 26: Val Loss 7395.98682
Epoch 27: Val Loss 7100.65625
Epoch 28: Val Loss 6821.71436
Epoch 29: Val Loss 6607.33789
Epoch 30: Val Loss 6415.70850
Epoch 31: Val Loss 6243.25146
Epoch 32: Val Loss 6100.15967
Epoch 33: Val Loss 5948.52734
Epoch 34: Val Loss 5823.99463
Epoch 35: Val Loss 5696.65088
Epoch 36: Val Loss 5563.55615
Epoch 37: Val Loss 5459.74902
Epoch 38: Val Loss 5360.84131
Epoch 39: Val Loss 5259.97705
Epoch 40: Val Loss 5154.34473
Epoch 41: Val Loss 5061.18945
Epoch 42: Val Loss 4960.64062
Epoch 43: Val Loss 4880.39307
Epoch 44: Val Loss 4787.63867
Epoch 45: Val Loss 4701.24854
Epoch 46: Val Loss 4627.30273
Epoch 47: Val Loss 4558.38672
Epoch 48: Val Loss 4483.70264
Epoch 49: Val Loss 4398.01416
Epoch 50: Val Loss 4342.17822
Epoch 51: Val Loss 4287.37354
Epoch 52: Val Loss 4226.07715
Epoch 53: Val Loss 4164.78125
Epoch 54: Val Loss 4102.62354
Epoch 55: Val Loss 4054.16919
Epoch 56: Val Loss 4005.10327
Epoch 57: Val Loss 3959.51880
Epoch 58: Val Loss 3903.47388
Epoch 59: Val Loss 3866.72852
Epoch 60: Val Loss 3822.94897
Epoch 61: Val Loss 3791.82397
Epoch 62: Val Loss 3741.36743
Epoch 63: Val Loss 3709.13232
Epoch 64: Val Loss 3688.36499
Epoch 65: Val Loss 3631.94434
Epoch 66: Val Loss 3624.05054
Epoch 67: Val Loss 3596.22485
Epoch 68: Val Loss 3542.97729
Epoch 69: Val Loss 3519.16211
Epoch 70: Val Loss 3497.52441
Epoch 71: Val Loss 3474.42456
Epoch 72: Val Loss 3440.93945
Epoch 73: Val Loss 3403.31592
Epoch 74: Val Loss 3385.94995
Epoch 75: Val Loss 3367.33472
Epoch 76: Val Loss 3344.82153
Epoch 77: Val Loss 3307.75073
Epoch 78: Val Loss 3293.83008
Epoch 79: Val Loss 3281.67627
Epoch 80: Val Loss 3257.26196
Epoch 81: Val Loss 3228.99902
Epoch 82: Val Loss 3211.06836
Epoch 83: Val Loss 3173.35181
Epoch 84: Val Loss 3164.81226
Epoch 85: Val Loss 3148.60229
Epoch 86: Val Loss 3125.04468
Epoch 87: Val Loss 3135.84033
Epoch 88: Val Loss 3105.84058
Epoch 89: Val Loss 3057.27832
Epoch 90: Val Loss 3059.75195
Epoch 91: Val Loss 3031.88135
Epoch 92: Val Loss 3007.02026
Epoch 93: Val Loss 2996.03564
Epoch 94: Val Loss 2976.70874
Epoch 95: Val Loss 2967.41040
Epoch 96: Val Loss 2927.55273
Epoch 97: Val Loss 2934.34131
Epoch 98: Val Loss 2933.20215
Epoch 99: Val Loss 2895.15454
{'MSE - mean': 3137.8510515239573, 'MSE - std': 242.69670547918258, 'R2 - mean': 0.6140085770475538, 'R2 - std': 0.0070680978346689405} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518080.28125
Epoch 1: Val Loss 517699.15625
Epoch 2: Val Loss 517036.50000
Epoch 3: Val Loss 515723.87500
Epoch 4: Val Loss 513168.75000
Epoch 5: Val Loss 508404.75000
Epoch 6: Val Loss 500068.65625
Epoch 7: Val Loss 486415.31250
Epoch 8: Val Loss 464957.50000
Epoch 9: Val Loss 433274.28125
Epoch 10: Val Loss 389739.56250
Epoch 11: Val Loss 335054.12500
Epoch 12: Val Loss 270697.75000
Epoch 13: Val Loss 202387.35938
Epoch 14: Val Loss 138105.46875
Epoch 15: Val Loss 86063.71875
Epoch 16: Val Loss 51473.71875
Epoch 17: Val Loss 33072.81250
Epoch 18: Val Loss 24446.17383
Epoch 19: Val Loss 19769.25977
Epoch 20: Val Loss 16690.72852
Epoch 21: Val Loss 14518.73145
Epoch 22: Val Loss 12867.97168
Epoch 23: Val Loss 11482.08594
Epoch 24: Val Loss 10412.10254
Epoch 25: Val Loss 9535.76465
Epoch 26: Val Loss 8770.37598
Epoch 27: Val Loss 8156.82715
Epoch 28: Val Loss 7630.73633
Epoch 29: Val Loss 7155.55273
Epoch 30: Val Loss 6789.42920
Epoch 31: Val Loss 6467.56934
Epoch 32: Val Loss 6169.80469
Epoch 33: Val Loss 5927.62793
Epoch 34: Val Loss 5675.04639
Epoch 35: Val Loss 5488.47314
Epoch 36: Val Loss 5339.75684
Epoch 37: Val Loss 5149.78320
Epoch 38: Val Loss 4995.31787
Epoch 39: Val Loss 4858.22510
Epoch 40: Val Loss 4749.38721
Epoch 41: Val Loss 4631.30273
Epoch 42: Val Loss 4537.31494
Epoch 43: Val Loss 4408.45117
Epoch 44: Val Loss 4333.65869
Epoch 45: Val Loss 4234.00928
Epoch 46: Val Loss 4172.96045
Epoch 47: Val Loss 4080.86084
Epoch 48: Val Loss 4014.55005
Epoch 49: Val Loss 3923.26001
Epoch 50: Val Loss 3881.52979
Epoch 51: Val Loss 3823.66235
Epoch 52: Val Loss 3767.15796
Epoch 53: Val Loss 3710.93481
Epoch 54: Val Loss 3663.94263
Epoch 55: Val Loss 3611.31226
Epoch 56: Val Loss 3559.87842
Epoch 57: Val Loss 3515.93677
Epoch 58: Val Loss 3492.36768
Epoch 59: Val Loss 3432.17676
Epoch 60: Val Loss 3386.55469
Epoch 61: Val Loss 3373.90942
Epoch 62: Val Loss 3330.05884
Epoch 63: Val Loss 3314.37134
Epoch 64: Val Loss 3268.00537
Epoch 65: Val Loss 3231.08301
Epoch 66: Val Loss 3199.33228
Epoch 67: Val Loss 3190.63257
Epoch 68: Val Loss 3146.17871
Epoch 69: Val Loss 3121.77417
Epoch 70: Val Loss 3110.54565
Epoch 71: Val Loss 3069.69995
Epoch 72: Val Loss 3064.11865
Epoch 73: Val Loss 3024.50806
Epoch 74: Val Loss 3002.33398
Epoch 75: Val Loss 2965.77832
Epoch 76: Val Loss 2960.48218
Epoch 77: Val Loss 2932.28101
Epoch 78: Val Loss 2913.16577
Epoch 79: Val Loss 2888.75122
Epoch 80: Val Loss 2857.78296
Epoch 81: Val Loss 2842.33472
Epoch 82: Val Loss 2855.86768
Epoch 83: Val Loss 2809.66504
Epoch 84: Val Loss 2769.58081
Epoch 85: Val Loss 2771.64258
Epoch 86: Val Loss 2754.90430
Epoch 87: Val Loss 2730.59497
Epoch 88: Val Loss 2717.51123
Epoch 89: Val Loss 2702.72925
Epoch 90: Val Loss 2665.45166
Epoch 91: Val Loss 2660.04004
Epoch 92: Val Loss 2631.98877
Epoch 93: Val Loss 2627.07104
Epoch 94: Val Loss 2594.24683
Epoch 95: Val Loss 2583.61694
Epoch 96: Val Loss 2574.98877
Epoch 97: Val Loss 2544.56201
Epoch 98: Val Loss 2537.48633
Epoch 99: Val Loss 2518.45166
{'MSE - mean': 2931.3845907615855, 'MSE - std': 352.8804227210393, 'R2 - mean': 0.6524870847847384, 'R2 - std': 0.05472199241037773} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520334.68750
Epoch 1: Val Loss 519874.37500
Epoch 2: Val Loss 518959.03125
Epoch 3: Val Loss 517102.50000
Epoch 4: Val Loss 513356.43750
Epoch 5: Val Loss 506229.68750
Epoch 6: Val Loss 493504.03125
Epoch 7: Val Loss 472530.71875
Epoch 8: Val Loss 439986.59375
Epoch 9: Val Loss 393962.18750
Epoch 10: Val Loss 334205.59375
Epoch 11: Val Loss 262621.65625
Epoch 12: Val Loss 186187.59375
Epoch 13: Val Loss 115650.27344
Epoch 14: Val Loss 62369.21875
Epoch 15: Val Loss 31717.13477
Epoch 16: Val Loss 19112.27344
Epoch 17: Val Loss 14704.07227
Epoch 18: Val Loss 12377.18359
Epoch 19: Val Loss 10719.37598
Epoch 20: Val Loss 9509.11914
Epoch 21: Val Loss 8594.41602
Epoch 22: Val Loss 7956.60059
Epoch 23: Val Loss 7440.04346
Epoch 24: Val Loss 7051.09424
Epoch 25: Val Loss 6761.61279
Epoch 26: Val Loss 6467.17969
Epoch 27: Val Loss 6239.20801
Epoch 28: Val Loss 6038.70801
Epoch 29: Val Loss 5865.54883
Epoch 30: Val Loss 5688.61914
Epoch 31: Val Loss 5567.07275
Epoch 32: Val Loss 5395.23340
Epoch 33: Val Loss 5253.41455
Epoch 34: Val Loss 5116.02637
Epoch 35: Val Loss 4996.31982
Epoch 36: Val Loss 4878.06641
Epoch 37: Val Loss 4777.89502
Epoch 38: Val Loss 4668.34375
Epoch 39: Val Loss 4559.11963
Epoch 40: Val Loss 4473.92480
Epoch 41: Val Loss 4384.45020
Epoch 42: Val Loss 4296.85352
Epoch 43: Val Loss 4220.93555
Epoch 44: Val Loss 4146.07422
Epoch 45: Val Loss 4080.34033
Epoch 46: Val Loss 4002.79395
Epoch 47: Val Loss 3951.96704
Epoch 48: Val Loss 3883.13184
Epoch 49: Val Loss 3837.46338
Epoch 50: Val Loss 3783.28101
Epoch 51: Val Loss 3730.47876
Epoch 52: Val Loss 3667.22144
Epoch 53: Val Loss 3628.19043
Epoch 54: Val Loss 3592.95557
Epoch 55: Val Loss 3537.84253
Epoch 56: Val Loss 3498.48169
Epoch 57: Val Loss 3477.52930
Epoch 58: Val Loss 3435.27344
Epoch 59: Val Loss 3403.02661
Epoch 60: Val Loss 3369.81909
Epoch 61: Val Loss 3329.95801
Epoch 62: Val Loss 3296.31299
Epoch 63: Val Loss 3265.93237
Epoch 64: Val Loss 3253.07910
Epoch 65: Val Loss 3207.83765
Epoch 66: Val Loss 3184.17676
Epoch 67: Val Loss 3149.27832
Epoch 68: Val Loss 3124.45068
Epoch 69: Val Loss 3101.50488
Epoch 70: Val Loss 3080.82251
Epoch 71: Val Loss 3044.85620
Epoch 72: Val Loss 3015.03491
Epoch 73: Val Loss 2999.29053
Epoch 74: Val Loss 2986.67847
Epoch 75: Val Loss 2942.19165
Epoch 76: Val Loss 2933.42529
Epoch 77: Val Loss 2915.45361
Epoch 78: Val Loss 2876.40649
Epoch 79: Val Loss 2880.57471
Epoch 80: Val Loss 2850.65454
Epoch 81: Val Loss 2821.24023
Epoch 82: Val Loss 2813.30981
Epoch 83: Val Loss 2799.56787
Epoch 84: Val Loss 2755.15259
Epoch 85: Val Loss 2742.21143
Epoch 86: Val Loss 2730.33740
Epoch 87: Val Loss 2693.59839
Epoch 88: Val Loss 2682.78296
Epoch 89: Val Loss 2659.09229
Epoch 90: Val Loss 2640.05762
Epoch 91: Val Loss 2643.21484
Epoch 92: Val Loss 2596.40405
Epoch 93: Val Loss 2589.69849
Epoch 94: Val Loss 2570.89526
Epoch 95: Val Loss 2546.13452
Epoch 96: Val Loss 2544.20532
Epoch 97: Val Loss 2509.43188
Epoch 98: Val Loss 2512.06030
Epoch 99: Val Loss 2491.76514
{'MSE - mean': 2821.479697400789, 'MSE - std': 360.04263818161695, 'R2 - mean': 0.6643619862567339, 'R2 - std': 0.051661515608137976} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517717.12500
Epoch 1: Val Loss 517320.56250
Epoch 2: Val Loss 516574.75000
Epoch 3: Val Loss 515066.28125
Epoch 4: Val Loss 511981.59375
Epoch 5: Val Loss 506252.43750
Epoch 6: Val Loss 496215.31250
Epoch 7: Val Loss 479882.25000
Epoch 8: Val Loss 454817.18750
Epoch 9: Val Loss 419140.46875
Epoch 10: Val Loss 371095.25000
Epoch 11: Val Loss 311635.75000
Epoch 12: Val Loss 245041.70312
Epoch 13: Val Loss 177164.68750
Epoch 14: Val Loss 117097.29688
Epoch 15: Val Loss 72351.98438
Epoch 16: Val Loss 45016.07812
Epoch 17: Val Loss 31456.91016
Epoch 18: Val Loss 24243.12891
Epoch 19: Val Loss 19942.70508
Epoch 20: Val Loss 16871.58789
Epoch 21: Val Loss 14516.85742
Epoch 22: Val Loss 12624.68848
Epoch 23: Val Loss 11180.52441
Epoch 24: Val Loss 9979.08301
Epoch 25: Val Loss 9013.19238
Epoch 26: Val Loss 8210.17383
Epoch 27: Val Loss 7514.27881
Epoch 28: Val Loss 6995.30518
Epoch 29: Val Loss 6528.06641
Epoch 30: Val Loss 6141.66504
Epoch 31: Val Loss 5832.09277
Epoch 32: Val Loss 5552.55566
Epoch 33: Val Loss 5309.09277
Epoch 34: Val Loss 5115.53857
Epoch 35: Val Loss 4934.46777
Epoch 36: Val Loss 4799.29932
Epoch 37: Val Loss 4656.81689
Epoch 38: Val Loss 4548.11572
Epoch 39: Val Loss 4445.71436
Epoch 40: Val Loss 4363.62402
Epoch 41: Val Loss 4273.99561
Epoch 42: Val Loss 4187.85107
Epoch 43: Val Loss 4116.53857
Epoch 44: Val Loss 4065.19629
Epoch 45: Val Loss 4000.84692
Epoch 46: Val Loss 3936.96265
Epoch 47: Val Loss 3882.88647
Epoch 48: Val Loss 3826.28955
Epoch 49: Val Loss 3780.61865
Epoch 50: Val Loss 3736.84326
Epoch 51: Val Loss 3685.84985
Epoch 52: Val Loss 3644.26929
Epoch 53: Val Loss 3607.29297
Epoch 54: Val Loss 3568.80298
Epoch 55: Val Loss 3521.89429
Epoch 56: Val Loss 3479.89038
Epoch 57: Val Loss 3444.78296
Epoch 58: Val Loss 3405.46338
Epoch 59: Val Loss 3367.30371
Epoch 60: Val Loss 3349.41138
Epoch 61: Val Loss 3307.79639
Epoch 62: Val Loss 3271.19849
Epoch 63: Val Loss 3236.57251
Epoch 64: Val Loss 3207.22168
Epoch 65: Val Loss 3180.17944
Epoch 66: Val Loss 3151.15845
Epoch 67: Val Loss 3115.67578
Epoch 68: Val Loss 3087.99878
Epoch 69: Val Loss 3062.95508
Epoch 70: Val Loss 3032.66943
Epoch 71: Val Loss 3008.82275
Epoch 72: Val Loss 2974.64160
Epoch 73: Val Loss 2944.79688
Epoch 74: Val Loss 2930.90283
Epoch 75: Val Loss 2897.16846
Epoch 76: Val Loss 2878.19751
Epoch 77: Val Loss 2849.56738
Epoch 78: Val Loss 2816.48413
Epoch 79: Val Loss 2793.54370
Epoch 80: Val Loss 2771.42358
Epoch 81: Val Loss 2744.51660
Epoch 82: Val Loss 2722.32080
Epoch 83: Val Loss 2707.96948
Epoch 84: Val Loss 2680.16602
Epoch 85: Val Loss 2650.32153
Epoch 86: Val Loss 2640.48071
Epoch 87: Val Loss 2616.23730
Epoch 88: Val Loss 2597.33521
Epoch 89: Val Loss 2565.63062
Epoch 90: Val Loss 2554.11963
Epoch 91: Val Loss 2530.89478
Epoch 92: Val Loss 2508.57959
Epoch 93: Val Loss 2491.54492
Epoch 94: Val Loss 2469.00659
Epoch 95: Val Loss 2455.95361
Epoch 96: Val Loss 2430.96338
Epoch 97: Val Loss 2428.52954
Epoch 98: Val Loss 2398.87646
Epoch 99: Val Loss 2376.16821
{'MSE - mean': 2732.417380320647, 'MSE - std': 368.0121551549051, 'R2 - mean': 0.6719218534982297, 'R2 - std': 0.04861826947537388} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 2732.417380320647 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516418.34375
Epoch 1: Val Loss 516183.00000
Epoch 2: Val Loss 515764.34375
Epoch 3: Val Loss 514869.46875
Epoch 4: Val Loss 513035.28125
Epoch 5: Val Loss 509478.68750
Epoch 6: Val Loss 503082.09375
Epoch 7: Val Loss 492202.00000
Epoch 8: Val Loss 474893.34375
Epoch 9: Val Loss 448614.93750
Epoch 10: Val Loss 411130.62500
Epoch 11: Val Loss 361708.96875
Epoch 12: Val Loss 301474.56250
Epoch 13: Val Loss 233260.92188
Epoch 14: Val Loss 163267.78125
Epoch 15: Val Loss 101416.32031
Epoch 16: Val Loss 55620.19531
Epoch 17: Val Loss 29269.13477
Epoch 18: Val Loss 17855.25000
Epoch 19: Val Loss 13665.79590
Epoch 20: Val Loss 11645.71191
Epoch 21: Val Loss 10355.59375
Epoch 22: Val Loss 9439.77148
Epoch 23: Val Loss 8739.00586
Epoch 24: Val Loss 8211.13184
Epoch 25: Val Loss 7812.13672
Epoch 26: Val Loss 7493.02002
Epoch 27: Val Loss 7239.06348
Epoch 28: Val Loss 7024.05078
Epoch 29: Val Loss 6851.97510
Epoch 30: Val Loss 6691.90723
Epoch 31: Val Loss 6553.60742
Epoch 32: Val Loss 6421.94141
Epoch 33: Val Loss 6302.85352
Epoch 34: Val Loss 6191.01416
Epoch 35: Val Loss 6085.41553
Epoch 36: Val Loss 5979.90039
Epoch 37: Val Loss 5880.71338
Epoch 38: Val Loss 5787.00879
Epoch 39: Val Loss 5696.39111
Epoch 40: Val Loss 5600.94727
Epoch 41: Val Loss 5509.29443
Epoch 42: Val Loss 5418.21191
Epoch 43: Val Loss 5338.88525
Epoch 44: Val Loss 5265.60352
Epoch 45: Val Loss 5186.47461
Epoch 46: Val Loss 5109.36182
Epoch 47: Val Loss 5040.45996
Epoch 48: Val Loss 4966.37305
Epoch 49: Val Loss 4905.84717
Epoch 50: Val Loss 4836.61963
Epoch 51: Val Loss 4772.29590
Epoch 52: Val Loss 4703.95312
Epoch 53: Val Loss 4648.92334
Epoch 54: Val Loss 4593.38867
Epoch 55: Val Loss 4536.09961
Epoch 56: Val Loss 4477.70020
Epoch 57: Val Loss 4424.28223
Epoch 58: Val Loss 4375.08203
Epoch 59: Val Loss 4331.46973
Epoch 60: Val Loss 4278.86230
Epoch 61: Val Loss 4230.52344
Epoch 62: Val Loss 4200.20801
Epoch 63: Val Loss 4144.94189
Epoch 64: Val Loss 4097.97070
Epoch 65: Val Loss 4057.93945
Epoch 66: Val Loss 4027.25024
Epoch 67: Val Loss 3984.79614
Epoch 68: Val Loss 3949.38745
Epoch 69: Val Loss 3926.05200
Epoch 70: Val Loss 3876.26831
Epoch 71: Val Loss 3849.47827
Epoch 72: Val Loss 3809.06128
Epoch 73: Val Loss 3781.39600
Epoch 74: Val Loss 3745.25879
Epoch 75: Val Loss 3723.00171
Epoch 76: Val Loss 3686.70850
Epoch 77: Val Loss 3656.84229
Epoch 78: Val Loss 3629.34473
Epoch 79: Val Loss 3613.97363
Epoch 80: Val Loss 3571.58472
Epoch 81: Val Loss 3544.96167
Epoch 82: Val Loss 3526.81714
Epoch 83: Val Loss 3487.72266
Epoch 84: Val Loss 3463.06201
Epoch 85: Val Loss 3444.72046
Epoch 86: Val Loss 3422.75000
Epoch 87: Val Loss 3393.87354
Epoch 88: Val Loss 3364.98389
Epoch 89: Val Loss 3343.30737
Epoch 90: Val Loss 3319.07642
Epoch 91: Val Loss 3290.06641
Epoch 92: Val Loss 3273.38159
Epoch 93: Val Loss 3252.77295
Epoch 94: Val Loss 3231.64038
Epoch 95: Val Loss 3207.99658
Epoch 96: Val Loss 3181.86890
Epoch 97: Val Loss 3172.23950
Epoch 98: Val Loss 3141.51367
Epoch 99: Val Loss 3121.65649
{'MSE - mean': 3121.6566051417917, 'MSE - std': 0.0, 'R2 - mean': 0.6370420010375184, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524606.62500
Epoch 1: Val Loss 524015.93750
Epoch 2: Val Loss 522917.18750
Epoch 3: Val Loss 520786.37500
Epoch 4: Val Loss 516794.12500
Epoch 5: Val Loss 509854.12500
Epoch 6: Val Loss 498233.65625
Epoch 7: Val Loss 479912.96875
Epoch 8: Val Loss 452764.84375
Epoch 9: Val Loss 413947.46875
Epoch 10: Val Loss 363218.34375
Epoch 11: Val Loss 300983.96875
Epoch 12: Val Loss 231327.37500
Epoch 13: Val Loss 161545.59375
Epoch 14: Val Loss 101014.05469
Epoch 15: Val Loss 57985.28906
Epoch 16: Val Loss 33973.94922
Epoch 17: Val Loss 23147.50977
Epoch 18: Val Loss 18363.62891
Epoch 19: Val Loss 15479.60059
Epoch 20: Val Loss 13294.72461
Epoch 21: Val Loss 11603.14844
Epoch 22: Val Loss 10295.80859
Epoch 23: Val Loss 9237.45508
Epoch 24: Val Loss 8392.12695
Epoch 25: Val Loss 7704.35449
Epoch 26: Val Loss 7107.71973
Epoch 27: Val Loss 6625.24561
Epoch 28: Val Loss 6193.63281
Epoch 29: Val Loss 5875.72168
Epoch 30: Val Loss 5593.87256
Epoch 31: Val Loss 5383.81885
Epoch 32: Val Loss 5195.45947
Epoch 33: Val Loss 5046.53320
Epoch 34: Val Loss 4900.12158
Epoch 35: Val Loss 4790.91309
Epoch 36: Val Loss 4707.84619
Epoch 37: Val Loss 4615.08643
Epoch 38: Val Loss 4559.62598
Epoch 39: Val Loss 4488.51221
Epoch 40: Val Loss 4425.02930
Epoch 41: Val Loss 4421.73682
Epoch 42: Val Loss 4355.75146
Epoch 43: Val Loss 4308.57080
Epoch 44: Val Loss 4268.31592
Epoch 45: Val Loss 4227.22119
Epoch 46: Val Loss 4189.18506
Epoch 47: Val Loss 4175.85889
Epoch 48: Val Loss 4109.83643
Epoch 49: Val Loss 4117.82471
Epoch 50: Val Loss 4088.25342
Epoch 51: Val Loss 4028.52954
Epoch 52: Val Loss 4032.16406
Epoch 53: Val Loss 3982.30884
Epoch 54: Val Loss 3965.70557
Epoch 55: Val Loss 3947.59033
Epoch 56: Val Loss 3938.18066
Epoch 57: Val Loss 3864.12256
Epoch 58: Val Loss 3879.29077
Epoch 59: Val Loss 3859.42651
Epoch 60: Val Loss 3824.83569
Epoch 61: Val Loss 3797.22534
Epoch 62: Val Loss 3771.99023
Epoch 63: Val Loss 3745.85400
Epoch 64: Val Loss 3725.73022
Epoch 65: Val Loss 3687.57202
Epoch 66: Val Loss 3698.06274
Epoch 67: Val Loss 3690.21924
Epoch 68: Val Loss 3632.00073
Epoch 69: Val Loss 3622.80518
Epoch 70: Val Loss 3585.90747
Epoch 71: Val Loss 3582.77734
Epoch 72: Val Loss 3580.81543
Epoch 73: Val Loss 3551.31836
Epoch 74: Val Loss 3512.69702
Epoch 75: Val Loss 3499.34424
Epoch 76: Val Loss 3494.13867
Epoch 77: Val Loss 3454.05371
Epoch 78: Val Loss 3429.68188
Epoch 79: Val Loss 3415.81396
Epoch 80: Val Loss 3395.52441
Epoch 81: Val Loss 3402.14038
Epoch 82: Val Loss 3388.93237
Epoch 83: Val Loss 3329.45190
Epoch 84: Val Loss 3329.28345
Epoch 85: Val Loss 3319.83838
Epoch 86: Val Loss 3299.01685
Epoch 87: Val Loss 3266.08984
Epoch 88: Val Loss 3255.47754
Epoch 89: Val Loss 3249.31641
Epoch 90: Val Loss 3228.04248
Epoch 91: Val Loss 3198.77637
Epoch 92: Val Loss 3182.93896
Epoch 93: Val Loss 3152.89404
Epoch 94: Val Loss 3145.74854
Epoch 95: Val Loss 3120.44287
Epoch 96: Val Loss 3124.26343
Epoch 97: Val Loss 3089.43115
Epoch 98: Val Loss 3067.01831
Epoch 99: Val Loss 3057.06885
{'MSE - mean': 3089.3627862736303, 'MSE - std': 32.293818868161225, 'R2 - mean': 0.6184634906616087, 'R2 - std': 0.018578510375909796} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518140.59375
Epoch 1: Val Loss 517736.25000
Epoch 2: Val Loss 517038.28125
Epoch 3: Val Loss 515727.90625
Epoch 4: Val Loss 513341.87500
Epoch 5: Val Loss 509130.90625
Epoch 6: Val Loss 502028.75000
Epoch 7: Val Loss 490651.87500
Epoch 8: Val Loss 473480.31250
Epoch 9: Val Loss 448805.15625
Epoch 10: Val Loss 415409.46875
Epoch 11: Val Loss 372680.12500
Epoch 12: Val Loss 321296.90625
Epoch 13: Val Loss 262776.75000
Epoch 14: Val Loss 201743.26562
Epoch 15: Val Loss 143200.39062
Epoch 16: Val Loss 93598.11719
Epoch 17: Val Loss 57482.00781
Epoch 18: Val Loss 35152.30078
Epoch 19: Val Loss 23519.66211
Epoch 20: Val Loss 17691.46680
Epoch 21: Val Loss 14506.42773
Epoch 22: Val Loss 12289.36621
Epoch 23: Val Loss 10751.52051
Epoch 24: Val Loss 9536.22363
Epoch 25: Val Loss 8623.06250
Epoch 26: Val Loss 7878.05762
Epoch 27: Val Loss 7255.80664
Epoch 28: Val Loss 6819.11572
Epoch 29: Val Loss 6398.40430
Epoch 30: Val Loss 6126.59131
Epoch 31: Val Loss 5905.54150
Epoch 32: Val Loss 5712.68213
Epoch 33: Val Loss 5540.55322
Epoch 34: Val Loss 5403.11377
Epoch 35: Val Loss 5300.86328
Epoch 36: Val Loss 5219.63330
Epoch 37: Val Loss 5113.84424
Epoch 38: Val Loss 5041.28320
Epoch 39: Val Loss 4955.87988
Epoch 40: Val Loss 4888.97461
Epoch 41: Val Loss 4844.90674
Epoch 42: Val Loss 4754.81006
Epoch 43: Val Loss 4705.22949
Epoch 44: Val Loss 4648.04883
Epoch 45: Val Loss 4572.73975
Epoch 46: Val Loss 4539.31738
Epoch 47: Val Loss 4471.90137
Epoch 48: Val Loss 4421.42139
Epoch 49: Val Loss 4369.31494
Epoch 50: Val Loss 4322.85107
Epoch 51: Val Loss 4269.12451
Epoch 52: Val Loss 4217.41162
Epoch 53: Val Loss 4175.29541
Epoch 54: Val Loss 4121.01270
Epoch 55: Val Loss 4066.96045
Epoch 56: Val Loss 4023.80420
Epoch 57: Val Loss 3977.45874
Epoch 58: Val Loss 3912.40137
Epoch 59: Val Loss 3897.57446
Epoch 60: Val Loss 3869.81055
Epoch 61: Val Loss 3827.24731
Epoch 62: Val Loss 3791.67334
Epoch 63: Val Loss 3753.17456
Epoch 64: Val Loss 3710.54858
Epoch 65: Val Loss 3656.51147
Epoch 66: Val Loss 3638.55737
Epoch 67: Val Loss 3593.69849
Epoch 68: Val Loss 3564.75952
Epoch 69: Val Loss 3540.74561
Epoch 70: Val Loss 3510.79053
Epoch 71: Val Loss 3479.80176
Epoch 72: Val Loss 3456.12695
Epoch 73: Val Loss 3431.97607
Epoch 74: Val Loss 3382.74170
Epoch 75: Val Loss 3348.73511
Epoch 76: Val Loss 3337.63013
Epoch 77: Val Loss 3307.75024
Epoch 78: Val Loss 3265.28052
Epoch 79: Val Loss 3254.03198
Epoch 80: Val Loss 3222.81763
Epoch 81: Val Loss 3216.17212
Epoch 82: Val Loss 3156.09741
Epoch 83: Val Loss 3150.37183
Epoch 84: Val Loss 3143.21118
Epoch 85: Val Loss 3105.74951
Epoch 86: Val Loss 3082.53955
Epoch 87: Val Loss 3063.91748
Epoch 88: Val Loss 3032.85889
Epoch 89: Val Loss 3021.70483
Epoch 90: Val Loss 2987.37842
Epoch 91: Val Loss 3003.38965
Epoch 92: Val Loss 2949.56128
Epoch 93: Val Loss 2937.93530
Epoch 94: Val Loss 2924.16650
Epoch 95: Val Loss 2906.85522
Epoch 96: Val Loss 2870.30005
Epoch 97: Val Loss 2866.69238
Epoch 98: Val Loss 2860.75757
Epoch 99: Val Loss 2840.64722
{'MSE - mean': 3006.457659552994, 'MSE - std': 120.17395959409285, 'R2 - mean': 0.6439192559849591, 'R2 - std': 0.03906532147041473} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520460.93750
Epoch 1: Val Loss 519886.28125
Epoch 2: Val Loss 518798.31250
Epoch 3: Val Loss 516647.93750
Epoch 4: Val Loss 512573.18750
Epoch 5: Val Loss 505173.03125
Epoch 6: Val Loss 492824.43750
Epoch 7: Val Loss 473175.34375
Epoch 8: Val Loss 444078.59375
Epoch 9: Val Loss 403222.40625
Epoch 10: Val Loss 348852.56250
Epoch 11: Val Loss 281954.40625
Epoch 12: Val Loss 207092.46875
Epoch 13: Val Loss 134169.14062
Epoch 14: Val Loss 75210.46875
Epoch 15: Val Loss 37333.28125
Epoch 16: Val Loss 19501.42578
Epoch 17: Val Loss 13117.84082
Epoch 18: Val Loss 10487.46875
Epoch 19: Val Loss 8792.13965
Epoch 20: Val Loss 7562.20752
Epoch 21: Val Loss 6690.64648
Epoch 22: Val Loss 6056.62354
Epoch 23: Val Loss 5574.39062
Epoch 24: Val Loss 5213.24268
Epoch 25: Val Loss 4927.45264
Epoch 26: Val Loss 4715.31396
Epoch 27: Val Loss 4544.27490
Epoch 28: Val Loss 4417.07178
Epoch 29: Val Loss 4309.95459
Epoch 30: Val Loss 4221.48486
Epoch 31: Val Loss 4156.22217
Epoch 32: Val Loss 4079.00952
Epoch 33: Val Loss 4023.60645
Epoch 34: Val Loss 3986.59375
Epoch 35: Val Loss 3914.81494
Epoch 36: Val Loss 3874.48315
Epoch 37: Val Loss 3828.07373
Epoch 38: Val Loss 3787.81104
Epoch 39: Val Loss 3754.04053
Epoch 40: Val Loss 3704.48364
Epoch 41: Val Loss 3681.44312
Epoch 42: Val Loss 3640.74878
Epoch 43: Val Loss 3607.83984
Epoch 44: Val Loss 3550.34595
Epoch 45: Val Loss 3527.09180
Epoch 46: Val Loss 3492.32153
Epoch 47: Val Loss 3456.08472
Epoch 48: Val Loss 3436.13867
Epoch 49: Val Loss 3388.48022
Epoch 50: Val Loss 3363.22021
Epoch 51: Val Loss 3343.42310
Epoch 52: Val Loss 3320.25244
Epoch 53: Val Loss 3277.04199
Epoch 54: Val Loss 3255.78247
Epoch 55: Val Loss 3231.73608
Epoch 56: Val Loss 3205.43628
Epoch 57: Val Loss 3176.00854
Epoch 58: Val Loss 3151.20312
Epoch 59: Val Loss 3124.90723
Epoch 60: Val Loss 3107.41699
Epoch 61: Val Loss 3082.79858
Epoch 62: Val Loss 3044.84106
Epoch 63: Val Loss 3025.16748
Epoch 64: Val Loss 3023.81592
Epoch 65: Val Loss 2989.72681
Epoch 66: Val Loss 2958.32300
Epoch 67: Val Loss 2956.46411
Epoch 68: Val Loss 2913.34570
Epoch 69: Val Loss 2916.50146
Epoch 70: Val Loss 2890.05078
Epoch 71: Val Loss 2857.00488
Epoch 72: Val Loss 2850.76562
Epoch 73: Val Loss 2824.66602
Epoch 74: Val Loss 2813.47314
Epoch 75: Val Loss 2791.92358
Epoch 76: Val Loss 2777.21338
Epoch 77: Val Loss 2749.50903
Epoch 78: Val Loss 2725.06934
Epoch 79: Val Loss 2716.81274
Epoch 80: Val Loss 2706.18237
Epoch 81: Val Loss 2671.71265
Epoch 82: Val Loss 2675.15796
Epoch 83: Val Loss 2635.94336
Epoch 84: Val Loss 2629.70654
Epoch 85: Val Loss 2618.99780
Epoch 86: Val Loss 2582.42896
Epoch 87: Val Loss 2592.89819
Epoch 88: Val Loss 2559.81348
Epoch 89: Val Loss 2547.27490
Epoch 90: Val Loss 2534.19263
Epoch 91: Val Loss 2511.06812
Epoch 92: Val Loss 2494.67969
Epoch 93: Val Loss 2484.03711
Epoch 94: Val Loss 2471.72070
Epoch 95: Val Loss 2451.86011
Epoch 96: Val Loss 2428.13110
Epoch 97: Val Loss 2425.98877
Epoch 98: Val Loss 2401.42017
Epoch 99: Val Loss 2388.73462
{'MSE - mean': 2852.0269496105957, 'MSE - std': 287.0154489628239, 'R2 - mean': 0.661037373233479, 'R2 - std': 0.04498515666964332} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517738.00000
Epoch 1: Val Loss 517216.18750
Epoch 2: Val Loss 516305.65625
Epoch 3: Val Loss 514647.84375
Epoch 4: Val Loss 511617.40625
Epoch 5: Val Loss 506310.06250
Epoch 6: Val Loss 497473.56250
Epoch 7: Val Loss 483675.50000
Epoch 8: Val Loss 463227.75000
Epoch 9: Val Loss 434494.34375
Epoch 10: Val Loss 395851.03125
Epoch 11: Val Loss 347102.53125
Epoch 12: Val Loss 289544.71875
Epoch 13: Val Loss 226317.15625
Epoch 14: Val Loss 163404.42188
Epoch 15: Val Loss 108117.68750
Epoch 16: Val Loss 66475.20312
Epoch 17: Val Loss 40636.25000
Epoch 18: Val Loss 27020.42969
Epoch 19: Val Loss 20500.65430
Epoch 20: Val Loss 16788.32422
Epoch 21: Val Loss 14379.32031
Epoch 22: Val Loss 12420.45312
Epoch 23: Val Loss 10948.61523
Epoch 24: Val Loss 9712.46484
Epoch 25: Val Loss 8621.42871
Epoch 26: Val Loss 7747.44238
Epoch 27: Val Loss 7035.91113
Epoch 28: Val Loss 6410.56934
Epoch 29: Val Loss 5941.31104
Epoch 30: Val Loss 5496.86035
Epoch 31: Val Loss 5181.42236
Epoch 32: Val Loss 4899.58301
Epoch 33: Val Loss 4677.01123
Epoch 34: Val Loss 4484.29492
Epoch 35: Val Loss 4339.43799
Epoch 36: Val Loss 4203.15039
Epoch 37: Val Loss 4096.39453
Epoch 38: Val Loss 4016.91187
Epoch 39: Val Loss 3934.33472
Epoch 40: Val Loss 3864.03955
Epoch 41: Val Loss 3795.34375
Epoch 42: Val Loss 3743.51172
Epoch 43: Val Loss 3692.06006
Epoch 44: Val Loss 3640.95361
Epoch 45: Val Loss 3594.86865
Epoch 46: Val Loss 3559.25879
Epoch 47: Val Loss 3516.70264
Epoch 48: Val Loss 3471.89771
Epoch 49: Val Loss 3437.45996
Epoch 50: Val Loss 3404.14087
Epoch 51: Val Loss 3370.33984
Epoch 52: Val Loss 3340.03955
Epoch 53: Val Loss 3312.38208
Epoch 54: Val Loss 3277.83545
Epoch 55: Val Loss 3240.08252
Epoch 56: Val Loss 3208.09375
Epoch 57: Val Loss 3181.43579
Epoch 58: Val Loss 3152.48071
Epoch 59: Val Loss 3137.03857
Epoch 60: Val Loss 3098.80688
Epoch 61: Val Loss 3072.54150
Epoch 62: Val Loss 3052.44800
Epoch 63: Val Loss 3017.09790
Epoch 64: Val Loss 2989.33667
Epoch 65: Val Loss 2964.83691
Epoch 66: Val Loss 2943.72388
Epoch 67: Val Loss 2925.24902
Epoch 68: Val Loss 2896.33423
Epoch 69: Val Loss 2877.98438
Epoch 70: Val Loss 2848.08032
Epoch 71: Val Loss 2831.09399
Epoch 72: Val Loss 2802.24194
Epoch 73: Val Loss 2784.16675
Epoch 74: Val Loss 2765.29077
Epoch 75: Val Loss 2746.67700
Epoch 76: Val Loss 2719.56836
Epoch 77: Val Loss 2709.55884
Epoch 78: Val Loss 2676.20215
Epoch 79: Val Loss 2671.22998
Epoch 80: Val Loss 2640.28101
Epoch 81: Val Loss 2625.44604
Epoch 82: Val Loss 2613.12183
Epoch 83: Val Loss 2588.72559
Epoch 84: Val Loss 2566.14111
Epoch 85: Val Loss 2547.16504
Epoch 86: Val Loss 2530.41504
Epoch 87: Val Loss 2513.13428
Epoch 88: Val Loss 2504.67676
Epoch 89: Val Loss 2478.58130
Epoch 90: Val Loss 2458.39087
Epoch 91: Val Loss 2448.10815
Epoch 92: Val Loss 2425.25952
Epoch 93: Val Loss 2406.93335
Epoch 94: Val Loss 2398.40625
Epoch 95: Val Loss 2378.22681
Epoch 96: Val Loss 2355.77588
Epoch 97: Val Loss 2350.03149
Epoch 98: Val Loss 2331.85864
Epoch 99: Val Loss 2317.50342
{'MSE - mean': 2745.1222584968777, 'MSE - std': 334.090925173287, 'R2 - mean': 0.6707328159465282, 'R2 - std': 0.04466472762798356} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 2745.1222584968777 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.4}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516635.53125
Epoch 1: Val Loss 516613.18750
Epoch 2: Val Loss 516590.59375
Epoch 3: Val Loss 516567.21875
Epoch 4: Val Loss 516542.84375
Epoch 5: Val Loss 516517.28125
Epoch 6: Val Loss 516490.18750
Epoch 7: Val Loss 516461.21875
Epoch 8: Val Loss 516430.21875
Epoch 9: Val Loss 516396.59375
Epoch 10: Val Loss 516360.21875
Epoch 11: Val Loss 516320.59375
Epoch 12: Val Loss 516277.06250
Epoch 13: Val Loss 516229.25000
Epoch 14: Val Loss 516176.28125
Epoch 15: Val Loss 516117.90625
Epoch 16: Val Loss 516052.93750
Epoch 17: Val Loss 515981.03125
Epoch 18: Val Loss 515902.68750
Epoch 19: Val Loss 515816.43750
Epoch 20: Val Loss 515721.43750
Epoch 21: Val Loss 515617.68750
Epoch 22: Val Loss 515503.28125
Epoch 23: Val Loss 515376.12500
Epoch 24: Val Loss 515234.68750
Epoch 25: Val Loss 515078.93750
Epoch 26: Val Loss 514908.84375
Epoch 27: Val Loss 514720.46875
Epoch 28: Val Loss 514516.40625
Epoch 29: Val Loss 514296.90625
Epoch 30: Val Loss 514059.81250
Epoch 31: Val Loss 513802.90625
Epoch 32: Val Loss 513521.81250
Epoch 33: Val Loss 513220.90625
Epoch 34: Val Loss 512900.15625
Epoch 35: Val Loss 512552.18750
Epoch 36: Val Loss 512181.06250
Epoch 37: Val Loss 511782.21875
Epoch 38: Val Loss 511353.21875
Epoch 39: Val Loss 510897.84375
Epoch 40: Val Loss 510403.71875
Epoch 41: Val Loss 509883.37500
Epoch 42: Val Loss 509335.18750
Epoch 43: Val Loss 508746.06250
Epoch 44: Val Loss 508112.84375
Epoch 45: Val Loss 507455.25000
Epoch 46: Val Loss 506762.90625
Epoch 47: Val Loss 506017.00000
Epoch 48: Val Loss 505240.84375
Epoch 49: Val Loss 504409.09375
Epoch 50: Val Loss 503539.18750
Epoch 51: Val Loss 502629.18750
Epoch 52: Val Loss 501663.34375
Epoch 53: Val Loss 500657.15625
Epoch 54: Val Loss 499587.46875
Epoch 55: Val Loss 498480.68750
Epoch 56: Val Loss 497317.06250
Epoch 57: Val Loss 496095.43750
Epoch 58: Val Loss 494821.87500
Epoch 59: Val Loss 493507.06250
Epoch 60: Val Loss 492129.18750
Epoch 61: Val Loss 490699.93750
Epoch 62: Val Loss 489218.34375
Epoch 63: Val Loss 487664.00000
Epoch 64: Val Loss 486048.93750
Epoch 65: Val Loss 484365.75000
Epoch 66: Val Loss 482642.59375
Epoch 67: Val Loss 480847.87500
Epoch 68: Val Loss 478996.93750
Epoch 69: Val Loss 477090.34375
Epoch 70: Val Loss 475092.62500
Epoch 71: Val Loss 473074.84375
Epoch 72: Val Loss 470951.09375
Epoch 73: Val Loss 468776.00000
Epoch 74: Val Loss 466527.37500
Epoch 75: Val Loss 464225.43750
Epoch 76: Val Loss 461877.65625
Epoch 77: Val Loss 459455.37500
Epoch 78: Val Loss 456957.25000
Epoch 79: Val Loss 454390.12500
Epoch 80: Val Loss 451760.40625
Epoch 81: Val Loss 449077.31250
Epoch 82: Val Loss 446316.84375
Epoch 83: Val Loss 443470.53125
Epoch 84: Val Loss 440590.90625
Epoch 85: Val Loss 437624.56250
Epoch 86: Val Loss 434597.96875
Epoch 87: Val Loss 431509.37500
Epoch 88: Val Loss 428338.71875
Epoch 89: Val Loss 425086.59375
Epoch 90: Val Loss 421819.03125
Epoch 91: Val Loss 418439.78125
Epoch 92: Val Loss 415020.96875
Epoch 93: Val Loss 411548.81250
Epoch 94: Val Loss 408020.46875
Epoch 95: Val Loss 404390.56250
Epoch 96: Val Loss 400717.21875
Epoch 97: Val Loss 397002.62500
Epoch 98: Val Loss 393202.87500
Epoch 99: Val Loss 389352.93750
{'MSE - mean': 389352.94750044233, 'MSE - std': 0.0, 'R2 - mean': -44.27043957433804, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524633.37500
Epoch 1: Val Loss 524607.75000
Epoch 2: Val Loss 524584.18750
Epoch 3: Val Loss 524562.68750
Epoch 4: Val Loss 524542.06250
Epoch 5: Val Loss 524521.50000
Epoch 6: Val Loss 524500.43750
Epoch 7: Val Loss 524478.62500
Epoch 8: Val Loss 524455.81250
Epoch 9: Val Loss 524431.87500
Epoch 10: Val Loss 524406.37500
Epoch 11: Val Loss 524379.12500
Epoch 12: Val Loss 524350.37500
Epoch 13: Val Loss 524318.81250
Epoch 14: Val Loss 524284.62500
Epoch 15: Val Loss 524247.00000
Epoch 16: Val Loss 524205.65625
Epoch 17: Val Loss 524160.25000
Epoch 18: Val Loss 524110.37500
Epoch 19: Val Loss 524054.59375
Epoch 20: Val Loss 523992.87500
Epoch 21: Val Loss 523924.21875
Epoch 22: Val Loss 523849.21875
Epoch 23: Val Loss 523765.78125
Epoch 24: Val Loss 523675.03125
Epoch 25: Val Loss 523575.78125
Epoch 26: Val Loss 523465.21875
Epoch 27: Val Loss 523344.53125
Epoch 28: Val Loss 523210.50000
Epoch 29: Val Loss 523062.43750
Epoch 30: Val Loss 522899.78125
Epoch 31: Val Loss 522723.25000
Epoch 32: Val Loss 522528.53125
Epoch 33: Val Loss 522318.37500
Epoch 34: Val Loss 522087.06250
Epoch 35: Val Loss 521836.81250
Epoch 36: Val Loss 521562.96875
Epoch 37: Val Loss 521268.68750
Epoch 38: Val Loss 520944.62500
Epoch 39: Val Loss 520597.78125
Epoch 40: Val Loss 520229.90625
Epoch 41: Val Loss 519820.25000
Epoch 42: Val Loss 519379.25000
Epoch 43: Val Loss 518911.62500
Epoch 44: Val Loss 518407.21875
Epoch 45: Val Loss 517866.09375
Epoch 46: Val Loss 517288.78125
Epoch 47: Val Loss 516667.21875
Epoch 48: Val Loss 516016.71875
Epoch 49: Val Loss 515318.56250
Epoch 50: Val Loss 514581.34375
Epoch 51: Val Loss 513812.43750
Epoch 52: Val Loss 512986.15625
Epoch 53: Val Loss 512119.90625
Epoch 54: Val Loss 511211.68750
Epoch 55: Val Loss 510254.56250
Epoch 56: Val Loss 509253.46875
Epoch 57: Val Loss 508194.59375
Epoch 58: Val Loss 507100.78125
Epoch 59: Val Loss 505937.96875
Epoch 60: Val Loss 504732.00000
Epoch 61: Val Loss 503502.06250
Epoch 62: Val Loss 502192.96875
Epoch 63: Val Loss 500821.21875
Epoch 64: Val Loss 499389.81250
Epoch 65: Val Loss 497909.96875
Epoch 66: Val Loss 496381.12500
Epoch 67: Val Loss 494785.21875
Epoch 68: Val Loss 493131.78125
Epoch 69: Val Loss 491428.71875
Epoch 70: Val Loss 489642.96875
Epoch 71: Val Loss 487803.46875
Epoch 72: Val Loss 485902.78125
Epoch 73: Val Loss 483933.90625
Epoch 74: Val Loss 481885.21875
Epoch 75: Val Loss 479796.50000
Epoch 76: Val Loss 477595.34375
Epoch 77: Val Loss 475366.56250
Epoch 78: Val Loss 473049.21875
Epoch 79: Val Loss 470655.71875
Epoch 80: Val Loss 468191.25000
Epoch 81: Val Loss 465682.18750
Epoch 82: Val Loss 463108.53125
Epoch 83: Val Loss 460458.12500
Epoch 84: Val Loss 457733.93750
Epoch 85: Val Loss 454956.81250
Epoch 86: Val Loss 452105.25000
Epoch 87: Val Loss 449171.12500
Epoch 88: Val Loss 446164.75000
Epoch 89: Val Loss 443113.81250
Epoch 90: Val Loss 439953.06250
Epoch 91: Val Loss 436788.28125
Epoch 92: Val Loss 433502.65625
Epoch 93: Val Loss 430167.59375
Epoch 94: Val Loss 426779.93750
Epoch 95: Val Loss 423312.43750
Epoch 96: Val Loss 419808.65625
Epoch 97: Val Loss 416202.53125
Epoch 98: Val Loss 412587.28125
Epoch 99: Val Loss 408878.53125
{'MSE - mean': 399115.72301738197, 'MSE - std': 9762.775516939611, 'R2 - mean': -48.39261882626556, 'R2 - std': 4.122179251927523} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518690.65625
Epoch 1: Val Loss 518667.43750
Epoch 2: Val Loss 518642.43750
Epoch 3: Val Loss 518615.93750
Epoch 4: Val Loss 518586.81250
Epoch 5: Val Loss 518554.93750
Epoch 6: Val Loss 518521.09375
Epoch 7: Val Loss 518484.21875
Epoch 8: Val Loss 518444.84375
Epoch 9: Val Loss 518402.96875
Epoch 10: Val Loss 518357.96875
Epoch 11: Val Loss 518310.25000
Epoch 12: Val Loss 518259.43750
Epoch 13: Val Loss 518204.62500
Epoch 14: Val Loss 518146.15625
Epoch 15: Val Loss 518083.75000
Epoch 16: Val Loss 518017.06250
Epoch 17: Val Loss 517945.43750
Epoch 18: Val Loss 517868.21875
Epoch 19: Val Loss 517784.68750
Epoch 20: Val Loss 517694.31250
Epoch 21: Val Loss 517595.62500
Epoch 22: Val Loss 517489.56250
Epoch 23: Val Loss 517375.53125
Epoch 24: Val Loss 517252.21875
Epoch 25: Val Loss 517118.31250
Epoch 26: Val Loss 516972.21875
Epoch 27: Val Loss 516813.71875
Epoch 28: Val Loss 516641.00000
Epoch 29: Val Loss 516454.75000
Epoch 30: Val Loss 516250.96875
Epoch 31: Val Loss 516028.46875
Epoch 32: Val Loss 515791.06250
Epoch 33: Val Loss 515534.25000
Epoch 34: Val Loss 515256.96875
Epoch 35: Val Loss 514957.00000
Epoch 36: Val Loss 514636.53125
Epoch 37: Val Loss 514289.06250
Epoch 38: Val Loss 513914.18750
Epoch 39: Val Loss 513508.93750
Epoch 40: Val Loss 513075.59375
Epoch 41: Val Loss 512605.87500
Epoch 42: Val Loss 512105.81250
Epoch 43: Val Loss 511566.75000
Epoch 44: Val Loss 510997.03125
Epoch 45: Val Loss 510384.37500
Epoch 46: Val Loss 509738.43750
Epoch 47: Val Loss 509042.50000
Epoch 48: Val Loss 508312.90625
Epoch 49: Val Loss 507530.25000
Epoch 50: Val Loss 506716.93750
Epoch 51: Val Loss 505849.81250
Epoch 52: Val Loss 504941.31250
Epoch 53: Val Loss 503974.37500
Epoch 54: Val Loss 502955.43750
Epoch 55: Val Loss 501892.71875
Epoch 56: Val Loss 500779.62500
Epoch 57: Val Loss 499610.25000
Epoch 58: Val Loss 498380.56250
Epoch 59: Val Loss 497099.09375
Epoch 60: Val Loss 495766.75000
Epoch 61: Val Loss 494369.15625
Epoch 62: Val Loss 492908.46875
Epoch 63: Val Loss 491391.71875
Epoch 64: Val Loss 489800.90625
Epoch 65: Val Loss 488151.00000
Epoch 66: Val Loss 486431.31250
Epoch 67: Val Loss 484657.34375
Epoch 68: Val Loss 482817.40625
Epoch 69: Val Loss 480914.25000
Epoch 70: Val Loss 478952.59375
Epoch 71: Val Loss 476903.34375
Epoch 72: Val Loss 474787.68750
Epoch 73: Val Loss 472609.25000
Epoch 74: Val Loss 470325.12500
Epoch 75: Val Loss 468029.09375
Epoch 76: Val Loss 465641.03125
Epoch 77: Val Loss 463165.71875
Epoch 78: Val Loss 460595.31250
Epoch 79: Val Loss 458007.03125
Epoch 80: Val Loss 455315.43750
Epoch 81: Val Loss 452559.78125
Epoch 82: Val Loss 449697.46875
Epoch 83: Val Loss 446733.00000
Epoch 84: Val Loss 443704.09375
Epoch 85: Val Loss 440620.40625
Epoch 86: Val Loss 437445.25000
Epoch 87: Val Loss 434185.68750
Epoch 88: Val Loss 430869.53125
Epoch 89: Val Loss 427485.12500
Epoch 90: Val Loss 424001.40625
Epoch 91: Val Loss 420442.90625
Epoch 92: Val Loss 416806.03125
Epoch 93: Val Loss 413149.06250
Epoch 94: Val Loss 409369.09375
Epoch 95: Val Loss 405567.84375
Epoch 96: Val Loss 401671.71875
Epoch 97: Val Loss 397697.28125
Epoch 98: Val Loss 393645.96875
Epoch 99: Val Loss 389537.50000
{'MSE - mean': 395922.9902425576, 'MSE - std': 9161.23767142365, 'R2 - mean': -45.87768103241393, 'R2 - std': 4.896740283484633} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520459.00000
Epoch 1: Val Loss 520432.18750
Epoch 2: Val Loss 520403.53125
Epoch 3: Val Loss 520373.06250
Epoch 4: Val Loss 520340.43750
Epoch 5: Val Loss 520305.09375
Epoch 6: Val Loss 520267.31250
Epoch 7: Val Loss 520225.65625
Epoch 8: Val Loss 520180.93750
Epoch 9: Val Loss 520131.37500
Epoch 10: Val Loss 520078.03125
Epoch 11: Val Loss 520019.00000
Epoch 12: Val Loss 519954.43750
Epoch 13: Val Loss 519883.50000
Epoch 14: Val Loss 519806.28125
Epoch 15: Val Loss 519722.06250
Epoch 16: Val Loss 519629.93750
Epoch 17: Val Loss 519529.34375
Epoch 18: Val Loss 519421.59375
Epoch 19: Val Loss 519305.12500
Epoch 20: Val Loss 519180.00000
Epoch 21: Val Loss 519045.18750
Epoch 22: Val Loss 518899.21875
Epoch 23: Val Loss 518741.75000
Epoch 24: Val Loss 518574.03125
Epoch 25: Val Loss 518392.03125
Epoch 26: Val Loss 518195.40625
Epoch 27: Val Loss 517985.75000
Epoch 28: Val Loss 517761.03125
Epoch 29: Val Loss 517519.62500
Epoch 30: Val Loss 517260.34375
Epoch 31: Val Loss 516981.93750
Epoch 32: Val Loss 516686.03125
Epoch 33: Val Loss 516367.46875
Epoch 34: Val Loss 516025.81250
Epoch 35: Val Loss 515658.40625
Epoch 36: Val Loss 515269.78125
Epoch 37: Val Loss 514850.78125
Epoch 38: Val Loss 514402.40625
Epoch 39: Val Loss 513925.56250
Epoch 40: Val Loss 513416.12500
Epoch 41: Val Loss 512878.03125
Epoch 42: Val Loss 512302.40625
Epoch 43: Val Loss 511683.65625
Epoch 44: Val Loss 511035.75000
Epoch 45: Val Loss 510341.12500
Epoch 46: Val Loss 509614.59375
Epoch 47: Val Loss 508842.37500
Epoch 48: Val Loss 508029.43750
Epoch 49: Val Loss 507161.75000
Epoch 50: Val Loss 506257.18750
Epoch 51: Val Loss 505295.59375
Epoch 52: Val Loss 504278.03125
Epoch 53: Val Loss 503221.28125
Epoch 54: Val Loss 502097.03125
Epoch 55: Val Loss 500922.90625
Epoch 56: Val Loss 499669.40625
Epoch 57: Val Loss 498374.96875
Epoch 58: Val Loss 497020.87500
Epoch 59: Val Loss 495608.18750
Epoch 60: Val Loss 494136.31250
Epoch 61: Val Loss 492591.96875
Epoch 62: Val Loss 490981.18750
Epoch 63: Val Loss 489310.56250
Epoch 64: Val Loss 487575.37500
Epoch 65: Val Loss 485779.21875
Epoch 66: Val Loss 483919.65625
Epoch 67: Val Loss 481971.28125
Epoch 68: Val Loss 479961.59375
Epoch 69: Val Loss 477899.78125
Epoch 70: Val Loss 475763.81250
Epoch 71: Val Loss 473540.06250
Epoch 72: Val Loss 471228.81250
Epoch 73: Val Loss 468864.43750
Epoch 74: Val Loss 466422.12500
Epoch 75: Val Loss 463896.09375
Epoch 76: Val Loss 461331.75000
Epoch 77: Val Loss 458667.40625
Epoch 78: Val Loss 455937.53125
Epoch 79: Val Loss 453137.31250
Epoch 80: Val Loss 450232.31250
Epoch 81: Val Loss 447276.40625
Epoch 82: Val Loss 444236.93750
Epoch 83: Val Loss 441126.75000
Epoch 84: Val Loss 437934.78125
Epoch 85: Val Loss 434669.15625
Epoch 86: Val Loss 431343.84375
Epoch 87: Val Loss 427962.96875
Epoch 88: Val Loss 424455.25000
Epoch 89: Val Loss 420905.53125
Epoch 90: Val Loss 417268.65625
Epoch 91: Val Loss 413571.15625
Epoch 92: Val Loss 409801.15625
Epoch 93: Val Loss 405932.65625
Epoch 94: Val Loss 402010.84375
Epoch 95: Val Loss 398053.03125
Epoch 96: Val Loss 394017.87500
Epoch 97: Val Loss 389856.53125
Epoch 98: Val Loss 385642.53125
Epoch 99: Val Loss 381426.09375
{'MSE - mean': 392298.7718763301, 'MSE - std': 10116.871208656175, 'R2 - mean': -45.639370626461194, 'R2 - std': 4.260742259475683} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517676.15625
Epoch 1: Val Loss 517638.09375
Epoch 2: Val Loss 517597.00000
Epoch 3: Val Loss 517552.46875
Epoch 4: Val Loss 517503.59375
Epoch 5: Val Loss 517450.53125
Epoch 6: Val Loss 517391.96875
Epoch 7: Val Loss 517328.34375
Epoch 8: Val Loss 517258.37500
Epoch 9: Val Loss 517180.84375
Epoch 10: Val Loss 517096.84375
Epoch 11: Val Loss 517004.18750
Epoch 12: Val Loss 516902.62500
Epoch 13: Val Loss 516791.25000
Epoch 14: Val Loss 516668.46875
Epoch 15: Val Loss 516535.25000
Epoch 16: Val Loss 516387.81250
Epoch 17: Val Loss 516227.53125
Epoch 18: Val Loss 516051.84375
Epoch 19: Val Loss 515858.71875
Epoch 20: Val Loss 515647.90625
Epoch 21: Val Loss 515416.62500
Epoch 22: Val Loss 515167.53125
Epoch 23: Val Loss 514896.75000
Epoch 24: Val Loss 514601.59375
Epoch 25: Val Loss 514284.78125
Epoch 26: Val Loss 513941.12500
Epoch 27: Val Loss 513571.34375
Epoch 28: Val Loss 513172.81250
Epoch 29: Val Loss 512743.56250
Epoch 30: Val Loss 512283.62500
Epoch 31: Val Loss 511785.68750
Epoch 32: Val Loss 511262.28125
Epoch 33: Val Loss 510704.03125
Epoch 34: Val Loss 510104.12500
Epoch 35: Val Loss 509467.43750
Epoch 36: Val Loss 508796.84375
Epoch 37: Val Loss 508085.00000
Epoch 38: Val Loss 507330.12500
Epoch 39: Val Loss 506533.81250
Epoch 40: Val Loss 505688.43750
Epoch 41: Val Loss 504799.59375
Epoch 42: Val Loss 503868.40625
Epoch 43: Val Loss 502879.96875
Epoch 44: Val Loss 501846.34375
Epoch 45: Val Loss 500753.53125
Epoch 46: Val Loss 499602.75000
Epoch 47: Val Loss 498390.21875
Epoch 48: Val Loss 497111.28125
Epoch 49: Val Loss 495785.15625
Epoch 50: Val Loss 494388.09375
Epoch 51: Val Loss 492934.50000
Epoch 52: Val Loss 491402.00000
Epoch 53: Val Loss 489801.34375
Epoch 54: Val Loss 488140.09375
Epoch 55: Val Loss 486407.87500
Epoch 56: Val Loss 484631.12500
Epoch 57: Val Loss 482737.50000
Epoch 58: Val Loss 480780.46875
Epoch 59: Val Loss 478769.09375
Epoch 60: Val Loss 476638.68750
Epoch 61: Val Loss 474480.81250
Epoch 62: Val Loss 472213.21875
Epoch 63: Val Loss 469858.06250
Epoch 64: Val Loss 467428.68750
Epoch 65: Val Loss 464912.37500
Epoch 66: Val Loss 462304.65625
Epoch 67: Val Loss 459631.43750
Epoch 68: Val Loss 456876.40625
Epoch 69: Val Loss 454041.56250
Epoch 70: Val Loss 451104.56250
Epoch 71: Val Loss 448097.09375
Epoch 72: Val Loss 444994.84375
Epoch 73: Val Loss 441834.37500
Epoch 74: Val Loss 438575.28125
Epoch 75: Val Loss 435211.43750
Epoch 76: Val Loss 431793.96875
Epoch 77: Val Loss 428294.53125
Epoch 78: Val Loss 424715.06250
Epoch 79: Val Loss 421061.81250
Epoch 80: Val Loss 417334.03125
Epoch 81: Val Loss 413504.71875
Epoch 82: Val Loss 409630.34375
Epoch 83: Val Loss 405648.06250
Epoch 84: Val Loss 401586.56250
Epoch 85: Val Loss 397456.84375
Epoch 86: Val Loss 393284.81250
Epoch 87: Val Loss 389025.53125
Epoch 88: Val Loss 384675.81250
Epoch 89: Val Loss 380271.81250
Epoch 90: Val Loss 375831.40625
Epoch 91: Val Loss 371286.65625
Epoch 92: Val Loss 366739.68750
Epoch 93: Val Loss 362099.31250
Epoch 94: Val Loss 357382.43750
Epoch 95: Val Loss 352620.59375
Epoch 96: Val Loss 347793.65625
Epoch 97: Val Loss 342900.40625
Epoch 98: Val Loss 338000.93750
Epoch 99: Val Loss 333031.53125
{'MSE - mean': 380445.3254590278, 'MSE - std': 25375.138114175017, 'R2 - mean': -44.66020490170514, 'R2 - std': 4.284647213889812} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 380445.3254590278 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.5}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516017.21875
Epoch 1: Val Loss 515468.65625
Epoch 2: Val Loss 514423.84375
Epoch 3: Val Loss 512496.84375
Epoch 4: Val Loss 508978.62500
Epoch 5: Val Loss 502863.75000
Epoch 6: Val Loss 492767.43750
Epoch 7: Val Loss 476888.56250
Epoch 8: Val Loss 453270.62500
Epoch 9: Val Loss 419833.75000
Epoch 10: Val Loss 375267.00000
Epoch 11: Val Loss 320092.37500
Epoch 12: Val Loss 256784.28125
Epoch 13: Val Loss 189989.64062
Epoch 14: Val Loss 127545.20312
Epoch 15: Val Loss 77186.62500
Epoch 16: Val Loss 44040.73047
Epoch 17: Val Loss 26272.46680
Epoch 18: Val Loss 18347.09375
Epoch 19: Val Loss 14599.02734
Epoch 20: Val Loss 12389.57227
Epoch 21: Val Loss 10842.57422
Epoch 22: Val Loss 9593.99023
Epoch 23: Val Loss 8657.65430
Epoch 24: Val Loss 7894.02197
Epoch 25: Val Loss 7287.81592
Epoch 26: Val Loss 6818.58740
Epoch 27: Val Loss 6436.39062
Epoch 28: Val Loss 6102.73486
Epoch 29: Val Loss 5840.11230
Epoch 30: Val Loss 5623.86133
Epoch 31: Val Loss 5447.76611
Epoch 32: Val Loss 5306.33496
Epoch 33: Val Loss 5175.84180
Epoch 34: Val Loss 5060.71045
Epoch 35: Val Loss 4954.79248
Epoch 36: Val Loss 4847.73096
Epoch 37: Val Loss 4796.51172
Epoch 38: Val Loss 4716.52832
Epoch 39: Val Loss 4641.95996
Epoch 40: Val Loss 4585.14014
Epoch 41: Val Loss 4515.74072
Epoch 42: Val Loss 4467.13623
Epoch 43: Val Loss 4412.67529
Epoch 44: Val Loss 4365.12061
Epoch 45: Val Loss 4321.15381
Epoch 46: Val Loss 4263.08008
Epoch 47: Val Loss 4231.27832
Epoch 48: Val Loss 4184.52148
Epoch 49: Val Loss 4153.49072
Epoch 50: Val Loss 4093.93066
Epoch 51: Val Loss 4077.60229
Epoch 52: Val Loss 4050.55981
Epoch 53: Val Loss 4011.65161
Epoch 54: Val Loss 3967.27075
Epoch 55: Val Loss 3937.79688
Epoch 56: Val Loss 3903.21509
Epoch 57: Val Loss 3869.59180
Epoch 58: Val Loss 3862.21387
Epoch 59: Val Loss 3815.39282
Epoch 60: Val Loss 3797.73804
Epoch 61: Val Loss 3761.80225
Epoch 62: Val Loss 3740.72852
Epoch 63: Val Loss 3695.66357
Epoch 64: Val Loss 3669.06323
Epoch 65: Val Loss 3650.08057
Epoch 66: Val Loss 3627.43896
Epoch 67: Val Loss 3591.68994
Epoch 68: Val Loss 3575.70435
Epoch 69: Val Loss 3546.57886
Epoch 70: Val Loss 3515.74438
Epoch 71: Val Loss 3506.46875
Epoch 72: Val Loss 3487.87109
Epoch 73: Val Loss 3461.32202
Epoch 74: Val Loss 3421.45093
Epoch 75: Val Loss 3409.29297
Epoch 76: Val Loss 3399.41284
Epoch 77: Val Loss 3353.11548
Epoch 78: Val Loss 3337.01782
Epoch 79: Val Loss 3316.83960
Epoch 80: Val Loss 3301.82910
Epoch 81: Val Loss 3269.03174
Epoch 82: Val Loss 3248.71313
Epoch 83: Val Loss 3228.62402
Epoch 84: Val Loss 3229.17603
Epoch 85: Val Loss 3204.41748
Epoch 86: Val Loss 3158.44238
Epoch 87: Val Loss 3165.27246
Epoch 88: Val Loss 3131.71606
Epoch 89: Val Loss 3117.61499
Epoch 90: Val Loss 3082.05444
Epoch 91: Val Loss 3069.12109
Epoch 92: Val Loss 3051.39331
Epoch 93: Val Loss 3022.46240
Epoch 94: Val Loss 3025.22681
Epoch 95: Val Loss 2982.83105
Epoch 96: Val Loss 2972.95728
Epoch 97: Val Loss 2951.69385
Epoch 98: Val Loss 2931.66577
Epoch 99: Val Loss 2919.03589
{'MSE - mean': 2919.036135381868, 'MSE - std': 0.0, 'R2 - mean': 0.6606008768381959, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524676.87500
Epoch 1: Val Loss 524247.71875
Epoch 2: Val Loss 523388.59375
Epoch 3: Val Loss 521703.06250
Epoch 4: Val Loss 518541.00000
Epoch 5: Val Loss 512784.15625
Epoch 6: Val Loss 502635.53125
Epoch 7: Val Loss 485725.40625
Epoch 8: Val Loss 459832.34375
Epoch 9: Val Loss 422827.53125
Epoch 10: Val Loss 373206.43750
Epoch 11: Val Loss 311647.84375
Epoch 12: Val Loss 242390.51562
Epoch 13: Val Loss 171875.53125
Epoch 14: Val Loss 109988.75000
Epoch 15: Val Loss 65011.35547
Epoch 16: Val Loss 39279.05469
Epoch 17: Val Loss 27568.35156
Epoch 18: Val Loss 22256.52734
Epoch 19: Val Loss 19033.36914
Epoch 20: Val Loss 16539.28906
Epoch 21: Val Loss 14614.83887
Epoch 22: Val Loss 13130.43066
Epoch 23: Val Loss 11865.94922
Epoch 24: Val Loss 10831.04785
Epoch 25: Val Loss 9960.22559
Epoch 26: Val Loss 9230.45410
Epoch 27: Val Loss 8612.53906
Epoch 28: Val Loss 8086.76660
Epoch 29: Val Loss 7637.40527
Epoch 30: Val Loss 7234.48633
Epoch 31: Val Loss 6885.37451
Epoch 32: Val Loss 6587.88281
Epoch 33: Val Loss 6323.68213
Epoch 34: Val Loss 6085.24609
Epoch 35: Val Loss 5873.33545
Epoch 36: Val Loss 5670.30420
Epoch 37: Val Loss 5486.93945
Epoch 38: Val Loss 5318.68115
Epoch 39: Val Loss 5147.61182
Epoch 40: Val Loss 5008.62451
Epoch 41: Val Loss 4866.87305
Epoch 42: Val Loss 4742.13818
Epoch 43: Val Loss 4627.11035
Epoch 44: Val Loss 4512.07275
Epoch 45: Val Loss 4405.14697
Epoch 46: Val Loss 4294.22656
Epoch 47: Val Loss 4197.10498
Epoch 48: Val Loss 4107.90039
Epoch 49: Val Loss 4012.90210
Epoch 50: Val Loss 3935.27393
Epoch 51: Val Loss 3855.85229
Epoch 52: Val Loss 3787.51807
Epoch 53: Val Loss 3707.73584
Epoch 54: Val Loss 3645.17041
Epoch 55: Val Loss 3586.31079
Epoch 56: Val Loss 3517.98340
Epoch 57: Val Loss 3470.41919
Epoch 58: Val Loss 3423.83154
Epoch 59: Val Loss 3365.33398
Epoch 60: Val Loss 3320.65552
Epoch 61: Val Loss 3267.11084
Epoch 62: Val Loss 3234.36890
Epoch 63: Val Loss 3193.21338
Epoch 64: Val Loss 3157.77637
Epoch 65: Val Loss 3124.69360
Epoch 66: Val Loss 3086.62817
Epoch 67: Val Loss 3058.37109
Epoch 68: Val Loss 3019.57471
Epoch 69: Val Loss 2995.65088
Epoch 70: Val Loss 2968.41455
Epoch 71: Val Loss 2935.63672
Epoch 72: Val Loss 2914.99731
Epoch 73: Val Loss 2888.03760
Epoch 74: Val Loss 2870.21606
Epoch 75: Val Loss 2840.99194
Epoch 76: Val Loss 2825.82495
Epoch 77: Val Loss 2794.13867
Epoch 78: Val Loss 2782.10913
Epoch 79: Val Loss 2750.98145
Epoch 80: Val Loss 2741.26416
Epoch 81: Val Loss 2721.91650
Epoch 82: Val Loss 2685.88965
Epoch 83: Val Loss 2679.00049
Epoch 84: Val Loss 2659.19189
Epoch 85: Val Loss 2621.60791
Epoch 86: Val Loss 2632.82788
Epoch 87: Val Loss 2597.48413
Epoch 88: Val Loss 2575.76733
Epoch 89: Val Loss 2571.32617
Epoch 90: Val Loss 2545.89624
Epoch 91: Val Loss 2525.64258
Epoch 92: Val Loss 2524.82739
Epoch 93: Val Loss 2503.84814
Epoch 94: Val Loss 2480.47583
Epoch 95: Val Loss 2464.82446
Epoch 96: Val Loss 2445.32593
Epoch 97: Val Loss 2431.21167
Epoch 98: Val Loss 2413.46313
Epoch 99: Val Loss 2415.87354
{'MSE - mean': 2666.2496839364603, 'MSE - std': 252.78645144540792, 'R2 - mean': 0.6723611018938485, 'R2 - std': 0.011760225055652607} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517595.09375
Epoch 1: Val Loss 516887.34375
Epoch 2: Val Loss 515646.62500
Epoch 3: Val Loss 513397.87500
Epoch 4: Val Loss 509347.15625
Epoch 5: Val Loss 502327.59375
Epoch 6: Val Loss 490668.21875
Epoch 7: Val Loss 472280.31250
Epoch 8: Val Loss 444936.21875
Epoch 9: Val Loss 406503.00000
Epoch 10: Val Loss 356453.68750
Epoch 11: Val Loss 295891.53125
Epoch 12: Val Loss 228929.09375
Epoch 13: Val Loss 161982.92188
Epoch 14: Val Loss 103365.98438
Epoch 15: Val Loss 60512.79688
Epoch 16: Val Loss 35076.73828
Epoch 17: Val Loss 22775.51953
Epoch 18: Val Loss 17185.33203
Epoch 19: Val Loss 14030.56738
Epoch 20: Val Loss 11922.45020
Epoch 21: Val Loss 10345.40332
Epoch 22: Val Loss 9110.05762
Epoch 23: Val Loss 8094.19629
Epoch 24: Val Loss 7302.37598
Epoch 25: Val Loss 6627.88574
Epoch 26: Val Loss 6111.13281
Epoch 27: Val Loss 5663.69482
Epoch 28: Val Loss 5290.94385
Epoch 29: Val Loss 5035.66797
Epoch 30: Val Loss 4794.35449
Epoch 31: Val Loss 4612.45020
Epoch 32: Val Loss 4458.91748
Epoch 33: Val Loss 4325.93457
Epoch 34: Val Loss 4224.23291
Epoch 35: Val Loss 4165.59912
Epoch 36: Val Loss 4087.80420
Epoch 37: Val Loss 4023.78735
Epoch 38: Val Loss 3955.27637
Epoch 39: Val Loss 3929.96216
Epoch 40: Val Loss 3860.51343
Epoch 41: Val Loss 3820.88696
Epoch 42: Val Loss 3767.13501
Epoch 43: Val Loss 3736.51709
Epoch 44: Val Loss 3695.76099
Epoch 45: Val Loss 3667.40552
Epoch 46: Val Loss 3640.51392
Epoch 47: Val Loss 3589.92676
Epoch 48: Val Loss 3545.04883
Epoch 49: Val Loss 3555.38867
Epoch 50: Val Loss 3491.74951
Epoch 51: Val Loss 3457.04150
Epoch 52: Val Loss 3436.51050
Epoch 53: Val Loss 3378.21631
Epoch 54: Val Loss 3365.59155
Epoch 55: Val Loss 3345.83984
Epoch 56: Val Loss 3309.09131
Epoch 57: Val Loss 3287.63696
Epoch 58: Val Loss 3255.36743
Epoch 59: Val Loss 3210.35449
Epoch 60: Val Loss 3206.91797
Epoch 61: Val Loss 3164.33301
Epoch 62: Val Loss 3140.64697
Epoch 63: Val Loss 3100.21777
Epoch 64: Val Loss 3099.38477
Epoch 65: Val Loss 3068.48071
Epoch 66: Val Loss 3053.89722
Epoch 67: Val Loss 3022.86670
Epoch 68: Val Loss 3002.41089
Epoch 69: Val Loss 2968.25513
Epoch 70: Val Loss 2961.46265
Epoch 71: Val Loss 2915.07690
Epoch 72: Val Loss 2903.95142
Epoch 73: Val Loss 2899.05518
Epoch 74: Val Loss 2850.72119
Epoch 75: Val Loss 2847.03662
Epoch 76: Val Loss 2820.56030
Epoch 77: Val Loss 2811.79297
Epoch 78: Val Loss 2793.09180
Epoch 79: Val Loss 2761.37744
Epoch 80: Val Loss 2750.41968
Epoch 81: Val Loss 2716.55176
Epoch 82: Val Loss 2703.23218
Epoch 83: Val Loss 2686.36938
Epoch 84: Val Loss 2688.88135
Epoch 85: Val Loss 2645.18628
Epoch 86: Val Loss 2641.37476
Epoch 87: Val Loss 2602.38721
Epoch 88: Val Loss 2612.48340
Epoch 89: Val Loss 2569.05029
Epoch 90: Val Loss 2581.80029
Epoch 91: Val Loss 2543.05640
Epoch 92: Val Loss 2533.83887
Epoch 93: Val Loss 2494.55225
Epoch 94: Val Loss 2512.71240
Epoch 95: Val Loss 2485.10791
Epoch 96: Val Loss 2475.30029
Epoch 97: Val Loss 2459.07031
Epoch 98: Val Loss 2434.44678
Epoch 99: Val Loss 2434.01880
{'MSE - mean': 2588.8393388101167, 'MSE - std': 233.6351495132249, 'R2 - mean': 0.6944122992533059, 'R2 - std': 0.03262993317305274} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520901.37500
Epoch 1: Val Loss 520577.71875
Epoch 2: Val Loss 519952.75000
Epoch 3: Val Loss 518659.46875
Epoch 4: Val Loss 515977.53125
Epoch 5: Val Loss 510772.93750
Epoch 6: Val Loss 501608.40625
Epoch 7: Val Loss 486490.84375
Epoch 8: Val Loss 462871.62500
Epoch 9: Val Loss 428300.06250
Epoch 10: Val Loss 380925.18750
Epoch 11: Val Loss 320694.62500
Epoch 12: Val Loss 250333.48438
Epoch 13: Val Loss 177035.14062
Epoch 14: Val Loss 110408.33594
Epoch 15: Val Loss 60537.29688
Epoch 16: Val Loss 31481.55664
Epoch 17: Val Loss 18877.21289
Epoch 18: Val Loss 14558.89648
Epoch 19: Val Loss 12302.38965
Epoch 20: Val Loss 10546.63574
Epoch 21: Val Loss 9165.07031
Epoch 22: Val Loss 8140.24365
Epoch 23: Val Loss 7376.65771
Epoch 24: Val Loss 6717.34814
Epoch 25: Val Loss 6247.14404
Epoch 26: Val Loss 5867.25146
Epoch 27: Val Loss 5585.65137
Epoch 28: Val Loss 5343.70752
Epoch 29: Val Loss 5142.58252
Epoch 30: Val Loss 4992.20264
Epoch 31: Val Loss 4856.87354
Epoch 32: Val Loss 4723.91943
Epoch 33: Val Loss 4611.41113
Epoch 34: Val Loss 4511.98145
Epoch 35: Val Loss 4432.64697
Epoch 36: Val Loss 4348.60059
Epoch 37: Val Loss 4264.09180
Epoch 38: Val Loss 4194.77197
Epoch 39: Val Loss 4133.18994
Epoch 40: Val Loss 4070.70093
Epoch 41: Val Loss 3997.79541
Epoch 42: Val Loss 3940.07568
Epoch 43: Val Loss 3878.14136
Epoch 44: Val Loss 3837.58911
Epoch 45: Val Loss 3782.38550
Epoch 46: Val Loss 3733.77197
Epoch 47: Val Loss 3687.98218
Epoch 48: Val Loss 3646.84424
Epoch 49: Val Loss 3602.96777
Epoch 50: Val Loss 3556.96973
Epoch 51: Val Loss 3524.03491
Epoch 52: Val Loss 3489.45557
Epoch 53: Val Loss 3448.75415
Epoch 54: Val Loss 3412.76465
Epoch 55: Val Loss 3375.34399
Epoch 56: Val Loss 3346.19849
Epoch 57: Val Loss 3306.22803
Epoch 58: Val Loss 3277.98608
Epoch 59: Val Loss 3247.74829
Epoch 60: Val Loss 3218.68140
Epoch 61: Val Loss 3194.43848
Epoch 62: Val Loss 3155.93140
Epoch 63: Val Loss 3130.11865
Epoch 64: Val Loss 3101.19165
Epoch 65: Val Loss 3076.03833
Epoch 66: Val Loss 3059.47998
Epoch 67: Val Loss 3019.09033
Epoch 68: Val Loss 3001.83350
Epoch 69: Val Loss 2970.86279
Epoch 70: Val Loss 2951.79004
Epoch 71: Val Loss 2920.63037
Epoch 72: Val Loss 2895.36084
Epoch 73: Val Loss 2878.00513
Epoch 74: Val Loss 2847.88647
Epoch 75: Val Loss 2828.69873
Epoch 76: Val Loss 2818.30444
Epoch 77: Val Loss 2786.04712
Epoch 78: Val Loss 2768.37134
Epoch 79: Val Loss 2738.36426
Epoch 80: Val Loss 2721.60229
Epoch 81: Val Loss 2704.15112
Epoch 82: Val Loss 2678.49219
Epoch 83: Val Loss 2659.90747
Epoch 84: Val Loss 2633.63818
Epoch 85: Val Loss 2616.30884
Epoch 86: Val Loss 2596.53296
Epoch 87: Val Loss 2573.45361
Epoch 88: Val Loss 2558.55688
Epoch 89: Val Loss 2543.75146
Epoch 90: Val Loss 2520.70142
Epoch 91: Val Loss 2497.60010
Epoch 92: Val Loss 2474.39844
Epoch 93: Val Loss 2467.28369
Epoch 94: Val Loss 2439.75781
Epoch 95: Val Loss 2436.12183
Epoch 96: Val Loss 2406.74438
Epoch 97: Val Loss 2388.21191
Epoch 98: Val Loss 2377.39771
Epoch 99: Val Loss 2357.84741
{'MSE - mean': 2531.0914298037933, 'MSE - std': 225.70666870931836, 'R2 - mean': 0.6998368727850592, 'R2 - std': 0.0297793955305109} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517594.84375
Epoch 1: Val Loss 517094.81250
Epoch 2: Val Loss 516206.50000
Epoch 3: Val Loss 514500.75000
Epoch 4: Val Loss 511173.34375
Epoch 5: Val Loss 505291.15625
Epoch 6: Val Loss 495495.53125
Epoch 7: Val Loss 479889.21875
Epoch 8: Val Loss 456456.31250
Epoch 9: Val Loss 422903.12500
Epoch 10: Val Loss 378063.71875
Epoch 11: Val Loss 322957.09375
Epoch 12: Val Loss 259534.14062
Epoch 13: Val Loss 194004.60938
Epoch 14: Val Loss 133269.76562
Epoch 15: Val Loss 84549.92188
Epoch 16: Val Loss 51763.39062
Epoch 17: Val Loss 33466.81641
Epoch 18: Val Loss 24343.79102
Epoch 19: Val Loss 19724.10742
Epoch 20: Val Loss 16703.93750
Epoch 21: Val Loss 14538.83691
Epoch 22: Val Loss 12853.82715
Epoch 23: Val Loss 11534.05664
Epoch 24: Val Loss 10417.76172
Epoch 25: Val Loss 9455.46875
Epoch 26: Val Loss 8691.18066
Epoch 27: Val Loss 8019.47656
Epoch 28: Val Loss 7448.45508
Epoch 29: Val Loss 6965.00049
Epoch 30: Val Loss 6541.65723
Epoch 31: Val Loss 6169.67383
Epoch 32: Val Loss 5869.72070
Epoch 33: Val Loss 5593.34375
Epoch 34: Val Loss 5355.15137
Epoch 35: Val Loss 5146.16162
Epoch 36: Val Loss 4957.28613
Epoch 37: Val Loss 4799.03760
Epoch 38: Val Loss 4641.51611
Epoch 39: Val Loss 4480.87793
Epoch 40: Val Loss 4364.64209
Epoch 41: Val Loss 4247.14355
Epoch 42: Val Loss 4148.91699
Epoch 43: Val Loss 4044.39697
Epoch 44: Val Loss 3982.85645
Epoch 45: Val Loss 3870.24951
Epoch 46: Val Loss 3774.16357
Epoch 47: Val Loss 3711.95972
Epoch 48: Val Loss 3657.16748
Epoch 49: Val Loss 3604.56177
Epoch 50: Val Loss 3549.79370
Epoch 51: Val Loss 3492.46436
Epoch 52: Val Loss 3422.81006
Epoch 53: Val Loss 3384.63159
Epoch 54: Val Loss 3334.13037
Epoch 55: Val Loss 3289.28027
Epoch 56: Val Loss 3244.32715
Epoch 57: Val Loss 3212.44873
Epoch 58: Val Loss 3176.45117
Epoch 59: Val Loss 3143.42017
Epoch 60: Val Loss 3105.88208
Epoch 61: Val Loss 3052.47339
Epoch 62: Val Loss 3013.79858
Epoch 63: Val Loss 2994.63110
Epoch 64: Val Loss 2957.04102
Epoch 65: Val Loss 2925.40796
Epoch 66: Val Loss 2913.71777
Epoch 67: Val Loss 2873.97803
Epoch 68: Val Loss 2841.06396
Epoch 69: Val Loss 2810.71631
Epoch 70: Val Loss 2785.13306
Epoch 71: Val Loss 2748.70679
Epoch 72: Val Loss 2733.59375
Epoch 73: Val Loss 2694.15601
Epoch 74: Val Loss 2671.54712
Epoch 75: Val Loss 2644.78198
Epoch 76: Val Loss 2629.36328
Epoch 77: Val Loss 2596.99561
Epoch 78: Val Loss 2568.74463
Epoch 79: Val Loss 2558.98535
Epoch 80: Val Loss 2523.84985
Epoch 81: Val Loss 2509.04199
Epoch 82: Val Loss 2495.72900
Epoch 83: Val Loss 2472.58228
Epoch 84: Val Loss 2462.45337
Epoch 85: Val Loss 2413.17480
Epoch 86: Val Loss 2394.37720
Epoch 87: Val Loss 2373.80493
Epoch 88: Val Loss 2360.52637
Epoch 89: Val Loss 2350.31689
Epoch 90: Val Loss 2320.33521
Epoch 91: Val Loss 2309.47095
Epoch 92: Val Loss 2285.42554
Epoch 93: Val Loss 2257.12305
Epoch 94: Val Loss 2259.30444
Epoch 95: Val Loss 2225.87427
Epoch 96: Val Loss 2198.87549
Epoch 97: Val Loss 2199.92700
Epoch 98: Val Loss 2162.32202
Epoch 99: Val Loss 2166.92969
{'MSE - mean': 2457.3376133944025, 'MSE - std': 250.02660254398324, 'R2 - mean': 0.7056626241260391, 'R2 - std': 0.02907245144940146} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 2457.3376133944025 and parameters: {'dim': 128, 'depth': 6, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516797.09375
Epoch 1: Val Loss 516766.34375
Epoch 2: Val Loss 516734.00000
Epoch 3: Val Loss 516699.87500
Epoch 4: Val Loss 516663.78125
Epoch 5: Val Loss 516625.12500
Epoch 6: Val Loss 516584.31250
Epoch 7: Val Loss 516540.71875
Epoch 8: Val Loss 516494.59375
Epoch 9: Val Loss 516445.09375
Epoch 10: Val Loss 516392.56250
Epoch 11: Val Loss 516336.00000
Epoch 12: Val Loss 516275.56250
Epoch 13: Val Loss 516210.40625
Epoch 14: Val Loss 516139.90625
Epoch 15: Val Loss 516063.81250
Epoch 16: Val Loss 515981.43750
Epoch 17: Val Loss 515891.75000
Epoch 18: Val Loss 515793.68750
Epoch 19: Val Loss 515686.62500
Epoch 20: Val Loss 515569.90625
Epoch 21: Val Loss 515442.15625
Epoch 22: Val Loss 515301.81250
Epoch 23: Val Loss 515150.28125
Epoch 24: Val Loss 514984.31250
Epoch 25: Val Loss 514806.15625
Epoch 26: Val Loss 514614.81250
Epoch 27: Val Loss 514407.81250
Epoch 28: Val Loss 514184.90625
Epoch 29: Val Loss 513948.09375
Epoch 30: Val Loss 513690.18750
Epoch 31: Val Loss 513415.53125
Epoch 32: Val Loss 513119.31250
Epoch 33: Val Loss 512801.96875
Epoch 34: Val Loss 512465.43750
Epoch 35: Val Loss 512102.09375
Epoch 36: Val Loss 511714.46875
Epoch 37: Val Loss 511305.93750
Epoch 38: Val Loss 510867.56250
Epoch 39: Val Loss 510397.96875
Epoch 40: Val Loss 509899.68750
Epoch 41: Val Loss 509368.37500
Epoch 42: Val Loss 508801.75000
Epoch 43: Val Loss 508198.68750
Epoch 44: Val Loss 507563.18750
Epoch 45: Val Loss 506873.15625
Epoch 46: Val Loss 506158.71875
Epoch 47: Val Loss 505391.81250
Epoch 48: Val Loss 504583.59375
Epoch 49: Val Loss 503720.68750
Epoch 50: Val Loss 502817.28125
Epoch 51: Val Loss 501860.68750
Epoch 52: Val Loss 500846.96875
Epoch 53: Val Loss 499776.34375
Epoch 54: Val Loss 498657.90625
Epoch 55: Val Loss 497487.53125
Epoch 56: Val Loss 496259.03125
Epoch 57: Val Loss 494963.71875
Epoch 58: Val Loss 493597.56250
Epoch 59: Val Loss 492170.53125
Epoch 60: Val Loss 490683.34375
Epoch 61: Val Loss 489138.21875
Epoch 62: Val Loss 487519.75000
Epoch 63: Val Loss 485843.28125
Epoch 64: Val Loss 484095.31250
Epoch 65: Val Loss 482290.93750
Epoch 66: Val Loss 480397.62500
Epoch 67: Val Loss 478459.03125
Epoch 68: Val Loss 476442.84375
Epoch 69: Val Loss 474353.93750
Epoch 70: Val Loss 472204.62500
Epoch 71: Val Loss 469994.96875
Epoch 72: Val Loss 467702.75000
Epoch 73: Val Loss 465326.31250
Epoch 74: Val Loss 462905.59375
Epoch 75: Val Loss 460405.59375
Epoch 76: Val Loss 457810.12500
Epoch 77: Val Loss 455148.78125
Epoch 78: Val Loss 452425.93750
Epoch 79: Val Loss 449626.50000
Epoch 80: Val Loss 446745.06250
Epoch 81: Val Loss 443801.40625
Epoch 82: Val Loss 440797.75000
Epoch 83: Val Loss 437704.96875
Epoch 84: Val Loss 434555.50000
Epoch 85: Val Loss 431287.96875
Epoch 86: Val Loss 427997.81250
Epoch 87: Val Loss 424618.78125
Epoch 88: Val Loss 421163.81250
Epoch 89: Val Loss 417661.37500
Epoch 90: Val Loss 414075.59375
Epoch 91: Val Loss 410431.68750
Epoch 92: Val Loss 406706.12500
Epoch 93: Val Loss 402924.90625
Epoch 94: Val Loss 399056.40625
Epoch 95: Val Loss 395138.43750
Epoch 96: Val Loss 391142.56250
Epoch 97: Val Loss 387095.46875
Epoch 98: Val Loss 382987.25000
Epoch 99: Val Loss 378841.09375
{'MSE - mean': 378841.0377282031, 'MSE - std': 0.0, 'R2 - mean': -43.04820977176409, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525015.50000
Epoch 1: Val Loss 524985.93750
Epoch 2: Val Loss 524955.81250
Epoch 3: Val Loss 524924.81250
Epoch 4: Val Loss 524892.31250
Epoch 5: Val Loss 524858.00000
Epoch 6: Val Loss 524821.43750
Epoch 7: Val Loss 524781.81250
Epoch 8: Val Loss 524738.31250
Epoch 9: Val Loss 524689.81250
Epoch 10: Val Loss 524636.00000
Epoch 11: Val Loss 524576.62500
Epoch 12: Val Loss 524511.25000
Epoch 13: Val Loss 524441.75000
Epoch 14: Val Loss 524366.43750
Epoch 15: Val Loss 524285.50000
Epoch 16: Val Loss 524198.78125
Epoch 17: Val Loss 524103.78125
Epoch 18: Val Loss 524000.93750
Epoch 19: Val Loss 523889.18750
Epoch 20: Val Loss 523766.81250
Epoch 21: Val Loss 523635.12500
Epoch 22: Val Loss 523491.75000
Epoch 23: Val Loss 523338.06250
Epoch 24: Val Loss 523171.31250
Epoch 25: Val Loss 522990.90625
Epoch 26: Val Loss 522794.18750
Epoch 27: Val Loss 522581.09375
Epoch 28: Val Loss 522350.90625
Epoch 29: Val Loss 522099.65625
Epoch 30: Val Loss 521827.25000
Epoch 31: Val Loss 521534.43750
Epoch 32: Val Loss 521215.25000
Epoch 33: Val Loss 520878.56250
Epoch 34: Val Loss 520512.56250
Epoch 35: Val Loss 520116.40625
Epoch 36: Val Loss 519690.90625
Epoch 37: Val Loss 519229.43750
Epoch 38: Val Loss 518741.84375
Epoch 39: Val Loss 518218.37500
Epoch 40: Val Loss 517653.78125
Epoch 41: Val Loss 517056.81250
Epoch 42: Val Loss 516424.06250
Epoch 43: Val Loss 515750.00000
Epoch 44: Val Loss 515030.96875
Epoch 45: Val Loss 514269.53125
Epoch 46: Val Loss 513468.71875
Epoch 47: Val Loss 512623.37500
Epoch 48: Val Loss 511729.96875
Epoch 49: Val Loss 510786.21875
Epoch 50: Val Loss 509797.50000
Epoch 51: Val Loss 508758.62500
Epoch 52: Val Loss 507678.34375
Epoch 53: Val Loss 506540.18750
Epoch 54: Val Loss 505342.28125
Epoch 55: Val Loss 504088.93750
Epoch 56: Val Loss 502777.25000
Epoch 57: Val Loss 501423.03125
Epoch 58: Val Loss 499992.00000
Epoch 59: Val Loss 498513.56250
Epoch 60: Val Loss 496957.21875
Epoch 61: Val Loss 495361.93750
Epoch 62: Val Loss 493694.18750
Epoch 63: Val Loss 491951.50000
Epoch 64: Val Loss 490150.68750
Epoch 65: Val Loss 488264.93750
Epoch 66: Val Loss 486359.81250
Epoch 67: Val Loss 484379.06250
Epoch 68: Val Loss 482306.53125
Epoch 69: Val Loss 480182.81250
Epoch 70: Val Loss 477976.68750
Epoch 71: Val Loss 475742.59375
Epoch 72: Val Loss 473395.56250
Epoch 73: Val Loss 470987.12500
Epoch 74: Val Loss 468534.56250
Epoch 75: Val Loss 465963.90625
Epoch 76: Val Loss 463375.37500
Epoch 77: Val Loss 460726.71875
Epoch 78: Val Loss 457983.56250
Epoch 79: Val Loss 455148.78125
Epoch 80: Val Loss 452234.46875
Epoch 81: Val Loss 449312.71875
Epoch 82: Val Loss 446338.46875
Epoch 83: Val Loss 443253.78125
Epoch 84: Val Loss 440106.75000
Epoch 85: Val Loss 436851.40625
Epoch 86: Val Loss 433543.90625
Epoch 87: Val Loss 430215.40625
Epoch 88: Val Loss 426806.53125
Epoch 89: Val Loss 423333.00000
Epoch 90: Val Loss 419744.90625
Epoch 91: Val Loss 416090.31250
Epoch 92: Val Loss 412406.25000
Epoch 93: Val Loss 408653.34375
Epoch 94: Val Loss 404832.40625
Epoch 95: Val Loss 400974.65625
Epoch 96: Val Loss 397018.93750
Epoch 97: Val Loss 393004.90625
Epoch 98: Val Loss 388908.96875
Epoch 99: Val Loss 384813.84375
{'MSE - mean': 381827.4331213505, 'MSE - std': 2986.395393147337, 'R2 - mean': -46.20668895971927, 'R2 - std': 3.158479187955184} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517941.53125
Epoch 1: Val Loss 517903.53125
Epoch 2: Val Loss 517865.53125
Epoch 3: Val Loss 517827.15625
Epoch 4: Val Loss 517787.90625
Epoch 5: Val Loss 517747.93750
Epoch 6: Val Loss 517706.75000
Epoch 7: Val Loss 517664.43750
Epoch 8: Val Loss 517620.15625
Epoch 9: Val Loss 517574.15625
Epoch 10: Val Loss 517525.84375
Epoch 11: Val Loss 517474.65625
Epoch 12: Val Loss 517419.75000
Epoch 13: Val Loss 517360.62500
Epoch 14: Val Loss 517296.96875
Epoch 15: Val Loss 517228.84375
Epoch 16: Val Loss 517154.06250
Epoch 17: Val Loss 517072.25000
Epoch 18: Val Loss 516982.81250
Epoch 19: Val Loss 516886.15625
Epoch 20: Val Loss 516781.12500
Epoch 21: Val Loss 516667.75000
Epoch 22: Val Loss 516543.50000
Epoch 23: Val Loss 516409.90625
Epoch 24: Val Loss 516265.78125
Epoch 25: Val Loss 516109.25000
Epoch 26: Val Loss 515940.00000
Epoch 27: Val Loss 515756.78125
Epoch 28: Val Loss 515558.81250
Epoch 29: Val Loss 515344.68750
Epoch 30: Val Loss 515111.40625
Epoch 31: Val Loss 514862.03125
Epoch 32: Val Loss 514594.56250
Epoch 33: Val Loss 514303.68750
Epoch 34: Val Loss 513992.81250
Epoch 35: Val Loss 513651.03125
Epoch 36: Val Loss 513287.43750
Epoch 37: Val Loss 512893.84375
Epoch 38: Val Loss 512479.09375
Epoch 39: Val Loss 512027.65625
Epoch 40: Val Loss 511555.75000
Epoch 41: Val Loss 511045.28125
Epoch 42: Val Loss 510500.46875
Epoch 43: Val Loss 509921.81250
Epoch 44: Val Loss 509301.09375
Epoch 45: Val Loss 508650.37500
Epoch 46: Val Loss 507960.90625
Epoch 47: Val Loss 507220.53125
Epoch 48: Val Loss 506446.87500
Epoch 49: Val Loss 505628.56250
Epoch 50: Val Loss 504770.09375
Epoch 51: Val Loss 503857.78125
Epoch 52: Val Loss 502908.09375
Epoch 53: Val Loss 501889.81250
Epoch 54: Val Loss 500823.15625
Epoch 55: Val Loss 499686.15625
Epoch 56: Val Loss 498491.87500
Epoch 57: Val Loss 497245.78125
Epoch 58: Val Loss 495922.31250
Epoch 59: Val Loss 494529.90625
Epoch 60: Val Loss 493076.78125
Epoch 61: Val Loss 491544.12500
Epoch 62: Val Loss 489929.40625
Epoch 63: Val Loss 488254.75000
Epoch 64: Val Loss 486488.90625
Epoch 65: Val Loss 484648.21875
Epoch 66: Val Loss 482724.15625
Epoch 67: Val Loss 480718.81250
Epoch 68: Val Loss 478633.93750
Epoch 69: Val Loss 476439.59375
Epoch 70: Val Loss 474190.68750
Epoch 71: Val Loss 471834.18750
Epoch 72: Val Loss 469382.93750
Epoch 73: Val Loss 466832.96875
Epoch 74: Val Loss 464182.75000
Epoch 75: Val Loss 461429.75000
Epoch 76: Val Loss 458607.46875
Epoch 77: Val Loss 455699.50000
Epoch 78: Val Loss 452680.93750
Epoch 79: Val Loss 449587.18750
Epoch 80: Val Loss 446411.31250
Epoch 81: Val Loss 443124.40625
Epoch 82: Val Loss 439751.09375
Epoch 83: Val Loss 436287.06250
Epoch 84: Val Loss 432732.15625
Epoch 85: Val Loss 429103.59375
Epoch 86: Val Loss 425362.65625
Epoch 87: Val Loss 421547.56250
Epoch 88: Val Loss 417668.03125
Epoch 89: Val Loss 413692.31250
Epoch 90: Val Loss 409580.59375
Epoch 91: Val Loss 405477.71875
Epoch 92: Val Loss 401270.06250
Epoch 93: Val Loss 396941.84375
Epoch 94: Val Loss 392587.56250
Epoch 95: Val Loss 388115.71875
Epoch 96: Val Loss 383578.40625
Epoch 97: Val Loss 378984.28125
Epoch 98: Val Loss 374308.81250
Epoch 99: Val Loss 369546.62500
{'MSE - mean': 377733.8332555232, 'MSE - std': 6281.785151197882, 'R2 - mean': -43.70452424857225, 'R2 - std': 4.378620445290536} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520672.90625
Epoch 1: Val Loss 520636.40625
Epoch 2: Val Loss 520599.62500
Epoch 3: Val Loss 520562.00000
Epoch 4: Val Loss 520523.46875
Epoch 5: Val Loss 520483.21875
Epoch 6: Val Loss 520441.18750
Epoch 7: Val Loss 520396.40625
Epoch 8: Val Loss 520348.96875
Epoch 9: Val Loss 520297.78125
Epoch 10: Val Loss 520242.96875
Epoch 11: Val Loss 520183.12500
Epoch 12: Val Loss 520118.09375
Epoch 13: Val Loss 520046.68750
Epoch 14: Val Loss 519968.00000
Epoch 15: Val Loss 519880.50000
Epoch 16: Val Loss 519784.03125
Epoch 17: Val Loss 519675.75000
Epoch 18: Val Loss 519554.96875
Epoch 19: Val Loss 519419.87500
Epoch 20: Val Loss 519270.50000
Epoch 21: Val Loss 519104.46875
Epoch 22: Val Loss 518922.96875
Epoch 23: Val Loss 518720.78125
Epoch 24: Val Loss 518499.03125
Epoch 25: Val Loss 518254.81250
Epoch 26: Val Loss 517985.84375
Epoch 27: Val Loss 517693.84375
Epoch 28: Val Loss 517373.03125
Epoch 29: Val Loss 517026.40625
Epoch 30: Val Loss 516655.21875
Epoch 31: Val Loss 516250.68750
Epoch 32: Val Loss 515817.75000
Epoch 33: Val Loss 515348.50000
Epoch 34: Val Loss 514842.43750
Epoch 35: Val Loss 514297.28125
Epoch 36: Val Loss 513718.81250
Epoch 37: Val Loss 513098.96875
Epoch 38: Val Loss 512433.84375
Epoch 39: Val Loss 511725.31250
Epoch 40: Val Loss 510975.46875
Epoch 41: Val Loss 510174.40625
Epoch 42: Val Loss 509322.06250
Epoch 43: Val Loss 508424.75000
Epoch 44: Val Loss 507477.96875
Epoch 45: Val Loss 506473.09375
Epoch 46: Val Loss 505410.37500
Epoch 47: Val Loss 504300.21875
Epoch 48: Val Loss 503131.03125
Epoch 49: Val Loss 501902.78125
Epoch 50: Val Loss 500622.34375
Epoch 51: Val Loss 499257.81250
Epoch 52: Val Loss 497815.78125
Epoch 53: Val Loss 496334.56250
Epoch 54: Val Loss 494778.50000
Epoch 55: Val Loss 493139.62500
Epoch 56: Val Loss 491434.40625
Epoch 57: Val Loss 489656.78125
Epoch 58: Val Loss 487805.62500
Epoch 59: Val Loss 485870.21875
Epoch 60: Val Loss 483853.03125
Epoch 61: Val Loss 481771.50000
Epoch 62: Val Loss 479600.50000
Epoch 63: Val Loss 477321.78125
Epoch 64: Val Loss 474984.93750
Epoch 65: Val Loss 472560.06250
Epoch 66: Val Loss 470038.00000
Epoch 67: Val Loss 467425.75000
Epoch 68: Val Loss 464732.06250
Epoch 69: Val Loss 461949.18750
Epoch 70: Val Loss 459098.43750
Epoch 71: Val Loss 456150.34375
Epoch 72: Val Loss 453131.15625
Epoch 73: Val Loss 449970.18750
Epoch 74: Val Loss 446765.90625
Epoch 75: Val Loss 443439.46875
Epoch 76: Val Loss 440047.96875
Epoch 77: Val Loss 436590.87500
Epoch 78: Val Loss 433004.46875
Epoch 79: Val Loss 429365.93750
Epoch 80: Val Loss 425635.59375
Epoch 81: Val Loss 421768.50000
Epoch 82: Val Loss 417867.28125
Epoch 83: Val Loss 413869.96875
Epoch 84: Val Loss 409779.37500
Epoch 85: Val Loss 405560.25000
Epoch 86: Val Loss 401336.18750
Epoch 87: Val Loss 396988.40625
Epoch 88: Val Loss 392564.53125
Epoch 89: Val Loss 388063.78125
Epoch 90: Val Loss 383491.25000
Epoch 91: Val Loss 378844.25000
Epoch 92: Val Loss 374142.81250
Epoch 93: Val Loss 369340.90625
Epoch 94: Val Loss 364479.75000
Epoch 95: Val Loss 359523.68750
Epoch 96: Val Loss 354520.40625
Epoch 97: Val Loss 349440.90625
Epoch 98: Val Loss 344320.18750
Epoch 99: Val Loss 339121.21875
{'MSE - mean': 368080.68559253187, 'MSE - std': 17582.53104433212, 'R2 - mean': -42.73610594721798, 'R2 - std': 4.146412880084528} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517516.43750
Epoch 1: Val Loss 517490.18750
Epoch 2: Val Loss 517465.59375
Epoch 3: Val Loss 517442.43750
Epoch 4: Val Loss 517420.12500
Epoch 5: Val Loss 517397.93750
Epoch 6: Val Loss 517375.12500
Epoch 7: Val Loss 517350.75000
Epoch 8: Val Loss 517324.53125
Epoch 9: Val Loss 517295.65625
Epoch 10: Val Loss 517263.71875
Epoch 11: Val Loss 517228.15625
Epoch 12: Val Loss 517187.93750
Epoch 13: Val Loss 517144.03125
Epoch 14: Val Loss 517094.87500
Epoch 15: Val Loss 517039.84375
Epoch 16: Val Loss 516978.81250
Epoch 17: Val Loss 516910.93750
Epoch 18: Val Loss 516835.53125
Epoch 19: Val Loss 516752.62500
Epoch 20: Val Loss 516661.53125
Epoch 21: Val Loss 516561.59375
Epoch 22: Val Loss 516452.59375
Epoch 23: Val Loss 516332.28125
Epoch 24: Val Loss 516200.56250
Epoch 25: Val Loss 516056.40625
Epoch 26: Val Loss 515898.84375
Epoch 27: Val Loss 515724.40625
Epoch 28: Val Loss 515535.34375
Epoch 29: Val Loss 515330.00000
Epoch 30: Val Loss 515108.21875
Epoch 31: Val Loss 514865.62500
Epoch 32: Val Loss 514603.78125
Epoch 33: Val Loss 514326.25000
Epoch 34: Val Loss 514026.43750
Epoch 35: Val Loss 513699.84375
Epoch 36: Val Loss 513353.40625
Epoch 37: Val Loss 512978.40625
Epoch 38: Val Loss 512581.37500
Epoch 39: Val Loss 512156.62500
Epoch 40: Val Loss 511697.34375
Epoch 41: Val Loss 511211.21875
Epoch 42: Val Loss 510688.71875
Epoch 43: Val Loss 510127.25000
Epoch 44: Val Loss 509546.21875
Epoch 45: Val Loss 508927.12500
Epoch 46: Val Loss 508269.25000
Epoch 47: Val Loss 507571.06250
Epoch 48: Val Loss 506835.50000
Epoch 49: Val Loss 506061.90625
Epoch 50: Val Loss 505246.56250
Epoch 51: Val Loss 504388.03125
Epoch 52: Val Loss 503486.28125
Epoch 53: Val Loss 502541.90625
Epoch 54: Val Loss 501559.90625
Epoch 55: Val Loss 500535.00000
Epoch 56: Val Loss 499459.18750
Epoch 57: Val Loss 498339.68750
Epoch 58: Val Loss 497174.09375
Epoch 59: Val Loss 495953.56250
Epoch 60: Val Loss 494692.40625
Epoch 61: Val Loss 493365.06250
Epoch 62: Val Loss 491983.34375
Epoch 63: Val Loss 490549.78125
Epoch 64: Val Loss 489052.53125
Epoch 65: Val Loss 487525.62500
Epoch 66: Val Loss 485910.06250
Epoch 67: Val Loss 484238.59375
Epoch 68: Val Loss 482561.68750
Epoch 69: Val Loss 480782.03125
Epoch 70: Val Loss 478946.62500
Epoch 71: Val Loss 477072.12500
Epoch 72: Val Loss 475110.09375
Epoch 73: Val Loss 473087.90625
Epoch 74: Val Loss 471016.62500
Epoch 75: Val Loss 468892.53125
Epoch 76: Val Loss 466679.84375
Epoch 77: Val Loss 464400.15625
Epoch 78: Val Loss 462087.65625
Epoch 79: Val Loss 459706.25000
Epoch 80: Val Loss 457266.53125
Epoch 81: Val Loss 454751.15625
Epoch 82: Val Loss 452173.56250
Epoch 83: Val Loss 449555.37500
Epoch 84: Val Loss 446868.53125
Epoch 85: Val Loss 444085.43750
Epoch 86: Val Loss 441272.06250
Epoch 87: Val Loss 438405.78125
Epoch 88: Val Loss 435470.62500
Epoch 89: Val Loss 432448.59375
Epoch 90: Val Loss 429360.68750
Epoch 91: Val Loss 426248.81250
Epoch 92: Val Loss 423042.90625
Epoch 93: Val Loss 419800.18750
Epoch 94: Val Loss 416485.84375
Epoch 95: Val Loss 413111.53125
Epoch 96: Val Loss 409672.25000
Epoch 97: Val Loss 406183.06250
Epoch 98: Val Loss 402643.68750
Epoch 99: Val Loss 399017.87500
{'MSE - mean': 374268.1199155555, 'MSE - std': 20011.339094294715, 'R2 - mean': -43.991792405908505, 'R2 - std': 4.478971505586935} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 374268.1199155555 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -2, 'learning_rate': -4, 'dropout': 0.3}. Best is trial 8 with value: 2417.3120980283456.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516636.71875
Epoch 1: Val Loss 516196.50000
Epoch 2: Val Loss 515468.90625
Epoch 3: Val Loss 514132.37500
Epoch 4: Val Loss 511658.84375
Epoch 5: Val Loss 507115.62500
Epoch 6: Val Loss 499244.09375
Epoch 7: Val Loss 486248.31250
Epoch 8: Val Loss 466047.78125
Epoch 9: Val Loss 436351.62500
Epoch 10: Val Loss 394848.78125
Epoch 11: Val Loss 342062.59375
Epoch 12: Val Loss 279469.62500
Epoch 13: Val Loss 211562.01562
Epoch 14: Val Loss 145865.04688
Epoch 15: Val Loss 91838.26562
Epoch 16: Val Loss 54912.20312
Epoch 17: Val Loss 34599.41016
Epoch 18: Val Loss 25088.48242
Epoch 19: Val Loss 20207.12891
Epoch 20: Val Loss 17118.19336
Epoch 21: Val Loss 14824.94141
Epoch 22: Val Loss 13116.03711
Epoch 23: Val Loss 11727.37109
Epoch 24: Val Loss 10588.69043
Epoch 25: Val Loss 9713.79395
Epoch 26: Val Loss 8951.65820
Epoch 27: Val Loss 8288.78809
Epoch 28: Val Loss 7760.03174
Epoch 29: Val Loss 7332.60547
Epoch 30: Val Loss 6945.42334
Epoch 31: Val Loss 6607.94727
Epoch 32: Val Loss 6322.13867
Epoch 33: Val Loss 6100.40918
Epoch 34: Val Loss 5878.00488
Epoch 35: Val Loss 5716.51514
Epoch 36: Val Loss 5558.80273
Epoch 37: Val Loss 5393.48193
Epoch 38: Val Loss 5285.47852
Epoch 39: Val Loss 5159.26025
Epoch 40: Val Loss 5070.40967
Epoch 41: Val Loss 4981.04199
Epoch 42: Val Loss 4874.75537
Epoch 43: Val Loss 4783.13574
Epoch 44: Val Loss 4733.12256
Epoch 45: Val Loss 4649.25244
Epoch 46: Val Loss 4581.44482
Epoch 47: Val Loss 4519.06885
Epoch 48: Val Loss 4471.54346
Epoch 49: Val Loss 4410.00342
Epoch 50: Val Loss 4335.63232
Epoch 51: Val Loss 4297.97754
Epoch 52: Val Loss 4241.52490
Epoch 53: Val Loss 4211.43408
Epoch 54: Val Loss 4179.39404
Epoch 55: Val Loss 4104.88428
Epoch 56: Val Loss 4048.70654
Epoch 57: Val Loss 4025.77588
Epoch 58: Val Loss 3981.30225
Epoch 59: Val Loss 3943.75342
Epoch 60: Val Loss 3895.93237
Epoch 61: Val Loss 3862.39282
Epoch 62: Val Loss 3842.40552
Epoch 63: Val Loss 3778.82690
Epoch 64: Val Loss 3759.85132
Epoch 65: Val Loss 3719.03784
Epoch 66: Val Loss 3705.43945
Epoch 67: Val Loss 3672.87500
Epoch 68: Val Loss 3629.34644
Epoch 69: Val Loss 3598.38965
Epoch 70: Val Loss 3567.19751
Epoch 71: Val Loss 3552.52783
Epoch 72: Val Loss 3505.68896
Epoch 73: Val Loss 3474.01636
Epoch 74: Val Loss 3462.68555
Epoch 75: Val Loss 3434.86377
Epoch 76: Val Loss 3391.97729
Epoch 77: Val Loss 3391.73682
Epoch 78: Val Loss 3344.94922
Epoch 79: Val Loss 3304.47070
Epoch 80: Val Loss 3303.95142
Epoch 81: Val Loss 3277.66748
Epoch 82: Val Loss 3237.31445
Epoch 83: Val Loss 3214.22729
Epoch 84: Val Loss 3201.11963
Epoch 85: Val Loss 3176.20898
Epoch 86: Val Loss 3167.00171
Epoch 87: Val Loss 3141.53027
Epoch 88: Val Loss 3094.43994
Epoch 89: Val Loss 3089.70459
Epoch 90: Val Loss 3072.15503
Epoch 91: Val Loss 3029.25928
Epoch 92: Val Loss 3020.93164
Epoch 93: Val Loss 2991.51074
Epoch 94: Val Loss 2983.94702
Epoch 95: Val Loss 2957.55518
Epoch 96: Val Loss 2923.25562
Epoch 97: Val Loss 2899.28076
Epoch 98: Val Loss 2913.08447
Epoch 99: Val Loss 2865.73877
{'MSE - mean': 2865.7386261203933, 'MSE - std': 0.0, 'R2 - mean': 0.6667978292125748, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524252.68750
Epoch 1: Val Loss 523656.50000
Epoch 2: Val Loss 522310.81250
Epoch 3: Val Loss 519435.62500
Epoch 4: Val Loss 513813.75000
Epoch 5: Val Loss 503778.62500
Epoch 6: Val Loss 487030.90625
Epoch 7: Val Loss 460967.00000
Epoch 8: Val Loss 423618.46875
Epoch 9: Val Loss 373526.46875
Epoch 10: Val Loss 311598.18750
Epoch 11: Val Loss 241634.65625
Epoch 12: Val Loss 170853.10938
Epoch 13: Val Loss 107783.83594
Epoch 14: Val Loss 61883.64062
Epoch 15: Val Loss 35540.64453
Epoch 16: Val Loss 23644.30469
Epoch 17: Val Loss 18410.57227
Epoch 18: Val Loss 15408.40332
Epoch 19: Val Loss 13284.09375
Epoch 20: Val Loss 11698.05176
Epoch 21: Val Loss 10472.10254
Epoch 22: Val Loss 9492.23340
Epoch 23: Val Loss 8738.87500
Epoch 24: Val Loss 8136.50537
Epoch 25: Val Loss 7622.81787
Epoch 26: Val Loss 7207.36963
Epoch 27: Val Loss 6866.82324
Epoch 28: Val Loss 6562.52295
Epoch 29: Val Loss 6301.65820
Epoch 30: Val Loss 6087.41504
Epoch 31: Val Loss 5884.27979
Epoch 32: Val Loss 5678.90088
Epoch 33: Val Loss 5509.81201
Epoch 34: Val Loss 5346.77441
Epoch 35: Val Loss 5198.20508
Epoch 36: Val Loss 5054.14453
Epoch 37: Val Loss 4927.15430
Epoch 38: Val Loss 4799.47998
Epoch 39: Val Loss 4686.13184
Epoch 40: Val Loss 4567.68115
Epoch 41: Val Loss 4465.06982
Epoch 42: Val Loss 4364.00439
Epoch 43: Val Loss 4267.97021
Epoch 44: Val Loss 4183.85107
Epoch 45: Val Loss 4091.66626
Epoch 46: Val Loss 4014.39624
Epoch 47: Val Loss 3944.78955
Epoch 48: Val Loss 3877.30737
Epoch 49: Val Loss 3799.94092
Epoch 50: Val Loss 3745.97290
Epoch 51: Val Loss 3685.87915
Epoch 52: Val Loss 3639.56128
Epoch 53: Val Loss 3578.62720
Epoch 54: Val Loss 3530.18262
Epoch 55: Val Loss 3479.01123
Epoch 56: Val Loss 3438.32349
Epoch 57: Val Loss 3399.37012
Epoch 58: Val Loss 3360.27173
Epoch 59: Val Loss 3317.38818
Epoch 60: Val Loss 3294.17163
Epoch 61: Val Loss 3239.87744
Epoch 62: Val Loss 3206.60693
Epoch 63: Val Loss 3188.64160
Epoch 64: Val Loss 3137.75977
Epoch 65: Val Loss 3118.72803
Epoch 66: Val Loss 3081.48438
Epoch 67: Val Loss 3052.91089
Epoch 68: Val Loss 3030.74316
Epoch 69: Val Loss 2994.84473
Epoch 70: Val Loss 2967.05615
Epoch 71: Val Loss 2935.58862
Epoch 72: Val Loss 2924.86621
Epoch 73: Val Loss 2873.93115
Epoch 74: Val Loss 2860.18286
Epoch 75: Val Loss 2829.46167
Epoch 76: Val Loss 2809.71216
Epoch 77: Val Loss 2781.37354
Epoch 78: Val Loss 2749.47583
Epoch 79: Val Loss 2728.13550
Epoch 80: Val Loss 2710.00488
Epoch 81: Val Loss 2678.73584
Epoch 82: Val Loss 2658.16064
Epoch 83: Val Loss 2628.50806
Epoch 84: Val Loss 2610.66504
Epoch 85: Val Loss 2585.54199
Epoch 86: Val Loss 2556.17334
Epoch 87: Val Loss 2538.98755
Epoch 88: Val Loss 2541.55469
Epoch 89: Val Loss 2490.53955
Epoch 90: Val Loss 2475.46411
Epoch 91: Val Loss 2455.82739
Epoch 92: Val Loss 2426.59009
Epoch 93: Val Loss 2410.61694
Epoch 94: Val Loss 2392.59961
Epoch 95: Val Loss 2363.12183
Epoch 96: Val Loss 2358.78613
Epoch 97: Val Loss 2334.04956
Epoch 98: Val Loss 2308.29028
Epoch 99: Val Loss 2288.90723
{'MSE - mean': 2577.3228904028847, 'MSE - std': 288.41573571750837, 'R2 - mean': 0.6836106466467318, 'R2 - std': 0.016812817434157024} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518202.53125
Epoch 1: Val Loss 517751.65625
Epoch 2: Val Loss 516824.25000
Epoch 3: Val Loss 514811.81250
Epoch 4: Val Loss 510712.31250
Epoch 5: Val Loss 503093.25000
Epoch 6: Val Loss 489812.09375
Epoch 7: Val Loss 468356.53125
Epoch 8: Val Loss 436293.56250
Epoch 9: Val Loss 391297.93750
Epoch 10: Val Loss 332339.34375
Epoch 11: Val Loss 262894.87500
Epoch 12: Val Loss 189206.43750
Epoch 13: Val Loss 120654.99219
Epoch 14: Val Loss 67751.10938
Epoch 15: Val Loss 35685.87500
Epoch 16: Val Loss 21379.06055
Epoch 17: Val Loss 15629.94922
Epoch 18: Val Loss 12847.41797
Epoch 19: Val Loss 11261.25977
Epoch 20: Val Loss 10203.20703
Epoch 21: Val Loss 9389.82715
Epoch 22: Val Loss 8768.58496
Epoch 23: Val Loss 8309.33691
Epoch 24: Val Loss 7859.36865
Epoch 25: Val Loss 7495.15332
Epoch 26: Val Loss 7179.18018
Epoch 27: Val Loss 6923.34766
Epoch 28: Val Loss 6751.07373
Epoch 29: Val Loss 6500.61475
Epoch 30: Val Loss 6255.68652
Epoch 31: Val Loss 6074.07715
Epoch 32: Val Loss 5925.92676
Epoch 33: Val Loss 5765.19482
Epoch 34: Val Loss 5561.58301
Epoch 35: Val Loss 5439.99805
Epoch 36: Val Loss 5289.53662
Epoch 37: Val Loss 5141.38330
Epoch 38: Val Loss 5077.23633
Epoch 39: Val Loss 4944.38721
Epoch 40: Val Loss 4838.09229
Epoch 41: Val Loss 4714.03760
Epoch 42: Val Loss 4647.15723
Epoch 43: Val Loss 4587.69824
Epoch 44: Val Loss 4469.33643
Epoch 45: Val Loss 4391.15674
Epoch 46: Val Loss 4328.80176
Epoch 47: Val Loss 4241.73486
Epoch 48: Val Loss 4226.63525
Epoch 49: Val Loss 4163.23389
Epoch 50: Val Loss 4072.60522
Epoch 51: Val Loss 4036.40479
Epoch 52: Val Loss 3953.52783
Epoch 53: Val Loss 3883.26489
Epoch 54: Val Loss 3851.90625
Epoch 55: Val Loss 3830.58887
Epoch 56: Val Loss 3775.91162
Epoch 57: Val Loss 3722.72070
Epoch 58: Val Loss 3692.05444
Epoch 59: Val Loss 3631.28589
Epoch 60: Val Loss 3611.96289
Epoch 61: Val Loss 3554.56836
Epoch 62: Val Loss 3514.06519
Epoch 63: Val Loss 3511.83325
Epoch 64: Val Loss 3440.61328
Epoch 65: Val Loss 3425.33569
Epoch 66: Val Loss 3380.78687
Epoch 67: Val Loss 3337.94287
Epoch 68: Val Loss 3311.44043
Epoch 69: Val Loss 3282.85522
Epoch 70: Val Loss 3222.05127
Epoch 71: Val Loss 3212.06543
Epoch 72: Val Loss 3177.67993
Epoch 73: Val Loss 3145.66333
Epoch 74: Val Loss 3127.66016
Epoch 75: Val Loss 3097.57007
Epoch 76: Val Loss 3039.54150
Epoch 77: Val Loss 3037.78784
Epoch 78: Val Loss 2991.63306
Epoch 79: Val Loss 2973.55371
Epoch 80: Val Loss 2956.55566
Epoch 81: Val Loss 2927.06445
Epoch 82: Val Loss 2900.09229
Epoch 83: Val Loss 2882.38013
Epoch 84: Val Loss 2830.28809
Epoch 85: Val Loss 2826.50195
Epoch 86: Val Loss 2804.74170
Epoch 87: Val Loss 2764.05811
Epoch 88: Val Loss 2748.81079
Epoch 89: Val Loss 2725.20483
Epoch 90: Val Loss 2717.91577
Epoch 91: Val Loss 2661.41064
Epoch 92: Val Loss 2670.10718
Epoch 93: Val Loss 2639.59180
Epoch 94: Val Loss 2611.60498
Epoch 95: Val Loss 2601.33569
Epoch 96: Val Loss 2552.63501
Epoch 97: Val Loss 2574.47656
Epoch 98: Val Loss 2529.39038
Epoch 99: Val Loss 2496.04248
{'MSE - mean': 2550.229452029044, 'MSE - std': 238.58723047447856, 'R2 - mean': 0.6996909301291746, 'R2 - std': 0.026563099476547884} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521074.68750
Epoch 1: Val Loss 520801.03125
Epoch 2: Val Loss 520425.65625
Epoch 3: Val Loss 519768.31250
Epoch 4: Val Loss 518579.50000
Epoch 5: Val Loss 516293.50000
Epoch 6: Val Loss 512093.28125
Epoch 7: Val Loss 504879.12500
Epoch 8: Val Loss 493135.46875
Epoch 9: Val Loss 474742.28125
Epoch 10: Val Loss 447932.46875
Epoch 11: Val Loss 411525.43750
Epoch 12: Val Loss 365313.62500
Epoch 13: Val Loss 309876.06250
Epoch 14: Val Loss 247696.73438
Epoch 15: Val Loss 183711.39062
Epoch 16: Val Loss 124069.28125
Epoch 17: Val Loss 76166.03125
Epoch 18: Val Loss 43878.73047
Epoch 19: Val Loss 26395.10352
Epoch 20: Val Loss 18851.03711
Epoch 21: Val Loss 15662.39648
Epoch 22: Val Loss 13918.68652
Epoch 23: Val Loss 12552.13574
Epoch 24: Val Loss 11429.84277
Epoch 25: Val Loss 10504.31836
Epoch 26: Val Loss 9749.59961
Epoch 27: Val Loss 9140.27246
Epoch 28: Val Loss 8644.22754
Epoch 29: Val Loss 8200.39258
Epoch 30: Val Loss 7844.46924
Epoch 31: Val Loss 7516.87793
Epoch 32: Val Loss 7227.18262
Epoch 33: Val Loss 6979.66211
Epoch 34: Val Loss 6760.63623
Epoch 35: Val Loss 6554.69287
Epoch 36: Val Loss 6382.47119
Epoch 37: Val Loss 6191.51709
Epoch 38: Val Loss 6051.42041
Epoch 39: Val Loss 5900.60156
Epoch 40: Val Loss 5751.00977
Epoch 41: Val Loss 5611.15283
Epoch 42: Val Loss 5497.69238
Epoch 43: Val Loss 5401.03418
Epoch 44: Val Loss 5270.08936
Epoch 45: Val Loss 5156.12744
Epoch 46: Val Loss 5053.49072
Epoch 47: Val Loss 4941.32031
Epoch 48: Val Loss 4842.58398
Epoch 49: Val Loss 4743.26904
Epoch 50: Val Loss 4640.27637
Epoch 51: Val Loss 4565.33496
Epoch 52: Val Loss 4463.16895
Epoch 53: Val Loss 4371.15771
Epoch 54: Val Loss 4301.56348
Epoch 55: Val Loss 4218.70361
Epoch 56: Val Loss 4127.89746
Epoch 57: Val Loss 4058.61328
Epoch 58: Val Loss 3971.83545
Epoch 59: Val Loss 3902.11108
Epoch 60: Val Loss 3842.73633
Epoch 61: Val Loss 3771.81348
Epoch 62: Val Loss 3699.78540
Epoch 63: Val Loss 3626.48877
Epoch 64: Val Loss 3561.93726
Epoch 65: Val Loss 3504.59863
Epoch 66: Val Loss 3444.95166
Epoch 67: Val Loss 3382.26318
Epoch 68: Val Loss 3327.28662
Epoch 69: Val Loss 3262.80737
Epoch 70: Val Loss 3214.44751
Epoch 71: Val Loss 3160.35449
Epoch 72: Val Loss 3106.22290
Epoch 73: Val Loss 3062.47998
Epoch 74: Val Loss 3011.35986
Epoch 75: Val Loss 2964.12158
Epoch 76: Val Loss 2921.26416
Epoch 77: Val Loss 2884.19312
Epoch 78: Val Loss 2833.07910
Epoch 79: Val Loss 2788.07129
Epoch 80: Val Loss 2744.95361
Epoch 81: Val Loss 2709.95508
Epoch 82: Val Loss 2670.91064
Epoch 83: Val Loss 2645.22070
Epoch 84: Val Loss 2608.37109
Epoch 85: Val Loss 2569.11426
Epoch 86: Val Loss 2530.71680
Epoch 87: Val Loss 2505.36670
Epoch 88: Val Loss 2469.86572
Epoch 89: Val Loss 2443.08472
Epoch 90: Val Loss 2412.07324
Epoch 91: Val Loss 2392.82446
Epoch 92: Val Loss 2354.68433
Epoch 93: Val Loss 2331.66357
Epoch 94: Val Loss 2313.26221
Epoch 95: Val Loss 2278.85986
Epoch 96: Val Loss 2262.10864
Epoch 97: Val Loss 2238.07959
Epoch 98: Val Loss 2217.43481
Epoch 99: Val Loss 2196.92090
{'MSE - mean': 2461.9023235033, 'MSE - std': 257.095205731109, 'R2 - mean': 0.7086398190843969, 'R2 - std': 0.027738899238243537} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517276.81250
Epoch 1: Val Loss 516459.75000
Epoch 2: Val Loss 515003.40625
Epoch 3: Val Loss 512415.43750
Epoch 4: Val Loss 507767.18750
Epoch 5: Val Loss 499768.81250
Epoch 6: Val Loss 486655.21875
Epoch 7: Val Loss 465994.71875
Epoch 8: Val Loss 435108.59375
Epoch 9: Val Loss 392214.34375
Epoch 10: Val Loss 336225.43750
Epoch 11: Val Loss 270026.50000
Epoch 12: Val Loss 200009.59375
Epoch 13: Val Loss 134425.31250
Epoch 14: Val Loss 82702.15625
Epoch 15: Val Loss 50031.57812
Epoch 16: Val Loss 33082.40625
Epoch 17: Val Loss 25008.51562
Epoch 18: Val Loss 20484.42773
Epoch 19: Val Loss 17524.25781
Epoch 20: Val Loss 15333.20508
Epoch 21: Val Loss 13559.42285
Epoch 22: Val Loss 12158.79590
Epoch 23: Val Loss 11071.50977
Epoch 24: Val Loss 10094.21094
Epoch 25: Val Loss 9320.76172
Epoch 26: Val Loss 8628.74121
Epoch 27: Val Loss 8031.09424
Epoch 28: Val Loss 7526.98047
Epoch 29: Val Loss 7121.94385
Epoch 30: Val Loss 6757.17236
Epoch 31: Val Loss 6466.84717
Epoch 32: Val Loss 6173.31494
Epoch 33: Val Loss 5914.20508
Epoch 34: Val Loss 5690.38330
Epoch 35: Val Loss 5515.35938
Epoch 36: Val Loss 5324.96875
Epoch 37: Val Loss 5153.68164
Epoch 38: Val Loss 5011.09229
Epoch 39: Val Loss 4873.60400
Epoch 40: Val Loss 4746.65625
Epoch 41: Val Loss 4614.79346
Epoch 42: Val Loss 4489.11523
Epoch 43: Val Loss 4388.60791
Epoch 44: Val Loss 4299.64990
Epoch 45: Val Loss 4201.54883
Epoch 46: Val Loss 4135.02295
Epoch 47: Val Loss 4040.29370
Epoch 48: Val Loss 3944.35864
Epoch 49: Val Loss 3878.11401
Epoch 50: Val Loss 3808.03638
Epoch 51: Val Loss 3755.89966
Epoch 52: Val Loss 3672.06543
Epoch 53: Val Loss 3628.98120
Epoch 54: Val Loss 3554.31152
Epoch 55: Val Loss 3514.53491
Epoch 56: Val Loss 3434.43140
Epoch 57: Val Loss 3416.05713
Epoch 58: Val Loss 3350.22192
Epoch 59: Val Loss 3323.68677
Epoch 60: Val Loss 3265.46558
Epoch 61: Val Loss 3206.28906
Epoch 62: Val Loss 3203.12598
Epoch 63: Val Loss 3141.67065
Epoch 64: Val Loss 3105.20752
Epoch 65: Val Loss 3064.00928
Epoch 66: Val Loss 3027.58325
Epoch 67: Val Loss 3007.09033
Epoch 68: Val Loss 2971.16260
Epoch 69: Val Loss 2928.44189
Epoch 70: Val Loss 2901.64307
Epoch 71: Val Loss 2866.02271
Epoch 72: Val Loss 2848.30127
Epoch 73: Val Loss 2821.24878
Epoch 74: Val Loss 2776.87817
Epoch 75: Val Loss 2742.91284
Epoch 76: Val Loss 2721.35303
Epoch 77: Val Loss 2686.60254
Epoch 78: Val Loss 2673.12695
Epoch 79: Val Loss 2645.25391
Epoch 80: Val Loss 2602.70361
Epoch 81: Val Loss 2586.82959
Epoch 82: Val Loss 2558.61108
Epoch 83: Val Loss 2534.37695
Epoch 84: Val Loss 2511.05054
Epoch 85: Val Loss 2492.23438
Epoch 86: Val Loss 2462.38159
Epoch 87: Val Loss 2430.92554
Epoch 88: Val Loss 2424.40845
Epoch 89: Val Loss 2390.82471
Epoch 90: Val Loss 2364.98315
Epoch 91: Val Loss 2347.27100
Epoch 92: Val Loss 2325.22656
Epoch 93: Val Loss 2309.98511
Epoch 94: Val Loss 2290.41211
Epoch 95: Val Loss 2267.54810
Epoch 96: Val Loss 2248.29468
Epoch 97: Val Loss 2221.23730
Epoch 98: Val Loss 2209.53613
Epoch 99: Val Loss 2184.22607
{'MSE - mean': 2406.3670986507814, 'MSE - std': 255.37227853785578, 'R2 - mean': 0.7121558774109433, 'R2 - std': 0.025787746884562198} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 2406.3670986507814 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 20 with value: 2406.3670986507814.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516569.59375
Epoch 1: Val Loss 516541.78125
Epoch 2: Val Loss 516512.46875
Epoch 3: Val Loss 516481.25000
Epoch 4: Val Loss 516447.34375
Epoch 5: Val Loss 516410.37500
Epoch 6: Val Loss 516368.71875
Epoch 7: Val Loss 516322.40625
Epoch 8: Val Loss 516271.21875
Epoch 9: Val Loss 516214.68750
Epoch 10: Val Loss 516151.59375
Epoch 11: Val Loss 516080.68750
Epoch 12: Val Loss 516001.03125
Epoch 13: Val Loss 515913.28125
Epoch 14: Val Loss 515815.96875
Epoch 15: Val Loss 515707.15625
Epoch 16: Val Loss 515585.18750
Epoch 17: Val Loss 515451.43750
Epoch 18: Val Loss 515301.65625
Epoch 19: Val Loss 515135.56250
Epoch 20: Val Loss 514950.87500
Epoch 21: Val Loss 514743.50000
Epoch 22: Val Loss 514515.96875
Epoch 23: Val Loss 514265.62500
Epoch 24: Val Loss 513992.00000
Epoch 25: Val Loss 513694.75000
Epoch 26: Val Loss 513370.12500
Epoch 27: Val Loss 513024.62500
Epoch 28: Val Loss 512646.90625
Epoch 29: Val Loss 512234.18750
Epoch 30: Val Loss 511785.65625
Epoch 31: Val Loss 511304.03125
Epoch 32: Val Loss 510792.56250
Epoch 33: Val Loss 510236.37500
Epoch 34: Val Loss 509643.43750
Epoch 35: Val Loss 509011.56250
Epoch 36: Val Loss 508332.62500
Epoch 37: Val Loss 507608.37500
Epoch 38: Val Loss 506838.28125
Epoch 39: Val Loss 506012.65625
Epoch 40: Val Loss 505165.87500
Epoch 41: Val Loss 504241.78125
Epoch 42: Val Loss 503272.56250
Epoch 43: Val Loss 502245.65625
Epoch 44: Val Loss 501168.59375
Epoch 45: Val Loss 500030.00000
Epoch 46: Val Loss 498837.18750
Epoch 47: Val Loss 497558.93750
Epoch 48: Val Loss 496248.15625
Epoch 49: Val Loss 494858.40625
Epoch 50: Val Loss 493427.50000
Epoch 51: Val Loss 491918.96875
Epoch 52: Val Loss 490344.31250
Epoch 53: Val Loss 488688.96875
Epoch 54: Val Loss 486965.25000
Epoch 55: Val Loss 485193.09375
Epoch 56: Val Loss 483330.03125
Epoch 57: Val Loss 481381.06250
Epoch 58: Val Loss 479376.21875
Epoch 59: Val Loss 477287.06250
Epoch 60: Val Loss 475137.87500
Epoch 61: Val Loss 472904.03125
Epoch 62: Val Loss 470566.03125
Epoch 63: Val Loss 468171.03125
Epoch 64: Val Loss 465675.59375
Epoch 65: Val Loss 463155.56250
Epoch 66: Val Loss 460510.40625
Epoch 67: Val Loss 457804.75000
Epoch 68: Val Loss 455001.71875
Epoch 69: Val Loss 452122.28125
Epoch 70: Val Loss 449185.65625
Epoch 71: Val Loss 446149.25000
Epoch 72: Val Loss 443026.87500
Epoch 73: Val Loss 439857.06250
Epoch 74: Val Loss 436545.46875
Epoch 75: Val Loss 433191.90625
Epoch 76: Val Loss 429715.09375
Epoch 77: Val Loss 426200.18750
Epoch 78: Val Loss 422599.90625
Epoch 79: Val Loss 418945.62500
Epoch 80: Val Loss 415183.37500
Epoch 81: Val Loss 411315.87500
Epoch 82: Val Loss 407414.56250
Epoch 83: Val Loss 403419.03125
Epoch 84: Val Loss 399379.68750
Epoch 85: Val Loss 395247.81250
Epoch 86: Val Loss 391080.84375
Epoch 87: Val Loss 386809.43750
Epoch 88: Val Loss 382497.21875
Epoch 89: Val Loss 378029.28125
Epoch 90: Val Loss 373529.59375
Epoch 91: Val Loss 368984.81250
Epoch 92: Val Loss 364413.96875
Epoch 93: Val Loss 359772.81250
Epoch 94: Val Loss 355033.93750
Epoch 95: Val Loss 350258.75000
Epoch 96: Val Loss 345477.81250
Epoch 97: Val Loss 340633.65625
Epoch 98: Val Loss 335711.93750
Epoch 99: Val Loss 330798.25000
{'MSE - mean': 330798.26676268014, 'MSE - std': 0.0, 'R2 - mean': -37.46223084456977, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524564.37500
Epoch 1: Val Loss 524534.31250
Epoch 2: Val Loss 524502.93750
Epoch 3: Val Loss 524469.56250
Epoch 4: Val Loss 524433.75000
Epoch 5: Val Loss 524395.12500
Epoch 6: Val Loss 524353.43750
Epoch 7: Val Loss 524307.56250
Epoch 8: Val Loss 524257.56250
Epoch 9: Val Loss 524202.06250
Epoch 10: Val Loss 524140.96875
Epoch 11: Val Loss 524074.18750
Epoch 12: Val Loss 524000.84375
Epoch 13: Val Loss 523920.65625
Epoch 14: Val Loss 523831.78125
Epoch 15: Val Loss 523734.75000
Epoch 16: Val Loss 523627.71875
Epoch 17: Val Loss 523510.03125
Epoch 18: Val Loss 523380.81250
Epoch 19: Val Loss 523239.75000
Epoch 20: Val Loss 523082.68750
Epoch 21: Val Loss 522908.68750
Epoch 22: Val Loss 522719.09375
Epoch 23: Val Loss 522508.90625
Epoch 24: Val Loss 522273.65625
Epoch 25: Val Loss 522012.65625
Epoch 26: Val Loss 521716.56250
Epoch 27: Val Loss 521382.21875
Epoch 28: Val Loss 521011.56250
Epoch 29: Val Loss 520610.87500
Epoch 30: Val Loss 520170.84375
Epoch 31: Val Loss 519700.15625
Epoch 32: Val Loss 519190.37500
Epoch 33: Val Loss 518631.50000
Epoch 34: Val Loss 518038.81250
Epoch 35: Val Loss 517395.40625
Epoch 36: Val Loss 516694.12500
Epoch 37: Val Loss 515949.43750
Epoch 38: Val Loss 515141.28125
Epoch 39: Val Loss 514279.53125
Epoch 40: Val Loss 513371.75000
Epoch 41: Val Loss 512389.00000
Epoch 42: Val Loss 511345.75000
Epoch 43: Val Loss 510226.18750
Epoch 44: Val Loss 509050.40625
Epoch 45: Val Loss 507797.78125
Epoch 46: Val Loss 506495.06250
Epoch 47: Val Loss 505111.90625
Epoch 48: Val Loss 503643.34375
Epoch 49: Val Loss 502110.34375
Epoch 50: Val Loss 500485.78125
Epoch 51: Val Loss 498771.59375
Epoch 52: Val Loss 497001.59375
Epoch 53: Val Loss 495120.37500
Epoch 54: Val Loss 493194.68750
Epoch 55: Val Loss 491153.68750
Epoch 56: Val Loss 489034.18750
Epoch 57: Val Loss 486873.03125
Epoch 58: Val Loss 484577.96875
Epoch 59: Val Loss 482208.53125
Epoch 60: Val Loss 479723.03125
Epoch 61: Val Loss 477184.28125
Epoch 62: Val Loss 474572.96875
Epoch 63: Val Loss 471831.84375
Epoch 64: Val Loss 469018.34375
Epoch 65: Val Loss 466121.56250
Epoch 66: Val Loss 463130.43750
Epoch 67: Val Loss 460026.68750
Epoch 68: Val Loss 456815.43750
Epoch 69: Val Loss 453515.87500
Epoch 70: Val Loss 450141.59375
Epoch 71: Val Loss 446644.34375
Epoch 72: Val Loss 443047.28125
Epoch 73: Val Loss 439365.62500
Epoch 74: Val Loss 435641.09375
Epoch 75: Val Loss 431758.96875
Epoch 76: Val Loss 427844.75000
Epoch 77: Val Loss 423761.78125
Epoch 78: Val Loss 419654.43750
Epoch 79: Val Loss 415453.87500
Epoch 80: Val Loss 411148.50000
Epoch 81: Val Loss 406715.25000
Epoch 82: Val Loss 402285.40625
Epoch 83: Val Loss 397725.28125
Epoch 84: Val Loss 393113.56250
Epoch 85: Val Loss 388369.93750
Epoch 86: Val Loss 383540.03125
Epoch 87: Val Loss 378654.43750
Epoch 88: Val Loss 373718.06250
Epoch 89: Val Loss 368720.03125
Epoch 90: Val Loss 363583.62500
Epoch 91: Val Loss 358372.93750
Epoch 92: Val Loss 353142.40625
Epoch 93: Val Loss 347854.40625
Epoch 94: Val Loss 342525.90625
Epoch 95: Val Loss 337118.37500
Epoch 96: Val Loss 331577.84375
Epoch 97: Val Loss 326105.53125
Epoch 98: Val Loss 320530.53125
Epoch 99: Val Loss 314943.78125
{'MSE - mean': 322871.0226052456, 'MSE - std': 7927.2441574345285, 'R2 - mean': -38.84133675947149, 'R2 - std': 1.379105914901718} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517929.25000
Epoch 1: Val Loss 517890.59375
Epoch 2: Val Loss 517852.31250
Epoch 3: Val Loss 517813.84375
Epoch 4: Val Loss 517774.37500
Epoch 5: Val Loss 517732.68750
Epoch 6: Val Loss 517687.87500
Epoch 7: Val Loss 517638.68750
Epoch 8: Val Loss 517584.84375
Epoch 9: Val Loss 517524.93750
Epoch 10: Val Loss 517459.15625
Epoch 11: Val Loss 517386.18750
Epoch 12: Val Loss 517306.43750
Epoch 13: Val Loss 517217.87500
Epoch 14: Val Loss 517118.96875
Epoch 15: Val Loss 517009.65625
Epoch 16: Val Loss 516889.40625
Epoch 17: Val Loss 516757.06250
Epoch 18: Val Loss 516612.00000
Epoch 19: Val Loss 516451.84375
Epoch 20: Val Loss 516275.03125
Epoch 21: Val Loss 516082.06250
Epoch 22: Val Loss 515867.78125
Epoch 23: Val Loss 515634.09375
Epoch 24: Val Loss 515378.43750
Epoch 25: Val Loss 515097.40625
Epoch 26: Val Loss 514792.68750
Epoch 27: Val Loss 514456.81250
Epoch 28: Val Loss 514096.03125
Epoch 29: Val Loss 513703.46875
Epoch 30: Val Loss 513279.84375
Epoch 31: Val Loss 512810.81250
Epoch 32: Val Loss 512311.12500
Epoch 33: Val Loss 511770.53125
Epoch 34: Val Loss 511190.68750
Epoch 35: Val Loss 510560.81250
Epoch 36: Val Loss 509894.37500
Epoch 37: Val Loss 509170.78125
Epoch 38: Val Loss 508406.34375
Epoch 39: Val Loss 507577.03125
Epoch 40: Val Loss 506707.62500
Epoch 41: Val Loss 505765.21875
Epoch 42: Val Loss 504780.21875
Epoch 43: Val Loss 503732.65625
Epoch 44: Val Loss 502639.71875
Epoch 45: Val Loss 501465.87500
Epoch 46: Val Loss 500235.78125
Epoch 47: Val Loss 498935.18750
Epoch 48: Val Loss 497562.28125
Epoch 49: Val Loss 496141.46875
Epoch 50: Val Loss 494622.37500
Epoch 51: Val Loss 493020.40625
Epoch 52: Val Loss 491377.03125
Epoch 53: Val Loss 489643.68750
Epoch 54: Val Loss 487819.81250
Epoch 55: Val Loss 485932.59375
Epoch 56: Val Loss 483949.21875
Epoch 57: Val Loss 481883.15625
Epoch 58: Val Loss 479750.40625
Epoch 59: Val Loss 477518.90625
Epoch 60: Val Loss 475197.56250
Epoch 61: Val Loss 472787.65625
Epoch 62: Val Loss 470283.90625
Epoch 63: Val Loss 467762.37500
Epoch 64: Val Loss 465113.25000
Epoch 65: Val Loss 462345.59375
Epoch 66: Val Loss 459514.96875
Epoch 67: Val Loss 456590.15625
Epoch 68: Val Loss 453569.28125
Epoch 69: Val Loss 450473.96875
Epoch 70: Val Loss 447282.28125
Epoch 71: Val Loss 443913.37500
Epoch 72: Val Loss 440539.93750
Epoch 73: Val Loss 437089.68750
Epoch 74: Val Loss 433516.40625
Epoch 75: Val Loss 429848.25000
Epoch 76: Val Loss 426108.90625
Epoch 77: Val Loss 422286.15625
Epoch 78: Val Loss 418377.12500
Epoch 79: Val Loss 414360.59375
Epoch 80: Val Loss 410247.93750
Epoch 81: Val Loss 406098.34375
Epoch 82: Val Loss 401899.46875
Epoch 83: Val Loss 397568.62500
Epoch 84: Val Loss 393138.59375
Epoch 85: Val Loss 388622.81250
Epoch 86: Val Loss 384090.53125
Epoch 87: Val Loss 379417.25000
Epoch 88: Val Loss 374709.65625
Epoch 89: Val Loss 369936.40625
Epoch 90: Val Loss 365077.12500
Epoch 91: Val Loss 360201.71875
Epoch 92: Val Loss 355223.43750
Epoch 93: Val Loss 350168.12500
Epoch 94: Val Loss 345076.87500
Epoch 95: Val Loss 339945.00000
Epoch 96: Val Loss 334767.46875
Epoch 97: Val Loss 329569.21875
Epoch 98: Val Loss 324269.09375
Epoch 99: Val Loss 318915.93750
{'MSE - mean': 321552.6590278463, 'MSE - std': 6735.747793092163, 'R2 - mean': -36.98121315673436, 'R2 - std': 2.861481268725002} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520783.96875
Epoch 1: Val Loss 520765.34375
Epoch 2: Val Loss 520744.93750
Epoch 3: Val Loss 520722.28125
Epoch 4: Val Loss 520697.12500
Epoch 5: Val Loss 520668.71875
Epoch 6: Val Loss 520637.06250
Epoch 7: Val Loss 520601.03125
Epoch 8: Val Loss 520560.56250
Epoch 9: Val Loss 520515.06250
Epoch 10: Val Loss 520464.12500
Epoch 11: Val Loss 520406.56250
Epoch 12: Val Loss 520342.09375
Epoch 13: Val Loss 520270.21875
Epoch 14: Val Loss 520190.50000
Epoch 15: Val Loss 520101.84375
Epoch 16: Val Loss 520003.43750
Epoch 17: Val Loss 519893.78125
Epoch 18: Val Loss 519771.43750
Epoch 19: Val Loss 519636.15625
Epoch 20: Val Loss 519486.06250
Epoch 21: Val Loss 519320.03125
Epoch 22: Val Loss 519135.75000
Epoch 23: Val Loss 518933.71875
Epoch 24: Val Loss 518705.34375
Epoch 25: Val Loss 518452.00000
Epoch 26: Val Loss 518172.34375
Epoch 27: Val Loss 517866.68750
Epoch 28: Val Loss 517534.40625
Epoch 29: Val Loss 517179.03125
Epoch 30: Val Loss 516788.84375
Epoch 31: Val Loss 516369.25000
Epoch 32: Val Loss 515918.28125
Epoch 33: Val Loss 515430.34375
Epoch 34: Val Loss 514903.12500
Epoch 35: Val Loss 514340.00000
Epoch 36: Val Loss 513739.46875
Epoch 37: Val Loss 513084.37500
Epoch 38: Val Loss 512392.18750
Epoch 39: Val Loss 511648.78125
Epoch 40: Val Loss 510849.87500
Epoch 41: Val Loss 509992.12500
Epoch 42: Val Loss 509077.00000
Epoch 43: Val Loss 508104.03125
Epoch 44: Val Loss 507054.71875
Epoch 45: Val Loss 505954.87500
Epoch 46: Val Loss 504791.12500
Epoch 47: Val Loss 503566.34375
Epoch 48: Val Loss 502264.84375
Epoch 49: Val Loss 500875.87500
Epoch 50: Val Loss 499428.78125
Epoch 51: Val Loss 497918.28125
Epoch 52: Val Loss 496308.40625
Epoch 53: Val Loss 494638.81250
Epoch 54: Val Loss 492891.21875
Epoch 55: Val Loss 491040.34375
Epoch 56: Val Loss 489094.15625
Epoch 57: Val Loss 487096.00000
Epoch 58: Val Loss 484989.68750
Epoch 59: Val Loss 482784.75000
Epoch 60: Val Loss 480499.34375
Epoch 61: Val Loss 478136.50000
Epoch 62: Val Loss 475677.46875
Epoch 63: Val Loss 473126.56250
Epoch 64: Val Loss 470470.50000
Epoch 65: Val Loss 467710.68750
Epoch 66: Val Loss 464842.59375
Epoch 67: Val Loss 461932.90625
Epoch 68: Val Loss 458863.84375
Epoch 69: Val Loss 455714.25000
Epoch 70: Val Loss 452426.46875
Epoch 71: Val Loss 449081.53125
Epoch 72: Val Loss 445643.09375
Epoch 73: Val Loss 442113.31250
Epoch 74: Val Loss 438451.78125
Epoch 75: Val Loss 434682.84375
Epoch 76: Val Loss 430841.09375
Epoch 77: Val Loss 426892.68750
Epoch 78: Val Loss 422845.12500
Epoch 79: Val Loss 418672.96875
Epoch 80: Val Loss 414448.87500
Epoch 81: Val Loss 410092.65625
Epoch 82: Val Loss 405636.87500
Epoch 83: Val Loss 401129.37500
Epoch 84: Val Loss 396532.81250
Epoch 85: Val Loss 391826.18750
Epoch 86: Val Loss 387012.06250
Epoch 87: Val Loss 382173.43750
Epoch 88: Val Loss 377249.34375
Epoch 89: Val Loss 372203.68750
Epoch 90: Val Loss 367097.62500
Epoch 91: Val Loss 361961.15625
Epoch 92: Val Loss 356691.50000
Epoch 93: Val Loss 351376.12500
Epoch 94: Val Loss 345988.59375
Epoch 95: Val Loss 340565.68750
Epoch 96: Val Loss 335047.37500
Epoch 97: Val Loss 329469.75000
Epoch 98: Val Loss 323829.18750
Epoch 99: Val Loss 318202.96875
{'MSE - mean': 320715.2373306393, 'MSE - std': 6010.952424197732, 'R2 - mean': -37.06397261380668, 'R2 - std': 2.4822577770354055} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517687.40625
Epoch 1: Val Loss 517655.96875
Epoch 2: Val Loss 517624.68750
Epoch 3: Val Loss 517593.21875
Epoch 4: Val Loss 517560.50000
Epoch 5: Val Loss 517526.03125
Epoch 6: Val Loss 517488.93750
Epoch 7: Val Loss 517449.03125
Epoch 8: Val Loss 517404.84375
Epoch 9: Val Loss 517355.78125
Epoch 10: Val Loss 517300.81250
Epoch 11: Val Loss 517239.03125
Epoch 12: Val Loss 517169.28125
Epoch 13: Val Loss 517090.65625
Epoch 14: Val Loss 517002.78125
Epoch 15: Val Loss 516904.75000
Epoch 16: Val Loss 516796.28125
Epoch 17: Val Loss 516677.06250
Epoch 18: Val Loss 516544.90625
Epoch 19: Val Loss 516399.81250
Epoch 20: Val Loss 516239.37500
Epoch 21: Val Loss 516062.81250
Epoch 22: Val Loss 515867.75000
Epoch 23: Val Loss 515654.53125
Epoch 24: Val Loss 515422.81250
Epoch 25: Val Loss 515165.43750
Epoch 26: Val Loss 514885.59375
Epoch 27: Val Loss 514578.37500
Epoch 28: Val Loss 514243.15625
Epoch 29: Val Loss 513878.62500
Epoch 30: Val Loss 513481.25000
Epoch 31: Val Loss 513049.93750
Epoch 32: Val Loss 512583.03125
Epoch 33: Val Loss 512078.15625
Epoch 34: Val Loss 511528.84375
Epoch 35: Val Loss 510943.46875
Epoch 36: Val Loss 510302.40625
Epoch 37: Val Loss 509623.90625
Epoch 38: Val Loss 508889.12500
Epoch 39: Val Loss 508105.93750
Epoch 40: Val Loss 507260.71875
Epoch 41: Val Loss 506358.56250
Epoch 42: Val Loss 505390.09375
Epoch 43: Val Loss 504364.34375
Epoch 44: Val Loss 503280.03125
Epoch 45: Val Loss 502137.09375
Epoch 46: Val Loss 500902.21875
Epoch 47: Val Loss 499604.56250
Epoch 48: Val Loss 498239.53125
Epoch 49: Val Loss 496778.37500
Epoch 50: Val Loss 495254.87500
Epoch 51: Val Loss 493636.65625
Epoch 52: Val Loss 491938.46875
Epoch 53: Val Loss 490175.65625
Epoch 54: Val Loss 488311.53125
Epoch 55: Val Loss 486370.96875
Epoch 56: Val Loss 484338.37500
Epoch 57: Val Loss 482221.03125
Epoch 58: Val Loss 480009.28125
Epoch 59: Val Loss 477707.68750
Epoch 60: Val Loss 475320.12500
Epoch 61: Val Loss 472833.93750
Epoch 62: Val Loss 470273.37500
Epoch 63: Val Loss 467585.84375
Epoch 64: Val Loss 464806.96875
Epoch 65: Val Loss 461921.78125
Epoch 66: Val Loss 458988.21875
Epoch 67: Val Loss 455920.31250
Epoch 68: Val Loss 452754.53125
Epoch 69: Val Loss 449498.53125
Epoch 70: Val Loss 446138.09375
Epoch 71: Val Loss 442697.56250
Epoch 72: Val Loss 439144.03125
Epoch 73: Val Loss 435510.87500
Epoch 74: Val Loss 431851.50000
Epoch 75: Val Loss 428047.06250
Epoch 76: Val Loss 424147.75000
Epoch 77: Val Loss 420174.34375
Epoch 78: Val Loss 416087.87500
Epoch 79: Val Loss 411913.21875
Epoch 80: Val Loss 407629.37500
Epoch 81: Val Loss 403253.18750
Epoch 82: Val Loss 398802.18750
Epoch 83: Val Loss 394292.96875
Epoch 84: Val Loss 389660.81250
Epoch 85: Val Loss 384953.87500
Epoch 86: Val Loss 380157.21875
Epoch 87: Val Loss 375292.81250
Epoch 88: Val Loss 370331.81250
Epoch 89: Val Loss 365318.96875
Epoch 90: Val Loss 360254.09375
Epoch 91: Val Loss 355073.06250
Epoch 92: Val Loss 349858.84375
Epoch 93: Val Loss 344580.81250
Epoch 94: Val Loss 339211.87500
Epoch 95: Val Loss 333852.09375
Epoch 96: Val Loss 328344.81250
Epoch 97: Val Loss 322813.37500
Epoch 98: Val Loss 317183.25000
Epoch 99: Val Loss 311573.00000
{'MSE - mean': 318886.79332062445, 'MSE - std': 6502.1588128722215, 'R2 - mean': -37.261946609823895, 'R2 - std': 2.2552289794427285} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 318886.79332062445 and parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 20 with value: 2406.3670986507814.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 515803.59375
Epoch 1: Val Loss 515252.18750
Epoch 2: Val Loss 514208.00000
Epoch 3: Val Loss 512165.59375
Epoch 4: Val Loss 508275.81250
Epoch 5: Val Loss 501058.65625
Epoch 6: Val Loss 488443.28125
Epoch 7: Val Loss 467883.28125
Epoch 8: Val Loss 436585.68750
Epoch 9: Val Loss 392292.21875
Epoch 10: Val Loss 334886.65625
Epoch 11: Val Loss 265735.46875
Epoch 12: Val Loss 192123.25000
Epoch 13: Val Loss 123946.92188
Epoch 14: Val Loss 71432.59375
Epoch 15: Val Loss 40007.52344
Epoch 16: Val Loss 24927.45703
Epoch 17: Val Loss 18400.87891
Epoch 18: Val Loss 14827.56738
Epoch 19: Val Loss 12332.17090
Epoch 20: Val Loss 10460.21289
Epoch 21: Val Loss 9090.75586
Epoch 22: Val Loss 8053.67920
Epoch 23: Val Loss 7266.11768
Epoch 24: Val Loss 6648.05566
Epoch 25: Val Loss 6176.98096
Epoch 26: Val Loss 5822.23389
Epoch 27: Val Loss 5569.98047
Epoch 28: Val Loss 5360.56885
Epoch 29: Val Loss 5191.89062
Epoch 30: Val Loss 5063.73242
Epoch 31: Val Loss 4960.62207
Epoch 32: Val Loss 4876.58545
Epoch 33: Val Loss 4815.70654
Epoch 34: Val Loss 4755.79980
Epoch 35: Val Loss 4707.86328
Epoch 36: Val Loss 4651.54346
Epoch 37: Val Loss 4609.54053
Epoch 38: Val Loss 4572.62598
Epoch 39: Val Loss 4529.83301
Epoch 40: Val Loss 4488.41602
Epoch 41: Val Loss 4457.29688
Epoch 42: Val Loss 4418.92236
Epoch 43: Val Loss 4389.01416
Epoch 44: Val Loss 4343.81396
Epoch 45: Val Loss 4310.74268
Epoch 46: Val Loss 4264.70996
Epoch 47: Val Loss 4228.08936
Epoch 48: Val Loss 4191.08350
Epoch 49: Val Loss 4172.80127
Epoch 50: Val Loss 4129.22412
Epoch 51: Val Loss 4084.71777
Epoch 52: Val Loss 4061.09521
Epoch 53: Val Loss 4035.34180
Epoch 54: Val Loss 3996.56934
Epoch 55: Val Loss 3970.05518
Epoch 56: Val Loss 3946.13257
Epoch 57: Val Loss 3916.20752
Epoch 58: Val Loss 3890.20825
Epoch 59: Val Loss 3861.06445
Epoch 60: Val Loss 3830.91431
Epoch 61: Val Loss 3804.07275
Epoch 62: Val Loss 3782.53955
Epoch 63: Val Loss 3753.52100
Epoch 64: Val Loss 3732.06738
Epoch 65: Val Loss 3700.23242
Epoch 66: Val Loss 3692.28540
Epoch 67: Val Loss 3643.64453
Epoch 68: Val Loss 3627.77319
Epoch 69: Val Loss 3600.20679
Epoch 70: Val Loss 3577.50317
Epoch 71: Val Loss 3556.18286
Epoch 72: Val Loss 3545.27002
Epoch 73: Val Loss 3508.50244
Epoch 74: Val Loss 3483.54468
Epoch 75: Val Loss 3465.39746
Epoch 76: Val Loss 3451.29956
Epoch 77: Val Loss 3417.74585
Epoch 78: Val Loss 3404.46069
Epoch 79: Val Loss 3380.33032
Epoch 80: Val Loss 3366.70239
Epoch 81: Val Loss 3338.41040
Epoch 82: Val Loss 3328.94214
Epoch 83: Val Loss 3305.73682
Epoch 84: Val Loss 3285.28101
Epoch 85: Val Loss 3266.41992
Epoch 86: Val Loss 3241.29761
Epoch 87: Val Loss 3239.50635
Epoch 88: Val Loss 3204.86475
Epoch 89: Val Loss 3188.72168
Epoch 90: Val Loss 3166.05103
Epoch 91: Val Loss 3143.20483
Epoch 92: Val Loss 3132.91650
Epoch 93: Val Loss 3099.53979
Epoch 94: Val Loss 3089.99731
Epoch 95: Val Loss 3063.21729
Epoch 96: Val Loss 3063.14624
Epoch 97: Val Loss 3031.60864
Epoch 98: Val Loss 3016.98730
Epoch 99: Val Loss 2991.29858
{'MSE - mean': 2991.2984598761213, 'MSE - std': 0.0, 'R2 - mean': 0.6521988672591761, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524666.00000
Epoch 1: Val Loss 524085.18750
Epoch 2: Val Loss 522949.31250
Epoch 3: Val Loss 520575.12500
Epoch 4: Val Loss 515693.87500
Epoch 5: Val Loss 506660.87500
Epoch 6: Val Loss 491295.09375
Epoch 7: Val Loss 467388.96875
Epoch 8: Val Loss 432170.31250
Epoch 9: Val Loss 384365.62500
Epoch 10: Val Loss 323627.40625
Epoch 11: Val Loss 254085.71875
Epoch 12: Val Loss 181928.51562
Epoch 13: Val Loss 117789.95312
Epoch 14: Val Loss 70277.47656
Epoch 15: Val Loss 42115.71875
Epoch 16: Val Loss 28992.76562
Epoch 17: Val Loss 22760.33789
Epoch 18: Val Loss 18947.48242
Epoch 19: Val Loss 16067.16016
Epoch 20: Val Loss 13798.32227
Epoch 21: Val Loss 12013.89551
Epoch 22: Val Loss 10616.87305
Epoch 23: Val Loss 9501.26562
Epoch 24: Val Loss 8539.51074
Epoch 25: Val Loss 7741.96875
Epoch 26: Val Loss 7109.23096
Epoch 27: Val Loss 6580.12646
Epoch 28: Val Loss 6147.81592
Epoch 29: Val Loss 5801.35693
Epoch 30: Val Loss 5469.27100
Epoch 31: Val Loss 5213.88867
Epoch 32: Val Loss 4994.90186
Epoch 33: Val Loss 4817.33936
Epoch 34: Val Loss 4677.87793
Epoch 35: Val Loss 4525.13672
Epoch 36: Val Loss 4432.04004
Epoch 37: Val Loss 4339.92578
Epoch 38: Val Loss 4203.98682
Epoch 39: Val Loss 4175.99756
Epoch 40: Val Loss 4091.97729
Epoch 41: Val Loss 4023.72388
Epoch 42: Val Loss 3975.79834
Epoch 43: Val Loss 3943.98535
Epoch 44: Val Loss 3856.82080
Epoch 45: Val Loss 3846.88354
Epoch 46: Val Loss 3803.76953
Epoch 47: Val Loss 3760.22144
Epoch 48: Val Loss 3724.48022
Epoch 49: Val Loss 3697.37036
Epoch 50: Val Loss 3681.78101
Epoch 51: Val Loss 3669.27832
Epoch 52: Val Loss 3635.61133
Epoch 53: Val Loss 3581.69482
Epoch 54: Val Loss 3570.82544
Epoch 55: Val Loss 3553.00195
Epoch 56: Val Loss 3539.91748
Epoch 57: Val Loss 3483.56396
Epoch 58: Val Loss 3494.50757
Epoch 59: Val Loss 3489.67017
Epoch 60: Val Loss 3430.09204
Epoch 61: Val Loss 3417.72876
Epoch 62: Val Loss 3405.30835
Epoch 63: Val Loss 3402.65234
Epoch 64: Val Loss 3343.98853
Epoch 65: Val Loss 3358.21313
Epoch 66: Val Loss 3332.36133
Epoch 67: Val Loss 3297.14844
Epoch 68: Val Loss 3327.64233
Epoch 69: Val Loss 3285.75586
Epoch 70: Val Loss 3257.13159
Epoch 71: Val Loss 3249.84521
Epoch 72: Val Loss 3222.00757
Epoch 73: Val Loss 3213.91821
Epoch 74: Val Loss 3211.75391
Epoch 75: Val Loss 3175.74219
Epoch 76: Val Loss 3162.76636
Epoch 77: Val Loss 3147.64209
Epoch 78: Val Loss 3137.86401
Epoch 79: Val Loss 3121.09668
Epoch 80: Val Loss 3123.04712
Epoch 81: Val Loss 3092.45239
Epoch 82: Val Loss 3073.78345
Epoch 83: Val Loss 3073.20752
Epoch 84: Val Loss 3035.80933
Epoch 85: Val Loss 3027.19800
Epoch 86: Val Loss 3028.28442
Epoch 87: Val Loss 3003.97485
Epoch 88: Val Loss 2975.00513
Epoch 89: Val Loss 2980.60278
Epoch 90: Val Loss 2971.08667
Epoch 91: Val Loss 2928.35522
Epoch 92: Val Loss 2919.16748
Epoch 93: Val Loss 2916.10107
Epoch 94: Val Loss 2894.80786
Epoch 95: Val Loss 2890.46411
Epoch 96: Val Loss 2852.81470
Epoch 97: Val Loss 2844.33472
Epoch 98: Val Loss 2836.63330
Epoch 99: Val Loss 2829.89062
{'MSE - mean': 2910.594409411485, 'MSE - std': 80.7040504646368, 'R2 - mean': 0.6409087086522096, 'R2 - std': 0.011290158606966472} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517795.53125
Epoch 1: Val Loss 517106.15625
Epoch 2: Val Loss 515841.00000
Epoch 3: Val Loss 513430.31250
Epoch 4: Val Loss 508937.12500
Epoch 5: Val Loss 500989.78125
Epoch 6: Val Loss 487804.40625
Epoch 7: Val Loss 467406.59375
Epoch 8: Val Loss 437542.12500
Epoch 9: Val Loss 396561.78125
Epoch 10: Val Loss 344320.15625
Epoch 11: Val Loss 282248.06250
Epoch 12: Val Loss 214984.84375
Epoch 13: Val Loss 149665.65625
Epoch 14: Val Loss 95159.32812
Epoch 15: Val Loss 57032.55078
Epoch 16: Val Loss 35222.33203
Epoch 17: Val Loss 24728.83594
Epoch 18: Val Loss 19278.72070
Epoch 19: Val Loss 15898.30859
Epoch 20: Val Loss 13438.69434
Epoch 21: Val Loss 11589.63574
Epoch 22: Val Loss 10024.67578
Epoch 23: Val Loss 8859.51074
Epoch 24: Val Loss 7936.61035
Epoch 25: Val Loss 7162.04053
Epoch 26: Val Loss 6534.70312
Epoch 27: Val Loss 6035.00488
Epoch 28: Val Loss 5620.35254
Epoch 29: Val Loss 5308.97119
Epoch 30: Val Loss 5056.97803
Epoch 31: Val Loss 4852.25000
Epoch 32: Val Loss 4690.44141
Epoch 33: Val Loss 4545.46582
Epoch 34: Val Loss 4462.26758
Epoch 35: Val Loss 4352.67871
Epoch 36: Val Loss 4298.25977
Epoch 37: Val Loss 4216.23975
Epoch 38: Val Loss 4158.39990
Epoch 39: Val Loss 4097.02295
Epoch 40: Val Loss 4059.66846
Epoch 41: Val Loss 3992.86816
Epoch 42: Val Loss 3965.43188
Epoch 43: Val Loss 3940.27417
Epoch 44: Val Loss 3881.60132
Epoch 45: Val Loss 3825.78809
Epoch 46: Val Loss 3810.48633
Epoch 47: Val Loss 3769.22485
Epoch 48: Val Loss 3723.24902
Epoch 49: Val Loss 3692.44946
Epoch 50: Val Loss 3661.75073
Epoch 51: Val Loss 3635.45557
Epoch 52: Val Loss 3617.27344
Epoch 53: Val Loss 3580.67822
Epoch 54: Val Loss 3531.10107
Epoch 55: Val Loss 3509.01807
Epoch 56: Val Loss 3482.77319
Epoch 57: Val Loss 3446.13525
Epoch 58: Val Loss 3418.69116
Epoch 59: Val Loss 3388.58887
Epoch 60: Val Loss 3364.23730
Epoch 61: Val Loss 3355.16748
Epoch 62: Val Loss 3314.08130
Epoch 63: Val Loss 3283.22534
Epoch 64: Val Loss 3260.86597
Epoch 65: Val Loss 3244.99707
Epoch 66: Val Loss 3229.37305
Epoch 67: Val Loss 3185.50806
Epoch 68: Val Loss 3162.03320
Epoch 69: Val Loss 3145.58325
Epoch 70: Val Loss 3138.05908
Epoch 71: Val Loss 3101.30371
Epoch 72: Val Loss 3093.25781
Epoch 73: Val Loss 3059.87256
Epoch 74: Val Loss 3031.06543
Epoch 75: Val Loss 3001.86060
Epoch 76: Val Loss 3005.38037
Epoch 77: Val Loss 2962.75073
Epoch 78: Val Loss 2963.85278
Epoch 79: Val Loss 2945.90332
Epoch 80: Val Loss 2923.41968
Epoch 81: Val Loss 2888.27734
Epoch 82: Val Loss 2888.55103
Epoch 83: Val Loss 2876.63794
Epoch 84: Val Loss 2832.69092
Epoch 85: Val Loss 2830.68579
Epoch 86: Val Loss 2809.39551
Epoch 87: Val Loss 2790.70947
Epoch 88: Val Loss 2775.86646
Epoch 89: Val Loss 2763.01221
Epoch 90: Val Loss 2740.04907
Epoch 91: Val Loss 2718.29907
Epoch 92: Val Loss 2717.99585
Epoch 93: Val Loss 2708.75903
Epoch 94: Val Loss 2674.15332
Epoch 95: Val Loss 2678.25317
Epoch 96: Val Loss 2643.29419
Epoch 97: Val Loss 2627.70410
Epoch 98: Val Loss 2622.67969
Epoch 99: Val Loss 2613.49780
{'MSE - mean': 2811.562281984737, 'MSE - std': 154.779909440042, 'R2 - mean': 0.667016912925921, 'R2 - std': 0.038055947170483904} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520746.25000
Epoch 1: Val Loss 520373.71875
Epoch 2: Val Loss 519585.93750
Epoch 3: Val Loss 517998.21875
Epoch 4: Val Loss 514907.31250
Epoch 5: Val Loss 509170.00000
Epoch 6: Val Loss 499018.43750
Epoch 7: Val Loss 482313.21875
Epoch 8: Val Loss 456225.28125
Epoch 9: Val Loss 418453.31250
Epoch 10: Val Loss 366783.34375
Epoch 11: Val Loss 301274.21875
Epoch 12: Val Loss 226147.07812
Epoch 13: Val Loss 149844.90625
Epoch 14: Val Loss 84706.79688
Epoch 15: Val Loss 41796.48438
Epoch 16: Val Loss 21939.61133
Epoch 17: Val Loss 15796.48535
Epoch 18: Val Loss 14134.85547
Epoch 19: Val Loss 12980.95898
Epoch 20: Val Loss 11965.61133
Epoch 21: Val Loss 11159.64844
Epoch 22: Val Loss 10531.85742
Epoch 23: Val Loss 10017.85938
Epoch 24: Val Loss 9539.43750
Epoch 25: Val Loss 9126.35547
Epoch 26: Val Loss 8734.48730
Epoch 27: Val Loss 8416.90527
Epoch 28: Val Loss 8087.92139
Epoch 29: Val Loss 7759.79785
Epoch 30: Val Loss 7462.15332
Epoch 31: Val Loss 7220.92578
Epoch 32: Val Loss 6936.86963
Epoch 33: Val Loss 6683.87402
Epoch 34: Val Loss 6463.11328
Epoch 35: Val Loss 6238.50635
Epoch 36: Val Loss 6029.18799
Epoch 37: Val Loss 5842.81055
Epoch 38: Val Loss 5678.48096
Epoch 39: Val Loss 5494.11865
Epoch 40: Val Loss 5319.45605
Epoch 41: Val Loss 5186.58008
Epoch 42: Val Loss 5025.24756
Epoch 43: Val Loss 4901.44141
Epoch 44: Val Loss 4799.99707
Epoch 45: Val Loss 4675.44580
Epoch 46: Val Loss 4565.03516
Epoch 47: Val Loss 4507.68018
Epoch 48: Val Loss 4406.73047
Epoch 49: Val Loss 4313.42041
Epoch 50: Val Loss 4258.20068
Epoch 51: Val Loss 4170.91406
Epoch 52: Val Loss 4139.82324
Epoch 53: Val Loss 4060.97314
Epoch 54: Val Loss 3995.96826
Epoch 55: Val Loss 3933.89160
Epoch 56: Val Loss 3909.11060
Epoch 57: Val Loss 3856.48242
Epoch 58: Val Loss 3804.55078
Epoch 59: Val Loss 3753.96802
Epoch 60: Val Loss 3722.76562
Epoch 61: Val Loss 3685.86035
Epoch 62: Val Loss 3629.68628
Epoch 63: Val Loss 3602.06567
Epoch 64: Val Loss 3565.77856
Epoch 65: Val Loss 3532.86987
Epoch 66: Val Loss 3491.77222
Epoch 67: Val Loss 3453.69849
Epoch 68: Val Loss 3420.15649
Epoch 69: Val Loss 3396.08936
Epoch 70: Val Loss 3358.64600
Epoch 71: Val Loss 3331.96216
Epoch 72: Val Loss 3307.79028
Epoch 73: Val Loss 3259.19507
Epoch 74: Val Loss 3253.42920
Epoch 75: Val Loss 3225.90552
Epoch 76: Val Loss 3183.56030
Epoch 77: Val Loss 3159.79297
Epoch 78: Val Loss 3138.00195
Epoch 79: Val Loss 3118.29028
Epoch 80: Val Loss 3081.31299
Epoch 81: Val Loss 3076.74170
Epoch 82: Val Loss 3047.27832
Epoch 83: Val Loss 3013.29370
Epoch 84: Val Loss 2999.11987
Epoch 85: Val Loss 2969.23120
Epoch 86: Val Loss 2948.16357
Epoch 87: Val Loss 2927.06323
Epoch 88: Val Loss 2890.41675
Epoch 89: Val Loss 2881.83569
Epoch 90: Val Loss 2846.71240
Epoch 91: Val Loss 2827.82593
Epoch 92: Val Loss 2811.85156
Epoch 93: Val Loss 2792.14624
Epoch 94: Val Loss 2768.49268
Epoch 95: Val Loss 2744.56470
Epoch 96: Val Loss 2725.33740
Epoch 97: Val Loss 2698.51489
Epoch 98: Val Loss 2678.79663
Epoch 99: Val Loss 2667.76123
{'MSE - mean': 2775.6120910875306, 'MSE - std': 147.80008102615324, 'R2 - mean': 0.6699617779007431, 'R2 - std': 0.03334978298942584} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517668.59375
Epoch 1: Val Loss 517111.59375
Epoch 2: Val Loss 516079.53125
Epoch 3: Val Loss 514130.81250
Epoch 4: Val Loss 510417.37500
Epoch 5: Val Loss 503669.71875
Epoch 6: Val Loss 492254.06250
Epoch 7: Val Loss 474165.21875
Epoch 8: Val Loss 447071.40625
Epoch 9: Val Loss 408601.43750
Epoch 10: Val Loss 357305.18750
Epoch 11: Val Loss 293689.93750
Epoch 12: Val Loss 221649.84375
Epoch 13: Val Loss 148510.20312
Epoch 14: Val Loss 84879.60156
Epoch 15: Val Loss 40909.85156
Epoch 16: Val Loss 18821.90234
Epoch 17: Val Loss 11485.93262
Epoch 18: Val Loss 9606.51367
Epoch 19: Val Loss 8926.02637
Epoch 20: Val Loss 8491.06543
Epoch 21: Val Loss 8137.12061
Epoch 22: Val Loss 7866.70996
Epoch 23: Val Loss 7610.46582
Epoch 24: Val Loss 7381.91064
Epoch 25: Val Loss 7194.97754
Epoch 26: Val Loss 7020.70410
Epoch 27: Val Loss 6858.92578
Epoch 28: Val Loss 6733.72314
Epoch 29: Val Loss 6580.05029
Epoch 30: Val Loss 6434.72510
Epoch 31: Val Loss 6333.85938
Epoch 32: Val Loss 6219.49561
Epoch 33: Val Loss 6106.33398
Epoch 34: Val Loss 6003.84961
Epoch 35: Val Loss 5915.35059
Epoch 36: Val Loss 5836.02588
Epoch 37: Val Loss 5751.72119
Epoch 38: Val Loss 5672.58936
Epoch 39: Val Loss 5594.25537
Epoch 40: Val Loss 5525.74561
Epoch 41: Val Loss 5461.86816
Epoch 42: Val Loss 5383.95898
Epoch 43: Val Loss 5332.12256
Epoch 44: Val Loss 5280.22461
Epoch 45: Val Loss 5208.34326
Epoch 46: Val Loss 5156.97949
Epoch 47: Val Loss 5102.47266
Epoch 48: Val Loss 5054.71436
Epoch 49: Val Loss 4997.35938
Epoch 50: Val Loss 4955.47852
Epoch 51: Val Loss 4897.47119
Epoch 52: Val Loss 4847.58740
Epoch 53: Val Loss 4799.97754
Epoch 54: Val Loss 4752.48975
Epoch 55: Val Loss 4711.27588
Epoch 56: Val Loss 4663.57812
Epoch 57: Val Loss 4628.04639
Epoch 58: Val Loss 4572.70410
Epoch 59: Val Loss 4535.31689
Epoch 60: Val Loss 4487.50293
Epoch 61: Val Loss 4453.32080
Epoch 62: Val Loss 4412.57080
Epoch 63: Val Loss 4364.44580
Epoch 64: Val Loss 4321.46875
Epoch 65: Val Loss 4277.21289
Epoch 66: Val Loss 4248.07373
Epoch 67: Val Loss 4196.88574
Epoch 68: Val Loss 4162.15527
Epoch 69: Val Loss 4117.14062
Epoch 70: Val Loss 4098.10498
Epoch 71: Val Loss 4045.75806
Epoch 72: Val Loss 4006.16211
Epoch 73: Val Loss 3974.65771
Epoch 74: Val Loss 3942.04053
Epoch 75: Val Loss 3901.17432
Epoch 76: Val Loss 3863.82764
Epoch 77: Val Loss 3832.26099
Epoch 78: Val Loss 3790.56763
Epoch 79: Val Loss 3770.96533
Epoch 80: Val Loss 3726.28906
Epoch 81: Val Loss 3691.98511
Epoch 82: Val Loss 3658.33179
Epoch 83: Val Loss 3636.96021
Epoch 84: Val Loss 3597.84790
Epoch 85: Val Loss 3558.33789
Epoch 86: Val Loss 3533.24121
Epoch 87: Val Loss 3492.89771
Epoch 88: Val Loss 3462.76855
Epoch 89: Val Loss 3429.59692
Epoch 90: Val Loss 3422.45068
Epoch 91: Val Loss 3378.28833
Epoch 92: Val Loss 3335.46069
Epoch 93: Val Loss 3310.87866
Epoch 94: Val Loss 3279.10864
Epoch 95: Val Loss 3250.75073
Epoch 96: Val Loss 3208.86816
Epoch 97: Val Loss 3182.16943
Epoch 98: Val Loss 3166.10303
Epoch 99: Val Loss 3122.87158
{'MSE - mean': 2845.0639605253027, 'MSE - std': 191.75541670230663, 'R2 - mean': 0.6576827140196718, 'R2 - std': 0.038637650802489924} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 2845.0639605253027 and parameters: {'dim': 128, 'depth': 12, 'heads': 4, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 20 with value: 2406.3670986507814.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 515560.87500
Epoch 1: Val Loss 514869.50000
Epoch 2: Val Loss 513596.84375
Epoch 3: Val Loss 511085.75000
Epoch 4: Val Loss 506227.18750
Epoch 5: Val Loss 497360.68750
Epoch 6: Val Loss 482181.84375
Epoch 7: Val Loss 457777.50000
Epoch 8: Val Loss 421514.96875
Epoch 9: Val Loss 372259.68750
Epoch 10: Val Loss 309778.59375
Epoch 11: Val Loss 238410.89062
Epoch 12: Val Loss 165326.01562
Epoch 13: Val Loss 100624.22656
Epoch 14: Val Loss 54128.42578
Epoch 15: Val Loss 28672.31250
Epoch 16: Val Loss 17841.00195
Epoch 17: Val Loss 13554.34473
Epoch 18: Val Loss 11264.50000
Epoch 19: Val Loss 9777.79980
Epoch 20: Val Loss 8777.42188
Epoch 21: Val Loss 8100.99609
Epoch 22: Val Loss 7612.82080
Epoch 23: Val Loss 7224.31689
Epoch 24: Val Loss 6937.91211
Epoch 25: Val Loss 6699.69629
Epoch 26: Val Loss 6507.41016
Epoch 27: Val Loss 6327.09326
Epoch 28: Val Loss 6171.83838
Epoch 29: Val Loss 6039.08936
Epoch 30: Val Loss 5898.75537
Epoch 31: Val Loss 5772.62988
Epoch 32: Val Loss 5669.26318
Epoch 33: Val Loss 5537.21289
Epoch 34: Val Loss 5434.28662
Epoch 35: Val Loss 5337.05371
Epoch 36: Val Loss 5246.41162
Epoch 37: Val Loss 5161.93652
Epoch 38: Val Loss 5075.46631
Epoch 39: Val Loss 5005.03369
Epoch 40: Val Loss 4933.66309
Epoch 41: Val Loss 4861.53174
Epoch 42: Val Loss 4804.44727
Epoch 43: Val Loss 4764.00732
Epoch 44: Val Loss 4693.64600
Epoch 45: Val Loss 4643.22803
Epoch 46: Val Loss 4594.49268
Epoch 47: Val Loss 4547.44434
Epoch 48: Val Loss 4494.37549
Epoch 49: Val Loss 4461.68262
Epoch 50: Val Loss 4411.38672
Epoch 51: Val Loss 4370.05566
Epoch 52: Val Loss 4336.84570
Epoch 53: Val Loss 4290.85010
Epoch 54: Val Loss 4249.82666
Epoch 55: Val Loss 4216.19531
Epoch 56: Val Loss 4181.75342
Epoch 57: Val Loss 4138.53076
Epoch 58: Val Loss 4100.37109
Epoch 59: Val Loss 4077.22852
Epoch 60: Val Loss 4035.18237
Epoch 61: Val Loss 4001.18970
Epoch 62: Val Loss 3971.16113
Epoch 63: Val Loss 3933.88989
Epoch 64: Val Loss 3899.91455
Epoch 65: Val Loss 3880.50293
Epoch 66: Val Loss 3836.62354
Epoch 67: Val Loss 3813.26782
Epoch 68: Val Loss 3766.18677
Epoch 69: Val Loss 3737.29102
Epoch 70: Val Loss 3714.98438
Epoch 71: Val Loss 3691.59839
Epoch 72: Val Loss 3645.32178
Epoch 73: Val Loss 3609.59668
Epoch 74: Val Loss 3590.13330
Epoch 75: Val Loss 3561.30591
Epoch 76: Val Loss 3525.76831
Epoch 77: Val Loss 3508.02832
Epoch 78: Val Loss 3474.24731
Epoch 79: Val Loss 3443.97437
Epoch 80: Val Loss 3415.33594
Epoch 81: Val Loss 3389.42725
Epoch 82: Val Loss 3365.48853
Epoch 83: Val Loss 3340.76758
Epoch 84: Val Loss 3332.70508
Epoch 85: Val Loss 3287.04419
Epoch 86: Val Loss 3258.76709
Epoch 87: Val Loss 3237.09131
Epoch 88: Val Loss 3228.43530
Epoch 89: Val Loss 3177.62280
Epoch 90: Val Loss 3166.05225
Epoch 91: Val Loss 3145.38623
Epoch 92: Val Loss 3121.65088
Epoch 93: Val Loss 3083.08447
Epoch 94: Val Loss 3083.69946
Epoch 95: Val Loss 3054.18677
Epoch 96: Val Loss 3025.77393
Epoch 97: Val Loss 2987.88257
Epoch 98: Val Loss 2983.22119
Epoch 99: Val Loss 2958.72437
{'MSE - mean': 2958.724293500728, 'MSE - std': 0.0, 'R2 - mean': 0.6559862967368482, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524032.00000
Epoch 1: Val Loss 523020.84375
Epoch 2: Val Loss 520945.43750
Epoch 3: Val Loss 516945.71875
Epoch 4: Val Loss 509428.28125
Epoch 5: Val Loss 496189.90625
Epoch 6: Val Loss 474375.12500
Epoch 7: Val Loss 440993.93750
Epoch 8: Val Loss 393244.68750
Epoch 9: Val Loss 330157.28125
Epoch 10: Val Loss 254618.01562
Epoch 11: Val Loss 174078.64062
Epoch 12: Val Loss 101908.99219
Epoch 13: Val Loss 51122.30469
Epoch 14: Val Loss 25921.09961
Epoch 15: Val Loss 17473.74219
Epoch 16: Val Loss 14731.00098
Epoch 17: Val Loss 12990.76562
Epoch 18: Val Loss 11744.90820
Epoch 19: Val Loss 10819.12598
Epoch 20: Val Loss 10115.93164
Epoch 21: Val Loss 9490.43555
Epoch 22: Val Loss 8999.72656
Epoch 23: Val Loss 8550.02832
Epoch 24: Val Loss 8120.22998
Epoch 25: Val Loss 7744.15283
Epoch 26: Val Loss 7426.51025
Epoch 27: Val Loss 7103.54688
Epoch 28: Val Loss 6826.72998
Epoch 29: Val Loss 6557.07910
Epoch 30: Val Loss 6341.61572
Epoch 31: Val Loss 6118.31348
Epoch 32: Val Loss 5935.87207
Epoch 33: Val Loss 5759.26221
Epoch 34: Val Loss 5586.75635
Epoch 35: Val Loss 5436.84912
Epoch 36: Val Loss 5312.27344
Epoch 37: Val Loss 5191.77588
Epoch 38: Val Loss 5080.31006
Epoch 39: Val Loss 4983.82910
Epoch 40: Val Loss 4878.02295
Epoch 41: Val Loss 4785.35400
Epoch 42: Val Loss 4713.02734
Epoch 43: Val Loss 4617.07422
Epoch 44: Val Loss 4561.68018
Epoch 45: Val Loss 4482.09326
Epoch 46: Val Loss 4411.45850
Epoch 47: Val Loss 4372.66992
Epoch 48: Val Loss 4318.60010
Epoch 49: Val Loss 4237.92188
Epoch 50: Val Loss 4195.95264
Epoch 51: Val Loss 4125.06445
Epoch 52: Val Loss 4090.04028
Epoch 53: Val Loss 4025.00391
Epoch 54: Val Loss 3990.31738
Epoch 55: Val Loss 3932.74805
Epoch 56: Val Loss 3885.12256
Epoch 57: Val Loss 3835.67871
Epoch 58: Val Loss 3812.30713
Epoch 59: Val Loss 3769.57324
Epoch 60: Val Loss 3711.67383
Epoch 61: Val Loss 3672.36694
Epoch 62: Val Loss 3627.99951
Epoch 63: Val Loss 3619.18384
Epoch 64: Val Loss 3550.14746
Epoch 65: Val Loss 3509.24951
Epoch 66: Val Loss 3478.51245
Epoch 67: Val Loss 3458.14795
Epoch 68: Val Loss 3414.29614
Epoch 69: Val Loss 3369.72827
Epoch 70: Val Loss 3355.52319
Epoch 71: Val Loss 3331.04053
Epoch 72: Val Loss 3272.53345
Epoch 73: Val Loss 3250.72168
Epoch 74: Val Loss 3233.43164
Epoch 75: Val Loss 3185.11963
Epoch 76: Val Loss 3143.55444
Epoch 77: Val Loss 3123.39941
Epoch 78: Val Loss 3118.63208
Epoch 79: Val Loss 3053.60034
Epoch 80: Val Loss 3038.29248
Epoch 81: Val Loss 3008.34570
Epoch 82: Val Loss 2967.31079
Epoch 83: Val Loss 2945.32715
Epoch 84: Val Loss 2927.76514
Epoch 85: Val Loss 2898.94849
Epoch 86: Val Loss 2863.30957
Epoch 87: Val Loss 2828.70361
Epoch 88: Val Loss 2826.69043
Epoch 89: Val Loss 2805.23804
Epoch 90: Val Loss 2752.02563
Epoch 91: Val Loss 2734.28467
Epoch 92: Val Loss 2716.83350
Epoch 93: Val Loss 2683.63647
Epoch 94: Val Loss 2654.08154
Epoch 95: Val Loss 2645.59473
Epoch 96: Val Loss 2617.04663
Epoch 97: Val Loss 2583.46460
Epoch 98: Val Loss 2570.95044
Epoch 99: Val Loss 2536.97925
{'MSE - mean': 2747.8517499248846, 'MSE - std': 210.87254357584357, 'R2 - mean': 0.66197080867214, 'R2 - std': 0.005984511935291814} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517918.68750
Epoch 1: Val Loss 517509.18750
Epoch 2: Val Loss 516787.62500
Epoch 3: Val Loss 515412.18750
Epoch 4: Val Loss 512672.37500
Epoch 5: Val Loss 507621.12500
Epoch 6: Val Loss 498964.46875
Epoch 7: Val Loss 485014.59375
Epoch 8: Val Loss 463575.43750
Epoch 9: Val Loss 432930.71875
Epoch 10: Val Loss 391602.68750
Epoch 11: Val Loss 339249.28125
Epoch 12: Val Loss 279066.65625
Epoch 13: Val Loss 214824.56250
Epoch 14: Val Loss 153029.45312
Epoch 15: Val Loss 101136.00000
Epoch 16: Val Loss 64175.21094
Epoch 17: Val Loss 41980.78125
Epoch 18: Val Loss 30429.40625
Epoch 19: Val Loss 24163.68945
Epoch 20: Val Loss 20116.03516
Epoch 21: Val Loss 17315.19531
Epoch 22: Val Loss 15096.88281
Epoch 23: Val Loss 13392.23828
Epoch 24: Val Loss 11956.56641
Epoch 25: Val Loss 10825.15234
Epoch 26: Val Loss 9802.63281
Epoch 27: Val Loss 8974.07520
Epoch 28: Val Loss 8278.32617
Epoch 29: Val Loss 7655.51074
Epoch 30: Val Loss 7130.48438
Epoch 31: Val Loss 6731.57666
Epoch 32: Val Loss 6369.60645
Epoch 33: Val Loss 6071.47021
Epoch 34: Val Loss 5820.28760
Epoch 35: Val Loss 5561.11035
Epoch 36: Val Loss 5339.26611
Epoch 37: Val Loss 5143.27441
Epoch 38: Val Loss 5030.74756
Epoch 39: Val Loss 4878.49609
Epoch 40: Val Loss 4774.75391
Epoch 41: Val Loss 4654.44238
Epoch 42: Val Loss 4535.98682
Epoch 43: Val Loss 4449.57324
Epoch 44: Val Loss 4379.16699
Epoch 45: Val Loss 4322.51416
Epoch 46: Val Loss 4260.40771
Epoch 47: Val Loss 4165.65967
Epoch 48: Val Loss 4077.94531
Epoch 49: Val Loss 4048.48657
Epoch 50: Val Loss 3973.04858
Epoch 51: Val Loss 3925.20337
Epoch 52: Val Loss 3858.61206
Epoch 53: Val Loss 3815.49438
Epoch 54: Val Loss 3759.19287
Epoch 55: Val Loss 3697.73877
Epoch 56: Val Loss 3671.78198
Epoch 57: Val Loss 3606.39624
Epoch 58: Val Loss 3545.63354
Epoch 59: Val Loss 3515.48315
Epoch 60: Val Loss 3477.72070
Epoch 61: Val Loss 3407.30688
Epoch 62: Val Loss 3402.05029
Epoch 63: Val Loss 3362.01147
Epoch 64: Val Loss 3311.78394
Epoch 65: Val Loss 3264.89526
Epoch 66: Val Loss 3224.95679
Epoch 67: Val Loss 3193.98755
Epoch 68: Val Loss 3179.54956
Epoch 69: Val Loss 3147.76367
Epoch 70: Val Loss 3106.22437
Epoch 71: Val Loss 3091.60864
Epoch 72: Val Loss 3052.31055
Epoch 73: Val Loss 3026.17969
Epoch 74: Val Loss 3025.33130
Epoch 75: Val Loss 2945.48047
Epoch 76: Val Loss 2934.32959
Epoch 77: Val Loss 2935.20923
Epoch 78: Val Loss 2890.64307
Epoch 79: Val Loss 2867.27197
Epoch 80: Val Loss 2860.84692
Epoch 81: Val Loss 2827.70972
Epoch 82: Val Loss 2798.26685
Epoch 83: Val Loss 2792.29565
Epoch 84: Val Loss 2753.83325
Epoch 85: Val Loss 2736.19287
Epoch 86: Val Loss 2725.50708
Epoch 87: Val Loss 2699.54736
Epoch 88: Val Loss 2682.54565
Epoch 89: Val Loss 2661.78149
Epoch 90: Val Loss 2643.50635
Epoch 91: Val Loss 2636.00830
Epoch 92: Val Loss 2591.01025
Epoch 93: Val Loss 2586.15186
Epoch 94: Val Loss 2563.01636
Epoch 95: Val Loss 2552.59668
Epoch 96: Val Loss 2527.26318
Epoch 97: Val Loss 2512.19775
Epoch 98: Val Loss 2501.29736
Epoch 99: Val Loss 2479.64551
{'MSE - mean': 2658.4496913615385, 'MSE - std': 213.61244323967946, 'R2 - mean': 0.6858515451286884, 'R2 - std': 0.03412411761227927} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520827.93750
Epoch 1: Val Loss 520159.03125
Epoch 2: Val Loss 518741.15625
Epoch 3: Val Loss 515856.96875
Epoch 4: Val Loss 510306.12500
Epoch 5: Val Loss 500478.84375
Epoch 6: Val Loss 484145.12500
Epoch 7: Val Loss 458520.18750
Epoch 8: Val Loss 420931.84375
Epoch 9: Val Loss 369500.93750
Epoch 10: Val Loss 305523.12500
Epoch 11: Val Loss 232918.50000
Epoch 12: Val Loss 160339.96875
Epoch 13: Val Loss 98072.01562
Epoch 14: Val Loss 54818.25781
Epoch 15: Val Loss 31396.14844
Epoch 16: Val Loss 21555.20312
Epoch 17: Val Loss 17053.59570
Epoch 18: Val Loss 14088.88965
Epoch 19: Val Loss 11890.32910
Epoch 20: Val Loss 10246.50586
Epoch 21: Val Loss 9011.33105
Epoch 22: Val Loss 8006.83252
Epoch 23: Val Loss 7268.10693
Epoch 24: Val Loss 6666.79150
Epoch 25: Val Loss 6221.09619
Epoch 26: Val Loss 5852.14258
Epoch 27: Val Loss 5534.05566
Epoch 28: Val Loss 5304.87744
Epoch 29: Val Loss 5117.40869
Epoch 30: Val Loss 4943.34229
Epoch 31: Val Loss 4787.56445
Epoch 32: Val Loss 4662.72363
Epoch 33: Val Loss 4548.26709
Epoch 34: Val Loss 4452.07471
Epoch 35: Val Loss 4370.25732
Epoch 36: Val Loss 4282.03369
Epoch 37: Val Loss 4192.46484
Epoch 38: Val Loss 4114.43994
Epoch 39: Val Loss 4045.17456
Epoch 40: Val Loss 3980.39209
Epoch 41: Val Loss 3906.22095
Epoch 42: Val Loss 3851.27173
Epoch 43: Val Loss 3783.31543
Epoch 44: Val Loss 3742.99023
Epoch 45: Val Loss 3673.54663
Epoch 46: Val Loss 3624.56787
Epoch 47: Val Loss 3585.59204
Epoch 48: Val Loss 3534.06982
Epoch 49: Val Loss 3490.90503
Epoch 50: Val Loss 3444.31396
Epoch 51: Val Loss 3418.39868
Epoch 52: Val Loss 3362.17017
Epoch 53: Val Loss 3325.37256
Epoch 54: Val Loss 3287.98608
Epoch 55: Val Loss 3252.11987
Epoch 56: Val Loss 3216.98779
Epoch 57: Val Loss 3180.83325
Epoch 58: Val Loss 3147.02393
Epoch 59: Val Loss 3125.02417
Epoch 60: Val Loss 3091.88647
Epoch 61: Val Loss 3055.77319
Epoch 62: Val Loss 3032.28979
Epoch 63: Val Loss 3006.06372
Epoch 64: Val Loss 2984.39648
Epoch 65: Val Loss 2941.70288
Epoch 66: Val Loss 2923.64722
Epoch 67: Val Loss 2892.52686
Epoch 68: Val Loss 2867.06055
Epoch 69: Val Loss 2846.60449
Epoch 70: Val Loss 2817.75806
Epoch 71: Val Loss 2800.68164
Epoch 72: Val Loss 2769.89160
Epoch 73: Val Loss 2751.71240
Epoch 74: Val Loss 2736.62891
Epoch 75: Val Loss 2705.75146
Epoch 76: Val Loss 2686.19409
Epoch 77: Val Loss 2667.08374
Epoch 78: Val Loss 2642.40454
Epoch 79: Val Loss 2622.32471
Epoch 80: Val Loss 2594.92114
Epoch 81: Val Loss 2583.84570
Epoch 82: Val Loss 2563.23828
Epoch 83: Val Loss 2537.39526
Epoch 84: Val Loss 2518.72949
Epoch 85: Val Loss 2510.50635
Epoch 86: Val Loss 2483.26416
Epoch 87: Val Loss 2464.62207
Epoch 88: Val Loss 2455.17041
Epoch 89: Val Loss 2425.75195
Epoch 90: Val Loss 2405.30835
Epoch 91: Val Loss 2386.37695
Epoch 92: Val Loss 2375.74731
Epoch 93: Val Loss 2352.57349
Epoch 94: Val Loss 2346.22510
Epoch 95: Val Loss 2323.12085
Epoch 96: Val Loss 2308.33838
Epoch 97: Val Loss 2288.17920
Epoch 98: Val Loss 2271.15479
Epoch 99: Val Loss 2257.40649
{'MSE - mean': 2558.1888558399987, 'MSE - std': 253.73098421074008, 'R2 - mean': 0.696439640315932, 'R2 - std': 0.03478023621534066} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517360.31250
Epoch 1: Val Loss 517012.34375
Epoch 2: Val Loss 516421.31250
Epoch 3: Val Loss 515307.21875
Epoch 4: Val Loss 513157.25000
Epoch 5: Val Loss 509180.90625
Epoch 6: Val Loss 502018.09375
Epoch 7: Val Loss 489791.46875
Epoch 8: Val Loss 470469.28125
Epoch 9: Val Loss 442329.81250
Epoch 10: Val Loss 403756.31250
Epoch 11: Val Loss 354520.43750
Epoch 12: Val Loss 296249.00000
Epoch 13: Val Loss 232522.89062
Epoch 14: Val Loss 169300.21875
Epoch 15: Val Loss 113637.92969
Epoch 16: Val Loss 71775.89844
Epoch 17: Val Loss 45556.67578
Epoch 18: Val Loss 31493.88086
Epoch 19: Val Loss 24307.25781
Epoch 20: Val Loss 20014.77539
Epoch 21: Val Loss 17257.36914
Epoch 22: Val Loss 15158.50684
Epoch 23: Val Loss 13487.88379
Epoch 24: Val Loss 12086.71973
Epoch 25: Val Loss 10925.72363
Epoch 26: Val Loss 9930.03711
Epoch 27: Val Loss 9121.35938
Epoch 28: Val Loss 8499.08301
Epoch 29: Val Loss 7910.07812
Epoch 30: Val Loss 7378.82715
Epoch 31: Val Loss 6987.50635
Epoch 32: Val Loss 6580.10156
Epoch 33: Val Loss 6232.63916
Epoch 34: Val Loss 5975.92383
Epoch 35: Val Loss 5744.18018
Epoch 36: Val Loss 5519.32617
Epoch 37: Val Loss 5299.54541
Epoch 38: Val Loss 5133.20703
Epoch 39: Val Loss 4984.68457
Epoch 40: Val Loss 4829.02393
Epoch 41: Val Loss 4708.29639
Epoch 42: Val Loss 4577.01855
Epoch 43: Val Loss 4476.39355
Epoch 44: Val Loss 4370.79346
Epoch 45: Val Loss 4287.43115
Epoch 46: Val Loss 4181.59473
Epoch 47: Val Loss 4109.82812
Epoch 48: Val Loss 4015.10303
Epoch 49: Val Loss 3939.10303
Epoch 50: Val Loss 3864.88501
Epoch 51: Val Loss 3818.27832
Epoch 52: Val Loss 3728.58984
Epoch 53: Val Loss 3660.39917
Epoch 54: Val Loss 3609.97266
Epoch 55: Val Loss 3566.86743
Epoch 56: Val Loss 3503.79468
Epoch 57: Val Loss 3446.44800
Epoch 58: Val Loss 3395.20703
Epoch 59: Val Loss 3362.52002
Epoch 60: Val Loss 3313.05054
Epoch 61: Val Loss 3250.79517
Epoch 62: Val Loss 3232.35132
Epoch 63: Val Loss 3186.07910
Epoch 64: Val Loss 3135.16650
Epoch 65: Val Loss 3107.60229
Epoch 66: Val Loss 3056.61279
Epoch 67: Val Loss 3036.43677
Epoch 68: Val Loss 2984.66724
Epoch 69: Val Loss 2947.26343
Epoch 70: Val Loss 2926.88550
Epoch 71: Val Loss 2890.24512
Epoch 72: Val Loss 2869.17285
Epoch 73: Val Loss 2822.95923
Epoch 74: Val Loss 2799.72510
Epoch 75: Val Loss 2767.65601
Epoch 76: Val Loss 2738.56421
Epoch 77: Val Loss 2717.20068
Epoch 78: Val Loss 2680.74023
Epoch 79: Val Loss 2662.49438
Epoch 80: Val Loss 2644.83398
Epoch 81: Val Loss 2613.98145
Epoch 82: Val Loss 2578.17896
Epoch 83: Val Loss 2546.62720
Epoch 84: Val Loss 2559.15503
Epoch 85: Val Loss 2516.12964
Epoch 86: Val Loss 2489.32593
Epoch 87: Val Loss 2464.52075
Epoch 88: Val Loss 2433.67969
Epoch 89: Val Loss 2419.37378
Epoch 90: Val Loss 2420.20752
Epoch 91: Val Loss 2368.25610
Epoch 92: Val Loss 2365.83276
Epoch 93: Val Loss 2342.58594
Epoch 94: Val Loss 2307.16650
Epoch 95: Val Loss 2308.15698
Epoch 96: Val Loss 2282.11523
Epoch 97: Val Loss 2255.58740
Epoch 98: Val Loss 2244.33618
Epoch 99: Val Loss 2231.85742
{'MSE - mean': 2492.9225653340736, 'MSE - std': 261.8058146498202, 'R2 - mean': 0.7012016762183932, 'R2 - std': 0.0325336719215836} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 2492.9225653340736 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 20 with value: 2406.3670986507814.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516123.31250
Epoch 1: Val Loss 515814.43750
Epoch 2: Val Loss 515310.37500
Epoch 3: Val Loss 514349.68750
Epoch 4: Val Loss 512476.12500
Epoch 5: Val Loss 508909.28125
Epoch 6: Val Loss 502501.37500
Epoch 7: Val Loss 491711.50000
Epoch 8: Val Loss 474582.75000
Epoch 9: Val Loss 448924.50000
Epoch 10: Val Loss 413094.43750
Epoch 11: Val Loss 366141.75000
Epoch 12: Val Loss 308481.56250
Epoch 13: Val Loss 243457.17188
Epoch 14: Val Loss 176876.21875
Epoch 15: Val Loss 115917.08594
Epoch 16: Val Loss 68548.75781
Epoch 17: Val Loss 38299.83984
Epoch 18: Val Loss 23019.18359
Epoch 19: Val Loss 16488.71484
Epoch 20: Val Loss 13627.03027
Epoch 21: Val Loss 11947.22461
Epoch 22: Val Loss 10683.25293
Epoch 23: Val Loss 9763.33008
Epoch 24: Val Loss 9016.73340
Epoch 25: Val Loss 8360.61621
Epoch 26: Val Loss 7848.09229
Epoch 27: Val Loss 7430.28174
Epoch 28: Val Loss 7082.14941
Epoch 29: Val Loss 6763.69629
Epoch 30: Val Loss 6510.16797
Epoch 31: Val Loss 6289.61182
Epoch 32: Val Loss 6078.49707
Epoch 33: Val Loss 5899.08008
Epoch 34: Val Loss 5741.53857
Epoch 35: Val Loss 5597.68164
Epoch 36: Val Loss 5491.15479
Epoch 37: Val Loss 5354.83984
Epoch 38: Val Loss 5254.79883
Epoch 39: Val Loss 5165.81885
Epoch 40: Val Loss 5054.66895
Epoch 41: Val Loss 4969.72998
Epoch 42: Val Loss 4893.92822
Epoch 43: Val Loss 4818.20410
Epoch 44: Val Loss 4745.51172
Epoch 45: Val Loss 4666.91016
Epoch 46: Val Loss 4611.41699
Epoch 47: Val Loss 4544.70654
Epoch 48: Val Loss 4486.77539
Epoch 49: Val Loss 4416.26123
Epoch 50: Val Loss 4371.90820
Epoch 51: Val Loss 4312.97998
Epoch 52: Val Loss 4252.13281
Epoch 53: Val Loss 4186.46387
Epoch 54: Val Loss 4134.63770
Epoch 55: Val Loss 4085.05103
Epoch 56: Val Loss 4029.43457
Epoch 57: Val Loss 3984.40918
Epoch 58: Val Loss 3932.75537
Epoch 59: Val Loss 3886.85449
Epoch 60: Val Loss 3833.19336
Epoch 61: Val Loss 3787.42993
Epoch 62: Val Loss 3750.13232
Epoch 63: Val Loss 3703.27734
Epoch 64: Val Loss 3653.68823
Epoch 65: Val Loss 3615.04761
Epoch 66: Val Loss 3567.91846
Epoch 67: Val Loss 3541.72852
Epoch 68: Val Loss 3504.57715
Epoch 69: Val Loss 3471.33813
Epoch 70: Val Loss 3428.73315
Epoch 71: Val Loss 3390.41162
Epoch 72: Val Loss 3361.91113
Epoch 73: Val Loss 3335.34058
Epoch 74: Val Loss 3320.44019
Epoch 75: Val Loss 3271.62109
Epoch 76: Val Loss 3253.16455
Epoch 77: Val Loss 3220.44360
Epoch 78: Val Loss 3191.99463
Epoch 79: Val Loss 3176.32593
Epoch 80: Val Loss 3155.89551
Epoch 81: Val Loss 3131.53857
Epoch 82: Val Loss 3105.76099
Epoch 83: Val Loss 3090.97437
Epoch 84: Val Loss 3060.71875
Epoch 85: Val Loss 3044.57568
Epoch 86: Val Loss 3032.73950
Epoch 87: Val Loss 3004.51270
Epoch 88: Val Loss 2985.26831
Epoch 89: Val Loss 2966.05298
Epoch 90: Val Loss 2958.64331
Epoch 91: Val Loss 2932.50610
Epoch 92: Val Loss 2909.75195
Epoch 93: Val Loss 2896.77734
Epoch 94: Val Loss 2878.49438
Epoch 95: Val Loss 2860.60693
Epoch 96: Val Loss 2842.97119
Epoch 97: Val Loss 2829.52148
Epoch 98: Val Loss 2811.11182
Epoch 99: Val Loss 2787.66382
{'MSE - mean': 2787.663496927558, 'MSE - std': 0.0, 'R2 - mean': 0.6758756991531353, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524353.18750
Epoch 1: Val Loss 523985.06250
Epoch 2: Val Loss 523297.90625
Epoch 3: Val Loss 521828.34375
Epoch 4: Val Loss 518828.00000
Epoch 5: Val Loss 513243.03125
Epoch 6: Val Loss 503511.00000
Epoch 7: Val Loss 487571.21875
Epoch 8: Val Loss 463053.34375
Epoch 9: Val Loss 427599.84375
Epoch 10: Val Loss 379761.75000
Epoch 11: Val Loss 319725.68750
Epoch 12: Val Loss 251546.75000
Epoch 13: Val Loss 180081.09375
Epoch 14: Val Loss 115126.21875
Epoch 15: Val Loss 65757.54688
Epoch 16: Val Loss 36090.65625
Epoch 17: Val Loss 22219.36914
Epoch 18: Val Loss 16809.94141
Epoch 19: Val Loss 14097.11328
Epoch 20: Val Loss 12295.59863
Epoch 21: Val Loss 10931.99902
Epoch 22: Val Loss 9908.03223
Epoch 23: Val Loss 9088.14453
Epoch 24: Val Loss 8452.73242
Epoch 25: Val Loss 7912.59961
Epoch 26: Val Loss 7471.62793
Epoch 27: Val Loss 7105.91113
Epoch 28: Val Loss 6804.12207
Epoch 29: Val Loss 6521.14209
Epoch 30: Val Loss 6284.51025
Epoch 31: Val Loss 6060.29053
Epoch 32: Val Loss 5861.85449
Epoch 33: Val Loss 5677.18262
Epoch 34: Val Loss 5514.93213
Epoch 35: Val Loss 5352.06592
Epoch 36: Val Loss 5210.49121
Epoch 37: Val Loss 5070.62939
Epoch 38: Val Loss 4941.09131
Epoch 39: Val Loss 4822.17432
Epoch 40: Val Loss 4701.12061
Epoch 41: Val Loss 4594.06006
Epoch 42: Val Loss 4495.51953
Epoch 43: Val Loss 4398.34912
Epoch 44: Val Loss 4305.73828
Epoch 45: Val Loss 4226.22900
Epoch 46: Val Loss 4136.73877
Epoch 47: Val Loss 4055.43018
Epoch 48: Val Loss 3997.03223
Epoch 49: Val Loss 3923.19360
Epoch 50: Val Loss 3859.38257
Epoch 51: Val Loss 3807.70190
Epoch 52: Val Loss 3751.97168
Epoch 53: Val Loss 3697.32300
Epoch 54: Val Loss 3652.30591
Epoch 55: Val Loss 3591.07251
Epoch 56: Val Loss 3545.83569
Epoch 57: Val Loss 3510.60864
Epoch 58: Val Loss 3468.75977
Epoch 59: Val Loss 3420.54102
Epoch 60: Val Loss 3388.80371
Epoch 61: Val Loss 3350.64355
Epoch 62: Val Loss 3314.63110
Epoch 63: Val Loss 3278.86768
Epoch 64: Val Loss 3253.38281
Epoch 65: Val Loss 3220.20508
Epoch 66: Val Loss 3176.28247
Epoch 67: Val Loss 3171.68628
Epoch 68: Val Loss 3116.72314
Epoch 69: Val Loss 3086.53394
Epoch 70: Val Loss 3062.52734
Epoch 71: Val Loss 3036.95850
Epoch 72: Val Loss 3012.43384
Epoch 73: Val Loss 2971.33350
Epoch 74: Val Loss 2960.40332
Epoch 75: Val Loss 2934.57007
Epoch 76: Val Loss 2899.78052
Epoch 77: Val Loss 2875.27466
Epoch 78: Val Loss 2843.61230
Epoch 79: Val Loss 2836.97949
Epoch 80: Val Loss 2792.31641
Epoch 81: Val Loss 2774.94385
Epoch 82: Val Loss 2758.53784
Epoch 83: Val Loss 2733.43921
Epoch 84: Val Loss 2697.81982
Epoch 85: Val Loss 2682.22168
Epoch 86: Val Loss 2668.13135
Epoch 87: Val Loss 2629.09277
Epoch 88: Val Loss 2620.91992
Epoch 89: Val Loss 2591.67480
Epoch 90: Val Loss 2558.48999
Epoch 91: Val Loss 2548.44434
Epoch 92: Val Loss 2522.15088
Epoch 93: Val Loss 2505.55518
Epoch 94: Val Loss 2476.05151
Epoch 95: Val Loss 2455.10474
Epoch 96: Val Loss 2444.97510
Epoch 97: Val Loss 2414.07568
Epoch 98: Val Loss 2399.60645
Epoch 99: Val Loss 2386.15649
{'MSE - mean': 2586.909926213022, 'MSE - std': 200.75357071453547, 'R2 - mean': 0.6817855011341367, 'R2 - std': 0.0059098019810014946} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518537.81250
Epoch 1: Val Loss 518229.56250
Epoch 2: Val Loss 517672.31250
Epoch 3: Val Loss 516536.31250
Epoch 4: Val Loss 514410.21875
Epoch 5: Val Loss 510529.59375
Epoch 6: Val Loss 503540.96875
Epoch 7: Val Loss 491635.28125
Epoch 8: Val Loss 473008.18750
Epoch 9: Val Loss 445487.06250
Epoch 10: Val Loss 407573.53125
Epoch 11: Val Loss 358364.59375
Epoch 12: Val Loss 298894.93750
Epoch 13: Val Loss 232802.23438
Epoch 14: Val Loss 165737.90625
Epoch 15: Val Loss 105931.85156
Epoch 16: Val Loss 60319.41016
Epoch 17: Val Loss 32198.09766
Epoch 18: Val Loss 18554.21680
Epoch 19: Val Loss 13125.97070
Epoch 20: Val Loss 10860.50781
Epoch 21: Val Loss 9864.48047
Epoch 22: Val Loss 9185.65723
Epoch 23: Val Loss 8658.19043
Epoch 24: Val Loss 8267.34863
Epoch 25: Val Loss 7963.38574
Epoch 26: Val Loss 7659.23438
Epoch 27: Val Loss 7427.77637
Epoch 28: Val Loss 7227.00098
Epoch 29: Val Loss 6999.08594
Epoch 30: Val Loss 6827.14062
Epoch 31: Val Loss 6625.58203
Epoch 32: Val Loss 6481.16113
Epoch 33: Val Loss 6295.08740
Epoch 34: Val Loss 6127.40137
Epoch 35: Val Loss 5973.37793
Epoch 36: Val Loss 5879.12695
Epoch 37: Val Loss 5701.55859
Epoch 38: Val Loss 5534.26611
Epoch 39: Val Loss 5408.86084
Epoch 40: Val Loss 5267.51807
Epoch 41: Val Loss 5171.71777
Epoch 42: Val Loss 5054.39648
Epoch 43: Val Loss 4898.31641
Epoch 44: Val Loss 4830.59570
Epoch 45: Val Loss 4695.34961
Epoch 46: Val Loss 4612.25195
Epoch 47: Val Loss 4518.49902
Epoch 48: Val Loss 4400.34814
Epoch 49: Val Loss 4365.60645
Epoch 50: Val Loss 4214.33740
Epoch 51: Val Loss 4157.54248
Epoch 52: Val Loss 4066.02075
Epoch 53: Val Loss 3993.41748
Epoch 54: Val Loss 3927.32031
Epoch 55: Val Loss 3847.37817
Epoch 56: Val Loss 3772.66504
Epoch 57: Val Loss 3725.11670
Epoch 58: Val Loss 3663.08228
Epoch 59: Val Loss 3622.93335
Epoch 60: Val Loss 3545.59595
Epoch 61: Val Loss 3530.54517
Epoch 62: Val Loss 3468.20044
Epoch 63: Val Loss 3428.48901
Epoch 64: Val Loss 3370.26367
Epoch 65: Val Loss 3309.62891
Epoch 66: Val Loss 3294.50659
Epoch 67: Val Loss 3247.57861
Epoch 68: Val Loss 3206.71216
Epoch 69: Val Loss 3181.38647
Epoch 70: Val Loss 3175.91406
Epoch 71: Val Loss 3129.46436
Epoch 72: Val Loss 3065.25342
Epoch 73: Val Loss 3012.73682
Epoch 74: Val Loss 3004.80859
Epoch 75: Val Loss 3000.37915
Epoch 76: Val Loss 2962.18994
Epoch 77: Val Loss 2915.11377
Epoch 78: Val Loss 2904.92676
Epoch 79: Val Loss 2874.90405
Epoch 80: Val Loss 2860.06665
Epoch 81: Val Loss 2849.04419
Epoch 82: Val Loss 2809.30981
Epoch 83: Val Loss 2774.14575
Epoch 84: Val Loss 2765.33179
Epoch 85: Val Loss 2751.12817
Epoch 86: Val Loss 2726.32568
Epoch 87: Val Loss 2680.10107
Epoch 88: Val Loss 2683.12134
Epoch 89: Val Loss 2671.72168
Epoch 90: Val Loss 2634.92017
Epoch 91: Val Loss 2614.12036
Epoch 92: Val Loss 2622.47046
Epoch 93: Val Loss 2572.54199
Epoch 94: Val Loss 2551.92163
Epoch 95: Val Loss 2530.98291
Epoch 96: Val Loss 2563.59985
Epoch 97: Val Loss 2477.04419
Epoch 98: Val Loss 2501.38647
Epoch 99: Val Loss 2475.64941
{'MSE - mean': 2549.82315782099, 'MSE - std': 172.1012905702699, 'R2 - mean': 0.699204434466448, 'R2 - std': 0.025102237280916564} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520272.68750
Epoch 1: Val Loss 519751.03125
Epoch 2: Val Loss 518717.25000
Epoch 3: Val Loss 516662.40625
Epoch 4: Val Loss 512851.15625
Epoch 5: Val Loss 506294.15625
Epoch 6: Val Loss 495479.46875
Epoch 7: Val Loss 478606.62500
Epoch 8: Val Loss 453730.43750
Epoch 9: Val Loss 419101.21875
Epoch 10: Val Loss 373496.03125
Epoch 11: Val Loss 317125.43750
Epoch 12: Val Loss 252627.32812
Epoch 13: Val Loss 185240.18750
Epoch 14: Val Loss 122416.51562
Epoch 15: Val Loss 72752.58594
Epoch 16: Val Loss 40900.62109
Epoch 17: Val Loss 24840.75000
Epoch 18: Val Loss 18383.59766
Epoch 19: Val Loss 15350.74902
Epoch 20: Val Loss 13299.70508
Epoch 21: Val Loss 11524.28516
Epoch 22: Val Loss 10159.68457
Epoch 23: Val Loss 9059.96680
Epoch 24: Val Loss 8172.92041
Epoch 25: Val Loss 7458.60938
Epoch 26: Val Loss 6804.35498
Epoch 27: Val Loss 6278.29736
Epoch 28: Val Loss 5835.91846
Epoch 29: Val Loss 5438.48975
Epoch 30: Val Loss 5137.97559
Epoch 31: Val Loss 4853.85156
Epoch 32: Val Loss 4637.34424
Epoch 33: Val Loss 4440.97705
Epoch 34: Val Loss 4260.68311
Epoch 35: Val Loss 4142.80127
Epoch 36: Val Loss 4015.04565
Epoch 37: Val Loss 3908.43896
Epoch 38: Val Loss 3828.46631
Epoch 39: Val Loss 3757.27271
Epoch 40: Val Loss 3675.19238
Epoch 41: Val Loss 3607.97754
Epoch 42: Val Loss 3562.82153
Epoch 43: Val Loss 3506.48804
Epoch 44: Val Loss 3460.27905
Epoch 45: Val Loss 3412.73853
Epoch 46: Val Loss 3368.17676
Epoch 47: Val Loss 3325.45215
Epoch 48: Val Loss 3289.83423
Epoch 49: Val Loss 3251.59766
Epoch 50: Val Loss 3223.29614
Epoch 51: Val Loss 3183.76172
Epoch 52: Val Loss 3152.03638
Epoch 53: Val Loss 3119.85205
Epoch 54: Val Loss 3088.95117
Epoch 55: Val Loss 3063.07715
Epoch 56: Val Loss 3035.02173
Epoch 57: Val Loss 3010.73047
Epoch 58: Val Loss 2986.37427
Epoch 59: Val Loss 2949.78052
Epoch 60: Val Loss 2926.76074
Epoch 61: Val Loss 2907.94409
Epoch 62: Val Loss 2874.07373
Epoch 63: Val Loss 2845.35156
Epoch 64: Val Loss 2829.90234
Epoch 65: Val Loss 2799.95972
Epoch 66: Val Loss 2774.73804
Epoch 67: Val Loss 2766.83667
Epoch 68: Val Loss 2740.84766
Epoch 69: Val Loss 2716.45386
Epoch 70: Val Loss 2686.25488
Epoch 71: Val Loss 2680.71460
Epoch 72: Val Loss 2658.44995
Epoch 73: Val Loss 2632.51367
Epoch 74: Val Loss 2616.41943
Epoch 75: Val Loss 2590.16235
Epoch 76: Val Loss 2574.70825
Epoch 77: Val Loss 2565.97095
Epoch 78: Val Loss 2536.12451
Epoch 79: Val Loss 2519.11182
Epoch 80: Val Loss 2503.55029
Epoch 81: Val Loss 2484.14624
Epoch 82: Val Loss 2476.21387
Epoch 83: Val Loss 2446.18628
Epoch 84: Val Loss 2428.94971
Epoch 85: Val Loss 2421.49683
Epoch 86: Val Loss 2403.28516
Epoch 87: Val Loss 2373.75342
Epoch 88: Val Loss 2365.83325
Epoch 89: Val Loss 2345.67651
Epoch 90: Val Loss 2333.93701
Epoch 91: Val Loss 2319.08765
Epoch 92: Val Loss 2301.15820
Epoch 93: Val Loss 2288.12646
Epoch 94: Val Loss 2266.87988
Epoch 95: Val Loss 2262.41431
Epoch 96: Val Loss 2245.61963
Epoch 97: Val Loss 2225.19482
Epoch 98: Val Loss 2220.11426
Epoch 99: Val Loss 2197.78931
{'MSE - mean': 2461.814715148044, 'MSE - std': 213.1914599769797, 'R2 - mean': 0.7082488065979959, 'R2 - std': 0.026795405187127554} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517634.96875
Epoch 1: Val Loss 517214.96875
Epoch 2: Val Loss 516425.81250
Epoch 3: Val Loss 514893.46875
Epoch 4: Val Loss 512009.25000
Epoch 5: Val Loss 507009.21875
Epoch 6: Val Loss 498753.15625
Epoch 7: Val Loss 485990.96875
Epoch 8: Val Loss 466860.43750
Epoch 9: Val Loss 440392.78125
Epoch 10: Val Loss 404984.87500
Epoch 11: Val Loss 360275.25000
Epoch 12: Val Loss 307579.65625
Epoch 13: Val Loss 249341.60938
Epoch 14: Val Loss 190301.67188
Epoch 15: Val Loss 136118.48438
Epoch 16: Val Loss 91771.23438
Epoch 17: Val Loss 60578.78125
Epoch 18: Val Loss 41624.21484
Epoch 19: Val Loss 31494.70703
Epoch 20: Val Loss 25619.82422
Epoch 21: Val Loss 21822.25586
Epoch 22: Val Loss 19041.47852
Epoch 23: Val Loss 16948.26172
Epoch 24: Val Loss 15257.70020
Epoch 25: Val Loss 13794.21875
Epoch 26: Val Loss 12569.47266
Epoch 27: Val Loss 11594.87500
Epoch 28: Val Loss 10720.75977
Epoch 29: Val Loss 9972.93848
Epoch 30: Val Loss 9310.38086
Epoch 31: Val Loss 8786.54688
Epoch 32: Val Loss 8256.05664
Epoch 33: Val Loss 7820.59961
Epoch 34: Val Loss 7408.02588
Epoch 35: Val Loss 7092.99170
Epoch 36: Val Loss 6786.13086
Epoch 37: Val Loss 6511.47168
Epoch 38: Val Loss 6228.72754
Epoch 39: Val Loss 6036.63379
Epoch 40: Val Loss 5782.30957
Epoch 41: Val Loss 5615.35742
Epoch 42: Val Loss 5428.04248
Epoch 43: Val Loss 5245.33105
Epoch 44: Val Loss 5106.54395
Epoch 45: Val Loss 4943.02930
Epoch 46: Val Loss 4812.26123
Epoch 47: Val Loss 4672.22852
Epoch 48: Val Loss 4574.98193
Epoch 49: Val Loss 4443.68262
Epoch 50: Val Loss 4339.87500
Epoch 51: Val Loss 4228.72021
Epoch 52: Val Loss 4119.25684
Epoch 53: Val Loss 4032.74487
Epoch 54: Val Loss 3946.66724
Epoch 55: Val Loss 3833.69312
Epoch 56: Val Loss 3754.62549
Epoch 57: Val Loss 3683.76099
Epoch 58: Val Loss 3607.40479
Epoch 59: Val Loss 3518.83325
Epoch 60: Val Loss 3460.35645
Epoch 61: Val Loss 3413.35864
Epoch 62: Val Loss 3332.92969
Epoch 63: Val Loss 3277.51929
Epoch 64: Val Loss 3233.70679
Epoch 65: Val Loss 3158.09497
Epoch 66: Val Loss 3111.86328
Epoch 67: Val Loss 3055.96899
Epoch 68: Val Loss 3020.67676
Epoch 69: Val Loss 2971.15845
Epoch 70: Val Loss 2921.67358
Epoch 71: Val Loss 2872.23120
Epoch 72: Val Loss 2845.93433
Epoch 73: Val Loss 2805.55005
Epoch 74: Val Loss 2764.85864
Epoch 75: Val Loss 2734.86768
Epoch 76: Val Loss 2687.87256
Epoch 77: Val Loss 2662.49536
Epoch 78: Val Loss 2633.22974
Epoch 79: Val Loss 2597.93359
Epoch 80: Val Loss 2553.54370
Epoch 81: Val Loss 2559.15649
Epoch 82: Val Loss 2498.56592
Epoch 83: Val Loss 2503.89722
Epoch 84: Val Loss 2452.03589
Epoch 85: Val Loss 2431.66504
Epoch 86: Val Loss 2415.27515
Epoch 87: Val Loss 2378.07373
Epoch 88: Val Loss 2358.17041
Epoch 89: Val Loss 2335.73315
Epoch 90: Val Loss 2317.46094
Epoch 91: Val Loss 2288.88623
Epoch 92: Val Loss 2279.77954
Epoch 93: Val Loss 2241.46484
Epoch 94: Val Loss 2235.61938
Epoch 95: Val Loss 2207.26709
Epoch 96: Val Loss 2185.54443
Epoch 97: Val Loss 2173.57983
Epoch 98: Val Loss 2156.07910
Epoch 99: Val Loss 2124.10352
{'MSE - mean': 2394.2725020802454, 'MSE - std': 233.68414809326813, 'R2 - mean': 0.7133502689133644, 'R2 - std': 0.02604793009692658} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 2394.2725020802454 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 515711.75000
Epoch 1: Val Loss 514976.53125
Epoch 2: Val Loss 513544.28125
Epoch 3: Val Loss 510723.93750
Epoch 4: Val Loss 505578.37500
Epoch 5: Val Loss 496825.90625
Epoch 6: Val Loss 482658.81250
Epoch 7: Val Loss 460505.50000
Epoch 8: Val Loss 427618.06250
Epoch 9: Val Loss 381471.75000
Epoch 10: Val Loss 320893.78125
Epoch 11: Val Loss 250128.70312
Epoch 12: Val Loss 175636.25000
Epoch 13: Val Loss 109090.54688
Epoch 14: Val Loss 60536.85156
Epoch 15: Val Loss 33095.49609
Epoch 16: Val Loss 21162.41602
Epoch 17: Val Loss 15914.58105
Epoch 18: Val Loss 12890.55664
Epoch 19: Val Loss 10799.54492
Epoch 20: Val Loss 9339.69043
Epoch 21: Val Loss 8323.80957
Epoch 22: Val Loss 7537.74707
Epoch 23: Val Loss 7029.87402
Epoch 24: Val Loss 6644.23730
Epoch 25: Val Loss 6354.35352
Epoch 26: Val Loss 6132.70410
Epoch 27: Val Loss 5965.62158
Epoch 28: Val Loss 5844.41650
Epoch 29: Val Loss 5732.71729
Epoch 30: Val Loss 5633.61963
Epoch 31: Val Loss 5548.21533
Epoch 32: Val Loss 5470.26953
Epoch 33: Val Loss 5388.77881
Epoch 34: Val Loss 5321.48486
Epoch 35: Val Loss 5246.51562
Epoch 36: Val Loss 5185.05664
Epoch 37: Val Loss 5131.31787
Epoch 38: Val Loss 5054.26953
Epoch 39: Val Loss 4989.96289
Epoch 40: Val Loss 4936.56396
Epoch 41: Val Loss 4886.41016
Epoch 42: Val Loss 4819.09131
Epoch 43: Val Loss 4745.68652
Epoch 44: Val Loss 4694.26953
Epoch 45: Val Loss 4621.42334
Epoch 46: Val Loss 4548.74463
Epoch 47: Val Loss 4482.56934
Epoch 48: Val Loss 4410.69580
Epoch 49: Val Loss 4339.73389
Epoch 50: Val Loss 4283.07812
Epoch 51: Val Loss 4204.72656
Epoch 52: Val Loss 4155.85205
Epoch 53: Val Loss 4087.36646
Epoch 54: Val Loss 4033.75659
Epoch 55: Val Loss 3976.51758
Epoch 56: Val Loss 3912.66895
Epoch 57: Val Loss 3859.03809
Epoch 58: Val Loss 3808.59546
Epoch 59: Val Loss 3756.48218
Epoch 60: Val Loss 3707.16455
Epoch 61: Val Loss 3666.31982
Epoch 62: Val Loss 3606.31519
Epoch 63: Val Loss 3568.10400
Epoch 64: Val Loss 3523.59009
Epoch 65: Val Loss 3474.07324
Epoch 66: Val Loss 3433.20044
Epoch 67: Val Loss 3393.63208
Epoch 68: Val Loss 3349.73145
Epoch 69: Val Loss 3310.69531
Epoch 70: Val Loss 3276.35986
Epoch 71: Val Loss 3241.56616
Epoch 72: Val Loss 3200.71216
Epoch 73: Val Loss 3167.28613
Epoch 74: Val Loss 3131.78101
Epoch 75: Val Loss 3103.02881
Epoch 76: Val Loss 3077.15332
Epoch 77: Val Loss 3043.15234
Epoch 78: Val Loss 3007.20361
Epoch 79: Val Loss 2997.72021
Epoch 80: Val Loss 2952.63477
Epoch 81: Val Loss 2934.65186
Epoch 82: Val Loss 2912.63257
Epoch 83: Val Loss 2879.02539
Epoch 84: Val Loss 2858.31689
Epoch 85: Val Loss 2848.35986
Epoch 86: Val Loss 2802.55542
Epoch 87: Val Loss 2801.04517
Epoch 88: Val Loss 2763.85425
Epoch 89: Val Loss 2732.94482
Epoch 90: Val Loss 2724.96826
Epoch 91: Val Loss 2696.01855
Epoch 92: Val Loss 2677.89478
Epoch 93: Val Loss 2671.67358
Epoch 94: Val Loss 2636.35693
Epoch 95: Val Loss 2624.06494
Epoch 96: Val Loss 2611.83154
Epoch 97: Val Loss 2587.26587
Epoch 98: Val Loss 2578.07446
Epoch 99: Val Loss 2553.95142
{'MSE - mean': 2553.95137161852, 'MSE - std': 0.0, 'R2 - mean': 0.7030496314798731, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524473.06250
Epoch 1: Val Loss 523786.65625
Epoch 2: Val Loss 522455.78125
Epoch 3: Val Loss 519903.37500
Epoch 4: Val Loss 515299.71875
Epoch 5: Val Loss 507388.59375
Epoch 6: Val Loss 494613.09375
Epoch 7: Val Loss 475065.59375
Epoch 8: Val Loss 446377.87500
Epoch 9: Val Loss 407415.59375
Epoch 10: Val Loss 357578.56250
Epoch 11: Val Loss 297537.31250
Epoch 12: Val Loss 231334.57812
Epoch 13: Val Loss 165449.20312
Epoch 14: Val Loss 107511.17188
Epoch 15: Val Loss 64877.23828
Epoch 16: Val Loss 39479.59766
Epoch 17: Val Loss 27290.02148
Epoch 18: Val Loss 21924.52930
Epoch 19: Val Loss 18682.90820
Epoch 20: Val Loss 16197.63184
Epoch 21: Val Loss 14292.87207
Epoch 22: Val Loss 12762.49023
Epoch 23: Val Loss 11564.72461
Epoch 24: Val Loss 10574.32812
Epoch 25: Val Loss 9733.36914
Epoch 26: Val Loss 9038.05664
Epoch 27: Val Loss 8438.89062
Epoch 28: Val Loss 7953.22070
Epoch 29: Val Loss 7507.44824
Epoch 30: Val Loss 7138.95312
Epoch 31: Val Loss 6794.87793
Epoch 32: Val Loss 6501.37988
Epoch 33: Val Loss 6244.26855
Epoch 34: Val Loss 6015.53271
Epoch 35: Val Loss 5815.02295
Epoch 36: Val Loss 5615.54834
Epoch 37: Val Loss 5451.07178
Epoch 38: Val Loss 5307.36133
Epoch 39: Val Loss 5141.06494
Epoch 40: Val Loss 5013.33350
Epoch 41: Val Loss 4888.34131
Epoch 42: Val Loss 4764.44336
Epoch 43: Val Loss 4645.79492
Epoch 44: Val Loss 4554.37744
Epoch 45: Val Loss 4446.75439
Epoch 46: Val Loss 4352.27637
Epoch 47: Val Loss 4274.90527
Epoch 48: Val Loss 4182.60059
Epoch 49: Val Loss 4087.64746
Epoch 50: Val Loss 4034.36304
Epoch 51: Val Loss 3959.52954
Epoch 52: Val Loss 3881.72095
Epoch 53: Val Loss 3847.71631
Epoch 54: Val Loss 3778.37817
Epoch 55: Val Loss 3745.40894
Epoch 56: Val Loss 3679.57104
Epoch 57: Val Loss 3634.49707
Epoch 58: Val Loss 3602.30908
Epoch 59: Val Loss 3560.84619
Epoch 60: Val Loss 3519.48779
Epoch 61: Val Loss 3478.42163
Epoch 62: Val Loss 3455.52441
Epoch 63: Val Loss 3429.52734
Epoch 64: Val Loss 3408.80420
Epoch 65: Val Loss 3375.58398
Epoch 66: Val Loss 3332.73560
Epoch 67: Val Loss 3320.49048
Epoch 68: Val Loss 3284.74902
Epoch 69: Val Loss 3258.64990
Epoch 70: Val Loss 3238.69556
Epoch 71: Val Loss 3226.07251
Epoch 72: Val Loss 3204.83765
Epoch 73: Val Loss 3172.40747
Epoch 74: Val Loss 3169.30176
Epoch 75: Val Loss 3150.23828
Epoch 76: Val Loss 3116.97778
Epoch 77: Val Loss 3097.86865
Epoch 78: Val Loss 3102.44946
Epoch 79: Val Loss 3078.80078
Epoch 80: Val Loss 3056.23901
Epoch 81: Val Loss 3044.65894
Epoch 82: Val Loss 3019.41846
Epoch 83: Val Loss 2989.95801
Epoch 84: Val Loss 3004.40112
Epoch 85: Val Loss 2957.16602
Epoch 86: Val Loss 2952.88062
Epoch 87: Val Loss 2946.58105
Epoch 88: Val Loss 2920.64136
Epoch 89: Val Loss 2900.81030
Epoch 90: Val Loss 2900.15894
Epoch 91: Val Loss 2877.75195
Epoch 92: Val Loss 2865.49341
Epoch 93: Val Loss 2832.01147
Epoch 94: Val Loss 2841.98779
Epoch 95: Val Loss 2816.57617
Epoch 96: Val Loss 2799.35449
Epoch 97: Val Loss 2790.71509
Epoch 98: Val Loss 2784.07373
Epoch 99: Val Loss 2766.35425
{'MSE - mean': 2660.1527782955286, 'MSE - std': 106.20140667700844, 'R2 - mean': 0.6704919586048198, 'R2 - std': 0.03255767287505329} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518024.03125
Epoch 1: Val Loss 517466.53125
Epoch 2: Val Loss 516401.59375
Epoch 3: Val Loss 514312.18750
Epoch 4: Val Loss 510361.87500
Epoch 5: Val Loss 503260.84375
Epoch 6: Val Loss 491449.12500
Epoch 7: Val Loss 473034.90625
Epoch 8: Val Loss 445995.62500
Epoch 9: Val Loss 408343.15625
Epoch 10: Val Loss 359218.65625
Epoch 11: Val Loss 299625.28125
Epoch 12: Val Loss 233132.93750
Epoch 13: Val Loss 166634.92188
Epoch 14: Val Loss 109045.22656
Epoch 15: Val Loss 66703.46875
Epoch 16: Val Loss 41410.25781
Epoch 17: Val Loss 28857.21094
Epoch 18: Val Loss 22739.79102
Epoch 19: Val Loss 19103.38086
Epoch 20: Val Loss 16521.12305
Epoch 21: Val Loss 14543.44434
Epoch 22: Val Loss 13043.28613
Epoch 23: Val Loss 11841.88574
Epoch 24: Val Loss 10879.91406
Epoch 25: Val Loss 10072.21973
Epoch 26: Val Loss 9442.53320
Epoch 27: Val Loss 8888.68066
Epoch 28: Val Loss 8415.16797
Epoch 29: Val Loss 8020.96240
Epoch 30: Val Loss 7692.31787
Epoch 31: Val Loss 7391.84668
Epoch 32: Val Loss 7115.00098
Epoch 33: Val Loss 6874.13574
Epoch 34: Val Loss 6675.49170
Epoch 35: Val Loss 6480.72705
Epoch 36: Val Loss 6305.12061
Epoch 37: Val Loss 6154.58203
Epoch 38: Val Loss 5990.98877
Epoch 39: Val Loss 5836.40723
Epoch 40: Val Loss 5717.70996
Epoch 41: Val Loss 5570.96240
Epoch 42: Val Loss 5465.87500
Epoch 43: Val Loss 5337.36279
Epoch 44: Val Loss 5230.50293
Epoch 45: Val Loss 5124.70312
Epoch 46: Val Loss 5025.04688
Epoch 47: Val Loss 4948.17725
Epoch 48: Val Loss 4853.23828
Epoch 49: Val Loss 4769.15430
Epoch 50: Val Loss 4691.95703
Epoch 51: Val Loss 4614.09424
Epoch 52: Val Loss 4543.30420
Epoch 53: Val Loss 4478.79834
Epoch 54: Val Loss 4424.01465
Epoch 55: Val Loss 4356.58057
Epoch 56: Val Loss 4288.20215
Epoch 57: Val Loss 4248.63574
Epoch 58: Val Loss 4179.08008
Epoch 59: Val Loss 4123.89453
Epoch 60: Val Loss 4095.42236
Epoch 61: Val Loss 4019.27295
Epoch 62: Val Loss 3976.30591
Epoch 63: Val Loss 3928.68188
Epoch 64: Val Loss 3882.45850
Epoch 65: Val Loss 3852.53516
Epoch 66: Val Loss 3794.36475
Epoch 67: Val Loss 3756.95581
Epoch 68: Val Loss 3718.51318
Epoch 69: Val Loss 3685.04102
Epoch 70: Val Loss 3641.77417
Epoch 71: Val Loss 3608.87695
Epoch 72: Val Loss 3579.02954
Epoch 73: Val Loss 3535.12476
Epoch 74: Val Loss 3495.71582
Epoch 75: Val Loss 3464.97705
Epoch 76: Val Loss 3439.60645
Epoch 77: Val Loss 3393.31299
Epoch 78: Val Loss 3363.45068
Epoch 79: Val Loss 3342.34595
Epoch 80: Val Loss 3307.43921
Epoch 81: Val Loss 3285.10034
Epoch 82: Val Loss 3248.80786
Epoch 83: Val Loss 3216.11084
Epoch 84: Val Loss 3188.93213
Epoch 85: Val Loss 3164.63037
Epoch 86: Val Loss 3129.49780
Epoch 87: Val Loss 3121.80664
Epoch 88: Val Loss 3088.44946
Epoch 89: Val Loss 3047.99121
Epoch 90: Val Loss 3022.34375
Epoch 91: Val Loss 3003.81592
Epoch 92: Val Loss 2980.66113
Epoch 93: Val Loss 2958.10547
Epoch 94: Val Loss 2935.30371
Epoch 95: Val Loss 2910.27637
Epoch 96: Val Loss 2883.70825
Epoch 97: Val Loss 2865.54517
Epoch 98: Val Loss 2849.50659
Epoch 99: Val Loss 2821.20679
{'MSE - mean': 2713.837413409545, 'MSE - std': 115.25293610764861, 'R2 - mean': 0.6793010700448708, 'R2 - std': 0.029357604308806632} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520502.28125
Epoch 1: Val Loss 520297.03125
Epoch 2: Val Loss 519911.62500
Epoch 3: Val Loss 519088.96875
Epoch 4: Val Loss 517389.12500
Epoch 5: Val Loss 514095.18750
Epoch 6: Val Loss 508098.84375
Epoch 7: Val Loss 498045.09375
Epoch 8: Val Loss 482281.87500
Epoch 9: Val Loss 458933.28125
Epoch 10: Val Loss 426344.68750
Epoch 11: Val Loss 383500.09375
Epoch 12: Val Loss 329924.40625
Epoch 13: Val Loss 267677.43750
Epoch 14: Val Loss 201088.48438
Epoch 15: Val Loss 136775.18750
Epoch 16: Val Loss 82120.96875
Epoch 17: Val Loss 43447.86719
Epoch 18: Val Loss 22185.71094
Epoch 19: Val Loss 13573.40430
Epoch 20: Val Loss 10704.84668
Epoch 21: Val Loss 9470.51953
Epoch 22: Val Loss 8642.53418
Epoch 23: Val Loss 8001.25342
Epoch 24: Val Loss 7502.81689
Epoch 25: Val Loss 7099.62891
Epoch 26: Val Loss 6806.46191
Epoch 27: Val Loss 6579.08398
Epoch 28: Val Loss 6367.21875
Epoch 29: Val Loss 6200.36133
Epoch 30: Val Loss 6044.65723
Epoch 31: Val Loss 5910.89941
Epoch 32: Val Loss 5781.04785
Epoch 33: Val Loss 5669.67773
Epoch 34: Val Loss 5568.84131
Epoch 35: Val Loss 5460.57861
Epoch 36: Val Loss 5361.02246
Epoch 37: Val Loss 5276.77539
Epoch 38: Val Loss 5184.08545
Epoch 39: Val Loss 5096.70410
Epoch 40: Val Loss 5008.26221
Epoch 41: Val Loss 4932.61377
Epoch 42: Val Loss 4826.90088
Epoch 43: Val Loss 4750.33203
Epoch 44: Val Loss 4689.99902
Epoch 45: Val Loss 4601.65918
Epoch 46: Val Loss 4531.64307
Epoch 47: Val Loss 4455.10059
Epoch 48: Val Loss 4373.89404
Epoch 49: Val Loss 4307.01221
Epoch 50: Val Loss 4229.39990
Epoch 51: Val Loss 4159.73975
Epoch 52: Val Loss 4080.77832
Epoch 53: Val Loss 4006.18896
Epoch 54: Val Loss 3931.08203
Epoch 55: Val Loss 3851.64966
Epoch 56: Val Loss 3787.38770
Epoch 57: Val Loss 3731.69507
Epoch 58: Val Loss 3651.21875
Epoch 59: Val Loss 3588.22925
Epoch 60: Val Loss 3527.85986
Epoch 61: Val Loss 3469.58667
Epoch 62: Val Loss 3410.22827
Epoch 63: Val Loss 3347.10498
Epoch 64: Val Loss 3301.31079
Epoch 65: Val Loss 3240.40723
Epoch 66: Val Loss 3190.94702
Epoch 67: Val Loss 3136.41455
Epoch 68: Val Loss 3103.11890
Epoch 69: Val Loss 3063.17920
Epoch 70: Val Loss 2998.82788
Epoch 71: Val Loss 2959.16748
Epoch 72: Val Loss 2914.94312
Epoch 73: Val Loss 2897.82422
Epoch 74: Val Loss 2836.02783
Epoch 75: Val Loss 2806.30908
Epoch 76: Val Loss 2763.85645
Epoch 77: Val Loss 2726.65771
Epoch 78: Val Loss 2685.61377
Epoch 79: Val Loss 2661.08130
Epoch 80: Val Loss 2622.34326
Epoch 81: Val Loss 2595.58618
Epoch 82: Val Loss 2551.12573
Epoch 83: Val Loss 2525.92285
Epoch 84: Val Loss 2496.05737
Epoch 85: Val Loss 2460.70679
Epoch 86: Val Loss 2433.89624
Epoch 87: Val Loss 2413.12329
Epoch 88: Val Loss 2368.64258
Epoch 89: Val Loss 2341.79126
Epoch 90: Val Loss 2328.74341
Epoch 91: Val Loss 2290.00098
Epoch 92: Val Loss 2278.43896
Epoch 93: Val Loss 2238.57715
Epoch 94: Val Loss 2209.98218
Epoch 95: Val Loss 2204.56909
Epoch 96: Val Loss 2173.19824
Epoch 97: Val Loss 2146.35303
Epoch 98: Val Loss 2121.42310
Epoch 99: Val Loss 2099.02344
{'MSE - mean': 2560.1339219734286, 'MSE - std': 284.3179897580036, 'R2 - mean': 0.6962941856363194, 'R2 - std': 0.038893439428486554} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517755.21875
Epoch 1: Val Loss 517248.34375
Epoch 2: Val Loss 516208.50000
Epoch 3: Val Loss 514164.46875
Epoch 4: Val Loss 510319.84375
Epoch 5: Val Loss 503412.12500
Epoch 6: Val Loss 491911.59375
Epoch 7: Val Loss 473808.50000
Epoch 8: Val Loss 446771.43750
Epoch 9: Val Loss 409078.00000
Epoch 10: Val Loss 359557.25000
Epoch 11: Val Loss 299142.03125
Epoch 12: Val Loss 231777.04688
Epoch 13: Val Loss 164336.14062
Epoch 14: Val Loss 106626.56250
Epoch 15: Val Loss 64760.41406
Epoch 16: Val Loss 40367.96094
Epoch 17: Val Loss 28348.61523
Epoch 18: Val Loss 22478.05273
Epoch 19: Val Loss 18780.32227
Epoch 20: Val Loss 16124.01953
Epoch 21: Val Loss 14028.04980
Epoch 22: Val Loss 12419.50586
Epoch 23: Val Loss 11035.64551
Epoch 24: Val Loss 9972.13086
Epoch 25: Val Loss 9091.36426
Epoch 26: Val Loss 8362.95215
Epoch 27: Val Loss 7777.28613
Epoch 28: Val Loss 7271.80078
Epoch 29: Val Loss 6879.77637
Epoch 30: Val Loss 6499.97754
Epoch 31: Val Loss 6187.18750
Epoch 32: Val Loss 5915.37793
Epoch 33: Val Loss 5693.85547
Epoch 34: Val Loss 5468.48486
Epoch 35: Val Loss 5292.83252
Epoch 36: Val Loss 5104.57178
Epoch 37: Val Loss 4942.21924
Epoch 38: Val Loss 4786.91943
Epoch 39: Val Loss 4662.04199
Epoch 40: Val Loss 4522.43604
Epoch 41: Val Loss 4391.51611
Epoch 42: Val Loss 4295.58740
Epoch 43: Val Loss 4209.73438
Epoch 44: Val Loss 4113.12305
Epoch 45: Val Loss 4014.91162
Epoch 46: Val Loss 3959.47363
Epoch 47: Val Loss 3874.13428
Epoch 48: Val Loss 3799.48413
Epoch 49: Val Loss 3733.46826
Epoch 50: Val Loss 3678.89722
Epoch 51: Val Loss 3640.62598
Epoch 52: Val Loss 3582.40479
Epoch 53: Val Loss 3544.10767
Epoch 54: Val Loss 3481.84399
Epoch 55: Val Loss 3436.29565
Epoch 56: Val Loss 3401.26978
Epoch 57: Val Loss 3370.50757
Epoch 58: Val Loss 3330.46216
Epoch 59: Val Loss 3292.35815
Epoch 60: Val Loss 3275.60327
Epoch 61: Val Loss 3230.87988
Epoch 62: Val Loss 3200.83521
Epoch 63: Val Loss 3166.40894
Epoch 64: Val Loss 3146.40332
Epoch 65: Val Loss 3113.25000
Epoch 66: Val Loss 3091.67456
Epoch 67: Val Loss 3061.71021
Epoch 68: Val Loss 3044.90161
Epoch 69: Val Loss 3010.73315
Epoch 70: Val Loss 2990.52710
Epoch 71: Val Loss 2964.61182
Epoch 72: Val Loss 2933.89917
Epoch 73: Val Loss 2902.64355
Epoch 74: Val Loss 2883.59399
Epoch 75: Val Loss 2866.18555
Epoch 76: Val Loss 2846.47485
Epoch 77: Val Loss 2814.11108
Epoch 78: Val Loss 2796.87671
Epoch 79: Val Loss 2781.83228
Epoch 80: Val Loss 2766.79395
Epoch 81: Val Loss 2735.80664
Epoch 82: Val Loss 2711.31299
Epoch 83: Val Loss 2700.85718
Epoch 84: Val Loss 2682.29956
Epoch 85: Val Loss 2654.03271
Epoch 86: Val Loss 2635.33911
Epoch 87: Val Loss 2628.96118
Epoch 88: Val Loss 2593.73560
Epoch 89: Val Loss 2570.06616
Epoch 90: Val Loss 2547.84033
Epoch 91: Val Loss 2545.18188
Epoch 92: Val Loss 2509.70605
Epoch 93: Val Loss 2496.52490
Epoch 94: Val Loss 2481.60815
Epoch 95: Val Loss 2459.64478
Epoch 96: Val Loss 2445.20898
Epoch 97: Val Loss 2435.09741
Epoch 98: Val Loss 2417.62549
Epoch 99: Val Loss 2393.18848
{'MSE - mean': 2526.744826035168, 'MSE - std': 262.92337733705483, 'R2 - mean': 0.6970409333712082, 'R2 - std': 0.03481939449760906} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 2526.744826035168 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516456.81250
Epoch 1: Val Loss 516421.50000
Epoch 2: Val Loss 516385.31250
Epoch 3: Val Loss 516348.25000
Epoch 4: Val Loss 516309.78125
Epoch 5: Val Loss 516269.84375
Epoch 6: Val Loss 516228.31250
Epoch 7: Val Loss 516185.09375
Epoch 8: Val Loss 516140.00000
Epoch 9: Val Loss 516092.21875
Epoch 10: Val Loss 516041.34375
Epoch 11: Val Loss 515987.00000
Epoch 12: Val Loss 515928.68750
Epoch 13: Val Loss 515865.78125
Epoch 14: Val Loss 515797.90625
Epoch 15: Val Loss 515724.71875
Epoch 16: Val Loss 515645.65625
Epoch 17: Val Loss 515559.65625
Epoch 18: Val Loss 515466.87500
Epoch 19: Val Loss 515365.00000
Epoch 20: Val Loss 515253.25000
Epoch 21: Val Loss 515130.34375
Epoch 22: Val Loss 514995.28125
Epoch 23: Val Loss 514846.87500
Epoch 24: Val Loss 514686.25000
Epoch 25: Val Loss 514513.06250
Epoch 26: Val Loss 514323.25000
Epoch 27: Val Loss 514117.59375
Epoch 28: Val Loss 513894.62500
Epoch 29: Val Loss 513653.12500
Epoch 30: Val Loss 513391.96875
Epoch 31: Val Loss 513111.00000
Epoch 32: Val Loss 512806.96875
Epoch 33: Val Loss 512476.96875
Epoch 34: Val Loss 512122.62500
Epoch 35: Val Loss 511742.65625
Epoch 36: Val Loss 511335.84375
Epoch 37: Val Loss 510902.93750
Epoch 38: Val Loss 510436.93750
Epoch 39: Val Loss 509942.15625
Epoch 40: Val Loss 509416.71875
Epoch 41: Val Loss 508857.40625
Epoch 42: Val Loss 508267.21875
Epoch 43: Val Loss 507632.06250
Epoch 44: Val Loss 506960.46875
Epoch 45: Val Loss 506249.68750
Epoch 46: Val Loss 505497.31250
Epoch 47: Val Loss 504707.03125
Epoch 48: Val Loss 503864.25000
Epoch 49: Val Loss 502979.18750
Epoch 50: Val Loss 502046.28125
Epoch 51: Val Loss 501065.06250
Epoch 52: Val Loss 500029.09375
Epoch 53: Val Loss 498934.68750
Epoch 54: Val Loss 497804.15625
Epoch 55: Val Loss 496601.09375
Epoch 56: Val Loss 495357.62500
Epoch 57: Val Loss 494048.84375
Epoch 58: Val Loss 492685.12500
Epoch 59: Val Loss 491252.06250
Epoch 60: Val Loss 489752.25000
Epoch 61: Val Loss 488188.59375
Epoch 62: Val Loss 486566.21875
Epoch 63: Val Loss 484896.06250
Epoch 64: Val Loss 483168.56250
Epoch 65: Val Loss 481360.71875
Epoch 66: Val Loss 479482.75000
Epoch 67: Val Loss 477530.81250
Epoch 68: Val Loss 475503.03125
Epoch 69: Val Loss 473429.93750
Epoch 70: Val Loss 471280.53125
Epoch 71: Val Loss 469079.46875
Epoch 72: Val Loss 466778.62500
Epoch 73: Val Loss 464403.84375
Epoch 74: Val Loss 461960.46875
Epoch 75: Val Loss 459434.50000
Epoch 76: Val Loss 456848.90625
Epoch 77: Val Loss 454179.50000
Epoch 78: Val Loss 451438.03125
Epoch 79: Val Loss 448609.21875
Epoch 80: Val Loss 445743.43750
Epoch 81: Val Loss 442770.28125
Epoch 82: Val Loss 439748.93750
Epoch 83: Val Loss 436648.50000
Epoch 84: Val Loss 433503.00000
Epoch 85: Val Loss 430270.53125
Epoch 86: Val Loss 426949.31250
Epoch 87: Val Loss 423575.84375
Epoch 88: Val Loss 420148.09375
Epoch 89: Val Loss 416635.87500
Epoch 90: Val Loss 413090.78125
Epoch 91: Val Loss 409437.34375
Epoch 92: Val Loss 405718.09375
Epoch 93: Val Loss 401957.25000
Epoch 94: Val Loss 398124.09375
Epoch 95: Val Loss 394238.28125
Epoch 96: Val Loss 390329.28125
Epoch 97: Val Loss 386285.21875
Epoch 98: Val Loss 382203.06250
Epoch 99: Val Loss 378030.96875
{'MSE - mean': 378030.95722421317, 'MSE - std': 0.0, 'R2 - mean': -42.95402093682228, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525496.87500
Epoch 1: Val Loss 525469.87500
Epoch 2: Val Loss 525444.06250
Epoch 3: Val Loss 525418.87500
Epoch 4: Val Loss 525393.81250
Epoch 5: Val Loss 525368.43750
Epoch 6: Val Loss 525342.31250
Epoch 7: Val Loss 525315.37500
Epoch 8: Val Loss 525288.06250
Epoch 9: Val Loss 525260.50000
Epoch 10: Val Loss 525232.12500
Epoch 11: Val Loss 525203.12500
Epoch 12: Val Loss 525172.87500
Epoch 13: Val Loss 525140.31250
Epoch 14: Val Loss 525106.06250
Epoch 15: Val Loss 525069.50000
Epoch 16: Val Loss 525030.62500
Epoch 17: Val Loss 524987.68750
Epoch 18: Val Loss 524942.18750
Epoch 19: Val Loss 524893.18750
Epoch 20: Val Loss 524839.62500
Epoch 21: Val Loss 524781.56250
Epoch 22: Val Loss 524716.75000
Epoch 23: Val Loss 524646.06250
Epoch 24: Val Loss 524569.25000
Epoch 25: Val Loss 524486.62500
Epoch 26: Val Loss 524397.56250
Epoch 27: Val Loss 524300.68750
Epoch 28: Val Loss 524197.50000
Epoch 29: Val Loss 524085.62500
Epoch 30: Val Loss 523967.90625
Epoch 31: Val Loss 523840.15625
Epoch 32: Val Loss 523705.28125
Epoch 33: Val Loss 523560.84375
Epoch 34: Val Loss 523405.62500
Epoch 35: Val Loss 523240.25000
Epoch 36: Val Loss 523062.12500
Epoch 37: Val Loss 522870.43750
Epoch 38: Val Loss 522666.25000
Epoch 39: Val Loss 522447.81250
Epoch 40: Val Loss 522213.28125
Epoch 41: Val Loss 521961.84375
Epoch 42: Val Loss 521695.68750
Epoch 43: Val Loss 521407.87500
Epoch 44: Val Loss 521095.90625
Epoch 45: Val Loss 520765.00000
Epoch 46: Val Loss 520410.81250
Epoch 47: Val Loss 520035.25000
Epoch 48: Val Loss 519628.12500
Epoch 49: Val Loss 519193.28125
Epoch 50: Val Loss 518730.25000
Epoch 51: Val Loss 518235.40625
Epoch 52: Val Loss 517706.37500
Epoch 53: Val Loss 517158.37500
Epoch 54: Val Loss 516558.96875
Epoch 55: Val Loss 515944.00000
Epoch 56: Val Loss 515277.96875
Epoch 57: Val Loss 514583.90625
Epoch 58: Val Loss 513860.28125
Epoch 59: Val Loss 513106.59375
Epoch 60: Val Loss 512321.81250
Epoch 61: Val Loss 511490.06250
Epoch 62: Val Loss 510620.37500
Epoch 63: Val Loss 509715.46875
Epoch 64: Val Loss 508784.65625
Epoch 65: Val Loss 507801.96875
Epoch 66: Val Loss 506787.71875
Epoch 67: Val Loss 505725.68750
Epoch 68: Val Loss 504626.78125
Epoch 69: Val Loss 503506.53125
Epoch 70: Val Loss 502319.15625
Epoch 71: Val Loss 501107.53125
Epoch 72: Val Loss 499858.75000
Epoch 73: Val Loss 498553.46875
Epoch 74: Val Loss 497197.03125
Epoch 75: Val Loss 495800.59375
Epoch 76: Val Loss 494352.50000
Epoch 77: Val Loss 492850.46875
Epoch 78: Val Loss 491317.12500
Epoch 79: Val Loss 489716.65625
Epoch 80: Val Loss 488082.03125
Epoch 81: Val Loss 486406.59375
Epoch 82: Val Loss 484679.62500
Epoch 83: Val Loss 482902.71875
Epoch 84: Val Loss 481089.28125
Epoch 85: Val Loss 479238.43750
Epoch 86: Val Loss 477299.53125
Epoch 87: Val Loss 475337.56250
Epoch 88: Val Loss 473325.62500
Epoch 89: Val Loss 471239.12500
Epoch 90: Val Loss 469135.56250
Epoch 91: Val Loss 466978.03125
Epoch 92: Val Loss 464778.03125
Epoch 93: Val Loss 462510.06250
Epoch 94: Val Loss 460199.00000
Epoch 95: Val Loss 457868.18750
Epoch 96: Val Loss 455477.43750
Epoch 97: Val Loss 453017.34375
Epoch 98: Val Loss 450508.12500
Epoch 99: Val Loss 447948.75000
{'MSE - mean': 412989.8466396545, 'MSE - std': 34958.88941544134, 'R2 - mean': -50.291203126255255, 'R2 - std': 7.337182189432973} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518299.87500
Epoch 1: Val Loss 518263.00000
Epoch 2: Val Loss 518226.37500
Epoch 3: Val Loss 518189.25000
Epoch 4: Val Loss 518150.75000
Epoch 5: Val Loss 518110.50000
Epoch 6: Val Loss 518067.65625
Epoch 7: Val Loss 518022.21875
Epoch 8: Val Loss 517973.62500
Epoch 9: Val Loss 517920.93750
Epoch 10: Val Loss 517863.53125
Epoch 11: Val Loss 517801.28125
Epoch 12: Val Loss 517733.31250
Epoch 13: Val Loss 517658.53125
Epoch 14: Val Loss 517576.37500
Epoch 15: Val Loss 517486.09375
Epoch 16: Val Loss 517386.90625
Epoch 17: Val Loss 517277.71875
Epoch 18: Val Loss 517157.65625
Epoch 19: Val Loss 517025.18750
Epoch 20: Val Loss 516879.65625
Epoch 21: Val Loss 516719.56250
Epoch 22: Val Loss 516543.87500
Epoch 23: Val Loss 516350.37500
Epoch 24: Val Loss 516141.50000
Epoch 25: Val Loss 515910.15625
Epoch 26: Val Loss 515660.93750
Epoch 27: Val Loss 515389.93750
Epoch 28: Val Loss 515096.31250
Epoch 29: Val Loss 514778.50000
Epoch 30: Val Loss 514432.96875
Epoch 31: Val Loss 514061.56250
Epoch 32: Val Loss 513663.25000
Epoch 33: Val Loss 513231.53125
Epoch 34: Val Loss 512774.34375
Epoch 35: Val Loss 512275.84375
Epoch 36: Val Loss 511745.15625
Epoch 37: Val Loss 511178.06250
Epoch 38: Val Loss 510570.78125
Epoch 39: Val Loss 509923.25000
Epoch 40: Val Loss 509232.09375
Epoch 41: Val Loss 508499.65625
Epoch 42: Val Loss 507722.81250
Epoch 43: Val Loss 506893.40625
Epoch 44: Val Loss 506019.31250
Epoch 45: Val Loss 505091.37500
Epoch 46: Val Loss 504104.18750
Epoch 47: Val Loss 503074.90625
Epoch 48: Val Loss 501979.00000
Epoch 49: Val Loss 500830.12500
Epoch 50: Val Loss 499639.12500
Epoch 51: Val Loss 498371.03125
Epoch 52: Val Loss 497043.68750
Epoch 53: Val Loss 495654.34375
Epoch 54: Val Loss 494210.96875
Epoch 55: Val Loss 492695.56250
Epoch 56: Val Loss 491098.59375
Epoch 57: Val Loss 489441.34375
Epoch 58: Val Loss 487708.65625
Epoch 59: Val Loss 485899.43750
Epoch 60: Val Loss 484029.00000
Epoch 61: Val Loss 482063.78125
Epoch 62: Val Loss 480026.90625
Epoch 63: Val Loss 477906.31250
Epoch 64: Val Loss 475725.12500
Epoch 65: Val Loss 473464.25000
Epoch 66: Val Loss 471135.40625
Epoch 67: Val Loss 468700.09375
Epoch 68: Val Loss 466218.12500
Epoch 69: Val Loss 463622.81250
Epoch 70: Val Loss 460961.87500
Epoch 71: Val Loss 458190.56250
Epoch 72: Val Loss 455346.59375
Epoch 73: Val Loss 452407.03125
Epoch 74: Val Loss 449394.06250
Epoch 75: Val Loss 446297.81250
Epoch 76: Val Loss 443138.81250
Epoch 77: Val Loss 439836.71875
Epoch 78: Val Loss 436487.06250
Epoch 79: Val Loss 433063.81250
Epoch 80: Val Loss 429562.59375
Epoch 81: Val Loss 425959.12500
Epoch 82: Val Loss 422281.09375
Epoch 83: Val Loss 418513.81250
Epoch 84: Val Loss 414693.56250
Epoch 85: Val Loss 410763.15625
Epoch 86: Val Loss 406747.62500
Epoch 87: Val Loss 402695.53125
Epoch 88: Val Loss 398550.56250
Epoch 89: Val Loss 394326.68750
Epoch 90: Val Loss 390063.00000
Epoch 91: Val Loss 385695.71875
Epoch 92: Val Loss 381259.09375
Epoch 93: Val Loss 376756.96875
Epoch 94: Val Loss 372190.03125
Epoch 95: Val Loss 367559.46875
Epoch 96: Val Loss 362862.56250
Epoch 97: Val Loss 358140.21875
Epoch 98: Val Loss 353297.12500
Epoch 99: Val Loss 348410.12500
{'MSE - mean': 391463.2752091601, 'MSE - std': 41731.712815855695, 'R2 - mean': -45.670639399130124, 'R2 - std': 8.865027540856103} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520753.18750
Epoch 1: Val Loss 520718.93750
Epoch 2: Val Loss 520683.03125
Epoch 3: Val Loss 520644.78125
Epoch 4: Val Loss 520604.34375
Epoch 5: Val Loss 520561.37500
Epoch 6: Val Loss 520515.56250
Epoch 7: Val Loss 520466.18750
Epoch 8: Val Loss 520412.81250
Epoch 9: Val Loss 520355.15625
Epoch 10: Val Loss 520292.28125
Epoch 11: Val Loss 520223.96875
Epoch 12: Val Loss 520148.78125
Epoch 13: Val Loss 520067.03125
Epoch 14: Val Loss 519977.84375
Epoch 15: Val Loss 519879.71875
Epoch 16: Val Loss 519771.68750
Epoch 17: Val Loss 519652.78125
Epoch 18: Val Loss 519522.28125
Epoch 19: Val Loss 519378.53125
Epoch 20: Val Loss 519221.81250
Epoch 21: Val Loss 519049.56250
Epoch 22: Val Loss 518864.03125
Epoch 23: Val Loss 518661.53125
Epoch 24: Val Loss 518441.87500
Epoch 25: Val Loss 518203.87500
Epoch 26: Val Loss 517943.00000
Epoch 27: Val Loss 517663.03125
Epoch 28: Val Loss 517358.56250
Epoch 29: Val Loss 517026.96875
Epoch 30: Val Loss 516670.87500
Epoch 31: Val Loss 516286.06250
Epoch 32: Val Loss 515872.12500
Epoch 33: Val Loss 515418.28125
Epoch 34: Val Loss 514933.93750
Epoch 35: Val Loss 514420.75000
Epoch 36: Val Loss 513869.43750
Epoch 37: Val Loss 513281.40625
Epoch 38: Val Loss 512656.31250
Epoch 39: Val Loss 511989.46875
Epoch 40: Val Loss 511281.50000
Epoch 41: Val Loss 510534.68750
Epoch 42: Val Loss 509739.93750
Epoch 43: Val Loss 508900.46875
Epoch 44: Val Loss 508011.93750
Epoch 45: Val Loss 507070.40625
Epoch 46: Val Loss 506084.21875
Epoch 47: Val Loss 505029.56250
Epoch 48: Val Loss 503932.62500
Epoch 49: Val Loss 502771.31250
Epoch 50: Val Loss 501548.18750
Epoch 51: Val Loss 500279.12500
Epoch 52: Val Loss 498935.03125
Epoch 53: Val Loss 497538.62500
Epoch 54: Val Loss 496078.21875
Epoch 55: Val Loss 494551.00000
Epoch 56: Val Loss 492942.31250
Epoch 57: Val Loss 491276.62500
Epoch 58: Val Loss 489534.84375
Epoch 59: Val Loss 487734.62500
Epoch 60: Val Loss 485854.12500
Epoch 61: Val Loss 483895.56250
Epoch 62: Val Loss 481853.00000
Epoch 63: Val Loss 479740.00000
Epoch 64: Val Loss 477540.00000
Epoch 65: Val Loss 475276.21875
Epoch 66: Val Loss 472944.09375
Epoch 67: Val Loss 470542.09375
Epoch 68: Val Loss 468014.62500
Epoch 69: Val Loss 465416.87500
Epoch 70: Val Loss 462742.40625
Epoch 71: Val Loss 460003.75000
Epoch 72: Val Loss 457186.46875
Epoch 73: Val Loss 454276.06250
Epoch 74: Val Loss 451272.56250
Epoch 75: Val Loss 448207.78125
Epoch 76: Val Loss 445030.87500
Epoch 77: Val Loss 441802.96875
Epoch 78: Val Loss 438480.25000
Epoch 79: Val Loss 435044.62500
Epoch 80: Val Loss 431550.81250
Epoch 81: Val Loss 427961.78125
Epoch 82: Val Loss 424311.90625
Epoch 83: Val Loss 420561.03125
Epoch 84: Val Loss 416765.25000
Epoch 85: Val Loss 412820.84375
Epoch 86: Val Loss 408831.59375
Epoch 87: Val Loss 404804.50000
Epoch 88: Val Loss 400651.62500
Epoch 89: Val Loss 396415.18750
Epoch 90: Val Loss 392126.09375
Epoch 91: Val Loss 387759.34375
Epoch 92: Val Loss 383296.37500
Epoch 93: Val Loss 378767.28125
Epoch 94: Val Loss 374207.37500
Epoch 95: Val Loss 369558.00000
Epoch 96: Val Loss 364864.84375
Epoch 97: Val Loss 360110.37500
Epoch 98: Val Loss 355268.75000
Epoch 99: Val Loss 350395.37500
{'MSE - mean': 381196.29564852384, 'MSE - std': 40278.83437742907, 'R2 - mean': -44.550048583328945, 'R2 - std': 7.918882894984417} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517917.18750
Epoch 1: Val Loss 517871.90625
Epoch 2: Val Loss 517824.00000
Epoch 3: Val Loss 517773.59375
Epoch 4: Val Loss 517719.84375
Epoch 5: Val Loss 517662.21875
Epoch 6: Val Loss 517600.37500
Epoch 7: Val Loss 517534.34375
Epoch 8: Val Loss 517464.25000
Epoch 9: Val Loss 517388.71875
Epoch 10: Val Loss 517308.09375
Epoch 11: Val Loss 517221.21875
Epoch 12: Val Loss 517127.78125
Epoch 13: Val Loss 517026.28125
Epoch 14: Val Loss 516916.46875
Epoch 15: Val Loss 516798.21875
Epoch 16: Val Loss 516671.00000
Epoch 17: Val Loss 516532.31250
Epoch 18: Val Loss 516381.90625
Epoch 19: Val Loss 516220.15625
Epoch 20: Val Loss 516042.12500
Epoch 21: Val Loss 515850.40625
Epoch 22: Val Loss 515640.90625
Epoch 23: Val Loss 515415.03125
Epoch 24: Val Loss 515169.21875
Epoch 25: Val Loss 514904.03125
Epoch 26: Val Loss 514616.62500
Epoch 27: Val Loss 514302.50000
Epoch 28: Val Loss 513963.93750
Epoch 29: Val Loss 513595.68750
Epoch 30: Val Loss 513199.37500
Epoch 31: Val Loss 512770.87500
Epoch 32: Val Loss 512314.15625
Epoch 33: Val Loss 511823.18750
Epoch 34: Val Loss 511292.53125
Epoch 35: Val Loss 510728.96875
Epoch 36: Val Loss 510119.53125
Epoch 37: Val Loss 509480.65625
Epoch 38: Val Loss 508794.31250
Epoch 39: Val Loss 508066.31250
Epoch 40: Val Loss 507289.40625
Epoch 41: Val Loss 506464.37500
Epoch 42: Val Loss 505590.40625
Epoch 43: Val Loss 504670.68750
Epoch 44: Val Loss 503690.71875
Epoch 45: Val Loss 502658.28125
Epoch 46: Val Loss 501559.34375
Epoch 47: Val Loss 500409.59375
Epoch 48: Val Loss 499194.90625
Epoch 49: Val Loss 497921.15625
Epoch 50: Val Loss 496580.31250
Epoch 51: Val Loss 495167.78125
Epoch 52: Val Loss 493695.53125
Epoch 53: Val Loss 492147.62500
Epoch 54: Val Loss 490528.31250
Epoch 55: Val Loss 488816.31250
Epoch 56: Val Loss 487036.00000
Epoch 57: Val Loss 485190.96875
Epoch 58: Val Loss 483271.53125
Epoch 59: Val Loss 481264.15625
Epoch 60: Val Loss 479171.09375
Epoch 61: Val Loss 476991.78125
Epoch 62: Val Loss 474745.71875
Epoch 63: Val Loss 472419.68750
Epoch 64: Val Loss 470002.31250
Epoch 65: Val Loss 467509.46875
Epoch 66: Val Loss 464916.84375
Epoch 67: Val Loss 462268.78125
Epoch 68: Val Loss 459497.78125
Epoch 69: Val Loss 456645.18750
Epoch 70: Val Loss 453700.71875
Epoch 71: Val Loss 450718.18750
Epoch 72: Val Loss 447595.93750
Epoch 73: Val Loss 444433.43750
Epoch 74: Val Loss 441146.25000
Epoch 75: Val Loss 437817.81250
Epoch 76: Val Loss 434345.50000
Epoch 77: Val Loss 430813.84375
Epoch 78: Val Loss 427187.09375
Epoch 79: Val Loss 423500.87500
Epoch 80: Val Loss 419715.87500
Epoch 81: Val Loss 415838.50000
Epoch 82: Val Loss 411861.75000
Epoch 83: Val Loss 407761.34375
Epoch 84: Val Loss 403593.75000
Epoch 85: Val Loss 399341.34375
Epoch 86: Val Loss 395011.75000
Epoch 87: Val Loss 390620.03125
Epoch 88: Val Loss 386118.56250
Epoch 89: Val Loss 381552.53125
Epoch 90: Val Loss 376926.28125
Epoch 91: Val Loss 372229.43750
Epoch 92: Val Loss 367501.59375
Epoch 93: Val Loss 362678.93750
Epoch 94: Val Loss 357784.50000
Epoch 95: Val Loss 352827.62500
Epoch 96: Val Loss 347825.00000
Epoch 97: Val Loss 342753.25000
Epoch 98: Val Loss 337685.75000
Epoch 99: Val Loss 332503.50000
{'MSE - mean': 371457.7405617035, 'MSE - std': 40954.4310149662, 'R2 - mean': -43.7755104384367, 'R2 - std': 7.250282918283529} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 371457.7405617035 and parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516143.87500
Epoch 1: Val Loss 515746.93750
Epoch 2: Val Loss 514977.37500
Epoch 3: Val Loss 513564.75000
Epoch 4: Val Loss 510801.25000
Epoch 5: Val Loss 505585.09375
Epoch 6: Val Loss 496542.68750
Epoch 7: Val Loss 481746.68750
Epoch 8: Val Loss 458623.68750
Epoch 9: Val Loss 424248.62500
Epoch 10: Val Loss 376022.71875
Epoch 11: Val Loss 313413.12500
Epoch 12: Val Loss 239556.56250
Epoch 13: Val Loss 161664.71875
Epoch 14: Val Loss 92340.61719
Epoch 15: Val Loss 43363.20703
Epoch 16: Val Loss 18743.49023
Epoch 17: Val Loss 10974.91895
Epoch 18: Val Loss 9230.98828
Epoch 19: Val Loss 8503.43359
Epoch 20: Val Loss 8043.74316
Epoch 21: Val Loss 7681.05078
Epoch 22: Val Loss 7426.69287
Epoch 23: Val Loss 7210.70361
Epoch 24: Val Loss 7032.58887
Epoch 25: Val Loss 6868.51025
Epoch 26: Val Loss 6730.36475
Epoch 27: Val Loss 6615.32422
Epoch 28: Val Loss 6494.78418
Epoch 29: Val Loss 6376.86670
Epoch 30: Val Loss 6270.05371
Epoch 31: Val Loss 6191.47803
Epoch 32: Val Loss 6107.29688
Epoch 33: Val Loss 6026.81396
Epoch 34: Val Loss 5926.26611
Epoch 35: Val Loss 5849.18604
Epoch 36: Val Loss 5780.52930
Epoch 37: Val Loss 5715.87988
Epoch 38: Val Loss 5661.46484
Epoch 39: Val Loss 5597.78467
Epoch 40: Val Loss 5528.25000
Epoch 41: Val Loss 5467.82129
Epoch 42: Val Loss 5398.73438
Epoch 43: Val Loss 5359.90771
Epoch 44: Val Loss 5306.92041
Epoch 45: Val Loss 5250.35791
Epoch 46: Val Loss 5212.39600
Epoch 47: Val Loss 5159.80469
Epoch 48: Val Loss 5138.97168
Epoch 49: Val Loss 5072.59814
Epoch 50: Val Loss 5038.61426
Epoch 51: Val Loss 4991.23291
Epoch 52: Val Loss 4950.40527
Epoch 53: Val Loss 4927.04248
Epoch 54: Val Loss 4883.96533
Epoch 55: Val Loss 4843.35303
Epoch 56: Val Loss 4805.72998
Epoch 57: Val Loss 4775.15381
Epoch 58: Val Loss 4738.62256
Epoch 59: Val Loss 4697.66113
Epoch 60: Val Loss 4681.57715
Epoch 61: Val Loss 4649.57031
Epoch 62: Val Loss 4613.74219
Epoch 63: Val Loss 4587.69141
Epoch 64: Val Loss 4532.94922
Epoch 65: Val Loss 4518.26123
Epoch 66: Val Loss 4469.20947
Epoch 67: Val Loss 4430.82617
Epoch 68: Val Loss 4421.87354
Epoch 69: Val Loss 4375.90967
Epoch 70: Val Loss 4339.32373
Epoch 71: Val Loss 4306.16699
Epoch 72: Val Loss 4288.74365
Epoch 73: Val Loss 4237.25977
Epoch 74: Val Loss 4218.87451
Epoch 75: Val Loss 4196.91211
Epoch 76: Val Loss 4142.64746
Epoch 77: Val Loss 4123.51904
Epoch 78: Val Loss 4084.95703
Epoch 79: Val Loss 4055.75854
Epoch 80: Val Loss 4038.31738
Epoch 81: Val Loss 3997.52905
Epoch 82: Val Loss 3966.20557
Epoch 83: Val Loss 3957.58057
Epoch 84: Val Loss 3920.38232
Epoch 85: Val Loss 3874.01025
Epoch 86: Val Loss 3849.82617
Epoch 87: Val Loss 3808.79565
Epoch 88: Val Loss 3790.19727
Epoch 89: Val Loss 3781.16138
Epoch 90: Val Loss 3718.05713
Epoch 91: Val Loss 3699.01147
Epoch 92: Val Loss 3671.65234
Epoch 93: Val Loss 3659.36597
Epoch 94: Val Loss 3618.53345
Epoch 95: Val Loss 3586.77539
Epoch 96: Val Loss 3575.70361
Epoch 97: Val Loss 3522.99683
Epoch 98: Val Loss 3515.40234
Epoch 99: Val Loss 3481.74194
{'MSE - mean': 3481.741843150446, 'MSE - std': 0.0, 'R2 - mean': 0.5951745460367741, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524276.84375
Epoch 1: Val Loss 523664.40625
Epoch 2: Val Loss 522396.59375
Epoch 3: Val Loss 519863.71875
Epoch 4: Val Loss 515015.68750
Epoch 5: Val Loss 506080.65625
Epoch 6: Val Loss 490615.96875
Epoch 7: Val Loss 465642.31250
Epoch 8: Val Loss 428544.09375
Epoch 9: Val Loss 377640.03125
Epoch 10: Val Loss 313580.31250
Epoch 11: Val Loss 240399.90625
Epoch 12: Val Loss 165774.42188
Epoch 13: Val Loss 101368.52344
Epoch 14: Val Loss 56311.40625
Epoch 15: Val Loss 32403.38867
Epoch 16: Val Loss 22617.11719
Epoch 17: Val Loss 18452.86719
Epoch 18: Val Loss 15824.45215
Epoch 19: Val Loss 13915.04688
Epoch 20: Val Loss 12456.82227
Epoch 21: Val Loss 11404.30566
Epoch 22: Val Loss 10540.19043
Epoch 23: Val Loss 9847.76758
Epoch 24: Val Loss 9272.58301
Epoch 25: Val Loss 8786.74902
Epoch 26: Val Loss 8391.72461
Epoch 27: Val Loss 8017.98584
Epoch 28: Val Loss 7703.66797
Epoch 29: Val Loss 7401.14160
Epoch 30: Val Loss 7137.36914
Epoch 31: Val Loss 6889.55176
Epoch 32: Val Loss 6671.75537
Epoch 33: Val Loss 6443.73096
Epoch 34: Val Loss 6248.37158
Epoch 35: Val Loss 6050.39746
Epoch 36: Val Loss 5885.92041
Epoch 37: Val Loss 5710.22461
Epoch 38: Val Loss 5542.54443
Epoch 39: Val Loss 5384.07812
Epoch 40: Val Loss 5235.08398
Epoch 41: Val Loss 5094.18896
Epoch 42: Val Loss 4970.78760
Epoch 43: Val Loss 4839.26709
Epoch 44: Val Loss 4721.24219
Epoch 45: Val Loss 4605.54785
Epoch 46: Val Loss 4525.07910
Epoch 47: Val Loss 4430.14893
Epoch 48: Val Loss 4325.78516
Epoch 49: Val Loss 4236.80908
Epoch 50: Val Loss 4162.51611
Epoch 51: Val Loss 4111.79199
Epoch 52: Val Loss 4043.88916
Epoch 53: Val Loss 3990.28394
Epoch 54: Val Loss 3904.89526
Epoch 55: Val Loss 3863.52539
Epoch 56: Val Loss 3820.57397
Epoch 57: Val Loss 3768.86914
Epoch 58: Val Loss 3728.92603
Epoch 59: Val Loss 3680.71924
Epoch 60: Val Loss 3656.87256
Epoch 61: Val Loss 3633.74658
Epoch 62: Val Loss 3575.76685
Epoch 63: Val Loss 3552.35645
Epoch 64: Val Loss 3512.80981
Epoch 65: Val Loss 3492.79907
Epoch 66: Val Loss 3458.42651
Epoch 67: Val Loss 3445.98657
Epoch 68: Val Loss 3404.73950
Epoch 69: Val Loss 3380.45557
Epoch 70: Val Loss 3379.52661
Epoch 71: Val Loss 3335.44580
Epoch 72: Val Loss 3318.64160
Epoch 73: Val Loss 3294.39307
Epoch 74: Val Loss 3297.25269
Epoch 75: Val Loss 3248.82886
Epoch 76: Val Loss 3249.47168
Epoch 77: Val Loss 3215.51929
Epoch 78: Val Loss 3187.66821
Epoch 79: Val Loss 3191.40601
Epoch 80: Val Loss 3200.11523
Epoch 81: Val Loss 3133.13965
Epoch 82: Val Loss 3119.16602
Epoch 83: Val Loss 3118.72070
Epoch 84: Val Loss 3100.20874
Epoch 85: Val Loss 3056.12305
Epoch 86: Val Loss 3057.96924
Epoch 87: Val Loss 3031.80518
Epoch 88: Val Loss 3002.72900
Epoch 89: Val Loss 2996.87305
Epoch 90: Val Loss 3002.32495
Epoch 91: Val Loss 2986.09448
Epoch 92: Val Loss 2936.64282
Epoch 93: Val Loss 2936.65137
Epoch 94: Val Loss 2934.20044
Epoch 95: Val Loss 2890.61938
Epoch 96: Val Loss 2889.02734
Epoch 97: Val Loss 2878.51392
Epoch 98: Val Loss 2856.07202
Epoch 99: Val Loss 2831.07812
{'MSE - mean': 3156.4098997312876, 'MSE - std': 325.33194341915873, 'R2 - mean': 0.6123188305363844, 'R2 - std': 0.01714428449961025} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518105.06250
Epoch 1: Val Loss 517532.71875
Epoch 2: Val Loss 516328.50000
Epoch 3: Val Loss 513878.65625
Epoch 4: Val Loss 509208.56250
Epoch 5: Val Loss 500864.31250
Epoch 6: Val Loss 486722.78125
Epoch 7: Val Loss 463923.96875
Epoch 8: Val Loss 429494.87500
Epoch 9: Val Loss 380814.31250
Epoch 10: Val Loss 317491.87500
Epoch 11: Val Loss 244270.92188
Epoch 12: Val Loss 169331.14062
Epoch 13: Val Loss 105661.64062
Epoch 14: Val Loss 61814.83594
Epoch 15: Val Loss 38417.33984
Epoch 16: Val Loss 27441.63867
Epoch 17: Val Loss 21228.96289
Epoch 18: Val Loss 17067.45117
Epoch 19: Val Loss 14107.10254
Epoch 20: Val Loss 11963.42285
Epoch 21: Val Loss 10268.84766
Epoch 22: Val Loss 8934.39453
Epoch 23: Val Loss 7942.79297
Epoch 24: Val Loss 7192.85254
Epoch 25: Val Loss 6607.49121
Epoch 26: Val Loss 6195.16748
Epoch 27: Val Loss 5809.57568
Epoch 28: Val Loss 5553.47168
Epoch 29: Val Loss 5278.39795
Epoch 30: Val Loss 5113.71191
Epoch 31: Val Loss 4964.31787
Epoch 32: Val Loss 4856.21143
Epoch 33: Val Loss 4739.70996
Epoch 34: Val Loss 4635.98975
Epoch 35: Val Loss 4559.89307
Epoch 36: Val Loss 4499.33105
Epoch 37: Val Loss 4412.93359
Epoch 38: Val Loss 4354.73047
Epoch 39: Val Loss 4269.28613
Epoch 40: Val Loss 4233.48828
Epoch 41: Val Loss 4192.97363
Epoch 42: Val Loss 4131.88330
Epoch 43: Val Loss 4064.46558
Epoch 44: Val Loss 4059.89087
Epoch 45: Val Loss 3988.84692
Epoch 46: Val Loss 3956.36450
Epoch 47: Val Loss 3902.13574
Epoch 48: Val Loss 3850.06543
Epoch 49: Val Loss 3859.04053
Epoch 50: Val Loss 3818.23975
Epoch 51: Val Loss 3787.14087
Epoch 52: Val Loss 3732.85962
Epoch 53: Val Loss 3734.42212
Epoch 54: Val Loss 3694.30054
Epoch 55: Val Loss 3638.05322
Epoch 56: Val Loss 3633.04980
Epoch 57: Val Loss 3599.64575
Epoch 58: Val Loss 3566.56641
Epoch 59: Val Loss 3534.32007
Epoch 60: Val Loss 3524.45166
Epoch 61: Val Loss 3476.91748
Epoch 62: Val Loss 3471.68433
Epoch 63: Val Loss 3454.85132
Epoch 64: Val Loss 3402.33936
Epoch 65: Val Loss 3379.35864
Epoch 66: Val Loss 3360.44409
Epoch 67: Val Loss 3347.45825
Epoch 68: Val Loss 3307.47437
Epoch 69: Val Loss 3305.13354
Epoch 70: Val Loss 3263.49023
Epoch 71: Val Loss 3239.97900
Epoch 72: Val Loss 3241.25684
Epoch 73: Val Loss 3184.39209
Epoch 74: Val Loss 3171.41406
Epoch 75: Val Loss 3179.42065
Epoch 76: Val Loss 3140.24927
Epoch 77: Val Loss 3098.26050
Epoch 78: Val Loss 3080.03613
Epoch 79: Val Loss 3088.95337
Epoch 80: Val Loss 3039.75659
Epoch 81: Val Loss 3029.89014
Epoch 82: Val Loss 3009.37329
Epoch 83: Val Loss 2996.35669
Epoch 84: Val Loss 2949.83618
Epoch 85: Val Loss 2958.25928
Epoch 86: Val Loss 2933.61816
Epoch 87: Val Loss 2905.96240
Epoch 88: Val Loss 2888.64160
Epoch 89: Val Loss 2867.97485
Epoch 90: Val Loss 2852.11011
Epoch 91: Val Loss 2820.20679
Epoch 92: Val Loss 2823.88525
Epoch 93: Val Loss 2783.25439
Epoch 94: Val Loss 2762.47656
Epoch 95: Val Loss 2774.23364
Epoch 96: Val Loss 2736.27100
Epoch 97: Val Loss 2707.23730
Epoch 98: Val Loss 2712.61060
Epoch 99: Val Loss 2694.09888
{'MSE - mean': 3002.3062797688303, 'MSE - std': 343.59341327164304, 'R2 - mean': 0.6450706864582775, 'R2 - std': 0.04838717936250541} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520519.56250
Epoch 1: Val Loss 519727.43750
Epoch 2: Val Loss 518125.31250
Epoch 3: Val Loss 514841.34375
Epoch 4: Val Loss 508492.40625
Epoch 5: Val Loss 497118.75000
Epoch 6: Val Loss 478229.65625
Epoch 7: Val Loss 448847.46875
Epoch 8: Val Loss 406268.87500
Epoch 9: Val Loss 349309.65625
Epoch 10: Val Loss 280989.81250
Epoch 11: Val Loss 205410.59375
Epoch 12: Val Loss 134142.32812
Epoch 13: Val Loss 79013.57812
Epoch 14: Val Loss 45836.70703
Epoch 15: Val Loss 30870.96875
Epoch 16: Val Loss 24358.72656
Epoch 17: Val Loss 20336.14062
Epoch 18: Val Loss 17309.61914
Epoch 19: Val Loss 14993.41699
Epoch 20: Val Loss 13223.10449
Epoch 21: Val Loss 11797.68457
Epoch 22: Val Loss 10662.48535
Epoch 23: Val Loss 9779.03613
Epoch 24: Val Loss 8973.74023
Epoch 25: Val Loss 8341.92285
Epoch 26: Val Loss 7798.32275
Epoch 27: Val Loss 7330.41846
Epoch 28: Val Loss 6951.60205
Epoch 29: Val Loss 6601.97559
Epoch 30: Val Loss 6343.52539
Epoch 31: Val Loss 6060.04688
Epoch 32: Val Loss 5818.16602
Epoch 33: Val Loss 5602.82715
Epoch 34: Val Loss 5402.89404
Epoch 35: Val Loss 5240.57715
Epoch 36: Val Loss 5056.56250
Epoch 37: Val Loss 4946.35889
Epoch 38: Val Loss 4794.91016
Epoch 39: Val Loss 4676.72363
Epoch 40: Val Loss 4514.56934
Epoch 41: Val Loss 4423.41602
Epoch 42: Val Loss 4331.59424
Epoch 43: Val Loss 4240.35986
Epoch 44: Val Loss 4148.71826
Epoch 45: Val Loss 4083.46045
Epoch 46: Val Loss 3985.76318
Epoch 47: Val Loss 3943.13110
Epoch 48: Val Loss 3866.43384
Epoch 49: Val Loss 3818.14893
Epoch 50: Val Loss 3739.71875
Epoch 51: Val Loss 3700.60547
Epoch 52: Val Loss 3651.13037
Epoch 53: Val Loss 3620.88379
Epoch 54: Val Loss 3553.23706
Epoch 55: Val Loss 3523.70605
Epoch 56: Val Loss 3476.45605
Epoch 57: Val Loss 3437.68677
Epoch 58: Val Loss 3400.57275
Epoch 59: Val Loss 3371.09814
Epoch 60: Val Loss 3354.77222
Epoch 61: Val Loss 3295.41943
Epoch 62: Val Loss 3271.18530
Epoch 63: Val Loss 3249.55127
Epoch 64: Val Loss 3211.44238
Epoch 65: Val Loss 3168.11938
Epoch 66: Val Loss 3169.80518
Epoch 67: Val Loss 3120.70825
Epoch 68: Val Loss 3117.81836
Epoch 69: Val Loss 3076.64062
Epoch 70: Val Loss 3048.46533
Epoch 71: Val Loss 3034.45874
Epoch 72: Val Loss 3017.03687
Epoch 73: Val Loss 2977.52124
Epoch 74: Val Loss 2960.07544
Epoch 75: Val Loss 2921.79468
Epoch 76: Val Loss 2891.94360
Epoch 77: Val Loss 2904.53198
Epoch 78: Val Loss 2879.12085
Epoch 79: Val Loss 2835.89282
Epoch 80: Val Loss 2825.15698
Epoch 81: Val Loss 2787.39868
Epoch 82: Val Loss 2771.43994
Epoch 83: Val Loss 2753.92578
Epoch 84: Val Loss 2761.63306
Epoch 85: Val Loss 2705.66357
Epoch 86: Val Loss 2709.90039
Epoch 87: Val Loss 2695.49146
Epoch 88: Val Loss 2670.60010
Epoch 89: Val Loss 2633.71631
Epoch 90: Val Loss 2643.81641
Epoch 91: Val Loss 2594.71021
Epoch 92: Val Loss 2592.95312
Epoch 93: Val Loss 2567.66821
Epoch 94: Val Loss 2545.38086
Epoch 95: Val Loss 2526.74487
Epoch 96: Val Loss 2515.26294
Epoch 97: Val Loss 2497.43091
Epoch 98: Val Loss 2475.44531
Epoch 99: Val Loss 2454.38354
{'MSE - mean': 2865.325625945528, 'MSE - std': 380.5698671706965, 'R2 - mean': 0.6599248842394634, 'R2 - std': 0.0491724609935503} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517720.46875
Epoch 1: Val Loss 517137.65625
Epoch 2: Val Loss 516086.43750
Epoch 3: Val Loss 514114.78125
Epoch 4: Val Loss 510432.43750
Epoch 5: Val Loss 503813.25000
Epoch 6: Val Loss 492420.40625
Epoch 7: Val Loss 473998.96875
Epoch 8: Val Loss 445926.03125
Epoch 9: Val Loss 406082.87500
Epoch 10: Val Loss 353983.34375
Epoch 11: Val Loss 291322.84375
Epoch 12: Val Loss 222052.31250
Epoch 13: Val Loss 155322.04688
Epoch 14: Val Loss 99064.81250
Epoch 15: Val Loss 60338.70312
Epoch 16: Val Loss 38768.47656
Epoch 17: Val Loss 28396.06445
Epoch 18: Val Loss 22874.92969
Epoch 19: Val Loss 19291.25000
Epoch 20: Val Loss 16789.28320
Epoch 21: Val Loss 14867.23242
Epoch 22: Val Loss 13241.87988
Epoch 23: Val Loss 11983.05469
Epoch 24: Val Loss 10902.62598
Epoch 25: Val Loss 9903.22559
Epoch 26: Val Loss 9135.64453
Epoch 27: Val Loss 8506.30859
Epoch 28: Val Loss 7936.92920
Epoch 29: Val Loss 7424.00195
Epoch 30: Val Loss 7008.25586
Epoch 31: Val Loss 6598.91357
Epoch 32: Val Loss 6300.29346
Epoch 33: Val Loss 5971.65430
Epoch 34: Val Loss 5721.25732
Epoch 35: Val Loss 5493.36377
Epoch 36: Val Loss 5253.25879
Epoch 37: Val Loss 5075.24268
Epoch 38: Val Loss 4922.93018
Epoch 39: Val Loss 4769.64062
Epoch 40: Val Loss 4623.18311
Epoch 41: Val Loss 4497.78320
Epoch 42: Val Loss 4363.78076
Epoch 43: Val Loss 4242.64014
Epoch 44: Val Loss 4137.93115
Epoch 45: Val Loss 4063.36548
Epoch 46: Val Loss 3972.59082
Epoch 47: Val Loss 3900.92212
Epoch 48: Val Loss 3799.60791
Epoch 49: Val Loss 3736.42700
Epoch 50: Val Loss 3693.86597
Epoch 51: Val Loss 3619.36670
Epoch 52: Val Loss 3566.55542
Epoch 53: Val Loss 3502.94727
Epoch 54: Val Loss 3452.46338
Epoch 55: Val Loss 3408.93164
Epoch 56: Val Loss 3357.37891
Epoch 57: Val Loss 3332.35547
Epoch 58: Val Loss 3288.64819
Epoch 59: Val Loss 3237.44849
Epoch 60: Val Loss 3195.37329
Epoch 61: Val Loss 3175.88257
Epoch 62: Val Loss 3140.50903
Epoch 63: Val Loss 3107.15283
Epoch 64: Val Loss 3071.89844
Epoch 65: Val Loss 3037.30176
Epoch 66: Val Loss 3015.62598
Epoch 67: Val Loss 2991.86011
Epoch 68: Val Loss 2955.08838
Epoch 69: Val Loss 2931.40210
Epoch 70: Val Loss 2898.77588
Epoch 71: Val Loss 2879.05420
Epoch 72: Val Loss 2850.29468
Epoch 73: Val Loss 2822.99487
Epoch 74: Val Loss 2803.74829
Epoch 75: Val Loss 2766.65381
Epoch 76: Val Loss 2748.71045
Epoch 77: Val Loss 2736.03027
Epoch 78: Val Loss 2696.04248
Epoch 79: Val Loss 2689.33325
Epoch 80: Val Loss 2662.91406
Epoch 81: Val Loss 2634.21069
Epoch 82: Val Loss 2609.49487
Epoch 83: Val Loss 2582.66357
Epoch 84: Val Loss 2568.87329
Epoch 85: Val Loss 2542.31030
Epoch 86: Val Loss 2546.86841
Epoch 87: Val Loss 2497.57568
Epoch 88: Val Loss 2483.02075
Epoch 89: Val Loss 2475.26587
Epoch 90: Val Loss 2443.33130
Epoch 91: Val Loss 2421.21436
Epoch 92: Val Loss 2409.93970
Epoch 93: Val Loss 2403.80933
Epoch 94: Val Loss 2375.22754
Epoch 95: Val Loss 2353.67114
Epoch 96: Val Loss 2330.02612
Epoch 97: Val Loss 2324.27637
Epoch 98: Val Loss 2296.80029
Epoch 99: Val Loss 2286.93750
{'MSE - mean': 2749.647982536376, 'MSE - std': 411.5726032882134, 'R2 - mean': 0.670609080499824, 'R2 - std': 0.04889737145337561} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 2749.647982536376 and parameters: {'dim': 64, 'depth': 2, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516659.00000
Epoch 1: Val Loss 516628.78125
Epoch 2: Val Loss 516598.37500
Epoch 3: Val Loss 516567.31250
Epoch 4: Val Loss 516534.75000
Epoch 5: Val Loss 516500.46875
Epoch 6: Val Loss 516464.31250
Epoch 7: Val Loss 516426.37500
Epoch 8: Val Loss 516386.46875
Epoch 9: Val Loss 516343.96875
Epoch 10: Val Loss 516298.18750
Epoch 11: Val Loss 516249.09375
Epoch 12: Val Loss 516196.56250
Epoch 13: Val Loss 516139.81250
Epoch 14: Val Loss 516078.12500
Epoch 15: Val Loss 516010.40625
Epoch 16: Val Loss 515936.15625
Epoch 17: Val Loss 515855.84375
Epoch 18: Val Loss 515767.25000
Epoch 19: Val Loss 515670.09375
Epoch 20: Val Loss 515563.15625
Epoch 21: Val Loss 515445.93750
Epoch 22: Val Loss 515317.25000
Epoch 23: Val Loss 515178.09375
Epoch 24: Val Loss 515026.18750
Epoch 25: Val Loss 514858.37500
Epoch 26: Val Loss 514677.25000
Epoch 27: Val Loss 514479.59375
Epoch 28: Val Loss 514268.00000
Epoch 29: Val Loss 514034.84375
Epoch 30: Val Loss 513783.09375
Epoch 31: Val Loss 513509.81250
Epoch 32: Val Loss 513213.75000
Epoch 33: Val Loss 512897.68750
Epoch 34: Val Loss 512552.03125
Epoch 35: Val Loss 512183.06250
Epoch 36: Val Loss 511782.93750
Epoch 37: Val Loss 511353.34375
Epoch 38: Val Loss 510896.03125
Epoch 39: Val Loss 510402.09375
Epoch 40: Val Loss 509877.53125
Epoch 41: Val Loss 509311.68750
Epoch 42: Val Loss 508707.43750
Epoch 43: Val Loss 508061.59375
Epoch 44: Val Loss 507376.90625
Epoch 45: Val Loss 506648.90625
Epoch 46: Val Loss 505872.12500
Epoch 47: Val Loss 505045.06250
Epoch 48: Val Loss 504177.06250
Epoch 49: Val Loss 503242.56250
Epoch 50: Val Loss 502261.37500
Epoch 51: Val Loss 501225.28125
Epoch 52: Val Loss 500130.40625
Epoch 53: Val Loss 498980.28125
Epoch 54: Val Loss 497765.06250
Epoch 55: Val Loss 496494.37500
Epoch 56: Val Loss 495155.06250
Epoch 57: Val Loss 493736.75000
Epoch 58: Val Loss 492261.06250
Epoch 59: Val Loss 490697.50000
Epoch 60: Val Loss 489081.03125
Epoch 61: Val Loss 487389.56250
Epoch 62: Val Loss 485627.81250
Epoch 63: Val Loss 483767.53125
Epoch 64: Val Loss 481841.50000
Epoch 65: Val Loss 479868.71875
Epoch 66: Val Loss 477789.34375
Epoch 67: Val Loss 475667.53125
Epoch 68: Val Loss 473437.93750
Epoch 69: Val Loss 471113.40625
Epoch 70: Val Loss 468739.00000
Epoch 71: Val Loss 466269.68750
Epoch 72: Val Loss 463723.21875
Epoch 73: Val Loss 461099.21875
Epoch 74: Val Loss 458387.25000
Epoch 75: Val Loss 455627.65625
Epoch 76: Val Loss 452767.37500
Epoch 77: Val Loss 449823.03125
Epoch 78: Val Loss 446802.34375
Epoch 79: Val Loss 443721.93750
Epoch 80: Val Loss 440552.84375
Epoch 81: Val Loss 437300.81250
Epoch 82: Val Loss 434010.37500
Epoch 83: Val Loss 430585.84375
Epoch 84: Val Loss 427097.65625
Epoch 85: Val Loss 423547.21875
Epoch 86: Val Loss 419902.00000
Epoch 87: Val Loss 416214.21875
Epoch 88: Val Loss 412442.50000
Epoch 89: Val Loss 408614.18750
Epoch 90: Val Loss 404674.65625
Epoch 91: Val Loss 400715.53125
Epoch 92: Val Loss 396631.78125
Epoch 93: Val Loss 392474.43750
Epoch 94: Val Loss 388279.90625
Epoch 95: Val Loss 384046.84375
Epoch 96: Val Loss 379738.06250
Epoch 97: Val Loss 375353.21875
Epoch 98: Val Loss 370907.09375
Epoch 99: Val Loss 366402.00000
{'MSE - mean': 366401.96229398577, 'MSE - std': 0.0, 'R2 - mean': -41.60190657457376, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 525254.75000
Epoch 1: Val Loss 525228.93750
Epoch 2: Val Loss 525204.50000
Epoch 3: Val Loss 525179.81250
Epoch 4: Val Loss 525154.00000
Epoch 5: Val Loss 525126.18750
Epoch 6: Val Loss 525095.06250
Epoch 7: Val Loss 525061.18750
Epoch 8: Val Loss 525023.06250
Epoch 9: Val Loss 524979.37500
Epoch 10: Val Loss 524930.56250
Epoch 11: Val Loss 524875.81250
Epoch 12: Val Loss 524813.81250
Epoch 13: Val Loss 524744.00000
Epoch 14: Val Loss 524665.31250
Epoch 15: Val Loss 524577.31250
Epoch 16: Val Loss 524479.31250
Epoch 17: Val Loss 524368.68750
Epoch 18: Val Loss 524244.03125
Epoch 19: Val Loss 524103.81250
Epoch 20: Val Loss 523943.90625
Epoch 21: Val Loss 523763.43750
Epoch 22: Val Loss 523565.31250
Epoch 23: Val Loss 523349.06250
Epoch 24: Val Loss 523116.12500
Epoch 25: Val Loss 522865.96875
Epoch 26: Val Loss 522598.81250
Epoch 27: Val Loss 522310.59375
Epoch 28: Val Loss 521999.84375
Epoch 29: Val Loss 521666.56250
Epoch 30: Val Loss 521308.87500
Epoch 31: Val Loss 520919.78125
Epoch 32: Val Loss 520497.90625
Epoch 33: Val Loss 520049.25000
Epoch 34: Val Loss 519564.90625
Epoch 35: Val Loss 519041.81250
Epoch 36: Val Loss 518491.28125
Epoch 37: Val Loss 517893.28125
Epoch 38: Val Loss 517255.59375
Epoch 39: Val Loss 516567.40625
Epoch 40: Val Loss 515841.34375
Epoch 41: Val Loss 515061.31250
Epoch 42: Val Loss 514234.18750
Epoch 43: Val Loss 513350.81250
Epoch 44: Val Loss 512416.56250
Epoch 45: Val Loss 511413.71875
Epoch 46: Val Loss 510357.96875
Epoch 47: Val Loss 509236.59375
Epoch 48: Val Loss 508049.18750
Epoch 49: Val Loss 506797.40625
Epoch 50: Val Loss 505493.59375
Epoch 51: Val Loss 504093.56250
Epoch 52: Val Loss 502651.81250
Epoch 53: Val Loss 501119.71875
Epoch 54: Val Loss 499514.93750
Epoch 55: Val Loss 497840.84375
Epoch 56: Val Loss 496084.75000
Epoch 57: Val Loss 494243.25000
Epoch 58: Val Loss 492334.34375
Epoch 59: Val Loss 490331.84375
Epoch 60: Val Loss 488228.96875
Epoch 61: Val Loss 486056.03125
Epoch 62: Val Loss 483796.93750
Epoch 63: Val Loss 481472.50000
Epoch 64: Val Loss 479046.78125
Epoch 65: Val Loss 476509.68750
Epoch 66: Val Loss 473876.18750
Epoch 67: Val Loss 471177.53125
Epoch 68: Val Loss 468429.06250
Epoch 69: Val Loss 465523.75000
Epoch 70: Val Loss 462578.81250
Epoch 71: Val Loss 459525.12500
Epoch 72: Val Loss 456372.09375
Epoch 73: Val Loss 453163.28125
Epoch 74: Val Loss 449835.81250
Epoch 75: Val Loss 446416.81250
Epoch 76: Val Loss 442872.62500
Epoch 77: Val Loss 439279.96875
Epoch 78: Val Loss 435570.09375
Epoch 79: Val Loss 431745.31250
Epoch 80: Val Loss 427870.75000
Epoch 81: Val Loss 423840.65625
Epoch 82: Val Loss 419763.84375
Epoch 83: Val Loss 415574.46875
Epoch 84: Val Loss 411340.78125
Epoch 85: Val Loss 407012.46875
Epoch 86: Val Loss 402598.87500
Epoch 87: Val Loss 398097.12500
Epoch 88: Val Loss 393497.53125
Epoch 89: Val Loss 388881.34375
Epoch 90: Val Loss 384171.46875
Epoch 91: Val Loss 379400.09375
Epoch 92: Val Loss 374572.81250
Epoch 93: Val Loss 369609.06250
Epoch 94: Val Loss 364628.34375
Epoch 95: Val Loss 359546.50000
Epoch 96: Val Loss 354465.71875
Epoch 97: Val Loss 349263.06250
Epoch 98: Val Loss 344031.68750
Epoch 99: Val Loss 338750.53125
{'MSE - mean': 352576.2295423418, 'MSE - std': 13825.732751643984, 'R2 - mean': -42.46910898589242, 'R2 - std': 0.8672024113186545} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518499.37500
Epoch 1: Val Loss 518464.12500
Epoch 2: Val Loss 518429.90625
Epoch 3: Val Loss 518396.59375
Epoch 4: Val Loss 518364.03125
Epoch 5: Val Loss 518331.59375
Epoch 6: Val Loss 518299.21875
Epoch 7: Val Loss 518266.43750
Epoch 8: Val Loss 518233.34375
Epoch 9: Val Loss 518199.06250
Epoch 10: Val Loss 518163.31250
Epoch 11: Val Loss 518125.12500
Epoch 12: Val Loss 518083.31250
Epoch 13: Val Loss 518036.46875
Epoch 14: Val Loss 517982.03125
Epoch 15: Val Loss 517920.50000
Epoch 16: Val Loss 517848.90625
Epoch 17: Val Loss 517764.40625
Epoch 18: Val Loss 517663.53125
Epoch 19: Val Loss 517547.71875
Epoch 20: Val Loss 517420.40625
Epoch 21: Val Loss 517278.93750
Epoch 22: Val Loss 517125.56250
Epoch 23: Val Loss 516959.81250
Epoch 24: Val Loss 516779.06250
Epoch 25: Val Loss 516585.09375
Epoch 26: Val Loss 516374.62500
Epoch 27: Val Loss 516146.65625
Epoch 28: Val Loss 515903.31250
Epoch 29: Val Loss 515640.68750
Epoch 30: Val Loss 515357.40625
Epoch 31: Val Loss 515047.31250
Epoch 32: Val Loss 514714.81250
Epoch 33: Val Loss 514355.09375
Epoch 34: Val Loss 513970.84375
Epoch 35: Val Loss 513556.28125
Epoch 36: Val Loss 513112.75000
Epoch 37: Val Loss 512635.87500
Epoch 38: Val Loss 512129.34375
Epoch 39: Val Loss 511585.21875
Epoch 40: Val Loss 511010.34375
Epoch 41: Val Loss 510392.75000
Epoch 42: Val Loss 509745.46875
Epoch 43: Val Loss 509053.50000
Epoch 44: Val Loss 508302.40625
Epoch 45: Val Loss 507526.81250
Epoch 46: Val Loss 506695.75000
Epoch 47: Val Loss 505821.28125
Epoch 48: Val Loss 504899.50000
Epoch 49: Val Loss 503918.75000
Epoch 50: Val Loss 502884.34375
Epoch 51: Val Loss 501790.75000
Epoch 52: Val Loss 500643.53125
Epoch 53: Val Loss 499434.12500
Epoch 54: Val Loss 498200.00000
Epoch 55: Val Loss 496897.53125
Epoch 56: Val Loss 495534.68750
Epoch 57: Val Loss 494118.50000
Epoch 58: Val Loss 492642.31250
Epoch 59: Val Loss 491080.28125
Epoch 60: Val Loss 489466.06250
Epoch 61: Val Loss 487799.68750
Epoch 62: Val Loss 486072.96875
Epoch 63: Val Loss 484285.62500
Epoch 64: Val Loss 482428.06250
Epoch 65: Val Loss 480496.90625
Epoch 66: Val Loss 478499.71875
Epoch 67: Val Loss 476427.21875
Epoch 68: Val Loss 474308.65625
Epoch 69: Val Loss 472140.09375
Epoch 70: Val Loss 469860.28125
Epoch 71: Val Loss 467531.90625
Epoch 72: Val Loss 465164.78125
Epoch 73: Val Loss 462679.53125
Epoch 74: Val Loss 460153.75000
Epoch 75: Val Loss 457560.96875
Epoch 76: Val Loss 454849.78125
Epoch 77: Val Loss 452141.78125
Epoch 78: Val Loss 449340.40625
Epoch 79: Val Loss 446449.06250
Epoch 80: Val Loss 443513.65625
Epoch 81: Val Loss 440481.90625
Epoch 82: Val Loss 437440.62500
Epoch 83: Val Loss 434276.93750
Epoch 84: Val Loss 431082.12500
Epoch 85: Val Loss 427794.46875
Epoch 86: Val Loss 424471.93750
Epoch 87: Val Loss 421044.40625
Epoch 88: Val Loss 417592.71875
Epoch 89: Val Loss 414006.31250
Epoch 90: Val Loss 410403.62500
Epoch 91: Val Loss 406799.06250
Epoch 92: Val Loss 403070.81250
Epoch 93: Val Loss 399290.18750
Epoch 94: Val Loss 395488.37500
Epoch 95: Val Loss 391581.78125
Epoch 96: Val Loss 387626.96875
Epoch 97: Val Loss 383603.25000
Epoch 98: Val Loss 379550.00000
Epoch 99: Val Loss 375421.09375
{'MSE - mean': 360191.19247935544, 'MSE - std': 15601.578290163556, 'R2 - mean': -41.423168507931734, 'R2 - std': 1.6399216388813127} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521085.06250
Epoch 1: Val Loss 521051.50000
Epoch 2: Val Loss 521016.12500
Epoch 3: Val Loss 520978.06250
Epoch 4: Val Loss 520937.09375
Epoch 5: Val Loss 520892.75000
Epoch 6: Val Loss 520844.78125
Epoch 7: Val Loss 520792.56250
Epoch 8: Val Loss 520736.34375
Epoch 9: Val Loss 520675.03125
Epoch 10: Val Loss 520608.31250
Epoch 11: Val Loss 520534.71875
Epoch 12: Val Loss 520453.43750
Epoch 13: Val Loss 520363.56250
Epoch 14: Val Loss 520264.09375
Epoch 15: Val Loss 520154.93750
Epoch 16: Val Loss 520034.71875
Epoch 17: Val Loss 519903.00000
Epoch 18: Val Loss 519757.90625
Epoch 19: Val Loss 519599.50000
Epoch 20: Val Loss 519428.31250
Epoch 21: Val Loss 519240.90625
Epoch 22: Val Loss 519035.87500
Epoch 23: Val Loss 518812.28125
Epoch 24: Val Loss 518567.06250
Epoch 25: Val Loss 518302.62500
Epoch 26: Val Loss 518013.56250
Epoch 27: Val Loss 517699.93750
Epoch 28: Val Loss 517360.68750
Epoch 29: Val Loss 516991.37500
Epoch 30: Val Loss 516592.50000
Epoch 31: Val Loss 516163.12500
Epoch 32: Val Loss 515705.28125
Epoch 33: Val Loss 515206.15625
Epoch 34: Val Loss 514671.84375
Epoch 35: Val Loss 514100.46875
Epoch 36: Val Loss 513486.18750
Epoch 37: Val Loss 512828.75000
Epoch 38: Val Loss 512128.43750
Epoch 39: Val Loss 511375.53125
Epoch 40: Val Loss 510582.06250
Epoch 41: Val Loss 509728.93750
Epoch 42: Val Loss 508832.31250
Epoch 43: Val Loss 507885.12500
Epoch 44: Val Loss 506885.06250
Epoch 45: Val Loss 505833.09375
Epoch 46: Val Loss 504718.21875
Epoch 47: Val Loss 503546.53125
Epoch 48: Val Loss 502326.65625
Epoch 49: Val Loss 501036.34375
Epoch 50: Val Loss 499685.06250
Epoch 51: Val Loss 498275.81250
Epoch 52: Val Loss 496806.62500
Epoch 53: Val Loss 495276.68750
Epoch 54: Val Loss 493664.25000
Epoch 55: Val Loss 491990.68750
Epoch 56: Val Loss 490237.50000
Epoch 57: Val Loss 488417.34375
Epoch 58: Val Loss 486517.90625
Epoch 59: Val Loss 484548.46875
Epoch 60: Val Loss 482491.62500
Epoch 61: Val Loss 480339.03125
Epoch 62: Val Loss 478124.21875
Epoch 63: Val Loss 475816.68750
Epoch 64: Val Loss 473454.75000
Epoch 65: Val Loss 470977.06250
Epoch 66: Val Loss 468445.71875
Epoch 67: Val Loss 465802.00000
Epoch 68: Val Loss 463078.40625
Epoch 69: Val Loss 460286.56250
Epoch 70: Val Loss 457397.90625
Epoch 71: Val Loss 454436.50000
Epoch 72: Val Loss 451393.50000
Epoch 73: Val Loss 448242.53125
Epoch 74: Val Loss 445010.53125
Epoch 75: Val Loss 441708.96875
Epoch 76: Val Loss 438348.06250
Epoch 77: Val Loss 434890.03125
Epoch 78: Val Loss 431340.34375
Epoch 79: Val Loss 427705.68750
Epoch 80: Val Loss 423991.28125
Epoch 81: Val Loss 420175.40625
Epoch 82: Val Loss 416306.18750
Epoch 83: Val Loss 412340.71875
Epoch 84: Val Loss 408336.40625
Epoch 85: Val Loss 404222.53125
Epoch 86: Val Loss 400056.81250
Epoch 87: Val Loss 395790.50000
Epoch 88: Val Loss 391453.71875
Epoch 89: Val Loss 387038.37500
Epoch 90: Val Loss 382551.65625
Epoch 91: Val Loss 378010.53125
Epoch 92: Val Loss 373395.31250
Epoch 93: Val Loss 368659.75000
Epoch 94: Val Loss 363884.96875
Epoch 95: Val Loss 359117.65625
Epoch 96: Val Loss 354224.31250
Epoch 97: Val Loss 349276.56250
Epoch 98: Val Loss 344307.62500
Epoch 99: Val Loss 339293.46875
{'MSE - mean': 354966.7655318375, 'MSE - std': 16261.637206872248, 'R2 - mean': -41.030273711488846, 'R2 - std': 1.5748352928065426} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518080.21875
Epoch 1: Val Loss 518049.46875
Epoch 2: Val Loss 518018.31250
Epoch 3: Val Loss 517985.75000
Epoch 4: Val Loss 517951.53125
Epoch 5: Val Loss 517914.90625
Epoch 6: Val Loss 517875.37500
Epoch 7: Val Loss 517832.65625
Epoch 8: Val Loss 517785.18750
Epoch 9: Val Loss 517733.87500
Epoch 10: Val Loss 517676.96875
Epoch 11: Val Loss 517614.78125
Epoch 12: Val Loss 517544.96875
Epoch 13: Val Loss 517469.12500
Epoch 14: Val Loss 517385.21875
Epoch 15: Val Loss 517294.09375
Epoch 16: Val Loss 517193.75000
Epoch 17: Val Loss 517084.43750
Epoch 18: Val Loss 516962.65625
Epoch 19: Val Loss 516830.50000
Epoch 20: Val Loss 516684.93750
Epoch 21: Val Loss 516528.21875
Epoch 22: Val Loss 516358.68750
Epoch 23: Val Loss 516175.34375
Epoch 24: Val Loss 515977.84375
Epoch 25: Val Loss 515763.59375
Epoch 26: Val Loss 515534.62500
Epoch 27: Val Loss 515293.18750
Epoch 28: Val Loss 515033.31250
Epoch 29: Val Loss 514755.81250
Epoch 30: Val Loss 514460.90625
Epoch 31: Val Loss 514142.50000
Epoch 32: Val Loss 513803.75000
Epoch 33: Val Loss 513445.50000
Epoch 34: Val Loss 513058.84375
Epoch 35: Val Loss 512647.06250
Epoch 36: Val Loss 512205.81250
Epoch 37: Val Loss 511737.53125
Epoch 38: Val Loss 511234.37500
Epoch 39: Val Loss 510712.62500
Epoch 40: Val Loss 510155.12500
Epoch 41: Val Loss 509558.50000
Epoch 42: Val Loss 508931.62500
Epoch 43: Val Loss 508280.68750
Epoch 44: Val Loss 507584.65625
Epoch 45: Val Loss 506853.37500
Epoch 46: Val Loss 506080.68750
Epoch 47: Val Loss 505279.37500
Epoch 48: Val Loss 504431.00000
Epoch 49: Val Loss 503552.37500
Epoch 50: Val Loss 502624.40625
Epoch 51: Val Loss 501666.28125
Epoch 52: Val Loss 500654.59375
Epoch 53: Val Loss 499592.46875
Epoch 54: Val Loss 498487.34375
Epoch 55: Val Loss 497332.09375
Epoch 56: Val Loss 496134.59375
Epoch 57: Val Loss 494877.84375
Epoch 58: Val Loss 493550.31250
Epoch 59: Val Loss 492199.12500
Epoch 60: Val Loss 490790.09375
Epoch 61: Val Loss 489322.78125
Epoch 62: Val Loss 487812.65625
Epoch 63: Val Loss 486222.50000
Epoch 64: Val Loss 484612.43750
Epoch 65: Val Loss 482931.37500
Epoch 66: Val Loss 481198.03125
Epoch 67: Val Loss 479405.46875
Epoch 68: Val Loss 477580.90625
Epoch 69: Val Loss 475658.00000
Epoch 70: Val Loss 473709.56250
Epoch 71: Val Loss 471700.71875
Epoch 72: Val Loss 469628.46875
Epoch 73: Val Loss 467503.65625
Epoch 74: Val Loss 465300.87500
Epoch 75: Val Loss 463063.34375
Epoch 76: Val Loss 460747.03125
Epoch 77: Val Loss 458364.93750
Epoch 78: Val Loss 455925.37500
Epoch 79: Val Loss 453464.68750
Epoch 80: Val Loss 450934.62500
Epoch 81: Val Loss 448322.78125
Epoch 82: Val Loss 445663.09375
Epoch 83: Val Loss 442938.03125
Epoch 84: Val Loss 440137.46875
Epoch 85: Val Loss 437298.28125
Epoch 86: Val Loss 434381.65625
Epoch 87: Val Loss 431393.75000
Epoch 88: Val Loss 428358.75000
Epoch 89: Val Loss 425225.21875
Epoch 90: Val Loss 422043.43750
Epoch 91: Val Loss 418812.62500
Epoch 92: Val Loss 415476.03125
Epoch 93: Val Loss 412147.09375
Epoch 94: Val Loss 408715.93750
Epoch 95: Val Loss 405224.50000
Epoch 96: Val Loss 401680.50000
Epoch 97: Val Loss 398085.84375
Epoch 98: Val Loss 394464.65625
Epoch 99: Val Loss 390715.93750
{'MSE - mean': 362116.60611215944, 'MSE - std': 20396.900671779, 'R2 - mean': -42.41900804515258, 'R2 - std': 3.114228180073039} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 362116.60611215944 and parameters: {'dim': 256, 'depth': 6, 'heads': 2, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516359.34375
Epoch 1: Val Loss 516061.15625
Epoch 2: Val Loss 515542.03125
Epoch 3: Val Loss 514517.96875
Epoch 4: Val Loss 512525.75000
Epoch 5: Val Loss 508798.00000
Epoch 6: Val Loss 502188.71875
Epoch 7: Val Loss 491011.71875
Epoch 8: Val Loss 473157.78125
Epoch 9: Val Loss 446169.25000
Epoch 10: Val Loss 407952.71875
Epoch 11: Val Loss 358099.12500
Epoch 12: Val Loss 297074.71875
Epoch 13: Val Loss 228510.15625
Epoch 14: Val Loss 159473.34375
Epoch 15: Val Loss 99302.08594
Epoch 16: Val Loss 56299.49609
Epoch 17: Val Loss 32419.14844
Epoch 18: Val Loss 21788.89648
Epoch 19: Val Loss 17359.86328
Epoch 20: Val Loss 14750.67676
Epoch 21: Val Loss 12910.60254
Epoch 22: Val Loss 11565.01758
Epoch 23: Val Loss 10508.55859
Epoch 24: Val Loss 9640.53809
Epoch 25: Val Loss 8939.01367
Epoch 26: Val Loss 8361.82812
Epoch 27: Val Loss 7909.37402
Epoch 28: Val Loss 7547.44482
Epoch 29: Val Loss 7205.53174
Epoch 30: Val Loss 6913.03369
Epoch 31: Val Loss 6672.58398
Epoch 32: Val Loss 6473.44141
Epoch 33: Val Loss 6280.75195
Epoch 34: Val Loss 6099.44873
Epoch 35: Val Loss 5946.67432
Epoch 36: Val Loss 5809.37061
Epoch 37: Val Loss 5671.01221
Epoch 38: Val Loss 5564.91211
Epoch 39: Val Loss 5442.33838
Epoch 40: Val Loss 5341.86377
Epoch 41: Val Loss 5240.95068
Epoch 42: Val Loss 5143.00732
Epoch 43: Val Loss 5048.54785
Epoch 44: Val Loss 4968.26709
Epoch 45: Val Loss 4907.62451
Epoch 46: Val Loss 4829.39990
Epoch 47: Val Loss 4753.44775
Epoch 48: Val Loss 4693.96875
Epoch 49: Val Loss 4638.03760
Epoch 50: Val Loss 4576.85986
Epoch 51: Val Loss 4522.82959
Epoch 52: Val Loss 4466.33594
Epoch 53: Val Loss 4426.03467
Epoch 54: Val Loss 4377.71484
Epoch 55: Val Loss 4328.28467
Epoch 56: Val Loss 4297.55322
Epoch 57: Val Loss 4263.24072
Epoch 58: Val Loss 4203.58838
Epoch 59: Val Loss 4182.21631
Epoch 60: Val Loss 4142.25781
Epoch 61: Val Loss 4104.13037
Epoch 62: Val Loss 4078.98779
Epoch 63: Val Loss 4046.40552
Epoch 64: Val Loss 4007.72266
Epoch 65: Val Loss 4005.81226
Epoch 66: Val Loss 3941.00000
Epoch 67: Val Loss 3913.38550
Epoch 68: Val Loss 3901.46533
Epoch 69: Val Loss 3859.73853
Epoch 70: Val Loss 3836.66162
Epoch 71: Val Loss 3803.30420
Epoch 72: Val Loss 3772.85718
Epoch 73: Val Loss 3751.22388
Epoch 74: Val Loss 3715.64917
Epoch 75: Val Loss 3697.62549
Epoch 76: Val Loss 3666.25903
Epoch 77: Val Loss 3654.59229
Epoch 78: Val Loss 3610.95654
Epoch 79: Val Loss 3588.96509
Epoch 80: Val Loss 3577.71313
Epoch 81: Val Loss 3547.12134
Epoch 82: Val Loss 3516.83960
Epoch 83: Val Loss 3492.38013
Epoch 84: Val Loss 3469.28687
Epoch 85: Val Loss 3441.69458
Epoch 86: Val Loss 3416.40088
Epoch 87: Val Loss 3397.87500
Epoch 88: Val Loss 3372.04761
Epoch 89: Val Loss 3338.08716
Epoch 90: Val Loss 3339.78735
Epoch 91: Val Loss 3306.23462
Epoch 92: Val Loss 3264.45020
Epoch 93: Val Loss 3266.57568
Epoch 94: Val Loss 3228.77417
Epoch 95: Val Loss 3217.52881
Epoch 96: Val Loss 3195.12207
Epoch 97: Val Loss 3179.07739
Epoch 98: Val Loss 3143.96509
Epoch 99: Val Loss 3119.56885
{'MSE - mean': 3119.5688264850633, 'MSE - std': 0.0, 'R2 - mean': 0.6372847490586411, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524581.06250
Epoch 1: Val Loss 524094.18750
Epoch 2: Val Loss 523222.87500
Epoch 3: Val Loss 521515.75000
Epoch 4: Val Loss 518300.96875
Epoch 5: Val Loss 512570.43750
Epoch 6: Val Loss 502741.18750
Epoch 7: Val Loss 486587.90625
Epoch 8: Val Loss 461492.34375
Epoch 9: Val Loss 425342.65625
Epoch 10: Val Loss 376892.37500
Epoch 11: Val Loss 316145.56250
Epoch 12: Val Loss 246331.73438
Epoch 13: Val Loss 173552.42188
Epoch 14: Val Loss 107302.45312
Epoch 15: Val Loss 57120.19141
Epoch 16: Val Loss 27446.87305
Epoch 17: Val Loss 14653.75586
Epoch 18: Val Loss 10612.33398
Epoch 19: Val Loss 8898.92676
Epoch 20: Val Loss 7804.17236
Epoch 21: Val Loss 7002.93457
Epoch 22: Val Loss 6400.35400
Epoch 23: Val Loss 5985.51123
Epoch 24: Val Loss 5656.35693
Epoch 25: Val Loss 5394.40186
Epoch 26: Val Loss 5188.71143
Epoch 27: Val Loss 5004.59229
Epoch 28: Val Loss 4859.49951
Epoch 29: Val Loss 4736.94971
Epoch 30: Val Loss 4616.50098
Epoch 31: Val Loss 4522.35400
Epoch 32: Val Loss 4419.53662
Epoch 33: Val Loss 4321.18359
Epoch 34: Val Loss 4267.78467
Epoch 35: Val Loss 4200.40527
Epoch 36: Val Loss 4117.83252
Epoch 37: Val Loss 4054.52954
Epoch 38: Val Loss 4009.35938
Epoch 39: Val Loss 3976.91260
Epoch 40: Val Loss 3935.26880
Epoch 41: Val Loss 3904.24121
Epoch 42: Val Loss 3858.57324
Epoch 43: Val Loss 3810.01660
Epoch 44: Val Loss 3778.54736
Epoch 45: Val Loss 3763.82666
Epoch 46: Val Loss 3732.71265
Epoch 47: Val Loss 3721.64917
Epoch 48: Val Loss 3706.84766
Epoch 49: Val Loss 3666.24561
Epoch 50: Val Loss 3638.26318
Epoch 51: Val Loss 3640.85376
Epoch 52: Val Loss 3624.99756
Epoch 53: Val Loss 3601.34839
Epoch 54: Val Loss 3567.51270
Epoch 55: Val Loss 3562.11450
Epoch 56: Val Loss 3540.48340
Epoch 57: Val Loss 3520.22339
Epoch 58: Val Loss 3506.66333
Epoch 59: Val Loss 3512.69385
Epoch 60: Val Loss 3461.67017
Epoch 61: Val Loss 3486.32300
Epoch 62: Val Loss 3441.71826
Epoch 63: Val Loss 3447.46777
Epoch 64: Val Loss 3418.79102
Epoch 65: Val Loss 3408.03271
Epoch 66: Val Loss 3378.51636
Epoch 67: Val Loss 3396.03662
Epoch 68: Val Loss 3349.37524
Epoch 69: Val Loss 3354.09937
Epoch 70: Val Loss 3340.62573
Epoch 71: Val Loss 3339.82593
Epoch 72: Val Loss 3322.96802
Epoch 73: Val Loss 3289.75098
Epoch 74: Val Loss 3277.39038
Epoch 75: Val Loss 3275.64453
Epoch 76: Val Loss 3279.06128
Epoch 77: Val Loss 3262.35815
Epoch 78: Val Loss 3233.78394
Epoch 79: Val Loss 3213.95020
Epoch 80: Val Loss 3206.80225
Epoch 81: Val Loss 3197.78247
Epoch 82: Val Loss 3178.24146
Epoch 83: Val Loss 3172.73096
Epoch 84: Val Loss 3144.91699
Epoch 85: Val Loss 3136.68457
Epoch 86: Val Loss 3128.08057
Epoch 87: Val Loss 3111.34399
Epoch 88: Val Loss 3101.15649
Epoch 89: Val Loss 3084.61499
Epoch 90: Val Loss 3063.70801
Epoch 91: Val Loss 3031.79053
Epoch 92: Val Loss 3040.79370
Epoch 93: Val Loss 3010.71411
Epoch 94: Val Loss 3009.74878
Epoch 95: Val Loss 3006.94385
Epoch 96: Val Loss 2980.98682
Epoch 97: Val Loss 2964.10132
Epoch 98: Val Loss 2945.79102
Epoch 99: Val Loss 2924.66943
{'MSE - mean': 3022.1190539592717, 'MSE - std': 97.44977252579179, 'R2 - mean': 0.6272492264395477, 'R2 - std': 0.010035522619093351} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518178.96875
Epoch 1: Val Loss 517544.37500
Epoch 2: Val Loss 516221.65625
Epoch 3: Val Loss 513613.00000
Epoch 4: Val Loss 508791.78125
Epoch 5: Val Loss 500344.87500
Epoch 6: Val Loss 486360.12500
Epoch 7: Val Loss 464463.06250
Epoch 8: Val Loss 432521.06250
Epoch 9: Val Loss 389471.31250
Epoch 10: Val Loss 334924.93750
Epoch 11: Val Loss 271595.68750
Epoch 12: Val Loss 205258.75000
Epoch 13: Val Loss 143069.82812
Epoch 14: Val Loss 93277.44531
Epoch 15: Val Loss 60328.27344
Epoch 16: Val Loss 41584.22656
Epoch 17: Val Loss 31691.31055
Epoch 18: Val Loss 25708.25977
Epoch 19: Val Loss 21478.92188
Epoch 20: Val Loss 18258.42188
Epoch 21: Val Loss 15794.29883
Epoch 22: Val Loss 13863.63672
Epoch 23: Val Loss 12256.84473
Epoch 24: Val Loss 10966.93555
Epoch 25: Val Loss 9892.48633
Epoch 26: Val Loss 9032.95703
Epoch 27: Val Loss 8295.27148
Epoch 28: Val Loss 7649.06689
Epoch 29: Val Loss 7141.76074
Epoch 30: Val Loss 6724.34717
Epoch 31: Val Loss 6330.37109
Epoch 32: Val Loss 6044.17188
Epoch 33: Val Loss 5777.57227
Epoch 34: Val Loss 5532.00977
Epoch 35: Val Loss 5331.54492
Epoch 36: Val Loss 5176.61328
Epoch 37: Val Loss 5028.49072
Epoch 38: Val Loss 4894.57715
Epoch 39: Val Loss 4803.79639
Epoch 40: Val Loss 4663.96436
Epoch 41: Val Loss 4575.75977
Epoch 42: Val Loss 4489.93701
Epoch 43: Val Loss 4418.53955
Epoch 44: Val Loss 4332.14697
Epoch 45: Val Loss 4273.04395
Epoch 46: Val Loss 4208.78125
Epoch 47: Val Loss 4151.24512
Epoch 48: Val Loss 4076.80664
Epoch 49: Val Loss 4043.72949
Epoch 50: Val Loss 4000.13989
Epoch 51: Val Loss 3926.68018
Epoch 52: Val Loss 3896.42407
Epoch 53: Val Loss 3838.04517
Epoch 54: Val Loss 3821.25635
Epoch 55: Val Loss 3771.63623
Epoch 56: Val Loss 3732.36426
Epoch 57: Val Loss 3707.55396
Epoch 58: Val Loss 3693.27588
Epoch 59: Val Loss 3623.66675
Epoch 60: Val Loss 3590.67529
Epoch 61: Val Loss 3570.94189
Epoch 62: Val Loss 3549.40479
Epoch 63: Val Loss 3502.45361
Epoch 64: Val Loss 3480.90234
Epoch 65: Val Loss 3451.33545
Epoch 66: Val Loss 3419.38892
Epoch 67: Val Loss 3387.60107
Epoch 68: Val Loss 3363.89917
Epoch 69: Val Loss 3354.35913
Epoch 70: Val Loss 3327.21411
Epoch 71: Val Loss 3276.61108
Epoch 72: Val Loss 3265.18018
Epoch 73: Val Loss 3223.38867
Epoch 74: Val Loss 3229.00049
Epoch 75: Val Loss 3216.37695
Epoch 76: Val Loss 3181.00195
Epoch 77: Val Loss 3145.03125
Epoch 78: Val Loss 3120.54712
Epoch 79: Val Loss 3097.96045
Epoch 80: Val Loss 3075.97241
Epoch 81: Val Loss 3046.90283
Epoch 82: Val Loss 3044.36719
Epoch 83: Val Loss 3017.52734
Epoch 84: Val Loss 2986.54980
Epoch 85: Val Loss 2978.45752
Epoch 86: Val Loss 2956.14014
Epoch 87: Val Loss 2936.91187
Epoch 88: Val Loss 2915.66602
Epoch 89: Val Loss 2898.16602
Epoch 90: Val Loss 2881.12549
Epoch 91: Val Loss 2861.93091
Epoch 92: Val Loss 2847.39673
Epoch 93: Val Loss 2826.93677
Epoch 94: Val Loss 2807.32202
Epoch 95: Val Loss 2777.46069
Epoch 96: Val Loss 2772.96094
Epoch 97: Val Loss 2738.51147
Epoch 98: Val Loss 2726.09741
Epoch 99: Val Loss 2715.11377
{'MSE - mean': 2919.7839947911525, 'MSE - std': 165.15417276019355, 'R2 - mean': 0.6542717462193655, 'R2 - std': 0.039084194930007726} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520811.37500
Epoch 1: Val Loss 520210.65625
Epoch 2: Val Loss 519043.68750
Epoch 3: Val Loss 516748.34375
Epoch 4: Val Loss 512374.28125
Epoch 5: Val Loss 504549.50000
Epoch 6: Val Loss 491556.65625
Epoch 7: Val Loss 471405.00000
Epoch 8: Val Loss 442415.12500
Epoch 9: Val Loss 402351.40625
Epoch 10: Val Loss 350777.21875
Epoch 11: Val Loss 288974.90625
Epoch 12: Val Loss 220756.76562
Epoch 13: Val Loss 152877.62500
Epoch 14: Val Loss 94402.22656
Epoch 15: Val Loss 52220.00000
Epoch 16: Val Loss 28245.89258
Epoch 17: Val Loss 17597.51172
Epoch 18: Val Loss 13217.08594
Epoch 19: Val Loss 10968.14844
Epoch 20: Val Loss 9328.45898
Epoch 21: Val Loss 8143.97656
Epoch 22: Val Loss 7216.15234
Epoch 23: Val Loss 6497.83643
Epoch 24: Val Loss 5944.43164
Epoch 25: Val Loss 5515.15674
Epoch 26: Val Loss 5161.17578
Epoch 27: Val Loss 4882.40820
Epoch 28: Val Loss 4652.04590
Epoch 29: Val Loss 4473.87891
Epoch 30: Val Loss 4325.41748
Epoch 31: Val Loss 4200.96191
Epoch 32: Val Loss 4095.43335
Epoch 33: Val Loss 4009.21533
Epoch 34: Val Loss 3936.23462
Epoch 35: Val Loss 3872.46143
Epoch 36: Val Loss 3816.00366
Epoch 37: Val Loss 3762.04639
Epoch 38: Val Loss 3714.42480
Epoch 39: Val Loss 3671.20508
Epoch 40: Val Loss 3625.62720
Epoch 41: Val Loss 3591.76660
Epoch 42: Val Loss 3547.56201
Epoch 43: Val Loss 3517.81104
Epoch 44: Val Loss 3484.27710
Epoch 45: Val Loss 3445.18896
Epoch 46: Val Loss 3412.76270
Epoch 47: Val Loss 3384.84229
Epoch 48: Val Loss 3356.04956
Epoch 49: Val Loss 3342.75049
Epoch 50: Val Loss 3290.27881
Epoch 51: Val Loss 3273.92041
Epoch 52: Val Loss 3249.66846
Epoch 53: Val Loss 3229.60498
Epoch 54: Val Loss 3189.03247
Epoch 55: Val Loss 3160.49585
Epoch 56: Val Loss 3145.02368
Epoch 57: Val Loss 3123.87744
Epoch 58: Val Loss 3089.83911
Epoch 59: Val Loss 3069.47168
Epoch 60: Val Loss 3063.24121
Epoch 61: Val Loss 3032.47144
Epoch 62: Val Loss 3003.23755
Epoch 63: Val Loss 2980.36279
Epoch 64: Val Loss 2968.81860
Epoch 65: Val Loss 2944.57007
Epoch 66: Val Loss 2923.53149
Epoch 67: Val Loss 2914.29858
Epoch 68: Val Loss 2877.02783
Epoch 69: Val Loss 2866.80200
Epoch 70: Val Loss 2856.11914
Epoch 71: Val Loss 2825.75000
Epoch 72: Val Loss 2813.93091
Epoch 73: Val Loss 2791.22949
Epoch 74: Val Loss 2784.23120
Epoch 75: Val Loss 2752.18994
Epoch 76: Val Loss 2744.07642
Epoch 77: Val Loss 2752.79102
Epoch 78: Val Loss 2702.63306
Epoch 79: Val Loss 2684.20264
Epoch 80: Val Loss 2680.27368
Epoch 81: Val Loss 2658.52222
Epoch 82: Val Loss 2655.61987
Epoch 83: Val Loss 2629.09839
Epoch 84: Val Loss 2622.21997
Epoch 85: Val Loss 2606.54541
Epoch 86: Val Loss 2578.53223
Epoch 87: Val Loss 2573.84473
Epoch 88: Val Loss 2550.47363
Epoch 89: Val Loss 2542.80786
Epoch 90: Val Loss 2518.11230
Epoch 91: Val Loss 2509.98926
Epoch 92: Val Loss 2484.25098
Epoch 93: Val Loss 2489.43213
Epoch 94: Val Loss 2471.09961
Epoch 95: Val Loss 2444.85059
Epoch 96: Val Loss 2429.39722
Epoch 97: Val Loss 2434.15894
Epoch 98: Val Loss 2418.18140
Epoch 99: Val Loss 2391.49121
{'MSE - mean': 2787.7108647377486, 'MSE - std': 269.7904012958727, 'R2 - mean': 0.6687187642720694, 'R2 - std': 0.04209310765244822} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517196.09375
Epoch 1: Val Loss 516272.96875
Epoch 2: Val Loss 514638.50000
Epoch 3: Val Loss 511685.25000
Epoch 4: Val Loss 506304.75000
Epoch 5: Val Loss 496935.34375
Epoch 6: Val Loss 481532.53125
Epoch 7: Val Loss 457713.34375
Epoch 8: Val Loss 422559.65625
Epoch 9: Val Loss 374478.37500
Epoch 10: Val Loss 313445.25000
Epoch 11: Val Loss 243958.73438
Epoch 12: Val Loss 172228.12500
Epoch 13: Val Loss 109908.61719
Epoch 14: Val Loss 64939.11328
Epoch 15: Val Loss 39045.12891
Epoch 16: Val Loss 26922.02930
Epoch 17: Val Loss 21101.37695
Epoch 18: Val Loss 17368.93164
Epoch 19: Val Loss 14692.29688
Epoch 20: Val Loss 12678.55078
Epoch 21: Val Loss 11191.00098
Epoch 22: Val Loss 9957.75391
Epoch 23: Val Loss 8999.61914
Epoch 24: Val Loss 8229.82227
Epoch 25: Val Loss 7597.47168
Epoch 26: Val Loss 7093.04150
Epoch 27: Val Loss 6675.09424
Epoch 28: Val Loss 6319.13965
Epoch 29: Val Loss 6029.70557
Epoch 30: Val Loss 5773.80518
Epoch 31: Val Loss 5558.02441
Epoch 32: Val Loss 5377.69678
Epoch 33: Val Loss 5217.97168
Epoch 34: Val Loss 5074.67480
Epoch 35: Val Loss 4942.51904
Epoch 36: Val Loss 4828.72998
Epoch 37: Val Loss 4724.75342
Epoch 38: Val Loss 4620.77539
Epoch 39: Val Loss 4539.68506
Epoch 40: Val Loss 4441.68604
Epoch 41: Val Loss 4371.35449
Epoch 42: Val Loss 4285.45361
Epoch 43: Val Loss 4216.81885
Epoch 44: Val Loss 4143.83594
Epoch 45: Val Loss 4078.09741
Epoch 46: Val Loss 4017.71973
Epoch 47: Val Loss 3958.26318
Epoch 48: Val Loss 3899.63257
Epoch 49: Val Loss 3845.89966
Epoch 50: Val Loss 3797.14209
Epoch 51: Val Loss 3750.99121
Epoch 52: Val Loss 3689.55615
Epoch 53: Val Loss 3650.17676
Epoch 54: Val Loss 3597.73682
Epoch 55: Val Loss 3549.52563
Epoch 56: Val Loss 3505.61328
Epoch 57: Val Loss 3466.71216
Epoch 58: Val Loss 3427.73608
Epoch 59: Val Loss 3391.35156
Epoch 60: Val Loss 3346.09082
Epoch 61: Val Loss 3303.03491
Epoch 62: Val Loss 3282.65454
Epoch 63: Val Loss 3239.47363
Epoch 64: Val Loss 3201.65845
Epoch 65: Val Loss 3167.08179
Epoch 66: Val Loss 3135.97705
Epoch 67: Val Loss 3104.80176
Epoch 68: Val Loss 3079.74268
Epoch 69: Val Loss 3038.70361
Epoch 70: Val Loss 3006.75293
Epoch 71: Val Loss 2987.99146
Epoch 72: Val Loss 2957.38306
Epoch 73: Val Loss 2931.80371
Epoch 74: Val Loss 2898.67773
Epoch 75: Val Loss 2876.74219
Epoch 76: Val Loss 2842.03760
Epoch 77: Val Loss 2817.94971
Epoch 78: Val Loss 2794.81982
Epoch 79: Val Loss 2763.77686
Epoch 80: Val Loss 2755.67871
Epoch 81: Val Loss 2716.90308
Epoch 82: Val Loss 2695.04492
Epoch 83: Val Loss 2671.34839
Epoch 84: Val Loss 2642.69604
Epoch 85: Val Loss 2627.20166
Epoch 86: Val Loss 2600.06226
Epoch 87: Val Loss 2580.65088
Epoch 88: Val Loss 2553.39136
Epoch 89: Val Loss 2536.26733
Epoch 90: Val Loss 2518.31274
Epoch 91: Val Loss 2498.42676
Epoch 92: Val Loss 2470.46509
Epoch 93: Val Loss 2458.57959
Epoch 94: Val Loss 2432.31592
Epoch 95: Val Loss 2419.26294
Epoch 96: Val Loss 2394.75659
Epoch 97: Val Loss 2387.56934
Epoch 98: Val Loss 2355.34692
Epoch 99: Val Loss 2340.73267
{'MSE - mean': 2698.315219869755, 'MSE - std': 300.3261789963841, 'R2 - mean': 0.6762956014415569, 'R2 - std': 0.040584450414465116} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 2698.315219869755 and parameters: {'dim': 32, 'depth': 12, 'heads': 2, 'weight_decay': -2, 'learning_rate': -3, 'dropout': 0.1}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516044.68750
Epoch 1: Val Loss 516041.46875
Epoch 2: Val Loss 516038.15625
Epoch 3: Val Loss 516034.93750
Epoch 4: Val Loss 516031.62500
Epoch 5: Val Loss 516028.31250
Epoch 6: Val Loss 516025.00000
Epoch 7: Val Loss 516021.68750
Epoch 8: Val Loss 516018.28125
Epoch 9: Val Loss 516014.90625
Epoch 10: Val Loss 516011.46875
Epoch 11: Val Loss 516008.00000
Epoch 12: Val Loss 516004.56250
Epoch 13: Val Loss 516001.06250
Epoch 14: Val Loss 515997.56250
Epoch 15: Val Loss 515994.03125
Epoch 16: Val Loss 515990.46875
Epoch 17: Val Loss 515986.93750
Epoch 18: Val Loss 515983.28125
Epoch 19: Val Loss 515979.62500
Epoch 20: Val Loss 515975.96875
Epoch 21: Val Loss 515972.28125
Epoch 22: Val Loss 515968.53125
Epoch 23: Val Loss 515964.75000
Epoch 24: Val Loss 515960.93750
Epoch 25: Val Loss 515957.06250
Epoch 26: Val Loss 515953.25000
Epoch 27: Val Loss 515949.34375
Epoch 28: Val Loss 515945.40625
Epoch 29: Val Loss 515941.46875
Epoch 30: Val Loss 515937.46875
Epoch 31: Val Loss 515933.43750
Epoch 32: Val Loss 515929.34375
Epoch 33: Val Loss 515925.25000
Epoch 34: Val Loss 515921.15625
Epoch 35: Val Loss 515917.00000
Epoch 36: Val Loss 515912.81250
Epoch 37: Val Loss 515908.59375
Epoch 38: Val Loss 515904.28125
Epoch 39: Val Loss 515899.96875
Epoch 40: Val Loss 515895.59375
Epoch 41: Val Loss 515891.25000
Epoch 42: Val Loss 515886.84375
Epoch 43: Val Loss 515882.43750
Epoch 44: Val Loss 515877.96875
Epoch 45: Val Loss 515873.50000
Epoch 46: Val Loss 515869.03125
Epoch 47: Val Loss 515864.46875
Epoch 48: Val Loss 515859.87500
Epoch 49: Val Loss 515855.25000
Epoch 50: Val Loss 515850.56250
Epoch 51: Val Loss 515845.87500
Epoch 52: Val Loss 515841.18750
Epoch 53: Val Loss 515836.37500
Epoch 54: Val Loss 515831.53125
Epoch 55: Val Loss 515826.65625
Epoch 56: Val Loss 515821.75000
Epoch 57: Val Loss 515816.78125
Epoch 58: Val Loss 515811.78125
Epoch 59: Val Loss 515806.71875
Epoch 60: Val Loss 515801.68750
Epoch 61: Val Loss 515796.56250
Epoch 62: Val Loss 515791.46875
Epoch 63: Val Loss 515786.28125
Epoch 64: Val Loss 515781.06250
Epoch 65: Val Loss 515775.78125
Epoch 66: Val Loss 515770.50000
Epoch 67: Val Loss 515765.12500
Epoch 68: Val Loss 515759.71875
Epoch 69: Val Loss 515754.31250
Epoch 70: Val Loss 515748.78125
Epoch 71: Val Loss 515743.25000
Epoch 72: Val Loss 515737.65625
Epoch 73: Val Loss 515732.06250
Epoch 74: Val Loss 515726.34375
Epoch 75: Val Loss 515720.59375
Epoch 76: Val Loss 515714.81250
Epoch 77: Val Loss 515709.06250
Epoch 78: Val Loss 515703.21875
Epoch 79: Val Loss 515697.34375
Epoch 80: Val Loss 515691.37500
Epoch 81: Val Loss 515685.34375
Epoch 82: Val Loss 515679.31250
Epoch 83: Val Loss 515673.28125
Epoch 84: Val Loss 515667.18750
Epoch 85: Val Loss 515661.03125
Epoch 86: Val Loss 515654.81250
Epoch 87: Val Loss 515648.46875
Epoch 88: Val Loss 515642.25000
Epoch 89: Val Loss 515635.87500
Epoch 90: Val Loss 515629.50000
Epoch 91: Val Loss 515622.96875
Epoch 92: Val Loss 515616.46875
Epoch 93: Val Loss 515609.93750
Epoch 94: Val Loss 515603.31250
Epoch 95: Val Loss 515596.65625
Epoch 96: Val Loss 515589.84375
Epoch 97: Val Loss 515583.09375
Epoch 98: Val Loss 515576.31250
Epoch 99: Val Loss 515569.37500
{'MSE - mean': 515569.37713490205, 'MSE - std': 0.0, 'R2 - mean': -58.94574455851055, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524931.00000
Epoch 1: Val Loss 524927.06250
Epoch 2: Val Loss 524923.12500
Epoch 3: Val Loss 524919.18750
Epoch 4: Val Loss 524915.31250
Epoch 5: Val Loss 524911.37500
Epoch 6: Val Loss 524907.43750
Epoch 7: Val Loss 524903.43750
Epoch 8: Val Loss 524899.50000
Epoch 9: Val Loss 524895.50000
Epoch 10: Val Loss 524891.43750
Epoch 11: Val Loss 524887.43750
Epoch 12: Val Loss 524883.43750
Epoch 13: Val Loss 524879.43750
Epoch 14: Val Loss 524875.37500
Epoch 15: Val Loss 524871.31250
Epoch 16: Val Loss 524867.18750
Epoch 17: Val Loss 524863.06250
Epoch 18: Val Loss 524858.93750
Epoch 19: Val Loss 524854.75000
Epoch 20: Val Loss 524850.62500
Epoch 21: Val Loss 524846.37500
Epoch 22: Val Loss 524842.12500
Epoch 23: Val Loss 524837.93750
Epoch 24: Val Loss 524833.62500
Epoch 25: Val Loss 524829.31250
Epoch 26: Val Loss 524825.00000
Epoch 27: Val Loss 524820.75000
Epoch 28: Val Loss 524816.37500
Epoch 29: Val Loss 524811.93750
Epoch 30: Val Loss 524807.50000
Epoch 31: Val Loss 524803.06250
Epoch 32: Val Loss 524798.56250
Epoch 33: Val Loss 524794.06250
Epoch 34: Val Loss 524789.56250
Epoch 35: Val Loss 524785.00000
Epoch 36: Val Loss 524780.50000
Epoch 37: Val Loss 524775.87500
Epoch 38: Val Loss 524771.25000
Epoch 39: Val Loss 524766.56250
Epoch 40: Val Loss 524761.87500
Epoch 41: Val Loss 524757.06250
Epoch 42: Val Loss 524752.18750
Epoch 43: Val Loss 524747.50000
Epoch 44: Val Loss 524742.56250
Epoch 45: Val Loss 524737.68750
Epoch 46: Val Loss 524732.75000
Epoch 47: Val Loss 524727.81250
Epoch 48: Val Loss 524722.75000
Epoch 49: Val Loss 524717.68750
Epoch 50: Val Loss 524712.62500
Epoch 51: Val Loss 524707.50000
Epoch 52: Val Loss 524702.31250
Epoch 53: Val Loss 524697.12500
Epoch 54: Val Loss 524691.87500
Epoch 55: Val Loss 524686.62500
Epoch 56: Val Loss 524681.31250
Epoch 57: Val Loss 524676.00000
Epoch 58: Val Loss 524670.62500
Epoch 59: Val Loss 524665.18750
Epoch 60: Val Loss 524659.75000
Epoch 61: Val Loss 524654.25000
Epoch 62: Val Loss 524648.75000
Epoch 63: Val Loss 524643.18750
Epoch 64: Val Loss 524637.50000
Epoch 65: Val Loss 524631.81250
Epoch 66: Val Loss 524626.12500
Epoch 67: Val Loss 524620.43750
Epoch 68: Val Loss 524614.62500
Epoch 69: Val Loss 524608.87500
Epoch 70: Val Loss 524603.00000
Epoch 71: Val Loss 524597.12500
Epoch 72: Val Loss 524591.18750
Epoch 73: Val Loss 524585.31250
Epoch 74: Val Loss 524579.31250
Epoch 75: Val Loss 524573.25000
Epoch 76: Val Loss 524567.18750
Epoch 77: Val Loss 524561.25000
Epoch 78: Val Loss 524555.12500
Epoch 79: Val Loss 524549.00000
Epoch 80: Val Loss 524542.81250
Epoch 81: Val Loss 524536.62500
Epoch 82: Val Loss 524530.25000
Epoch 83: Val Loss 524524.06250
Epoch 84: Val Loss 524517.68750
Epoch 85: Val Loss 524511.31250
Epoch 86: Val Loss 524504.87500
Epoch 87: Val Loss 524498.43750
Epoch 88: Val Loss 524491.93750
Epoch 89: Val Loss 524485.31250
Epoch 90: Val Loss 524478.75000
Epoch 91: Val Loss 524472.18750
Epoch 92: Val Loss 524465.50000
Epoch 93: Val Loss 524458.75000
Epoch 94: Val Loss 524452.00000
Epoch 95: Val Loss 524445.25000
Epoch 96: Val Loss 524438.43750
Epoch 97: Val Loss 524431.56250
Epoch 98: Val Loss 524424.75000
Epoch 99: Val Loss 524417.75000
{'MSE - mean': 519993.5541636354, 'MSE - std': 4424.177028733364, 'R2 - mean': -63.291268934965004, 'R2 - std': 4.34552437645446} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518370.71875
Epoch 1: Val Loss 518368.21875
Epoch 2: Val Loss 518365.62500
Epoch 3: Val Loss 518363.09375
Epoch 4: Val Loss 518360.62500
Epoch 5: Val Loss 518358.15625
Epoch 6: Val Loss 518355.62500
Epoch 7: Val Loss 518353.15625
Epoch 8: Val Loss 518350.62500
Epoch 9: Val Loss 518348.15625
Epoch 10: Val Loss 518345.71875
Epoch 11: Val Loss 518343.25000
Epoch 12: Val Loss 518340.78125
Epoch 13: Val Loss 518338.34375
Epoch 14: Val Loss 518335.90625
Epoch 15: Val Loss 518333.50000
Epoch 16: Val Loss 518331.09375
Epoch 17: Val Loss 518328.62500
Epoch 18: Val Loss 518326.21875
Epoch 19: Val Loss 518323.75000
Epoch 20: Val Loss 518321.34375
Epoch 21: Val Loss 518318.87500
Epoch 22: Val Loss 518316.40625
Epoch 23: Val Loss 518313.90625
Epoch 24: Val Loss 518311.46875
Epoch 25: Val Loss 518309.00000
Epoch 26: Val Loss 518306.43750
Epoch 27: Val Loss 518303.96875
Epoch 28: Val Loss 518301.43750
Epoch 29: Val Loss 518298.96875
Epoch 30: Val Loss 518296.46875
Epoch 31: Val Loss 518293.90625
Epoch 32: Val Loss 518291.43750
Epoch 33: Val Loss 518288.90625
Epoch 34: Val Loss 518286.34375
Epoch 35: Val Loss 518283.81250
Epoch 36: Val Loss 518281.21875
Epoch 37: Val Loss 518278.68750
Epoch 38: Val Loss 518276.00000
Epoch 39: Val Loss 518273.46875
Epoch 40: Val Loss 518270.81250
Epoch 41: Val Loss 518268.18750
Epoch 42: Val Loss 518265.53125
Epoch 43: Val Loss 518262.87500
Epoch 44: Val Loss 518260.21875
Epoch 45: Val Loss 518257.50000
Epoch 46: Val Loss 518254.81250
Epoch 47: Val Loss 518252.09375
Epoch 48: Val Loss 518249.34375
Epoch 49: Val Loss 518246.59375
Epoch 50: Val Loss 518243.81250
Epoch 51: Val Loss 518241.03125
Epoch 52: Val Loss 518238.21875
Epoch 53: Val Loss 518235.40625
Epoch 54: Val Loss 518232.50000
Epoch 55: Val Loss 518229.62500
Epoch 56: Val Loss 518226.75000
Epoch 57: Val Loss 518223.84375
Epoch 58: Val Loss 518220.93750
Epoch 59: Val Loss 518217.93750
Epoch 60: Val Loss 518214.96875
Epoch 61: Val Loss 518211.96875
Epoch 62: Val Loss 518208.96875
Epoch 63: Val Loss 518205.96875
Epoch 64: Val Loss 518202.90625
Epoch 65: Val Loss 518199.84375
Epoch 66: Val Loss 518196.78125
Epoch 67: Val Loss 518193.65625
Epoch 68: Val Loss 518190.50000
Epoch 69: Val Loss 518187.34375
Epoch 70: Val Loss 518184.18750
Epoch 71: Val Loss 518180.93750
Epoch 72: Val Loss 518177.71875
Epoch 73: Val Loss 518174.50000
Epoch 74: Val Loss 518171.15625
Epoch 75: Val Loss 518167.90625
Epoch 76: Val Loss 518164.53125
Epoch 77: Val Loss 518161.21875
Epoch 78: Val Loss 518157.78125
Epoch 79: Val Loss 518154.40625
Epoch 80: Val Loss 518150.96875
Epoch 81: Val Loss 518147.50000
Epoch 82: Val Loss 518143.96875
Epoch 83: Val Loss 518140.50000
Epoch 84: Val Loss 518136.93750
Epoch 85: Val Loss 518133.31250
Epoch 86: Val Loss 518129.71875
Epoch 87: Val Loss 518126.09375
Epoch 88: Val Loss 518122.37500
Epoch 89: Val Loss 518118.68750
Epoch 90: Val Loss 518114.90625
Epoch 91: Val Loss 518111.15625
Epoch 92: Val Loss 518107.37500
Epoch 93: Val Loss 518103.53125
Epoch 94: Val Loss 518099.62500
Epoch 95: Val Loss 518095.71875
Epoch 96: Val Loss 518091.81250
Epoch 97: Val Loss 518087.84375
Epoch 98: Val Loss 518083.81250
Epoch 99: Val Loss 518079.75000
{'MSE - mean': 519355.63302421133, 'MSE - std': 3723.2756654988734, 'R2 - mean': -60.4131897948398, 'R2 - std': 5.399604968071367} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 521168.31250
Epoch 1: Val Loss 521165.03125
Epoch 2: Val Loss 521161.81250
Epoch 3: Val Loss 521158.56250
Epoch 4: Val Loss 521155.31250
Epoch 5: Val Loss 521152.09375
Epoch 6: Val Loss 521148.78125
Epoch 7: Val Loss 521145.56250
Epoch 8: Val Loss 521142.28125
Epoch 9: Val Loss 521139.03125
Epoch 10: Val Loss 521135.78125
Epoch 11: Val Loss 521132.46875
Epoch 12: Val Loss 521129.21875
Epoch 13: Val Loss 521125.90625
Epoch 14: Val Loss 521122.56250
Epoch 15: Val Loss 521119.25000
Epoch 16: Val Loss 521115.90625
Epoch 17: Val Loss 521112.53125
Epoch 18: Val Loss 521109.18750
Epoch 19: Val Loss 521105.81250
Epoch 20: Val Loss 521102.40625
Epoch 21: Val Loss 521099.00000
Epoch 22: Val Loss 521095.59375
Epoch 23: Val Loss 521092.18750
Epoch 24: Val Loss 521088.71875
Epoch 25: Val Loss 521085.25000
Epoch 26: Val Loss 521081.78125
Epoch 27: Val Loss 521078.28125
Epoch 28: Val Loss 521074.75000
Epoch 29: Val Loss 521071.18750
Epoch 30: Val Loss 521067.59375
Epoch 31: Val Loss 521064.00000
Epoch 32: Val Loss 521060.37500
Epoch 33: Val Loss 521056.75000
Epoch 34: Val Loss 521053.06250
Epoch 35: Val Loss 521049.37500
Epoch 36: Val Loss 521045.75000
Epoch 37: Val Loss 521041.96875
Epoch 38: Val Loss 521038.21875
Epoch 39: Val Loss 521034.53125
Epoch 40: Val Loss 521030.68750
Epoch 41: Val Loss 521026.87500
Epoch 42: Val Loss 521023.06250
Epoch 43: Val Loss 521019.15625
Epoch 44: Val Loss 521015.21875
Epoch 45: Val Loss 521011.34375
Epoch 46: Val Loss 521007.34375
Epoch 47: Val Loss 521003.37500
Epoch 48: Val Loss 520999.34375
Epoch 49: Val Loss 520995.25000
Epoch 50: Val Loss 520991.18750
Epoch 51: Val Loss 520987.03125
Epoch 52: Val Loss 520982.93750
Epoch 53: Val Loss 520978.71875
Epoch 54: Val Loss 520974.50000
Epoch 55: Val Loss 520970.21875
Epoch 56: Val Loss 520965.87500
Epoch 57: Val Loss 520961.43750
Epoch 58: Val Loss 520957.03125
Epoch 59: Val Loss 520952.59375
Epoch 60: Val Loss 520948.03125
Epoch 61: Val Loss 520943.46875
Epoch 62: Val Loss 520938.84375
Epoch 63: Val Loss 520934.28125
Epoch 64: Val Loss 520929.62500
Epoch 65: Val Loss 520924.93750
Epoch 66: Val Loss 520920.25000
Epoch 67: Val Loss 520915.46875
Epoch 68: Val Loss 520910.68750
Epoch 69: Val Loss 520905.75000
Epoch 70: Val Loss 520900.84375
Epoch 71: Val Loss 520895.90625
Epoch 72: Val Loss 520890.96875
Epoch 73: Val Loss 520885.93750
Epoch 74: Val Loss 520880.84375
Epoch 75: Val Loss 520875.81250
Epoch 76: Val Loss 520870.68750
Epoch 77: Val Loss 520865.46875
Epoch 78: Val Loss 520860.25000
Epoch 79: Val Loss 520854.93750
Epoch 80: Val Loss 520849.59375
Epoch 81: Val Loss 520844.21875
Epoch 82: Val Loss 520838.81250
Epoch 83: Val Loss 520833.31250
Epoch 84: Val Loss 520827.81250
Epoch 85: Val Loss 520822.25000
Epoch 86: Val Loss 520816.62500
Epoch 87: Val Loss 520810.96875
Epoch 88: Val Loss 520805.25000
Epoch 89: Val Loss 520799.50000
Epoch 90: Val Loss 520793.65625
Epoch 91: Val Loss 520787.78125
Epoch 92: Val Loss 520781.81250
Epoch 93: Val Loss 520775.75000
Epoch 94: Val Loss 520769.75000
Epoch 95: Val Loss 520763.62500
Epoch 96: Val Loss 520757.40625
Epoch 97: Val Loss 520751.12500
Epoch 98: Val Loss 520744.81250
Epoch 99: Val Loss 520738.50000
{'MSE - mean': 519701.35264426516, 'MSE - std': 3279.581136066378, 'R2 - mean': -60.73437238786502, 'R2 - std': 4.709169261409379} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518159.65625
Epoch 1: Val Loss 518157.12500
Epoch 2: Val Loss 518154.59375
Epoch 3: Val Loss 518152.12500
Epoch 4: Val Loss 518149.56250
Epoch 5: Val Loss 518147.09375
Epoch 6: Val Loss 518144.62500
Epoch 7: Val Loss 518142.15625
Epoch 8: Val Loss 518139.68750
Epoch 9: Val Loss 518137.28125
Epoch 10: Val Loss 518134.75000
Epoch 11: Val Loss 518132.34375
Epoch 12: Val Loss 518129.87500
Epoch 13: Val Loss 518127.43750
Epoch 14: Val Loss 518125.00000
Epoch 15: Val Loss 518122.56250
Epoch 16: Val Loss 518120.12500
Epoch 17: Val Loss 518117.71875
Epoch 18: Val Loss 518115.25000
Epoch 19: Val Loss 518112.84375
Epoch 20: Val Loss 518110.37500
Epoch 21: Val Loss 518107.90625
Epoch 22: Val Loss 518105.50000
Epoch 23: Val Loss 518103.03125
Epoch 24: Val Loss 518100.53125
Epoch 25: Val Loss 518098.00000
Epoch 26: Val Loss 518095.53125
Epoch 27: Val Loss 518093.03125
Epoch 28: Val Loss 518090.43750
Epoch 29: Val Loss 518087.87500
Epoch 30: Val Loss 518085.31250
Epoch 31: Val Loss 518082.75000
Epoch 32: Val Loss 518080.12500
Epoch 33: Val Loss 518077.50000
Epoch 34: Val Loss 518074.87500
Epoch 35: Val Loss 518072.21875
Epoch 36: Val Loss 518069.56250
Epoch 37: Val Loss 518066.84375
Epoch 38: Val Loss 518064.12500
Epoch 39: Val Loss 518061.37500
Epoch 40: Val Loss 518058.65625
Epoch 41: Val Loss 518055.84375
Epoch 42: Val Loss 518053.06250
Epoch 43: Val Loss 518050.18750
Epoch 44: Val Loss 518047.34375
Epoch 45: Val Loss 518044.37500
Epoch 46: Val Loss 518041.46875
Epoch 47: Val Loss 518038.56250
Epoch 48: Val Loss 518035.56250
Epoch 49: Val Loss 518032.53125
Epoch 50: Val Loss 518029.50000
Epoch 51: Val Loss 518026.37500
Epoch 52: Val Loss 518023.34375
Epoch 53: Val Loss 518020.21875
Epoch 54: Val Loss 518017.09375
Epoch 55: Val Loss 518013.90625
Epoch 56: Val Loss 518010.65625
Epoch 57: Val Loss 518007.40625
Epoch 58: Val Loss 518004.09375
Epoch 59: Val Loss 518000.68750
Epoch 60: Val Loss 517997.28125
Epoch 61: Val Loss 517993.90625
Epoch 62: Val Loss 517990.50000
Epoch 63: Val Loss 517986.96875
Epoch 64: Val Loss 517983.46875
Epoch 65: Val Loss 517979.90625
Epoch 66: Val Loss 517976.31250
Epoch 67: Val Loss 517972.65625
Epoch 68: Val Loss 517969.00000
Epoch 69: Val Loss 517965.25000
Epoch 70: Val Loss 517961.46875
Epoch 71: Val Loss 517957.65625
Epoch 72: Val Loss 517953.84375
Epoch 73: Val Loss 517950.00000
Epoch 74: Val Loss 517946.15625
Epoch 75: Val Loss 517942.15625
Epoch 76: Val Loss 517938.25000
Epoch 77: Val Loss 517934.21875
Epoch 78: Val Loss 517930.21875
Epoch 79: Val Loss 517926.12500
Epoch 80: Val Loss 517922.06250
Epoch 81: Val Loss 517917.90625
Epoch 82: Val Loss 517913.75000
Epoch 83: Val Loss 517909.46875
Epoch 84: Val Loss 517905.25000
Epoch 85: Val Loss 517900.96875
Epoch 86: Val Loss 517896.65625
Epoch 87: Val Loss 517892.28125
Epoch 88: Val Loss 517887.87500
Epoch 89: Val Loss 517883.34375
Epoch 90: Val Loss 517878.81250
Epoch 91: Val Loss 517874.18750
Epoch 92: Val Loss 517869.62500
Epoch 93: Val Loss 517865.00000
Epoch 94: Val Loss 517860.31250
Epoch 95: Val Loss 517855.56250
Epoch 96: Val Loss 517850.84375
Epoch 97: Val Loss 517846.03125
Epoch 98: Val Loss 517841.21875
Epoch 99: Val Loss 517836.43750
{'MSE - mean': 519328.37078432215, 'MSE - std': 3026.711716338401, 'R2 - mean': -61.36904754785159, 'R2 - std': 4.3991215419556555} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 519328.37078432215 and parameters: {'dim': 64, 'depth': 12, 'heads': 4, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0}. Best is trial 24 with value: 2394.2725020802454.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516732.56250
Epoch 1: Val Loss 516391.65625
Epoch 2: Val Loss 515746.09375
Epoch 3: Val Loss 514372.09375
Epoch 4: Val Loss 511400.87500
Epoch 5: Val Loss 505845.18750
Epoch 6: Val Loss 496225.68750
Epoch 7: Val Loss 480654.65625
Epoch 8: Val Loss 456818.81250
Epoch 9: Val Loss 422793.53125
Epoch 10: Val Loss 377704.00000
Epoch 11: Val Loss 321767.65625
Epoch 12: Val Loss 257583.03125
Epoch 13: Val Loss 191081.85938
Epoch 14: Val Loss 130029.92969
Epoch 15: Val Loss 82117.50781
Epoch 16: Val Loss 50844.71484
Epoch 17: Val Loss 33977.76953
Epoch 18: Val Loss 25726.68164
Epoch 19: Val Loss 21031.75391
Epoch 20: Val Loss 17847.30859
Epoch 21: Val Loss 15406.83887
Epoch 22: Val Loss 13535.69434
Epoch 23: Val Loss 11935.41504
Epoch 24: Val Loss 10686.07520
Epoch 25: Val Loss 9650.63770
Epoch 26: Val Loss 8801.85352
Epoch 27: Val Loss 8064.60986
Epoch 28: Val Loss 7479.96094
Epoch 29: Val Loss 6973.35547
Epoch 30: Val Loss 6538.13770
Epoch 31: Val Loss 6173.52539
Epoch 32: Val Loss 5888.65186
Epoch 33: Val Loss 5623.76123
Epoch 34: Val Loss 5385.72168
Epoch 35: Val Loss 5198.25049
Epoch 36: Val Loss 5043.78467
Epoch 37: Val Loss 4907.12646
Epoch 38: Val Loss 4788.67188
Epoch 39: Val Loss 4681.81006
Epoch 40: Val Loss 4590.41504
Epoch 41: Val Loss 4506.00635
Epoch 42: Val Loss 4429.35645
Epoch 43: Val Loss 4378.00244
Epoch 44: Val Loss 4319.53564
Epoch 45: Val Loss 4254.45264
Epoch 46: Val Loss 4207.21680
Epoch 47: Val Loss 4163.40967
Epoch 48: Val Loss 4112.89062
Epoch 49: Val Loss 4072.44531
Epoch 50: Val Loss 4031.16431
Epoch 51: Val Loss 3981.92554
Epoch 52: Val Loss 3951.30176
Epoch 53: Val Loss 3911.05371
Epoch 54: Val Loss 3878.46924
Epoch 55: Val Loss 3843.35938
Epoch 56: Val Loss 3811.48779
Epoch 57: Val Loss 3777.82812
Epoch 58: Val Loss 3746.80981
Epoch 59: Val Loss 3722.03613
Epoch 60: Val Loss 3690.61719
Epoch 61: Val Loss 3663.63257
Epoch 62: Val Loss 3640.79980
Epoch 63: Val Loss 3615.86084
Epoch 64: Val Loss 3592.53955
Epoch 65: Val Loss 3562.84106
Epoch 66: Val Loss 3533.19214
Epoch 67: Val Loss 3517.24146
Epoch 68: Val Loss 3493.95605
Epoch 69: Val Loss 3475.70117
Epoch 70: Val Loss 3448.06738
Epoch 71: Val Loss 3423.74902
Epoch 72: Val Loss 3399.26221
Epoch 73: Val Loss 3378.85547
Epoch 74: Val Loss 3371.39819
Epoch 75: Val Loss 3334.90820
Epoch 76: Val Loss 3317.67432
Epoch 77: Val Loss 3308.62036
Epoch 78: Val Loss 3286.27344
Epoch 79: Val Loss 3262.65991
Epoch 80: Val Loss 3250.42188
Epoch 81: Val Loss 3230.93799
Epoch 82: Val Loss 3199.07812
Epoch 83: Val Loss 3192.29468
Epoch 84: Val Loss 3171.80811
Epoch 85: Val Loss 3153.00513
Epoch 86: Val Loss 3141.92725
Epoch 87: Val Loss 3126.95825
Epoch 88: Val Loss 3111.19507
Epoch 89: Val Loss 3086.48730
Epoch 90: Val Loss 3066.31567
Epoch 91: Val Loss 3063.37354
Epoch 92: Val Loss 3034.38599
Epoch 93: Val Loss 3024.08984
Epoch 94: Val Loss 3006.42871
Epoch 95: Val Loss 2990.34058
Epoch 96: Val Loss 2964.14697
Epoch 97: Val Loss 2970.84644
Epoch 98: Val Loss 2937.50366
Epoch 99: Val Loss 2923.58398
{'MSE - mean': 2923.5838962668095, 'MSE - std': 0.0, 'R2 - mean': 0.6600721043307268, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524558.31250
Epoch 1: Val Loss 524155.84375
Epoch 2: Val Loss 523410.43750
Epoch 3: Val Loss 521935.50000
Epoch 4: Val Loss 519065.15625
Epoch 5: Val Loss 513793.56250
Epoch 6: Val Loss 504791.50000
Epoch 7: Val Loss 490003.12500
Epoch 8: Val Loss 466860.81250
Epoch 9: Val Loss 432040.25000
Epoch 10: Val Loss 383095.46875
Epoch 11: Val Loss 320203.46875
Epoch 12: Val Loss 246465.10938
Epoch 13: Val Loss 169382.48438
Epoch 14: Val Loss 99038.02344
Epoch 15: Val Loss 47887.70312
Epoch 16: Val Loss 20806.27539
Epoch 17: Val Loss 12020.13184
Epoch 18: Val Loss 10420.95898
Epoch 19: Val Loss 9945.85449
Epoch 20: Val Loss 9478.49707
Epoch 21: Val Loss 9082.14453
Epoch 22: Val Loss 8759.72656
Epoch 23: Val Loss 8462.22754
Epoch 24: Val Loss 8215.33496
Epoch 25: Val Loss 7970.15723
Epoch 26: Val Loss 7751.30664
Epoch 27: Val Loss 7550.14795
Epoch 28: Val Loss 7353.11963
Epoch 29: Val Loss 7178.20654
Epoch 30: Val Loss 7011.30371
Epoch 31: Val Loss 6844.81055
Epoch 32: Val Loss 6697.02734
Epoch 33: Val Loss 6557.69434
Epoch 34: Val Loss 6413.11328
Epoch 35: Val Loss 6281.21387
Epoch 36: Val Loss 6140.18457
Epoch 37: Val Loss 6031.60547
Epoch 38: Val Loss 5898.84717
Epoch 39: Val Loss 5778.81396
Epoch 40: Val Loss 5687.34326
Epoch 41: Val Loss 5565.50342
Epoch 42: Val Loss 5464.10156
Epoch 43: Val Loss 5378.90918
Epoch 44: Val Loss 5287.49365
Epoch 45: Val Loss 5186.11670
Epoch 46: Val Loss 5113.91699
Epoch 47: Val Loss 5042.23193
Epoch 48: Val Loss 4948.95068
Epoch 49: Val Loss 4883.37158
Epoch 50: Val Loss 4814.17139
Epoch 51: Val Loss 4770.09375
Epoch 52: Val Loss 4685.43652
Epoch 53: Val Loss 4632.12256
Epoch 54: Val Loss 4589.00781
Epoch 55: Val Loss 4528.77979
Epoch 56: Val Loss 4488.09521
Epoch 57: Val Loss 4453.66553
Epoch 58: Val Loss 4383.35205
Epoch 59: Val Loss 4346.61377
Epoch 60: Val Loss 4300.18750
Epoch 61: Val Loss 4256.01953
Epoch 62: Val Loss 4221.16113
Epoch 63: Val Loss 4177.17725
Epoch 64: Val Loss 4157.06543
Epoch 65: Val Loss 4094.73242
Epoch 66: Val Loss 4077.33716
Epoch 67: Val Loss 4042.21191
Epoch 68: Val Loss 3988.09521
Epoch 69: Val Loss 3961.56470
Epoch 70: Val Loss 3944.75586
Epoch 71: Val Loss 3902.00439
Epoch 72: Val Loss 3871.21924
Epoch 73: Val Loss 3832.26025
Epoch 74: Val Loss 3798.70801
Epoch 75: Val Loss 3758.61035
Epoch 76: Val Loss 3736.97925
Epoch 77: Val Loss 3710.29980
Epoch 78: Val Loss 3680.18506
Epoch 79: Val Loss 3659.06738
Epoch 80: Val Loss 3621.93213
Epoch 81: Val Loss 3601.45142
Epoch 82: Val Loss 3557.32642
Epoch 83: Val Loss 3546.37354
Epoch 84: Val Loss 3496.54517
Epoch 85: Val Loss 3480.39258
Epoch 86: Val Loss 3445.87134
Epoch 87: Val Loss 3422.49097
Epoch 88: Val Loss 3379.60693
Epoch 89: Val Loss 3358.35693
Epoch 90: Val Loss 3338.80176
Epoch 91: Val Loss 3319.60889
Epoch 92: Val Loss 3277.25928
Epoch 93: Val Loss 3260.19604
Epoch 94: Val Loss 3220.87109
Epoch 95: Val Loss 3200.44312
Epoch 96: Val Loss 3173.78198
Epoch 97: Val Loss 3152.29736
Epoch 98: Val Loss 3114.06396
Epoch 99: Val Loss 3092.20044
{'MSE - mean': 3007.8922306597797, 'MSE - std': 84.30833439297044, 'R2 - mean': 0.6276794970404891, 'R2 - std': 0.032392607290237685} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518414.93750
Epoch 1: Val Loss 517947.56250
Epoch 2: Val Loss 517239.00000
Epoch 3: Val Loss 515897.59375
Epoch 4: Val Loss 513384.25000
Epoch 5: Val Loss 508888.68750
Epoch 6: Val Loss 501313.50000
Epoch 7: Val Loss 489209.84375
Epoch 8: Val Loss 471013.18750
Epoch 9: Val Loss 445372.53125
Epoch 10: Val Loss 410991.71875
Epoch 11: Val Loss 367547.46875
Epoch 12: Val Loss 315626.40625
Epoch 13: Val Loss 257628.93750
Epoch 14: Val Loss 197888.42188
Epoch 15: Val Loss 141574.03125
Epoch 16: Val Loss 94418.20312
Epoch 17: Val Loss 60156.55859
Epoch 18: Val Loss 39030.76172
Epoch 19: Val Loss 27442.96094
Epoch 20: Val Loss 21490.66211
Epoch 21: Val Loss 17793.05664
Epoch 22: Val Loss 15257.08789
Epoch 23: Val Loss 13298.42871
Epoch 24: Val Loss 11699.44629
Epoch 25: Val Loss 10416.86328
Epoch 26: Val Loss 9355.59863
Epoch 27: Val Loss 8480.14258
Epoch 28: Val Loss 7714.08008
Epoch 29: Val Loss 7071.47070
Epoch 30: Val Loss 6503.28223
Epoch 31: Val Loss 6054.90088
Epoch 32: Val Loss 5708.94580
Epoch 33: Val Loss 5401.26855
Epoch 34: Val Loss 5095.85645
Epoch 35: Val Loss 4851.14111
Epoch 36: Val Loss 4653.85938
Epoch 37: Val Loss 4484.13135
Epoch 38: Val Loss 4340.84033
Epoch 39: Val Loss 4240.51709
Epoch 40: Val Loss 4142.34082
Epoch 41: Val Loss 4067.01611
Epoch 42: Val Loss 4000.12964
Epoch 43: Val Loss 3937.15479
Epoch 44: Val Loss 3876.61255
Epoch 45: Val Loss 3838.63403
Epoch 46: Val Loss 3799.36157
Epoch 47: Val Loss 3765.47559
Epoch 48: Val Loss 3729.40479
Epoch 49: Val Loss 3711.23242
Epoch 50: Val Loss 3683.68896
Epoch 51: Val Loss 3651.84644
Epoch 52: Val Loss 3626.93066
Epoch 53: Val Loss 3607.31177
Epoch 54: Val Loss 3587.25439
Epoch 55: Val Loss 3556.85742
Epoch 56: Val Loss 3530.11377
Epoch 57: Val Loss 3514.81934
Epoch 58: Val Loss 3501.25732
Epoch 59: Val Loss 3474.82690
Epoch 60: Val Loss 3463.80225
Epoch 61: Val Loss 3420.10913
Epoch 62: Val Loss 3393.31592
Epoch 63: Val Loss 3364.83154
Epoch 64: Val Loss 3356.78101
Epoch 65: Val Loss 3324.83154
Epoch 66: Val Loss 3315.21289
Epoch 67: Val Loss 3301.10229
Epoch 68: Val Loss 3288.90674
Epoch 69: Val Loss 3250.86621
Epoch 70: Val Loss 3231.43164
Epoch 71: Val Loss 3208.17773
Epoch 72: Val Loss 3204.41187
Epoch 73: Val Loss 3173.04907
Epoch 74: Val Loss 3153.54663
Epoch 75: Val Loss 3135.15186
Epoch 76: Val Loss 3117.78662
Epoch 77: Val Loss 3098.09644
Epoch 78: Val Loss 3072.60669
Epoch 79: Val Loss 3061.87842
Epoch 80: Val Loss 3041.67773
Epoch 81: Val Loss 3022.75952
Epoch 82: Val Loss 2996.55566
Epoch 83: Val Loss 3000.13574
Epoch 84: Val Loss 2973.32861
Epoch 85: Val Loss 2941.94116
Epoch 86: Val Loss 2939.12085
Epoch 87: Val Loss 2907.01172
Epoch 88: Val Loss 2889.85400
Epoch 89: Val Loss 2888.40894
Epoch 90: Val Loss 2859.92212
Epoch 91: Val Loss 2834.07056
Epoch 92: Val Loss 2837.92676
Epoch 93: Val Loss 2813.69775
Epoch 94: Val Loss 2793.63306
Epoch 95: Val Loss 2775.50903
Epoch 96: Val Loss 2773.39331
Epoch 97: Val Loss 2752.74878
Epoch 98: Val Loss 2739.65771
Epoch 99: Val Loss 2719.30640
{'MSE - mean': 2911.6969879198004, 'MSE - std': 152.4652297531636, 'R2 - mean': 0.6544084561065442, 'R2 - std': 0.046134533452531414} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520237.90625
Epoch 1: Val Loss 519703.00000
Epoch 2: Val Loss 518803.75000
Epoch 3: Val Loss 517215.25000
Epoch 4: Val Loss 514320.43750
Epoch 5: Val Loss 509395.68750
Epoch 6: Val Loss 501313.25000
Epoch 7: Val Loss 488759.46875
Epoch 8: Val Loss 470092.71875
Epoch 9: Val Loss 443455.00000
Epoch 10: Val Loss 407225.81250
Epoch 11: Val Loss 360662.46875
Epoch 12: Val Loss 304679.59375
Epoch 13: Val Loss 241773.26562
Epoch 14: Val Loss 177133.23438
Epoch 15: Val Loss 117388.78906
Epoch 16: Val Loss 69431.90625
Epoch 17: Val Loss 37412.03125
Epoch 18: Val Loss 20383.27148
Epoch 19: Val Loss 13260.44824
Epoch 20: Val Loss 10390.09082
Epoch 21: Val Loss 8926.88379
Epoch 22: Val Loss 7761.33936
Epoch 23: Val Loss 6851.83887
Epoch 24: Val Loss 6114.69287
Epoch 25: Val Loss 5563.69385
Epoch 26: Val Loss 5119.36523
Epoch 27: Val Loss 4782.02832
Epoch 28: Val Loss 4522.97217
Epoch 29: Val Loss 4341.66748
Epoch 30: Val Loss 4163.38330
Epoch 31: Val Loss 4044.56860
Epoch 32: Val Loss 3935.77393
Epoch 33: Val Loss 3844.46680
Epoch 34: Val Loss 3769.09546
Epoch 35: Val Loss 3715.60205
Epoch 36: Val Loss 3665.59937
Epoch 37: Val Loss 3602.44067
Epoch 38: Val Loss 3561.78906
Epoch 39: Val Loss 3525.36060
Epoch 40: Val Loss 3483.36377
Epoch 41: Val Loss 3451.18652
Epoch 42: Val Loss 3425.95557
Epoch 43: Val Loss 3384.26758
Epoch 44: Val Loss 3361.55078
Epoch 45: Val Loss 3331.83594
Epoch 46: Val Loss 3309.59253
Epoch 47: Val Loss 3276.35425
Epoch 48: Val Loss 3250.20874
Epoch 49: Val Loss 3219.82959
Epoch 50: Val Loss 3206.80688
Epoch 51: Val Loss 3178.78271
Epoch 52: Val Loss 3152.56958
Epoch 53: Val Loss 3119.27734
Epoch 54: Val Loss 3100.11743
Epoch 55: Val Loss 3094.72290
Epoch 56: Val Loss 3069.86621
Epoch 57: Val Loss 3050.65625
Epoch 58: Val Loss 3020.45972
Epoch 59: Val Loss 3001.55444
Epoch 60: Val Loss 2994.82373
Epoch 61: Val Loss 2958.30469
Epoch 62: Val Loss 2941.23730
Epoch 63: Val Loss 2932.12085
Epoch 64: Val Loss 2904.22510
Epoch 65: Val Loss 2889.80371
Epoch 66: Val Loss 2873.30835
Epoch 67: Val Loss 2864.87402
Epoch 68: Val Loss 2847.30640
Epoch 69: Val Loss 2826.03857
Epoch 70: Val Loss 2808.37891
Epoch 71: Val Loss 2793.95410
Epoch 72: Val Loss 2778.09814
Epoch 73: Val Loss 2759.45410
Epoch 74: Val Loss 2738.50635
Epoch 75: Val Loss 2730.65796
Epoch 76: Val Loss 2720.43823
Epoch 77: Val Loss 2693.86060
Epoch 78: Val Loss 2681.28369
Epoch 79: Val Loss 2669.88184
Epoch 80: Val Loss 2653.65405
Epoch 81: Val Loss 2629.13037
Epoch 82: Val Loss 2631.37720
Epoch 83: Val Loss 2616.23535
Epoch 84: Val Loss 2591.46680
Epoch 85: Val Loss 2583.42627
Epoch 86: Val Loss 2575.06738
Epoch 87: Val Loss 2553.85010
Epoch 88: Val Loss 2552.50635
Epoch 89: Val Loss 2524.80737
Epoch 90: Val Loss 2511.81079
Epoch 91: Val Loss 2498.67358
Epoch 92: Val Loss 2502.15234
Epoch 93: Val Loss 2470.83130
Epoch 94: Val Loss 2458.34692
Epoch 95: Val Loss 2451.67114
Epoch 96: Val Loss 2441.89014
Epoch 97: Val Loss 2418.48511
Epoch 98: Val Loss 2408.91650
Epoch 99: Val Loss 2398.84204
{'MSE - mean': 2783.4833119186046, 'MSE - std': 258.3611326302444, 'R2 - mean': 0.6686000337854299, 'R2 - std': 0.04690947680063797} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517912.65625
Epoch 1: Val Loss 517396.71875
Epoch 2: Val Loss 516321.28125
Epoch 3: Val Loss 514080.56250
Epoch 4: Val Loss 509756.12500
Epoch 5: Val Loss 501915.43750
Epoch 6: Val Loss 488885.03125
Epoch 7: Val Loss 468705.40625
Epoch 8: Val Loss 439361.56250
Epoch 9: Val Loss 399243.09375
Epoch 10: Val Loss 347848.40625
Epoch 11: Val Loss 287855.09375
Epoch 12: Val Loss 223730.96875
Epoch 13: Val Loss 161798.23438
Epoch 14: Val Loss 109035.53906
Epoch 15: Val Loss 70987.71875
Epoch 16: Val Loss 48062.98047
Epoch 17: Val Loss 35460.57422
Epoch 18: Val Loss 28380.51172
Epoch 19: Val Loss 23637.76758
Epoch 20: Val Loss 20345.36523
Epoch 21: Val Loss 17729.78125
Epoch 22: Val Loss 15581.39258
Epoch 23: Val Loss 13837.10449
Epoch 24: Val Loss 12462.32031
Epoch 25: Val Loss 11234.38379
Epoch 26: Val Loss 10216.54590
Epoch 27: Val Loss 9354.10938
Epoch 28: Val Loss 8637.33594
Epoch 29: Val Loss 8024.88232
Epoch 30: Val Loss 7486.02637
Epoch 31: Val Loss 7024.73828
Epoch 32: Val Loss 6620.81641
Epoch 33: Val Loss 6299.40430
Epoch 34: Val Loss 6010.15186
Epoch 35: Val Loss 5747.19385
Epoch 36: Val Loss 5531.79248
Epoch 37: Val Loss 5342.81982
Epoch 38: Val Loss 5150.83398
Epoch 39: Val Loss 4984.29102
Epoch 40: Val Loss 4845.85059
Epoch 41: Val Loss 4713.73584
Epoch 42: Val Loss 4582.10498
Epoch 43: Val Loss 4489.52148
Epoch 44: Val Loss 4376.96924
Epoch 45: Val Loss 4265.00244
Epoch 46: Val Loss 4206.79199
Epoch 47: Val Loss 4128.90186
Epoch 48: Val Loss 4054.43555
Epoch 49: Val Loss 3958.43481
Epoch 50: Val Loss 3900.65625
Epoch 51: Val Loss 3857.97852
Epoch 52: Val Loss 3784.13867
Epoch 53: Val Loss 3729.68384
Epoch 54: Val Loss 3685.93530
Epoch 55: Val Loss 3624.62183
Epoch 56: Val Loss 3583.26782
Epoch 57: Val Loss 3538.99805
Epoch 58: Val Loss 3489.85059
Epoch 59: Val Loss 3446.24951
Epoch 60: Val Loss 3399.12695
Epoch 61: Val Loss 3369.59595
Epoch 62: Val Loss 3336.10156
Epoch 63: Val Loss 3299.75537
Epoch 64: Val Loss 3257.99902
Epoch 65: Val Loss 3219.87231
Epoch 66: Val Loss 3195.44824
Epoch 67: Val Loss 3160.58911
Epoch 68: Val Loss 3129.63428
Epoch 69: Val Loss 3097.59668
Epoch 70: Val Loss 3060.45825
Epoch 71: Val Loss 3038.19556
Epoch 72: Val Loss 3010.45898
Epoch 73: Val Loss 2996.14624
Epoch 74: Val Loss 2952.26367
Epoch 75: Val Loss 2932.91748
Epoch 76: Val Loss 2913.55225
Epoch 77: Val Loss 2877.49146
Epoch 78: Val Loss 2846.40845
Epoch 79: Val Loss 2829.71021
Epoch 80: Val Loss 2806.76611
Epoch 81: Val Loss 2776.63354
Epoch 82: Val Loss 2753.53613
Epoch 83: Val Loss 2726.09766
Epoch 84: Val Loss 2709.82202
Epoch 85: Val Loss 2687.82056
Epoch 86: Val Loss 2669.53638
Epoch 87: Val Loss 2633.75708
Epoch 88: Val Loss 2613.90283
Epoch 89: Val Loss 2599.92676
Epoch 90: Val Loss 2572.00952
Epoch 91: Val Loss 2548.37085
Epoch 92: Val Loss 2527.13818
Epoch 93: Val Loss 2513.05688
Epoch 94: Val Loss 2500.31689
Epoch 95: Val Loss 2462.21411
Epoch 96: Val Loss 2456.52832
Epoch 97: Val Loss 2428.35254
Epoch 98: Val Loss 2418.60303
Epoch 99: Val Loss 2392.99854
{'MSE - mean': 2705.38638099614, 'MSE - std': 278.92096077781434, 'R2 - mean': 0.6748903695762796, 'R2 - std': 0.0438026541252268} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 2705.38638099614 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 24 with value: 2394.2725020802454.
Best parameters: {'dim': 64, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.1}
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516373.21875
Epoch 1: Val Loss 515883.81250
Epoch 2: Val Loss 514995.53125
Epoch 3: Val Loss 513239.75000
Epoch 4: Val Loss 509809.84375
Epoch 5: Val Loss 503476.43750
Epoch 6: Val Loss 492465.50000
Epoch 7: Val Loss 474275.00000
Epoch 8: Val Loss 445991.90625
Epoch 9: Val Loss 404960.81250
Epoch 10: Val Loss 349776.06250
Epoch 11: Val Loss 282669.75000
Epoch 12: Val Loss 208286.89062
Epoch 13: Val Loss 136925.54688
Epoch 14: Val Loss 78694.82031
Epoch 15: Val Loss 41801.84375
Epoch 16: Val Loss 23976.87891
Epoch 17: Val Loss 16782.49219
Epoch 18: Val Loss 13314.67480
Epoch 19: Val Loss 11234.73145
Epoch 20: Val Loss 9809.54102
Epoch 21: Val Loss 8765.57227
Epoch 22: Val Loss 7977.19238
Epoch 23: Val Loss 7356.34180
Epoch 24: Val Loss 6909.90283
Epoch 25: Val Loss 6542.22607
Epoch 26: Val Loss 6287.11572
Epoch 27: Val Loss 6050.68799
Epoch 28: Val Loss 5854.03906
Epoch 29: Val Loss 5691.83057
Epoch 30: Val Loss 5544.35596
Epoch 31: Val Loss 5412.12500
Epoch 32: Val Loss 5270.58789
Epoch 33: Val Loss 5134.63867
Epoch 34: Val Loss 5048.02100
Epoch 35: Val Loss 4954.75684
Epoch 36: Val Loss 4853.20605
Epoch 37: Val Loss 4757.59521
Epoch 38: Val Loss 4674.57959
Epoch 39: Val Loss 4599.95703
Epoch 40: Val Loss 4553.27783
Epoch 41: Val Loss 4448.52637
Epoch 42: Val Loss 4390.98096
Epoch 43: Val Loss 4353.07715
Epoch 44: Val Loss 4277.44971
Epoch 45: Val Loss 4239.18018
Epoch 46: Val Loss 4182.08154
Epoch 47: Val Loss 4132.33594
Epoch 48: Val Loss 4087.95166
Epoch 49: Val Loss 4033.97437
Epoch 50: Val Loss 4015.35425
Epoch 51: Val Loss 3960.98022
Epoch 52: Val Loss 3898.47974
Epoch 53: Val Loss 3862.15210
Epoch 54: Val Loss 3826.65527
Epoch 55: Val Loss 3797.09302
Epoch 56: Val Loss 3753.73145
Epoch 57: Val Loss 3716.30420
Epoch 58: Val Loss 3682.73438
Epoch 59: Val Loss 3673.57764
Epoch 60: Val Loss 3620.95654
Epoch 61: Val Loss 3592.29712
Epoch 62: Val Loss 3580.60449
Epoch 63: Val Loss 3531.65259
Epoch 64: Val Loss 3518.37646
Epoch 65: Val Loss 3472.75757
Epoch 66: Val Loss 3443.57520
Epoch 67: Val Loss 3439.77563
Epoch 68: Val Loss 3385.79370
Epoch 69: Val Loss 3363.11597
Epoch 70: Val Loss 3352.59814
Epoch 71: Val Loss 3308.92212
Epoch 72: Val Loss 3297.68188
Epoch 73: Val Loss 3264.80884
Epoch 74: Val Loss 3241.96875
Epoch 75: Val Loss 3215.98486
Epoch 76: Val Loss 3217.28198
Epoch 77: Val Loss 3172.14551
Epoch 78: Val Loss 3144.48096
Epoch 79: Val Loss 3136.65234
Epoch 80: Val Loss 3102.04907
Epoch 81: Val Loss 3104.81494
Epoch 82: Val Loss 3068.75244
Epoch 83: Val Loss 3039.55249
Epoch 84: Val Loss 3016.61060
Epoch 85: Val Loss 2990.03149
Epoch 86: Val Loss 2984.44971
Epoch 87: Val Loss 2956.22119
Epoch 88: Val Loss 2949.94263
Epoch 89: Val Loss 2912.63037
Epoch 90: Val Loss 2895.94629
Epoch 91: Val Loss 2882.96362
Epoch 92: Val Loss 2883.07129
Epoch 93: Val Loss 2837.72754
Epoch 94: Val Loss 2815.71143
Epoch 95: Val Loss 2814.45557
Epoch 96: Val Loss 2784.69336
Epoch 97: Val Loss 2763.33350
Epoch 98: Val Loss 2733.49756
Epoch 99: Val Loss 2721.70703
Saved Losses
{'MSE - mean': 2721.707011407822, 'MSE - std': 0.0, 'R2 - mean': 0.6835445228038244, 'R2 - std': 0.0} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 524564.18750
Epoch 1: Val Loss 523724.78125
Epoch 2: Val Loss 522153.03125
Epoch 3: Val Loss 519244.37500
Epoch 4: Val Loss 514097.31250
Epoch 5: Val Loss 505497.03125
Epoch 6: Val Loss 491666.09375
Epoch 7: Val Loss 470542.40625
Epoch 8: Val Loss 439933.34375
Epoch 9: Val Loss 397969.25000
Epoch 10: Val Loss 343371.15625
Epoch 11: Val Loss 277760.56250
Epoch 12: Val Loss 206705.64062
Epoch 13: Val Loss 139064.54688
Epoch 14: Val Loss 84161.67188
Epoch 15: Val Loss 47834.75391
Epoch 16: Val Loss 28864.58594
Epoch 17: Val Loss 20786.95312
Epoch 18: Val Loss 16916.41797
Epoch 19: Val Loss 14405.41016
Epoch 20: Val Loss 12496.95801
Epoch 21: Val Loss 11033.96191
Epoch 22: Val Loss 9945.49023
Epoch 23: Val Loss 9044.60547
Epoch 24: Val Loss 8324.56543
Epoch 25: Val Loss 7761.68262
Epoch 26: Val Loss 7273.26709
Epoch 27: Val Loss 6894.87451
Epoch 28: Val Loss 6555.12012
Epoch 29: Val Loss 6267.01221
Epoch 30: Val Loss 6025.25879
Epoch 31: Val Loss 5814.85059
Epoch 32: Val Loss 5630.41602
Epoch 33: Val Loss 5466.01611
Epoch 34: Val Loss 5312.46777
Epoch 35: Val Loss 5169.63086
Epoch 36: Val Loss 5042.23682
Epoch 37: Val Loss 4919.43652
Epoch 38: Val Loss 4815.53369
Epoch 39: Val Loss 4714.52246
Epoch 40: Val Loss 4621.26611
Epoch 41: Val Loss 4530.60693
Epoch 42: Val Loss 4447.71484
Epoch 43: Val Loss 4363.74170
Epoch 44: Val Loss 4296.60840
Epoch 45: Val Loss 4233.66211
Epoch 46: Val Loss 4153.04346
Epoch 47: Val Loss 4098.56885
Epoch 48: Val Loss 4035.04199
Epoch 49: Val Loss 3982.24927
Epoch 50: Val Loss 3921.78174
Epoch 51: Val Loss 3874.80737
Epoch 52: Val Loss 3819.23853
Epoch 53: Val Loss 3761.98950
Epoch 54: Val Loss 3735.54175
Epoch 55: Val Loss 3692.68115
Epoch 56: Val Loss 3646.92725
Epoch 57: Val Loss 3605.80786
Epoch 58: Val Loss 3571.51880
Epoch 59: Val Loss 3530.20117
Epoch 60: Val Loss 3503.27002
Epoch 61: Val Loss 3462.48438
Epoch 62: Val Loss 3447.32397
Epoch 63: Val Loss 3412.50146
Epoch 64: Val Loss 3366.34521
Epoch 65: Val Loss 3344.73755
Epoch 66: Val Loss 3324.57739
Epoch 67: Val Loss 3276.59546
Epoch 68: Val Loss 3265.09937
Epoch 69: Val Loss 3238.45459
Epoch 70: Val Loss 3209.56812
Epoch 71: Val Loss 3197.97559
Epoch 72: Val Loss 3154.62061
Epoch 73: Val Loss 3137.99072
Epoch 74: Val Loss 3131.97778
Epoch 75: Val Loss 3091.58691
Epoch 76: Val Loss 3072.31421
Epoch 77: Val Loss 3038.70850
Epoch 78: Val Loss 3033.15942
Epoch 79: Val Loss 3023.31592
Epoch 80: Val Loss 2973.00952
Epoch 81: Val Loss 2953.52124
Epoch 82: Val Loss 2955.02661
Epoch 83: Val Loss 2910.66919
Epoch 84: Val Loss 2912.19946
Epoch 85: Val Loss 2882.99683
Epoch 86: Val Loss 2868.30835
Epoch 87: Val Loss 2837.38428
Epoch 88: Val Loss 2814.02417
Epoch 89: Val Loss 2812.34082
Epoch 90: Val Loss 2776.18677
Epoch 91: Val Loss 2758.21973
Epoch 92: Val Loss 2749.77148
Epoch 93: Val Loss 2724.91260
Epoch 94: Val Loss 2724.00684
Epoch 95: Val Loss 2688.92773
Epoch 96: Val Loss 2678.15186
Epoch 97: Val Loss 2660.29370
Epoch 98: Val Loss 2625.86597
Epoch 99: Val Loss 2643.40112
Saved Losses
{'MSE - mean': 2673.7864042590863, 'MSE - std': 47.9206071487356, 'R2 - mean': 0.6699330984029823, 'R2 - std': 0.013611424400842065} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 518442.28125
Epoch 1: Val Loss 517959.96875
Epoch 2: Val Loss 516970.31250
Epoch 3: Val Loss 514989.56250
Epoch 4: Val Loss 511182.87500
Epoch 5: Val Loss 504247.18750
Epoch 6: Val Loss 492280.90625
Epoch 7: Val Loss 473105.37500
Epoch 8: Val Loss 444115.34375
Epoch 9: Val Loss 402855.12500
Epoch 10: Val Loss 348943.81250
Epoch 11: Val Loss 282801.15625
Epoch 12: Val Loss 211325.42188
Epoch 13: Val Loss 142999.10938
Epoch 14: Val Loss 86923.20312
Epoch 15: Val Loss 50298.42969
Epoch 16: Val Loss 31355.59570
Epoch 17: Val Loss 22824.09180
Epoch 18: Val Loss 18376.31445
Epoch 19: Val Loss 15427.55469
Epoch 20: Val Loss 13287.48340
Epoch 21: Val Loss 11710.04883
Epoch 22: Val Loss 10430.55469
Epoch 23: Val Loss 9461.27832
Epoch 24: Val Loss 8686.01367
Epoch 25: Val Loss 8002.83398
Epoch 26: Val Loss 7452.94238
Epoch 27: Val Loss 6985.91846
Epoch 28: Val Loss 6589.24463
Epoch 29: Val Loss 6279.31006
Epoch 30: Val Loss 6015.26807
Epoch 31: Val Loss 5797.19482
Epoch 32: Val Loss 5587.84180
Epoch 33: Val Loss 5401.06201
Epoch 34: Val Loss 5245.29150
Epoch 35: Val Loss 5125.02441
Epoch 36: Val Loss 4951.44092
Epoch 37: Val Loss 4882.01221
Epoch 38: Val Loss 4808.83496
Epoch 39: Val Loss 4699.27344
Epoch 40: Val Loss 4581.79150
Epoch 41: Val Loss 4496.35938
Epoch 42: Val Loss 4399.04004
Epoch 43: Val Loss 4332.45557
Epoch 44: Val Loss 4280.72852
Epoch 45: Val Loss 4213.21240
Epoch 46: Val Loss 4137.10449
Epoch 47: Val Loss 4074.64014
Epoch 48: Val Loss 4046.01270
Epoch 49: Val Loss 3968.92090
Epoch 50: Val Loss 3921.14624
Epoch 51: Val Loss 3859.23657
Epoch 52: Val Loss 3801.89355
Epoch 53: Val Loss 3748.99072
Epoch 54: Val Loss 3688.61157
Epoch 55: Val Loss 3670.33423
Epoch 56: Val Loss 3612.15283
Epoch 57: Val Loss 3560.13159
Epoch 58: Val Loss 3532.67456
Epoch 59: Val Loss 3483.11938
Epoch 60: Val Loss 3475.26807
Epoch 61: Val Loss 3408.67529
Epoch 62: Val Loss 3341.38867
Epoch 63: Val Loss 3339.55835
Epoch 64: Val Loss 3317.10010
Epoch 65: Val Loss 3261.01758
Epoch 66: Val Loss 3218.98877
Epoch 67: Val Loss 3191.49390
Epoch 68: Val Loss 3168.83667
Epoch 69: Val Loss 3116.63330
Epoch 70: Val Loss 3089.26074
Epoch 71: Val Loss 3057.14941
Epoch 72: Val Loss 3063.84131
Epoch 73: Val Loss 2982.56787
Epoch 74: Val Loss 2987.90601
Epoch 75: Val Loss 2931.01855
Epoch 76: Val Loss 2913.85107
Epoch 77: Val Loss 2887.96313
Epoch 78: Val Loss 2873.26465
Epoch 79: Val Loss 2838.98413
Epoch 80: Val Loss 2804.66357
Epoch 81: Val Loss 2807.41309
Epoch 82: Val Loss 2746.26514
Epoch 83: Val Loss 2745.41919
Epoch 84: Val Loss 2716.07739
Epoch 85: Val Loss 2705.03931
Epoch 86: Val Loss 2660.26221
Epoch 87: Val Loss 2631.29321
Epoch 88: Val Loss 2634.59155
Epoch 89: Val Loss 2594.07739
Epoch 90: Val Loss 2573.80371
Epoch 91: Val Loss 2557.17139
Epoch 92: Val Loss 2557.26245
Epoch 93: Val Loss 2512.29956
Epoch 94: Val Loss 2487.25684
Epoch 95: Val Loss 2478.82593
Epoch 96: Val Loss 2474.60718
Epoch 97: Val Loss 2423.64478
Epoch 98: Val Loss 2412.29077
Epoch 99: Val Loss 2388.24048
Saved Losses
{'MSE - mean': 2578.604446742014, 'MSE - std': 140.17893255847213, 'R2 - mean': 0.6944329363648047, 'R2 - std': 0.03638678381579869} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 520365.12500
Epoch 1: Val Loss 520080.12500
Epoch 2: Val Loss 519572.68750
Epoch 3: Val Loss 518597.93750
Epoch 4: Val Loss 516749.25000
Epoch 5: Val Loss 513350.21875
Epoch 6: Val Loss 507455.90625
Epoch 7: Val Loss 497614.62500
Epoch 8: Val Loss 482293.84375
Epoch 9: Val Loss 459730.25000
Epoch 10: Val Loss 428015.50000
Epoch 11: Val Loss 386363.59375
Epoch 12: Val Loss 334228.59375
Epoch 13: Val Loss 273558.34375
Epoch 14: Val Loss 208081.87500
Epoch 15: Val Loss 143638.10938
Epoch 16: Val Loss 87735.10156
Epoch 17: Val Loss 46336.64453
Epoch 18: Val Loss 21778.84961
Epoch 19: Val Loss 10950.08203
Epoch 20: Val Loss 7544.76172
Epoch 21: Val Loss 6794.10303
Epoch 22: Val Loss 6573.70215
Epoch 23: Val Loss 6412.19287
Epoch 24: Val Loss 6264.42236
Epoch 25: Val Loss 6119.80176
Epoch 26: Val Loss 5982.62305
Epoch 27: Val Loss 5862.87012
Epoch 28: Val Loss 5745.57910
Epoch 29: Val Loss 5632.21924
Epoch 30: Val Loss 5524.03174
Epoch 31: Val Loss 5421.88818
Epoch 32: Val Loss 5315.59229
Epoch 33: Val Loss 5212.48047
Epoch 34: Val Loss 5112.01514
Epoch 35: Val Loss 5012.04395
Epoch 36: Val Loss 4915.10059
Epoch 37: Val Loss 4819.15918
Epoch 38: Val Loss 4727.96826
Epoch 39: Val Loss 4633.80762
Epoch 40: Val Loss 4539.01172
Epoch 41: Val Loss 4445.96973
Epoch 42: Val Loss 4361.64844
Epoch 43: Val Loss 4278.11670
Epoch 44: Val Loss 4198.91992
Epoch 45: Val Loss 4117.89648
Epoch 46: Val Loss 4045.66846
Epoch 47: Val Loss 3973.35498
Epoch 48: Val Loss 3902.83521
Epoch 49: Val Loss 3837.41650
Epoch 50: Val Loss 3776.03540
Epoch 51: Val Loss 3712.31299
Epoch 52: Val Loss 3656.22485
Epoch 53: Val Loss 3602.94922
Epoch 54: Val Loss 3549.45361
Epoch 55: Val Loss 3497.24194
Epoch 56: Val Loss 3453.01196
Epoch 57: Val Loss 3410.75952
Epoch 58: Val Loss 3364.44604
Epoch 59: Val Loss 3325.09009
Epoch 60: Val Loss 3287.77271
Epoch 61: Val Loss 3246.15894
Epoch 62: Val Loss 3210.96826
Epoch 63: Val Loss 3174.82715
Epoch 64: Val Loss 3143.13696
Epoch 65: Val Loss 3106.41553
Epoch 66: Val Loss 3072.56592
Epoch 67: Val Loss 3041.86743
Epoch 68: Val Loss 3013.92578
Epoch 69: Val Loss 2982.83545
Epoch 70: Val Loss 2951.48462
Epoch 71: Val Loss 2927.58423
Epoch 72: Val Loss 2898.61841
Epoch 73: Val Loss 2869.47559
Epoch 74: Val Loss 2841.79370
Epoch 75: Val Loss 2815.78809
Epoch 76: Val Loss 2789.28906
Epoch 77: Val Loss 2764.17456
Epoch 78: Val Loss 2740.54028
Epoch 79: Val Loss 2716.22363
Epoch 80: Val Loss 2691.60815
Epoch 81: Val Loss 2668.33081
Epoch 82: Val Loss 2649.38965
Epoch 83: Val Loss 2624.39209
Epoch 84: Val Loss 2603.57031
Epoch 85: Val Loss 2581.36206
Epoch 86: Val Loss 2560.92065
Epoch 87: Val Loss 2535.34180
Epoch 88: Val Loss 2514.89795
Epoch 89: Val Loss 2493.75000
Epoch 90: Val Loss 2475.56519
Epoch 91: Val Loss 2455.90088
Epoch 92: Val Loss 2434.81445
Epoch 93: Val Loss 2413.81104
Epoch 94: Val Loss 2394.57349
Epoch 95: Val Loss 2376.30933
Epoch 96: Val Loss 2358.54395
Epoch 97: Val Loss 2340.70044
Epoch 98: Val Loss 2319.04517
Epoch 99: Val Loss 2300.29321
Saved Losses
{'MSE - mean': 2509.0265939339515, 'MSE - std': 171.05798038695994, 'R2 - mean': 0.7015847738283771, 'R2 - std': 0.03385919173074459} 
 

In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 517707.56250
Epoch 1: Val Loss 517264.18750
Epoch 2: Val Loss 516390.84375
Epoch 3: Val Loss 514635.68750
Epoch 4: Val Loss 511239.84375
Epoch 5: Val Loss 505025.06250
Epoch 6: Val Loss 494228.15625
Epoch 7: Val Loss 476244.28125
Epoch 8: Val Loss 448346.90625
Epoch 9: Val Loss 408171.37500
Epoch 10: Val Loss 355104.18750
Epoch 11: Val Loss 289838.68750
Epoch 12: Val Loss 216998.85938
Epoch 13: Val Loss 144720.21875
Epoch 14: Val Loss 83545.72656
Epoch 15: Val Loss 41753.96094
Epoch 16: Val Loss 20796.94922
Epoch 17: Val Loss 13319.02832
Epoch 18: Val Loss 11030.88770
Epoch 19: Val Loss 10021.15625
Epoch 20: Val Loss 9444.05859
Epoch 21: Val Loss 8923.54199
Epoch 22: Val Loss 8516.09473
Epoch 23: Val Loss 8099.80225
Epoch 24: Val Loss 7729.49170
Epoch 25: Val Loss 7419.77295
Epoch 26: Val Loss 7100.95898
Epoch 27: Val Loss 6799.29932
Epoch 28: Val Loss 6500.61621
Epoch 29: Val Loss 6249.11768
Epoch 30: Val Loss 6065.96094
Epoch 31: Val Loss 5781.04199
Epoch 32: Val Loss 5619.38818
Epoch 33: Val Loss 5413.74561
Epoch 34: Val Loss 5204.03516
Epoch 35: Val Loss 5041.45654
Epoch 36: Val Loss 4891.63281
Epoch 37: Val Loss 4731.60059
Epoch 38: Val Loss 4604.93213
Epoch 39: Val Loss 4467.11084
Epoch 40: Val Loss 4340.41211
Epoch 41: Val Loss 4223.17725
Epoch 42: Val Loss 4109.24756
Epoch 43: Val Loss 3990.88965
Epoch 44: Val Loss 3912.61523
Epoch 45: Val Loss 3824.38647
Epoch 46: Val Loss 3711.44702
Epoch 47: Val Loss 3647.81860
Epoch 48: Val Loss 3571.59888
Epoch 49: Val Loss 3486.01807
Epoch 50: Val Loss 3404.12964
Epoch 51: Val Loss 3356.70215
Epoch 52: Val Loss 3309.56494
Epoch 53: Val Loss 3227.85986
Epoch 54: Val Loss 3185.95972
Epoch 55: Val Loss 3133.42578
Epoch 56: Val Loss 3097.89307
Epoch 57: Val Loss 3048.59448
Epoch 58: Val Loss 3017.62793
Epoch 59: Val Loss 2973.82983
Epoch 60: Val Loss 2940.73682
Epoch 61: Val Loss 2913.13281
Epoch 62: Val Loss 2881.26709
Epoch 63: Val Loss 2843.49878
Epoch 64: Val Loss 2811.50952
Epoch 65: Val Loss 2786.50073
Epoch 66: Val Loss 2757.22632
Epoch 67: Val Loss 2729.05835
Epoch 68: Val Loss 2696.87354
Epoch 69: Val Loss 2680.19043
Epoch 70: Val Loss 2639.84277
Epoch 71: Val Loss 2617.94458
Epoch 72: Val Loss 2614.18066
Epoch 73: Val Loss 2580.81641
Epoch 74: Val Loss 2563.54321
Epoch 75: Val Loss 2537.28638
Epoch 76: Val Loss 2515.46729
Epoch 77: Val Loss 2503.85840
Epoch 78: Val Loss 2480.75952
Epoch 79: Val Loss 2449.57007
Epoch 80: Val Loss 2438.95215
Epoch 81: Val Loss 2410.70581
Epoch 82: Val Loss 2406.44409
Epoch 83: Val Loss 2372.80835
Epoch 84: Val Loss 2355.53516
Epoch 85: Val Loss 2337.20581
Epoch 86: Val Loss 2326.53857
Epoch 87: Val Loss 2303.26831
Epoch 88: Val Loss 2297.59131
Epoch 89: Val Loss 2269.08569
Epoch 90: Val Loss 2251.88208
Epoch 91: Val Loss 2256.61523
Epoch 92: Val Loss 2217.95215
Epoch 93: Val Loss 2199.04004
Epoch 94: Val Loss 2179.22607
Epoch 95: Val Loss 2170.45581
Epoch 96: Val Loss 2154.89990
Epoch 97: Val Loss 2149.69189
Epoch 98: Val Loss 2120.60474
Epoch 99: Val Loss 2112.51245
Saved Losses
{'MSE - mean': 2429.723784585809, 'MSE - std': 220.37333868978757, 'R2 - mean': 0.7083096179862477, 'R2 - std': 0.03313683762096261} 
 

Saving model.....
Results After CV: {'MSE - mean': 2429.723784585809, 'MSE - std': 220.37333868978757, 'R2 - mean': 0.7083096179862477, 'R2 - std': 0.03313683762096261}
Train time: 61.53606433620007
Inference time: 0.10934636059973854
Finished cross validation


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/black_friday.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/black_friday.yml', data_parallel=False, dataset='Black_Friday', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 2, 3, 5, 6, 7, 8], num_classes=1, num_features=9, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=True, ordinal_idx=[1], scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Black_Friday...
Dataset loaded! 

X b4 encoding : ['F' '0-17' 10 'A' 2 0 1 6 14] 

(166821, 9)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 2, 3, 5, 6, 7, 8]
Ordinal Idx: [1]
Cat Dims: None 
 

Normonal Idx: [0, 2, 3, 5, 6, 7, 8]
Cat Idx Part II: [0, 1, 2, 3, 5, 6, 7, 8] 
ENDE 
 

X after Nominal Encoding: ['F' '0-17' 10 'A' 2 0 1 6 14] 
 

Scaling the data...
X after Scaling: ['F' '0-17' 10 'A' 0.1076520112629123 0 1 6 14] 
 

Ordinal Idx: [0]
One Hot Encoding...
X after One Hot Encoding: ['0-17' 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
 0.1076520112629123] 
 

args.num_features: 71
args.cat_idx: [0]
Cat Dims: [8]
New Shape: (166821, 71)
True 
 

Using an existing study with name 'TabTransformer_Black_Friday' instead of creating a new one.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17405394.00000
Epoch 1: Val Loss 13760163.00000
Epoch 2: Val Loss 13239181.00000
Epoch 3: Val Loss 13052081.00000
Epoch 4: Val Loss 13050407.00000
Epoch 5: Val Loss 12992993.00000
Epoch 6: Val Loss 12967330.00000
Epoch 7: Val Loss 12949040.00000
Epoch 8: Val Loss 12951723.00000
Epoch 9: Val Loss 12931666.00000
Epoch 10: Val Loss 12974753.00000
Epoch 11: Val Loss 12904116.00000
Epoch 12: Val Loss 12897694.00000
Epoch 13: Val Loss 12866165.00000
Epoch 14: Val Loss 12914435.00000
Epoch 15: Val Loss 12855809.00000
Epoch 16: Val Loss 12817908.00000
Epoch 17: Val Loss 12813992.00000
Epoch 18: Val Loss 12791583.00000
Epoch 19: Val Loss 12772359.00000
Epoch 20: Val Loss 12748223.00000
Epoch 21: Val Loss 12736944.00000
Epoch 22: Val Loss 12844326.00000
Epoch 23: Val Loss 12682630.00000
Epoch 24: Val Loss 12715831.00000
Epoch 25: Val Loss 12697941.00000
Epoch 26: Val Loss 12647263.00000
Epoch 27: Val Loss 12609443.00000
Epoch 28: Val Loss 12630318.00000
Epoch 29: Val Loss 12602298.00000
Epoch 30: Val Loss 12620707.00000
Epoch 31: Val Loss 12657993.00000
Epoch 32: Val Loss 12589218.00000
Epoch 33: Val Loss 12588226.00000
Epoch 34: Val Loss 12552259.00000
Epoch 35: Val Loss 12518124.00000
Epoch 36: Val Loss 12541821.00000
Epoch 37: Val Loss 12521557.00000
Epoch 38: Val Loss 12500052.00000
Epoch 39: Val Loss 12558927.00000
Epoch 40: Val Loss 12546703.00000
Epoch 41: Val Loss 12474508.00000
Epoch 42: Val Loss 12474571.00000
Epoch 43: Val Loss 12451872.00000
Epoch 44: Val Loss 12455002.00000
Epoch 45: Val Loss 12500273.00000
Epoch 46: Val Loss 12443886.00000
Epoch 47: Val Loss 12443331.00000
Epoch 48: Val Loss 12452300.00000
Epoch 49: Val Loss 12444239.00000
Epoch 50: Val Loss 12400924.00000
Epoch 51: Val Loss 12425850.00000
Epoch 52: Val Loss 12391560.00000
Epoch 53: Val Loss 12373993.00000
Epoch 54: Val Loss 12393667.00000
Epoch 55: Val Loss 12342023.00000
Epoch 56: Val Loss 12383206.00000
Epoch 57: Val Loss 12432554.00000
Epoch 58: Val Loss 12358212.00000
Epoch 59: Val Loss 12387723.00000
Epoch 60: Val Loss 12349087.00000
Epoch 61: Val Loss 12376044.00000
Epoch 62: Val Loss 12339754.00000
Epoch 63: Val Loss 12367005.00000
Epoch 64: Val Loss 12342616.00000
Epoch 65: Val Loss 12357654.00000
Epoch 66: Val Loss 12392523.00000
Epoch 67: Val Loss 12342305.00000
Epoch 68: Val Loss 12348652.00000
Epoch 69: Val Loss 12331168.00000
Epoch 70: Val Loss 12402220.00000
Epoch 71: Val Loss 12353328.00000
Epoch 72: Val Loss 12353281.00000
Epoch 73: Val Loss 12320761.00000
Epoch 74: Val Loss 12323399.00000
Epoch 75: Val Loss 12337500.00000
Epoch 76: Val Loss 12313972.00000
Epoch 77: Val Loss 12315425.00000
Epoch 78: Val Loss 12309350.00000
Epoch 79: Val Loss 12300185.00000
Epoch 80: Val Loss 12456597.00000
Epoch 81: Val Loss 12316813.00000
Epoch 82: Val Loss 12352407.00000
Epoch 83: Val Loss 12315539.00000
Epoch 84: Val Loss 12331128.00000
Epoch 85: Val Loss 12325234.00000
Epoch 86: Val Loss 12322551.00000
Epoch 87: Val Loss 12313807.00000
Epoch 88: Val Loss 12309670.00000
Epoch 89: Val Loss 12305804.00000
Epoch 90: Val Loss 12337985.00000
Epoch 91: Val Loss 12305579.00000
Epoch 92: Val Loss 12346695.00000
Epoch 93: Val Loss 12309135.00000
Epoch 94: Val Loss 12308313.00000
Epoch 95: Val Loss 12307522.00000
Epoch 96: Val Loss 12295080.00000
Epoch 97: Val Loss 12332665.00000
Epoch 98: Val Loss 12329210.00000
Epoch 99: Val Loss 12309889.00000
{'MSE - mean': 12295697.856182074, 'MSE - std': 0.0, 'R2 - mean': 0.5186730539488105, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20762092.00000
Epoch 1: Val Loss 14595462.00000
Epoch 2: Val Loss 13622585.00000
Epoch 3: Val Loss 13328597.00000
Epoch 4: Val Loss 13244725.00000
Epoch 5: Val Loss 13229064.00000
Epoch 6: Val Loss 13243036.00000
Epoch 7: Val Loss 13192833.00000
Epoch 8: Val Loss 13209872.00000
Epoch 9: Val Loss 13192204.00000
Epoch 10: Val Loss 13158037.00000
Epoch 11: Val Loss 13135768.00000
Epoch 12: Val Loss 13107494.00000
Epoch 13: Val Loss 13079373.00000
Epoch 14: Val Loss 13107344.00000
Epoch 15: Val Loss 13030773.00000
Epoch 16: Val Loss 13075191.00000
Epoch 17: Val Loss 12978283.00000
Epoch 18: Val Loss 12943162.00000
Epoch 19: Val Loss 12929459.00000
Epoch 20: Val Loss 12957857.00000
Epoch 21: Val Loss 12868085.00000
Epoch 22: Val Loss 12860041.00000
Epoch 23: Val Loss 12824572.00000
Epoch 24: Val Loss 12871388.00000
Epoch 25: Val Loss 12796629.00000
Epoch 26: Val Loss 12776401.00000
Epoch 27: Val Loss 12766931.00000
Epoch 28: Val Loss 12760718.00000
Epoch 29: Val Loss 12728653.00000
Epoch 30: Val Loss 12727543.00000
Epoch 31: Val Loss 12729976.00000
Epoch 32: Val Loss 12701717.00000
Epoch 33: Val Loss 12698407.00000
Epoch 34: Val Loss 12710944.00000
Epoch 35: Val Loss 12662578.00000
Epoch 36: Val Loss 12671584.00000
Epoch 37: Val Loss 12659420.00000
Epoch 38: Val Loss 12672722.00000
Epoch 39: Val Loss 12660987.00000
Epoch 40: Val Loss 12647013.00000
Epoch 41: Val Loss 12643907.00000
Epoch 42: Val Loss 12673024.00000
Epoch 43: Val Loss 12631594.00000
Epoch 44: Val Loss 12618956.00000
Epoch 45: Val Loss 12634099.00000
Epoch 46: Val Loss 12627874.00000
Epoch 47: Val Loss 12617492.00000
Epoch 48: Val Loss 12625685.00000
Epoch 49: Val Loss 12637393.00000
Epoch 50: Val Loss 12659605.00000
Epoch 51: Val Loss 12614898.00000
Epoch 52: Val Loss 12634062.00000
Epoch 53: Val Loss 12616726.00000
Epoch 54: Val Loss 12587575.00000
Epoch 55: Val Loss 12566476.00000
Epoch 56: Val Loss 12578192.00000
Epoch 57: Val Loss 12583877.00000
Epoch 58: Val Loss 12745219.00000
Epoch 59: Val Loss 12616946.00000
Epoch 60: Val Loss 12564102.00000
Epoch 61: Val Loss 12592654.00000
Epoch 62: Val Loss 12562592.00000
Epoch 63: Val Loss 12576028.00000
Epoch 64: Val Loss 12562189.00000
Epoch 65: Val Loss 12630863.00000
Epoch 66: Val Loss 12591328.00000
Epoch 67: Val Loss 12589472.00000
Epoch 68: Val Loss 12569856.00000
Epoch 69: Val Loss 12565361.00000
Epoch 70: Val Loss 12622577.00000
Epoch 71: Val Loss 12535093.00000
Epoch 72: Val Loss 12545310.00000
Epoch 73: Val Loss 12535435.00000
Epoch 74: Val Loss 12537618.00000
Epoch 75: Val Loss 12535278.00000
Epoch 76: Val Loss 12518455.00000
Epoch 77: Val Loss 12540570.00000
Epoch 78: Val Loss 12534419.00000
Epoch 79: Val Loss 12506246.00000
Epoch 80: Val Loss 12476033.00000
Epoch 81: Val Loss 12596135.00000
Epoch 82: Val Loss 12495062.00000
Epoch 83: Val Loss 12510304.00000
Epoch 84: Val Loss 12480552.00000
Epoch 85: Val Loss 12506894.00000
Epoch 86: Val Loss 12632216.00000
Epoch 87: Val Loss 12533150.00000
Epoch 88: Val Loss 12477365.00000
Epoch 89: Val Loss 12568043.00000
Epoch 90: Val Loss 12461028.00000
Epoch 91: Val Loss 12461553.00000
Epoch 92: Val Loss 12519196.00000
Epoch 93: Val Loss 12490000.00000
Epoch 94: Val Loss 12453085.00000
Epoch 95: Val Loss 12460900.00000
Epoch 96: Val Loss 12456776.00000
Epoch 97: Val Loss 12424579.00000
Epoch 98: Val Loss 12444056.00000
Epoch 99: Val Loss 12429483.00000
{'MSE - mean': 12363990.333685227, 'MSE - std': 68292.4775031535, 'R2 - mean': 0.5188442799807294, 'R2 - std': 0.00017122603191888075} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20686236.00000
Epoch 1: Val Loss 14846542.00000
Epoch 2: Val Loss 13684548.00000
Epoch 3: Val Loss 13384285.00000
Epoch 4: Val Loss 13275708.00000
Epoch 5: Val Loss 13260787.00000
Epoch 6: Val Loss 13205744.00000
Epoch 7: Val Loss 13222047.00000
Epoch 8: Val Loss 13187901.00000
Epoch 9: Val Loss 13195522.00000
Epoch 10: Val Loss 13166427.00000
Epoch 11: Val Loss 13133106.00000
Epoch 12: Val Loss 13110589.00000
Epoch 13: Val Loss 13113454.00000
Epoch 14: Val Loss 13080359.00000
Epoch 15: Val Loss 13096097.00000
Epoch 16: Val Loss 13005987.00000
Epoch 17: Val Loss 13020881.00000
Epoch 18: Val Loss 13017126.00000
Epoch 19: Val Loss 12943821.00000
Epoch 20: Val Loss 12932247.00000
Epoch 21: Val Loss 12934626.00000
Epoch 22: Val Loss 12905193.00000
Epoch 23: Val Loss 12884898.00000
Epoch 24: Val Loss 12894211.00000
Epoch 25: Val Loss 12872882.00000
Epoch 26: Val Loss 12848074.00000
Epoch 27: Val Loss 12875873.00000
Epoch 28: Val Loss 12834808.00000
Epoch 29: Val Loss 12820366.00000
Epoch 30: Val Loss 12828116.00000
Epoch 31: Val Loss 12877694.00000
Epoch 32: Val Loss 12860964.00000
Epoch 33: Val Loss 12794672.00000
Epoch 34: Val Loss 12795376.00000
Epoch 35: Val Loss 12808126.00000
Epoch 36: Val Loss 12793040.00000
Epoch 37: Val Loss 12840585.00000
Epoch 38: Val Loss 12786376.00000
Epoch 39: Val Loss 12781387.00000
Epoch 40: Val Loss 12755133.00000
Epoch 41: Val Loss 12779474.00000
Epoch 42: Val Loss 12777388.00000
Epoch 43: Val Loss 12767417.00000
Epoch 44: Val Loss 12744374.00000
Epoch 45: Val Loss 12738971.00000
Epoch 46: Val Loss 12727634.00000
Epoch 47: Val Loss 12737357.00000
Epoch 48: Val Loss 12725712.00000
Epoch 49: Val Loss 12787600.00000
Epoch 50: Val Loss 12738882.00000
Epoch 51: Val Loss 12749640.00000
Epoch 52: Val Loss 12729861.00000
Epoch 53: Val Loss 12719843.00000
Epoch 54: Val Loss 12707686.00000
Epoch 55: Val Loss 12710752.00000
Epoch 56: Val Loss 12700616.00000
Epoch 57: Val Loss 12728252.00000
Epoch 58: Val Loss 12672884.00000
Epoch 59: Val Loss 12669968.00000
Epoch 60: Val Loss 12694569.00000
Epoch 61: Val Loss 12676441.00000
Epoch 62: Val Loss 12684152.00000
Epoch 63: Val Loss 12676052.00000
Epoch 64: Val Loss 12673989.00000
Epoch 65: Val Loss 12678231.00000
Epoch 66: Val Loss 12664855.00000
Epoch 67: Val Loss 12659419.00000
Epoch 68: Val Loss 12676859.00000
Epoch 69: Val Loss 12659113.00000
Epoch 70: Val Loss 12675323.00000
Epoch 71: Val Loss 12654025.00000
Epoch 72: Val Loss 12651167.00000
Epoch 73: Val Loss 12663077.00000
Epoch 74: Val Loss 12670490.00000
Epoch 75: Val Loss 12662167.00000
Epoch 76: Val Loss 12652157.00000
Epoch 77: Val Loss 12654146.00000
Epoch 78: Val Loss 12656638.00000
Epoch 79: Val Loss 12684874.00000
Epoch 80: Val Loss 12652768.00000
Epoch 81: Val Loss 12685864.00000
Epoch 82: Val Loss 12651760.00000
Epoch 83: Val Loss 12653490.00000
Epoch 84: Val Loss 12663536.00000
Epoch 85: Val Loss 12669259.00000
Epoch 86: Val Loss 12664459.00000
Epoch 87: Val Loss 12697061.00000
Epoch 88: Val Loss 12676710.00000
Epoch 89: Val Loss 12627438.00000
Epoch 90: Val Loss 12656378.00000
Epoch 91: Val Loss 12637934.00000
Epoch 92: Val Loss 12656435.00000
Epoch 93: Val Loss 12637805.00000
Epoch 94: Val Loss 12672945.00000
Epoch 95: Val Loss 12652878.00000
Epoch 96: Val Loss 12642910.00000
Epoch 97: Val Loss 12666818.00000
Epoch 98: Val Loss 12624083.00000
Epoch 99: Val Loss 12628133.00000
{'MSE - mean': 12453819.029034846, 'MSE - std': 138735.83052733095, 'R2 - mean': 0.5160153237758981, 'R2 - std': 0.004003190226187597} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20413234.00000
Epoch 1: Val Loss 14537343.00000
Epoch 2: Val Loss 13554719.00000
Epoch 3: Val Loss 13260183.00000
Epoch 4: Val Loss 13199039.00000
Epoch 5: Val Loss 13154011.00000
Epoch 6: Val Loss 13166372.00000
Epoch 7: Val Loss 13109207.00000
Epoch 8: Val Loss 13122039.00000
Epoch 9: Val Loss 13069717.00000
Epoch 10: Val Loss 13048256.00000
Epoch 11: Val Loss 13053367.00000
Epoch 12: Val Loss 13028613.00000
Epoch 13: Val Loss 13049707.00000
Epoch 14: Val Loss 13065770.00000
Epoch 15: Val Loss 13000113.00000
Epoch 16: Val Loss 12976164.00000
Epoch 17: Val Loss 12949887.00000
Epoch 18: Val Loss 12963165.00000
Epoch 19: Val Loss 12910914.00000
Epoch 20: Val Loss 12911860.00000
Epoch 21: Val Loss 12939952.00000
Epoch 22: Val Loss 12976202.00000
Epoch 23: Val Loss 12929571.00000
Epoch 24: Val Loss 12892689.00000
Epoch 25: Val Loss 12832278.00000
Epoch 26: Val Loss 12872585.00000
Epoch 27: Val Loss 12827085.00000
Epoch 28: Val Loss 12871549.00000
Epoch 29: Val Loss 12796894.00000
Epoch 30: Val Loss 12781996.00000
Epoch 31: Val Loss 12792151.00000
Epoch 32: Val Loss 12759329.00000
Epoch 33: Val Loss 12800060.00000
Epoch 34: Val Loss 12742722.00000
Epoch 35: Val Loss 12753399.00000
Epoch 36: Val Loss 12737289.00000
Epoch 37: Val Loss 12722076.00000
Epoch 38: Val Loss 12683643.00000
Epoch 39: Val Loss 12701636.00000
Epoch 40: Val Loss 12694511.00000
Epoch 41: Val Loss 12690425.00000
Epoch 42: Val Loss 12661216.00000
Epoch 43: Val Loss 12645656.00000
Epoch 44: Val Loss 12627550.00000
Epoch 45: Val Loss 12731489.00000
Epoch 46: Val Loss 12611312.00000
Epoch 47: Val Loss 12646374.00000
Epoch 48: Val Loss 12688809.00000
Epoch 49: Val Loss 12682968.00000
Epoch 50: Val Loss 12574354.00000
Epoch 51: Val Loss 12552837.00000
Epoch 52: Val Loss 12638197.00000
Epoch 53: Val Loss 12555784.00000
Epoch 54: Val Loss 12546775.00000
Epoch 55: Val Loss 12556374.00000
Epoch 56: Val Loss 12562835.00000
Epoch 57: Val Loss 12554756.00000
Epoch 58: Val Loss 12526475.00000
Epoch 59: Val Loss 12554240.00000
Epoch 60: Val Loss 12528412.00000
Epoch 61: Val Loss 12532564.00000
Epoch 62: Val Loss 12510067.00000
Epoch 63: Val Loss 12488923.00000
Epoch 64: Val Loss 12478060.00000
Epoch 65: Val Loss 12483692.00000
Epoch 66: Val Loss 12513694.00000
Epoch 67: Val Loss 12512756.00000
Epoch 68: Val Loss 12534542.00000
Epoch 69: Val Loss 12490578.00000
Epoch 70: Val Loss 12486705.00000
Epoch 71: Val Loss 12598466.00000
Epoch 72: Val Loss 12458961.00000
Epoch 73: Val Loss 12514987.00000
Epoch 74: Val Loss 12495062.00000
Epoch 75: Val Loss 12480380.00000
Epoch 76: Val Loss 12475378.00000
Epoch 77: Val Loss 12463580.00000
Epoch 78: Val Loss 12496811.00000
Epoch 79: Val Loss 12483758.00000
Epoch 80: Val Loss 12483656.00000
Epoch 81: Val Loss 12480028.00000
Epoch 82: Val Loss 12462839.00000
Epoch 83: Val Loss 12468754.00000
Epoch 84: Val Loss 12469567.00000
Epoch 85: Val Loss 12484090.00000
Epoch 86: Val Loss 12463871.00000
Epoch 87: Val Loss 12464127.00000
Epoch 88: Val Loss 12463876.00000
Epoch 89: Val Loss 12512216.00000
Epoch 90: Val Loss 12439700.00000
Epoch 91: Val Loss 12508159.00000
Epoch 92: Val Loss 12428631.00000
Epoch 93: Val Loss 12464817.00000
Epoch 94: Val Loss 12436332.00000
Epoch 95: Val Loss 12450503.00000
Epoch 96: Val Loss 12422465.00000
Epoch 97: Val Loss 12464672.00000
Epoch 98: Val Loss 12432577.00000
Epoch 99: Val Loss 12435296.00000
{'MSE - mean': 12449284.461918265, 'MSE - std': 120405.1905064127, 'R2 - mean': 0.5168282315087883, 'R2 - std': 0.00374187198295997} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20425240.00000
Epoch 1: Val Loss 14590546.00000
Epoch 2: Val Loss 13614848.00000
Epoch 3: Val Loss 13273418.00000
Epoch 4: Val Loss 13207476.00000
Epoch 5: Val Loss 13198899.00000
Epoch 6: Val Loss 13140212.00000
Epoch 7: Val Loss 13130668.00000
Epoch 8: Val Loss 13133932.00000
Epoch 9: Val Loss 13114077.00000
Epoch 10: Val Loss 13083946.00000
Epoch 11: Val Loss 13129680.00000
Epoch 12: Val Loss 13122100.00000
Epoch 13: Val Loss 13136382.00000
Epoch 14: Val Loss 13042107.00000
Epoch 15: Val Loss 13051445.00000
Epoch 16: Val Loss 13034503.00000
Epoch 17: Val Loss 13027140.00000
Epoch 18: Val Loss 13015425.00000
Epoch 19: Val Loss 13042916.00000
Epoch 20: Val Loss 12986316.00000
Epoch 21: Val Loss 13000031.00000
Epoch 22: Val Loss 12990733.00000
Epoch 23: Val Loss 13019664.00000
Epoch 24: Val Loss 12966562.00000
Epoch 25: Val Loss 12966844.00000
Epoch 26: Val Loss 12944356.00000
Epoch 27: Val Loss 12943789.00000
Epoch 28: Val Loss 12936566.00000
Epoch 29: Val Loss 12889433.00000
Epoch 30: Val Loss 12927271.00000
Epoch 31: Val Loss 12891448.00000
Epoch 32: Val Loss 12859660.00000
Epoch 33: Val Loss 12860269.00000
Epoch 34: Val Loss 12820794.00000
Epoch 35: Val Loss 12819338.00000
Epoch 36: Val Loss 12763433.00000
Epoch 37: Val Loss 12772729.00000
Epoch 38: Val Loss 12727470.00000
Epoch 39: Val Loss 12718079.00000
Epoch 40: Val Loss 12727412.00000
Epoch 41: Val Loss 12670575.00000
Epoch 42: Val Loss 12685767.00000
Epoch 43: Val Loss 12647175.00000
Epoch 44: Val Loss 12670911.00000
Epoch 45: Val Loss 12631924.00000
Epoch 46: Val Loss 12671600.00000
Epoch 47: Val Loss 12598990.00000
Epoch 48: Val Loss 12638874.00000
Epoch 49: Val Loss 12593599.00000
Epoch 50: Val Loss 12576880.00000
Epoch 51: Val Loss 12567416.00000
Epoch 52: Val Loss 12554956.00000
Epoch 53: Val Loss 12528072.00000
Epoch 54: Val Loss 12525834.00000
Epoch 55: Val Loss 12527612.00000
Epoch 56: Val Loss 12558954.00000
Epoch 57: Val Loss 12519444.00000
Epoch 58: Val Loss 12515633.00000
Epoch 59: Val Loss 12516064.00000
Epoch 60: Val Loss 12567558.00000
Epoch 61: Val Loss 12664009.00000
Epoch 62: Val Loss 12663741.00000
Epoch 63: Val Loss 12498701.00000
Epoch 64: Val Loss 12495269.00000
Epoch 65: Val Loss 12540810.00000
Epoch 66: Val Loss 12623301.00000
Epoch 67: Val Loss 12496761.00000
Epoch 68: Val Loss 12481074.00000
Epoch 69: Val Loss 12492756.00000
Epoch 70: Val Loss 12507081.00000
Epoch 71: Val Loss 12474552.00000
Epoch 72: Val Loss 12466637.00000
Epoch 73: Val Loss 12556828.00000
Epoch 74: Val Loss 12459773.00000
Epoch 75: Val Loss 12468214.00000
Epoch 76: Val Loss 12506661.00000
Epoch 77: Val Loss 12500871.00000
Epoch 78: Val Loss 12478107.00000
Epoch 79: Val Loss 12459534.00000
Epoch 80: Val Loss 12455857.00000
Epoch 81: Val Loss 12467049.00000
Epoch 82: Val Loss 12478750.00000
Epoch 83: Val Loss 12467095.00000
Epoch 84: Val Loss 12442389.00000
Epoch 85: Val Loss 12454834.00000
Epoch 86: Val Loss 12475322.00000
Epoch 87: Val Loss 12468247.00000
Epoch 88: Val Loss 12415431.00000
Epoch 89: Val Loss 12424892.00000
Epoch 90: Val Loss 12419226.00000
Epoch 91: Val Loss 12417889.00000
Epoch 92: Val Loss 12431947.00000
Epoch 93: Val Loss 12430771.00000
Epoch 94: Val Loss 12455573.00000
Epoch 95: Val Loss 12447784.00000
Epoch 96: Val Loss 12425323.00000
Epoch 97: Val Loss 12448705.00000
Epoch 98: Val Loss 12412523.00000
Epoch 99: Val Loss 12424692.00000
{'MSE - mean': 12443717.191287499, 'MSE - std': 108267.75110721191, 'R2 - mean': 0.5182064080505867, 'R2 - std': 0.00433575449846309} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 12443717.191287499 and parameters: {'dim': 128, 'depth': 2, 'heads': 8, 'weight_decay': -5, 'learning_rate': -3, 'dropout': 0.4}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 28529708.00000
Epoch 1: Val Loss 27403378.00000
Epoch 2: Val Loss 26846126.00000
Epoch 3: Val Loss 26267066.00000
Epoch 4: Val Loss 25759862.00000
Epoch 5: Val Loss 25008174.00000
Epoch 6: Val Loss 24049576.00000
Epoch 7: Val Loss 23049592.00000
Epoch 8: Val Loss 21703682.00000
Epoch 9: Val Loss 20314156.00000
Epoch 10: Val Loss 19089596.00000
Epoch 11: Val Loss 17953980.00000
Epoch 12: Val Loss 17144848.00000
Epoch 13: Val Loss 16509065.00000
Epoch 14: Val Loss 16004588.00000
Epoch 15: Val Loss 15547349.00000
Epoch 16: Val Loss 15235073.00000
Epoch 17: Val Loss 14909866.00000
Epoch 18: Val Loss 14659281.00000
Epoch 19: Val Loss 14423411.00000
Epoch 20: Val Loss 14232691.00000
Epoch 21: Val Loss 14102780.00000
Epoch 22: Val Loss 13980259.00000
Epoch 23: Val Loss 13845282.00000
Epoch 24: Val Loss 13769903.00000
Epoch 25: Val Loss 13694910.00000
Epoch 26: Val Loss 13595879.00000
Epoch 27: Val Loss 13551365.00000
Epoch 28: Val Loss 13500537.00000
Epoch 29: Val Loss 13451844.00000
Epoch 30: Val Loss 13397234.00000
Epoch 31: Val Loss 13354989.00000
Epoch 32: Val Loss 13304902.00000
Epoch 33: Val Loss 13307780.00000
Epoch 34: Val Loss 13270409.00000
Epoch 35: Val Loss 13262797.00000
Epoch 36: Val Loss 13213102.00000
Epoch 37: Val Loss 13186437.00000
Epoch 38: Val Loss 13192813.00000
Epoch 39: Val Loss 13185295.00000
Epoch 40: Val Loss 13162133.00000
Epoch 41: Val Loss 13153535.00000
Epoch 42: Val Loss 13136152.00000
Epoch 43: Val Loss 13129192.00000
Epoch 44: Val Loss 13126101.00000
Epoch 45: Val Loss 13141565.00000
Epoch 46: Val Loss 13099864.00000
Epoch 47: Val Loss 13103482.00000
Epoch 48: Val Loss 13112572.00000
Epoch 49: Val Loss 13087820.00000
Epoch 50: Val Loss 13110056.00000
Epoch 51: Val Loss 13087098.00000
Epoch 52: Val Loss 13095158.00000
Epoch 53: Val Loss 13069087.00000
Epoch 54: Val Loss 13091571.00000
Epoch 55: Val Loss 13075958.00000
Epoch 56: Val Loss 13070355.00000
Epoch 57: Val Loss 13062731.00000
Epoch 58: Val Loss 13082974.00000
Epoch 59: Val Loss 13057589.00000
Epoch 60: Val Loss 13091761.00000
Epoch 61: Val Loss 13070992.00000
Epoch 62: Val Loss 13064699.00000
Epoch 63: Val Loss 13072349.00000
Epoch 64: Val Loss 13054079.00000
Epoch 65: Val Loss 13072588.00000
Epoch 66: Val Loss 13054402.00000
Epoch 67: Val Loss 13058184.00000
Epoch 68: Val Loss 13054982.00000
Epoch 69: Val Loss 13049672.00000
Epoch 70: Val Loss 13055488.00000
Epoch 71: Val Loss 13033278.00000
Epoch 72: Val Loss 13027220.00000
Epoch 73: Val Loss 13039468.00000
Epoch 74: Val Loss 13047325.00000
Epoch 75: Val Loss 13051322.00000
Epoch 76: Val Loss 13049794.00000
Epoch 77: Val Loss 13053089.00000
Epoch 78: Val Loss 13031468.00000
Epoch 79: Val Loss 13041412.00000
Epoch 80: Val Loss 13022440.00000
Epoch 81: Val Loss 13038387.00000
Epoch 82: Val Loss 13045309.00000
Epoch 83: Val Loss 13022669.00000
Epoch 84: Val Loss 13035477.00000
Epoch 85: Val Loss 13031810.00000
Epoch 86: Val Loss 13032106.00000
Epoch 87: Val Loss 13020780.00000
Epoch 88: Val Loss 13026495.00000
Epoch 89: Val Loss 13018946.00000
Epoch 90: Val Loss 13011149.00000
Epoch 91: Val Loss 13014033.00000
Epoch 92: Val Loss 13036977.00000
Epoch 93: Val Loss 13026790.00000
Epoch 94: Val Loss 13003435.00000
Epoch 95: Val Loss 13048374.00000
Epoch 96: Val Loss 13029043.00000
Epoch 97: Val Loss 13005727.00000
Epoch 98: Val Loss 13034242.00000
Epoch 99: Val Loss 13018352.00000
{'MSE - mean': 13079100.82120472, 'MSE - std': 0.0, 'R2 - mean': 0.48800598965589503, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 28785256.00000
Epoch 1: Val Loss 28128026.00000
Epoch 2: Val Loss 27542006.00000
Epoch 3: Val Loss 26853144.00000
Epoch 4: Val Loss 26219224.00000
Epoch 5: Val Loss 25580692.00000
Epoch 6: Val Loss 24841520.00000
Epoch 7: Val Loss 24028044.00000
Epoch 8: Val Loss 22990052.00000
Epoch 9: Val Loss 21535058.00000
Epoch 10: Val Loss 20347156.00000
Epoch 11: Val Loss 19123118.00000
Epoch 12: Val Loss 18111738.00000
Epoch 13: Val Loss 17321152.00000
Epoch 14: Val Loss 16781476.00000
Epoch 15: Val Loss 16239309.00000
Epoch 16: Val Loss 15846228.00000
Epoch 17: Val Loss 15528575.00000
Epoch 18: Val Loss 15241879.00000
Epoch 19: Val Loss 14970919.00000
Epoch 20: Val Loss 14758022.00000
Epoch 21: Val Loss 14549818.00000
Epoch 22: Val Loss 14416533.00000
Epoch 23: Val Loss 14292511.00000
Epoch 24: Val Loss 14174299.00000
Epoch 25: Val Loss 14086148.00000
Epoch 26: Val Loss 14007735.00000
Epoch 27: Val Loss 13912480.00000
Epoch 28: Val Loss 13869050.00000
Epoch 29: Val Loss 13816125.00000
Epoch 30: Val Loss 13735233.00000
Epoch 31: Val Loss 13706187.00000
Epoch 32: Val Loss 13628748.00000
Epoch 33: Val Loss 13607173.00000
Epoch 34: Val Loss 13573335.00000
Epoch 35: Val Loss 13545772.00000
Epoch 36: Val Loss 13529349.00000
Epoch 37: Val Loss 13516640.00000
Epoch 38: Val Loss 13482158.00000
Epoch 39: Val Loss 13496415.00000
Epoch 40: Val Loss 13459278.00000
Epoch 41: Val Loss 13406356.00000
Epoch 42: Val Loss 13422909.00000
Epoch 43: Val Loss 13401726.00000
Epoch 44: Val Loss 13387237.00000
Epoch 45: Val Loss 13399629.00000
Epoch 46: Val Loss 13386368.00000
Epoch 47: Val Loss 13369622.00000
Epoch 48: Val Loss 13373451.00000
Epoch 49: Val Loss 13373270.00000
Epoch 50: Val Loss 13373191.00000
Epoch 51: Val Loss 13383369.00000
Epoch 52: Val Loss 13337918.00000
Epoch 53: Val Loss 13326925.00000
Epoch 54: Val Loss 13347709.00000
Epoch 55: Val Loss 13318206.00000
Epoch 56: Val Loss 13326346.00000
Epoch 57: Val Loss 13334960.00000
Epoch 58: Val Loss 13331202.00000
Epoch 59: Val Loss 13323971.00000
Epoch 60: Val Loss 13327366.00000
Epoch 61: Val Loss 13300319.00000
Epoch 62: Val Loss 13306997.00000
Epoch 63: Val Loss 13316622.00000
Epoch 64: Val Loss 13293203.00000
Epoch 65: Val Loss 13300972.00000
Epoch 66: Val Loss 13318584.00000
Epoch 67: Val Loss 13307774.00000
Epoch 68: Val Loss 13288987.00000
Epoch 69: Val Loss 13277448.00000
Epoch 70: Val Loss 13300767.00000
Epoch 71: Val Loss 13281884.00000
Epoch 72: Val Loss 13287529.00000
Epoch 73: Val Loss 13292917.00000
Epoch 74: Val Loss 13283006.00000
Epoch 75: Val Loss 13278124.00000
Epoch 76: Val Loss 13285074.00000
Epoch 77: Val Loss 13288908.00000
Epoch 78: Val Loss 13294005.00000
Epoch 79: Val Loss 13274945.00000
Epoch 80: Val Loss 13281827.00000
Epoch 81: Val Loss 13284836.00000
Epoch 82: Val Loss 13270880.00000
Epoch 83: Val Loss 13281711.00000
Epoch 84: Val Loss 13307503.00000
Epoch 85: Val Loss 13301890.00000
Epoch 86: Val Loss 13286968.00000
Epoch 87: Val Loss 13276340.00000
Epoch 88: Val Loss 13276710.00000
Epoch 89: Val Loss 13259781.00000
Epoch 90: Val Loss 13268187.00000
Epoch 91: Val Loss 13279383.00000
Epoch 92: Val Loss 13290903.00000
Epoch 93: Val Loss 13249059.00000
Epoch 94: Val Loss 13257547.00000
Epoch 95: Val Loss 13281615.00000
Epoch 96: Val Loss 13258732.00000
Epoch 97: Val Loss 13277556.00000
Epoch 98: Val Loss 13235909.00000
Epoch 99: Val Loss 13271613.00000
{'MSE - mean': 13186274.319767658, 'MSE - std': 107173.49856293853, 'R2 - mean': 0.4868522217417366, 'R2 - std': 0.0011537679141584434} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 29339082.00000
Epoch 1: Val Loss 28525302.00000
Epoch 2: Val Loss 27881240.00000
Epoch 3: Val Loss 27158364.00000
Epoch 4: Val Loss 26486768.00000
Epoch 5: Val Loss 25749914.00000
Epoch 6: Val Loss 24819558.00000
Epoch 7: Val Loss 23679492.00000
Epoch 8: Val Loss 22467056.00000
Epoch 9: Val Loss 21108774.00000
Epoch 10: Val Loss 19812798.00000
Epoch 11: Val Loss 18724770.00000
Epoch 12: Val Loss 17880110.00000
Epoch 13: Val Loss 17226420.00000
Epoch 14: Val Loss 16664650.00000
Epoch 15: Val Loss 16212949.00000
Epoch 16: Val Loss 15855514.00000
Epoch 17: Val Loss 15543700.00000
Epoch 18: Val Loss 15287109.00000
Epoch 19: Val Loss 15053639.00000
Epoch 20: Val Loss 14864899.00000
Epoch 21: Val Loss 14708130.00000
Epoch 22: Val Loss 14522719.00000
Epoch 23: Val Loss 14414648.00000
Epoch 24: Val Loss 14328261.00000
Epoch 25: Val Loss 14205978.00000
Epoch 26: Val Loss 14141299.00000
Epoch 27: Val Loss 14061394.00000
Epoch 28: Val Loss 13977215.00000
Epoch 29: Val Loss 13897478.00000
Epoch 30: Val Loss 13880997.00000
Epoch 31: Val Loss 13799449.00000
Epoch 32: Val Loss 13756955.00000
Epoch 33: Val Loss 13708290.00000
Epoch 34: Val Loss 13689340.00000
Epoch 35: Val Loss 13627287.00000
Epoch 36: Val Loss 13609633.00000
Epoch 37: Val Loss 13572011.00000
Epoch 38: Val Loss 13562239.00000
Epoch 39: Val Loss 13532908.00000
Epoch 40: Val Loss 13517002.00000
Epoch 41: Val Loss 13468296.00000
Epoch 42: Val Loss 13471821.00000
Epoch 43: Val Loss 13455798.00000
Epoch 44: Val Loss 13426680.00000
Epoch 45: Val Loss 13430797.00000
Epoch 46: Val Loss 13430851.00000
Epoch 47: Val Loss 13409722.00000
Epoch 48: Val Loss 13412217.00000
Epoch 49: Val Loss 13398770.00000
Epoch 50: Val Loss 13371736.00000
Epoch 51: Val Loss 13387994.00000
Epoch 52: Val Loss 13370619.00000
Epoch 53: Val Loss 13375020.00000
Epoch 54: Val Loss 13365843.00000
Epoch 55: Val Loss 13365079.00000
Epoch 56: Val Loss 13338636.00000
Epoch 57: Val Loss 13352009.00000
Epoch 58: Val Loss 13335462.00000
Epoch 59: Val Loss 13362349.00000
Epoch 60: Val Loss 13333545.00000
Epoch 61: Val Loss 13364947.00000
Epoch 62: Val Loss 13341112.00000
Epoch 63: Val Loss 13352609.00000
Epoch 64: Val Loss 13326423.00000
Epoch 65: Val Loss 13334150.00000
Epoch 66: Val Loss 13349820.00000
Epoch 67: Val Loss 13356604.00000
Epoch 68: Val Loss 13346237.00000
Epoch 69: Val Loss 13337970.00000
Epoch 70: Val Loss 13329976.00000
Epoch 71: Val Loss 13319370.00000
Epoch 72: Val Loss 13318958.00000
Epoch 73: Val Loss 13316014.00000
Epoch 74: Val Loss 13303162.00000
Epoch 75: Val Loss 13342564.00000
Epoch 76: Val Loss 13305987.00000
Epoch 77: Val Loss 13316667.00000
Epoch 78: Val Loss 13325443.00000
Epoch 79: Val Loss 13332798.00000
Epoch 80: Val Loss 13306171.00000
Epoch 81: Val Loss 13318184.00000
Epoch 82: Val Loss 13326773.00000
Epoch 83: Val Loss 13313707.00000
Epoch 84: Val Loss 13298506.00000
Epoch 85: Val Loss 13322520.00000
Epoch 86: Val Loss 13335471.00000
Epoch 87: Val Loss 13310337.00000
Epoch 88: Val Loss 13335120.00000
Epoch 89: Val Loss 13309810.00000
Epoch 90: Val Loss 13298272.00000
Epoch 91: Val Loss 13306108.00000
Epoch 92: Val Loss 13321957.00000
Epoch 93: Val Loss 13307287.00000
Epoch 94: Val Loss 13308994.00000
Epoch 95: Val Loss 13315719.00000
Epoch 96: Val Loss 13293926.00000
Epoch 97: Val Loss 13304134.00000
Epoch 98: Val Loss 13334825.00000
Epoch 99: Val Loss 13322226.00000
{'MSE - mean': 13221767.736565365, 'MSE - std': 100881.14029935452, 'R2 - mean': 0.4861699497504282, 'R2 - std': 0.001348496844666475} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 28691340.00000
Epoch 1: Val Loss 27728904.00000
Epoch 2: Val Loss 26943412.00000
Epoch 3: Val Loss 26088934.00000
Epoch 4: Val Loss 25238594.00000
Epoch 5: Val Loss 24200330.00000
Epoch 6: Val Loss 22808774.00000
Epoch 7: Val Loss 21301072.00000
Epoch 8: Val Loss 19846692.00000
Epoch 9: Val Loss 18539436.00000
Epoch 10: Val Loss 17556182.00000
Epoch 11: Val Loss 16772874.00000
Epoch 12: Val Loss 16250090.00000
Epoch 13: Val Loss 15762898.00000
Epoch 14: Val Loss 15419557.00000
Epoch 15: Val Loss 15071052.00000
Epoch 16: Val Loss 14803238.00000
Epoch 17: Val Loss 14554650.00000
Epoch 18: Val Loss 14409700.00000
Epoch 19: Val Loss 14232126.00000
Epoch 20: Val Loss 14119319.00000
Epoch 21: Val Loss 14003334.00000
Epoch 22: Val Loss 13943063.00000
Epoch 23: Val Loss 13855303.00000
Epoch 24: Val Loss 13784622.00000
Epoch 25: Val Loss 13721312.00000
Epoch 26: Val Loss 13688108.00000
Epoch 27: Val Loss 13601047.00000
Epoch 28: Val Loss 13547537.00000
Epoch 29: Val Loss 13503468.00000
Epoch 30: Val Loss 13498427.00000
Epoch 31: Val Loss 13435092.00000
Epoch 32: Val Loss 13404850.00000
Epoch 33: Val Loss 13384429.00000
Epoch 34: Val Loss 13381968.00000
Epoch 35: Val Loss 13353136.00000
Epoch 36: Val Loss 13327897.00000
Epoch 37: Val Loss 13345210.00000
Epoch 38: Val Loss 13293960.00000
Epoch 39: Val Loss 13312853.00000
Epoch 40: Val Loss 13296030.00000
Epoch 41: Val Loss 13262069.00000
Epoch 42: Val Loss 13266904.00000
Epoch 43: Val Loss 13266868.00000
Epoch 44: Val Loss 13253886.00000
Epoch 45: Val Loss 13251065.00000
Epoch 46: Val Loss 13240736.00000
Epoch 47: Val Loss 13236824.00000
Epoch 48: Val Loss 13254118.00000
Epoch 49: Val Loss 13220141.00000
Epoch 50: Val Loss 13222527.00000
Epoch 51: Val Loss 13230203.00000
Epoch 52: Val Loss 13246621.00000
Epoch 53: Val Loss 13250580.00000
Epoch 54: Val Loss 13241669.00000
Epoch 55: Val Loss 13208391.00000
Epoch 56: Val Loss 13228009.00000
Epoch 57: Val Loss 13233932.00000
Epoch 58: Val Loss 13218947.00000
Epoch 59: Val Loss 13225564.00000
Epoch 60: Val Loss 13214801.00000
Epoch 61: Val Loss 13233355.00000
Epoch 62: Val Loss 13216549.00000
Epoch 63: Val Loss 13212281.00000
Epoch 64: Val Loss 13191853.00000
Epoch 65: Val Loss 13223135.00000
Epoch 66: Val Loss 13208421.00000
Epoch 67: Val Loss 13189703.00000
Epoch 68: Val Loss 13201307.00000
Epoch 69: Val Loss 13228129.00000
Epoch 70: Val Loss 13230321.00000
Epoch 71: Val Loss 13213847.00000
Epoch 72: Val Loss 13186788.00000
Epoch 73: Val Loss 13202451.00000
Epoch 74: Val Loss 13183604.00000
Epoch 75: Val Loss 13197897.00000
Epoch 76: Val Loss 13227174.00000
Epoch 77: Val Loss 13193760.00000
Epoch 78: Val Loss 13190655.00000
Epoch 79: Val Loss 13214333.00000
Epoch 80: Val Loss 13179952.00000
Epoch 81: Val Loss 13181653.00000
Epoch 82: Val Loss 13206719.00000
Epoch 83: Val Loss 13206271.00000
Epoch 84: Val Loss 13210234.00000
Epoch 85: Val Loss 13212658.00000
Epoch 86: Val Loss 13195769.00000
Epoch 87: Val Loss 13201792.00000
Epoch 88: Val Loss 13197764.00000
Epoch 89: Val Loss 13196871.00000
Epoch 90: Val Loss 13202092.00000
Epoch 91: Val Loss 13187451.00000
Epoch 92: Val Loss 13203759.00000
Epoch 93: Val Loss 13210227.00000
Epoch 94: Val Loss 13189003.00000
Epoch 95: Val Loss 13205747.00000
Epoch 96: Val Loss 13195191.00000
Epoch 97: Val Loss 13189762.00000
Epoch 98: Val Loss 13188211.00000
Epoch 99: Val Loss 13182429.00000
{'MSE - mean': 13211953.273810139, 'MSE - std': 89004.06950580761, 'R2 - mean': 0.4872265535107224, 'R2 - std': 0.002170960000520473} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 29724502.00000
Epoch 1: Val Loss 28490358.00000
Epoch 2: Val Loss 27586360.00000
Epoch 3: Val Loss 26812762.00000
Epoch 4: Val Loss 25980206.00000
Epoch 5: Val Loss 24854170.00000
Epoch 6: Val Loss 23678650.00000
Epoch 7: Val Loss 22368312.00000
Epoch 8: Val Loss 20802274.00000
Epoch 9: Val Loss 19506960.00000
Epoch 10: Val Loss 18386020.00000
Epoch 11: Val Loss 17559398.00000
Epoch 12: Val Loss 16934876.00000
Epoch 13: Val Loss 16404461.00000
Epoch 14: Val Loss 15944379.00000
Epoch 15: Val Loss 15608200.00000
Epoch 16: Val Loss 15309409.00000
Epoch 17: Val Loss 15050936.00000
Epoch 18: Val Loss 14879144.00000
Epoch 19: Val Loss 14640275.00000
Epoch 20: Val Loss 14443984.00000
Epoch 21: Val Loss 14334016.00000
Epoch 22: Val Loss 14203930.00000
Epoch 23: Val Loss 14085314.00000
Epoch 24: Val Loss 13991462.00000
Epoch 25: Val Loss 13909811.00000
Epoch 26: Val Loss 13831468.00000
Epoch 27: Val Loss 13770938.00000
Epoch 28: Val Loss 13710372.00000
Epoch 29: Val Loss 13646448.00000
Epoch 30: Val Loss 13618825.00000
Epoch 31: Val Loss 13550933.00000
Epoch 32: Val Loss 13523931.00000
Epoch 33: Val Loss 13489166.00000
Epoch 34: Val Loss 13466679.00000
Epoch 35: Val Loss 13435585.00000
Epoch 36: Val Loss 13433522.00000
Epoch 37: Val Loss 13409405.00000
Epoch 38: Val Loss 13376175.00000
Epoch 39: Val Loss 13369913.00000
Epoch 40: Val Loss 13362625.00000
Epoch 41: Val Loss 13330208.00000
Epoch 42: Val Loss 13301721.00000
Epoch 43: Val Loss 13343940.00000
Epoch 44: Val Loss 13303979.00000
Epoch 45: Val Loss 13298956.00000
Epoch 46: Val Loss 13304012.00000
Epoch 47: Val Loss 13286964.00000
Epoch 48: Val Loss 13312552.00000
Epoch 49: Val Loss 13271081.00000
Epoch 50: Val Loss 13265782.00000
Epoch 51: Val Loss 13290246.00000
Epoch 52: Val Loss 13288485.00000
Epoch 53: Val Loss 13264100.00000
Epoch 54: Val Loss 13274352.00000
Epoch 55: Val Loss 13279406.00000
Epoch 56: Val Loss 13257848.00000
Epoch 57: Val Loss 13275834.00000
Epoch 58: Val Loss 13273370.00000
Epoch 59: Val Loss 13273711.00000
Epoch 60: Val Loss 13245779.00000
Epoch 61: Val Loss 13258582.00000
Epoch 62: Val Loss 13246604.00000
Epoch 63: Val Loss 13258362.00000
Epoch 64: Val Loss 13278275.00000
Epoch 65: Val Loss 13243277.00000
Epoch 66: Val Loss 13246925.00000
Epoch 67: Val Loss 13256233.00000
Epoch 68: Val Loss 13267908.00000
Epoch 69: Val Loss 13251274.00000
Epoch 70: Val Loss 13250091.00000
Epoch 71: Val Loss 13248173.00000
Epoch 72: Val Loss 13272556.00000
Epoch 73: Val Loss 13245248.00000
Epoch 74: Val Loss 13260137.00000
Epoch 75: Val Loss 13276158.00000
Epoch 76: Val Loss 13246001.00000
Epoch 77: Val Loss 13245836.00000
Epoch 78: Val Loss 13236144.00000
Epoch 79: Val Loss 13253509.00000
Epoch 80: Val Loss 13260053.00000
Epoch 81: Val Loss 13232926.00000
Epoch 82: Val Loss 13249778.00000
Epoch 83: Val Loss 13246126.00000
Epoch 84: Val Loss 13247170.00000
Epoch 85: Val Loss 13246278.00000
Epoch 86: Val Loss 13234063.00000
Epoch 87: Val Loss 13230327.00000
Epoch 88: Val Loss 13269505.00000
Epoch 89: Val Loss 13251816.00000
Epoch 90: Val Loss 13249902.00000
Epoch 91: Val Loss 13246123.00000
Epoch 92: Val Loss 13269443.00000
Epoch 93: Val Loss 13220041.00000
Epoch 94: Val Loss 13248603.00000
Epoch 95: Val Loss 13269781.00000
Epoch 96: Val Loss 13248340.00000
Epoch 97: Val Loss 13232899.00000
Epoch 98: Val Loss 13241441.00000
Epoch 99: Val Loss 13232442.00000
{'MSE - mean': 13213996.726343187, 'MSE - std': 79712.49778983834, 'R2 - mean': 0.4883845773001075, 'R2 - std': 0.0030223385393126643} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 13213996.726343187 and parameters: {'dim': 128, 'depth': 2, 'heads': 2, 'weight_decay': -1, 'learning_rate': -4, 'dropout': 0.4}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161462272.00000
Epoch 1: Val Loss 159573440.00000
Epoch 2: Val Loss 152627936.00000
Epoch 3: Val Loss 136379856.00000
Epoch 4: Val Loss 106396280.00000
Epoch 5: Val Loss 66821284.00000
Epoch 6: Val Loss 35801076.00000
Epoch 7: Val Loss 29076676.00000
Epoch 8: Val Loss 29072286.00000
Epoch 9: Val Loss 28959964.00000
Epoch 10: Val Loss 28810746.00000
Epoch 11: Val Loss 28654764.00000
Epoch 12: Val Loss 28496934.00000
Epoch 13: Val Loss 28382216.00000
Epoch 14: Val Loss 28202092.00000
Epoch 15: Val Loss 28003798.00000
Epoch 16: Val Loss 27914330.00000
Epoch 17: Val Loss 27656190.00000
Epoch 18: Val Loss 27483252.00000
Epoch 19: Val Loss 27283010.00000
Epoch 20: Val Loss 27026044.00000
Epoch 21: Val Loss 26812732.00000
Epoch 22: Val Loss 26534440.00000
Epoch 23: Val Loss 26357808.00000
Epoch 24: Val Loss 26145118.00000
Epoch 25: Val Loss 26042558.00000
Epoch 26: Val Loss 25938194.00000
Epoch 27: Val Loss 25872530.00000
Epoch 28: Val Loss 25784226.00000
Epoch 29: Val Loss 25666024.00000
Epoch 30: Val Loss 25706022.00000
Epoch 31: Val Loss 25649912.00000
Epoch 32: Val Loss 25594204.00000
Epoch 33: Val Loss 25598786.00000
Epoch 34: Val Loss 25577468.00000
Epoch 35: Val Loss 25517490.00000
Epoch 36: Val Loss 25554376.00000
Epoch 37: Val Loss 25524440.00000
Epoch 38: Val Loss 25577958.00000
Epoch 39: Val Loss 25518778.00000
Epoch 40: Val Loss 25469324.00000
Epoch 41: Val Loss 25500420.00000
Epoch 42: Val Loss 25476882.00000
Epoch 43: Val Loss 25426886.00000
Epoch 44: Val Loss 25479478.00000
Epoch 45: Val Loss 25414812.00000
Epoch 46: Val Loss 25338720.00000
Epoch 47: Val Loss 25329274.00000
Epoch 48: Val Loss 25376014.00000
Epoch 49: Val Loss 25334170.00000
Epoch 50: Val Loss 25340974.00000
Epoch 51: Val Loss 25309980.00000
Epoch 52: Val Loss 25315306.00000
Epoch 53: Val Loss 25242028.00000
Epoch 54: Val Loss 25213430.00000
Epoch 55: Val Loss 25261486.00000
Epoch 56: Val Loss 25160372.00000
Epoch 57: Val Loss 25173474.00000
Epoch 58: Val Loss 25169054.00000
Epoch 59: Val Loss 25134492.00000
Epoch 60: Val Loss 25111606.00000
Epoch 61: Val Loss 25086856.00000
Epoch 62: Val Loss 25050004.00000
Epoch 63: Val Loss 24986994.00000
Epoch 64: Val Loss 25083556.00000
Epoch 65: Val Loss 24994098.00000
Epoch 66: Val Loss 24929370.00000
Epoch 67: Val Loss 24877806.00000
Epoch 68: Val Loss 24854318.00000
Epoch 69: Val Loss 24877986.00000
Epoch 70: Val Loss 24847058.00000
Epoch 71: Val Loss 24780976.00000
Epoch 72: Val Loss 24743204.00000
Epoch 73: Val Loss 24719286.00000
Epoch 74: Val Loss 24708098.00000
Epoch 75: Val Loss 24690980.00000
Epoch 76: Val Loss 24683224.00000
Epoch 77: Val Loss 24650976.00000
Epoch 78: Val Loss 24540618.00000
Epoch 79: Val Loss 24532346.00000
Epoch 80: Val Loss 24520356.00000
Epoch 81: Val Loss 24542818.00000
Epoch 82: Val Loss 24463690.00000
Epoch 83: Val Loss 24371084.00000
Epoch 84: Val Loss 24341656.00000
Epoch 85: Val Loss 24334424.00000
Epoch 86: Val Loss 24304330.00000
Epoch 87: Val Loss 24266478.00000
Epoch 88: Val Loss 24146678.00000
Epoch 89: Val Loss 24207294.00000
Epoch 90: Val Loss 24094682.00000
Epoch 91: Val Loss 24007460.00000
Epoch 92: Val Loss 23940020.00000
Epoch 93: Val Loss 23921332.00000
Epoch 94: Val Loss 23936296.00000
Epoch 95: Val Loss 23875428.00000
Epoch 96: Val Loss 23776748.00000
Epoch 97: Val Loss 23748330.00000
Epoch 98: Val Loss 23697780.00000
Epoch 99: Val Loss 23624164.00000
{'MSE - mean': 23366704.301123455, 'MSE - std': 0.0, 'R2 - mean': 0.08528783383481309, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162216256.00000
Epoch 1: Val Loss 160136864.00000
Epoch 2: Val Loss 152064816.00000
Epoch 3: Val Loss 132562536.00000
Epoch 4: Val Loss 98529816.00000
Epoch 5: Val Loss 58411944.00000
Epoch 6: Val Loss 36734004.00000
Epoch 7: Val Loss 34678392.00000
Epoch 8: Val Loss 34110780.00000
Epoch 9: Val Loss 33299726.00000
Epoch 10: Val Loss 32333378.00000
Epoch 11: Val Loss 31370038.00000
Epoch 12: Val Loss 30071418.00000
Epoch 13: Val Loss 28992176.00000
Epoch 14: Val Loss 27961548.00000
Epoch 15: Val Loss 27029812.00000
Epoch 16: Val Loss 26498532.00000
Epoch 17: Val Loss 26353844.00000
Epoch 18: Val Loss 26219130.00000
Epoch 19: Val Loss 26204626.00000
Epoch 20: Val Loss 26115208.00000
Epoch 21: Val Loss 26049490.00000
Epoch 22: Val Loss 26132922.00000
Epoch 23: Val Loss 26127492.00000
Epoch 24: Val Loss 26087530.00000
Epoch 25: Val Loss 26055942.00000
Epoch 26: Val Loss 26006478.00000
Epoch 27: Val Loss 26011102.00000
Epoch 28: Val Loss 26010054.00000
Epoch 29: Val Loss 25955262.00000
Epoch 30: Val Loss 25960484.00000
Epoch 31: Val Loss 25987658.00000
Epoch 32: Val Loss 26002138.00000
Epoch 33: Val Loss 25869828.00000
Epoch 34: Val Loss 25934960.00000
Epoch 35: Val Loss 25949004.00000
Epoch 36: Val Loss 25792882.00000
Epoch 37: Val Loss 25949458.00000
Epoch 38: Val Loss 25915192.00000
Epoch 39: Val Loss 25818280.00000
Epoch 40: Val Loss 25793892.00000
Epoch 41: Val Loss 25835902.00000
Epoch 42: Val Loss 25796524.00000
Epoch 43: Val Loss 25734750.00000
Epoch 44: Val Loss 25779872.00000
Epoch 45: Val Loss 25723800.00000
Epoch 46: Val Loss 25754130.00000
Epoch 47: Val Loss 25681114.00000
Epoch 48: Val Loss 25708256.00000
Epoch 49: Val Loss 25578792.00000
Epoch 50: Val Loss 25583832.00000
Epoch 51: Val Loss 25547280.00000
Epoch 52: Val Loss 25515668.00000
Epoch 53: Val Loss 25573638.00000
Epoch 54: Val Loss 25470514.00000
Epoch 55: Val Loss 25482852.00000
Epoch 56: Val Loss 25458606.00000
Epoch 57: Val Loss 25371650.00000
Epoch 58: Val Loss 25394312.00000
Epoch 59: Val Loss 25384030.00000
Epoch 60: Val Loss 25299310.00000
Epoch 61: Val Loss 25365438.00000
Epoch 62: Val Loss 25255754.00000
Epoch 63: Val Loss 25238576.00000
Epoch 64: Val Loss 25268214.00000
Epoch 65: Val Loss 25173966.00000
Epoch 66: Val Loss 25111432.00000
Epoch 67: Val Loss 25154584.00000
Epoch 68: Val Loss 25061884.00000
Epoch 69: Val Loss 25051210.00000
Epoch 70: Val Loss 25034332.00000
Epoch 71: Val Loss 24945112.00000
Epoch 72: Val Loss 24957306.00000
Epoch 73: Val Loss 24822978.00000
Epoch 74: Val Loss 24887710.00000
Epoch 75: Val Loss 24861128.00000
Epoch 76: Val Loss 24724830.00000
Epoch 77: Val Loss 24728358.00000
Epoch 78: Val Loss 24577610.00000
Epoch 79: Val Loss 24658536.00000
Epoch 80: Val Loss 24609098.00000
Epoch 81: Val Loss 24507596.00000
Epoch 82: Val Loss 24494468.00000
Epoch 83: Val Loss 24446854.00000
Epoch 84: Val Loss 24355326.00000
Epoch 85: Val Loss 24353658.00000
Epoch 86: Val Loss 24290318.00000
Epoch 87: Val Loss 24186328.00000
Epoch 88: Val Loss 24149206.00000
Epoch 89: Val Loss 24061616.00000
Epoch 90: Val Loss 24021974.00000
Epoch 91: Val Loss 24011396.00000
Epoch 92: Val Loss 23921032.00000
Epoch 93: Val Loss 23841604.00000
Epoch 94: Val Loss 23732236.00000
Epoch 95: Val Loss 23716178.00000
Epoch 96: Val Loss 23609380.00000
Epoch 97: Val Loss 23479556.00000
Epoch 98: Val Loss 23406688.00000
Epoch 99: Val Loss 23361590.00000
{'MSE - mean': 23207440.426599495, 'MSE - std': 159263.87452396005, 'R2 - mean': 0.09679596989939931, 'R2 - std': 0.011508136064586227} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162214624.00000
Epoch 1: Val Loss 160980336.00000
Epoch 2: Val Loss 156059936.00000
Epoch 3: Val Loss 143789120.00000
Epoch 4: Val Loss 120003728.00000
Epoch 5: Val Loss 84116032.00000
Epoch 6: Val Loss 46403388.00000
Epoch 7: Val Loss 27513298.00000
Epoch 8: Val Loss 26139598.00000
Epoch 9: Val Loss 26195220.00000
Epoch 10: Val Loss 26150510.00000
Epoch 11: Val Loss 26040980.00000
Epoch 12: Val Loss 26072818.00000
Epoch 13: Val Loss 26027912.00000
Epoch 14: Val Loss 26040282.00000
Epoch 15: Val Loss 26090674.00000
Epoch 16: Val Loss 25983022.00000
Epoch 17: Val Loss 26037452.00000
Epoch 18: Val Loss 26044998.00000
Epoch 19: Val Loss 26025710.00000
Epoch 20: Val Loss 26026318.00000
Epoch 21: Val Loss 26019094.00000
Epoch 22: Val Loss 25945284.00000
Epoch 23: Val Loss 25982930.00000
Epoch 24: Val Loss 26000784.00000
Epoch 25: Val Loss 25942754.00000
Epoch 26: Val Loss 25983050.00000
Epoch 27: Val Loss 25932330.00000
Epoch 28: Val Loss 25870834.00000
Epoch 29: Val Loss 25903706.00000
Epoch 30: Val Loss 25880684.00000
Epoch 31: Val Loss 25879914.00000
Epoch 32: Val Loss 25818920.00000
Epoch 33: Val Loss 25947660.00000
Epoch 34: Val Loss 25811228.00000
Epoch 35: Val Loss 25832642.00000
Epoch 36: Val Loss 25827698.00000
Epoch 37: Val Loss 25809026.00000
Epoch 38: Val Loss 25790728.00000
Epoch 39: Val Loss 25759482.00000
Epoch 40: Val Loss 25779470.00000
Epoch 41: Val Loss 25746638.00000
Epoch 42: Val Loss 25753732.00000
Epoch 43: Val Loss 25705730.00000
Epoch 44: Val Loss 25719048.00000
Epoch 45: Val Loss 25659906.00000
Epoch 46: Val Loss 25624570.00000
Epoch 47: Val Loss 25683386.00000
Epoch 48: Val Loss 25631132.00000
Epoch 49: Val Loss 25658222.00000
Epoch 50: Val Loss 25613050.00000
Epoch 51: Val Loss 25522804.00000
Epoch 52: Val Loss 25570212.00000
Epoch 53: Val Loss 25569742.00000
Epoch 54: Val Loss 25543726.00000
Epoch 55: Val Loss 25543294.00000
Epoch 56: Val Loss 25467048.00000
Epoch 57: Val Loss 25486152.00000
Epoch 58: Val Loss 25486778.00000
Epoch 59: Val Loss 25426838.00000
Epoch 60: Val Loss 25427488.00000
Epoch 61: Val Loss 25401828.00000
Epoch 62: Val Loss 25408142.00000
Epoch 63: Val Loss 25363868.00000
Epoch 64: Val Loss 25299558.00000
Epoch 65: Val Loss 25360706.00000
Epoch 66: Val Loss 25329146.00000
Epoch 67: Val Loss 25328370.00000
Epoch 68: Val Loss 25241892.00000
Epoch 69: Val Loss 25236656.00000
Epoch 70: Val Loss 25231350.00000
Epoch 71: Val Loss 25147158.00000
Epoch 72: Val Loss 25183960.00000
Epoch 73: Val Loss 25187680.00000
Epoch 74: Val Loss 25167122.00000
Epoch 75: Val Loss 25069698.00000
Epoch 76: Val Loss 25078238.00000
Epoch 77: Val Loss 25029238.00000
Epoch 78: Val Loss 25018254.00000
Epoch 79: Val Loss 25019428.00000
Epoch 80: Val Loss 24925604.00000
Epoch 81: Val Loss 24928838.00000
Epoch 82: Val Loss 24929014.00000
Epoch 83: Val Loss 24867106.00000
Epoch 84: Val Loss 24856020.00000
Epoch 85: Val Loss 24814546.00000
Epoch 86: Val Loss 24777040.00000
Epoch 87: Val Loss 24823080.00000
Epoch 88: Val Loss 24704920.00000
Epoch 89: Val Loss 24600978.00000
Epoch 90: Val Loss 24676998.00000
Epoch 91: Val Loss 24581306.00000
Epoch 92: Val Loss 24535414.00000
Epoch 93: Val Loss 24490756.00000
Epoch 94: Val Loss 24525786.00000
Epoch 95: Val Loss 24407988.00000
Epoch 96: Val Loss 24387580.00000
Epoch 97: Val Loss 24398608.00000
Epoch 98: Val Loss 24365740.00000
Epoch 99: Val Loss 24264574.00000
{'MSE - mean': 23486318.310923148, 'MSE - std': 415277.9027935339, 'R2 - mean': 0.08723418223046024, 'R2 - std': 0.01646654276522866} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 160363424.00000
Epoch 1: Val Loss 158649232.00000
Epoch 2: Val Loss 152804368.00000
Epoch 3: Val Loss 138590944.00000
Epoch 4: Val Loss 111986560.00000
Epoch 5: Val Loss 74083232.00000
Epoch 6: Val Loss 38796820.00000
Epoch 7: Val Loss 26445508.00000
Epoch 8: Val Loss 26183080.00000
Epoch 9: Val Loss 26172020.00000
Epoch 10: Val Loss 26092514.00000
Epoch 11: Val Loss 26129790.00000
Epoch 12: Val Loss 26067942.00000
Epoch 13: Val Loss 26079384.00000
Epoch 14: Val Loss 26129506.00000
Epoch 15: Val Loss 26059892.00000
Epoch 16: Val Loss 25949006.00000
Epoch 17: Val Loss 26083588.00000
Epoch 18: Val Loss 25991970.00000
Epoch 19: Val Loss 25991588.00000
Epoch 20: Val Loss 26042676.00000
Epoch 21: Val Loss 25950740.00000
Epoch 22: Val Loss 26048968.00000
Epoch 23: Val Loss 25977790.00000
Epoch 24: Val Loss 26001796.00000
Epoch 25: Val Loss 25973650.00000
Epoch 26: Val Loss 25982874.00000
Epoch 27: Val Loss 25921818.00000
Epoch 28: Val Loss 25953000.00000
Epoch 29: Val Loss 25881832.00000
Epoch 30: Val Loss 25852110.00000
Epoch 31: Val Loss 25821642.00000
Epoch 32: Val Loss 25798204.00000
Epoch 33: Val Loss 25836908.00000
Epoch 34: Val Loss 25797032.00000
Epoch 35: Val Loss 25797238.00000
Epoch 36: Val Loss 25788996.00000
Epoch 37: Val Loss 25730648.00000
Epoch 38: Val Loss 25793080.00000
Epoch 39: Val Loss 25792948.00000
Epoch 40: Val Loss 25736318.00000
Epoch 41: Val Loss 25699624.00000
Epoch 42: Val Loss 25649844.00000
Epoch 43: Val Loss 25628798.00000
Epoch 44: Val Loss 25687598.00000
Epoch 45: Val Loss 25646892.00000
Epoch 46: Val Loss 25618138.00000
Epoch 47: Val Loss 25606246.00000
Epoch 48: Val Loss 25593168.00000
Epoch 49: Val Loss 25526718.00000
Epoch 50: Val Loss 25551906.00000
Epoch 51: Val Loss 25490392.00000
Epoch 52: Val Loss 25463358.00000
Epoch 53: Val Loss 25428328.00000
Epoch 54: Val Loss 25495584.00000
Epoch 55: Val Loss 25418350.00000
Epoch 56: Val Loss 25348402.00000
Epoch 57: Val Loss 25424490.00000
Epoch 58: Val Loss 25355732.00000
Epoch 59: Val Loss 25328290.00000
Epoch 60: Val Loss 25244862.00000
Epoch 61: Val Loss 25292984.00000
Epoch 62: Val Loss 25177688.00000
Epoch 63: Val Loss 25188646.00000
Epoch 64: Val Loss 25157476.00000
Epoch 65: Val Loss 25177220.00000
Epoch 66: Val Loss 25129244.00000
Epoch 67: Val Loss 24996772.00000
Epoch 68: Val Loss 25036068.00000
Epoch 69: Val Loss 24970568.00000
Epoch 70: Val Loss 25030910.00000
Epoch 71: Val Loss 24905898.00000
Epoch 72: Val Loss 24949048.00000
Epoch 73: Val Loss 24852986.00000
Epoch 74: Val Loss 24858738.00000
Epoch 75: Val Loss 24805638.00000
Epoch 76: Val Loss 24813188.00000
Epoch 77: Val Loss 24748052.00000
Epoch 78: Val Loss 24647152.00000
Epoch 79: Val Loss 24620356.00000
Epoch 80: Val Loss 24591446.00000
Epoch 81: Val Loss 24531226.00000
Epoch 82: Val Loss 24492464.00000
Epoch 83: Val Loss 24462700.00000
Epoch 84: Val Loss 24412558.00000
Epoch 85: Val Loss 24435144.00000
Epoch 86: Val Loss 24307776.00000
Epoch 87: Val Loss 24285326.00000
Epoch 88: Val Loss 24263682.00000
Epoch 89: Val Loss 24190292.00000
Epoch 90: Val Loss 24127630.00000
Epoch 91: Val Loss 24116548.00000
Epoch 92: Val Loss 24039094.00000
Epoch 93: Val Loss 23960140.00000
Epoch 94: Val Loss 23919820.00000
Epoch 95: Val Loss 23856328.00000
Epoch 96: Val Loss 23778082.00000
Epoch 97: Val Loss 23809892.00000
Epoch 98: Val Loss 23652344.00000
Epoch 99: Val Loss 23559126.00000
{'MSE - mean': 23460207.765977196, 'MSE - std': 362473.563289201, 'R2 - mean': 0.08945408084099302, 'R2 - std': 0.01476970285762257} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161467328.00000
Epoch 1: Val Loss 159598880.00000
Epoch 2: Val Loss 152524912.00000
Epoch 3: Val Loss 135086880.00000
Epoch 4: Val Loss 103535912.00000
Epoch 5: Val Loss 62289740.00000
Epoch 6: Val Loss 31825192.00000
Epoch 7: Val Loss 26476770.00000
Epoch 8: Val Loss 26421010.00000
Epoch 9: Val Loss 26394176.00000
Epoch 10: Val Loss 26427036.00000
Epoch 11: Val Loss 26433680.00000
Epoch 12: Val Loss 26376478.00000
Epoch 13: Val Loss 26268672.00000
Epoch 14: Val Loss 26375246.00000
Epoch 15: Val Loss 26326054.00000
Epoch 16: Val Loss 26332326.00000
Epoch 17: Val Loss 26304534.00000
Epoch 18: Val Loss 26314306.00000
Epoch 19: Val Loss 26241292.00000
Epoch 20: Val Loss 26254442.00000
Epoch 21: Val Loss 26267076.00000
Epoch 22: Val Loss 26214318.00000
Epoch 23: Val Loss 26234466.00000
Epoch 24: Val Loss 26168966.00000
Epoch 25: Val Loss 26152152.00000
Epoch 26: Val Loss 26172082.00000
Epoch 27: Val Loss 26137084.00000
Epoch 28: Val Loss 26161880.00000
Epoch 29: Val Loss 26091430.00000
Epoch 30: Val Loss 26050104.00000
Epoch 31: Val Loss 26047544.00000
Epoch 32: Val Loss 26051396.00000
Epoch 33: Val Loss 26057954.00000
Epoch 34: Val Loss 26000558.00000
Epoch 35: Val Loss 25924738.00000
Epoch 36: Val Loss 25960134.00000
Epoch 37: Val Loss 25924560.00000
Epoch 38: Val Loss 25910134.00000
Epoch 39: Val Loss 25937664.00000
Epoch 40: Val Loss 25912908.00000
Epoch 41: Val Loss 25845066.00000
Epoch 42: Val Loss 25846798.00000
Epoch 43: Val Loss 25862068.00000
Epoch 44: Val Loss 25832502.00000
Epoch 45: Val Loss 25847096.00000
Epoch 46: Val Loss 25800364.00000
Epoch 47: Val Loss 25748964.00000
Epoch 48: Val Loss 25777362.00000
Epoch 49: Val Loss 25707572.00000
Epoch 50: Val Loss 25740056.00000
Epoch 51: Val Loss 25707484.00000
Epoch 52: Val Loss 25645152.00000
Epoch 53: Val Loss 25665056.00000
Epoch 54: Val Loss 25618850.00000
Epoch 55: Val Loss 25639140.00000
Epoch 56: Val Loss 25534412.00000
Epoch 57: Val Loss 25580500.00000
Epoch 58: Val Loss 25445148.00000
Epoch 59: Val Loss 25405756.00000
Epoch 60: Val Loss 25457030.00000
Epoch 61: Val Loss 25441006.00000
Epoch 62: Val Loss 25376278.00000
Epoch 63: Val Loss 25370684.00000
Epoch 64: Val Loss 25395938.00000
Epoch 65: Val Loss 25369594.00000
Epoch 66: Val Loss 25312972.00000
Epoch 67: Val Loss 25243152.00000
Epoch 68: Val Loss 25234860.00000
Epoch 69: Val Loss 25204202.00000
Epoch 70: Val Loss 25171704.00000
Epoch 71: Val Loss 25124300.00000
Epoch 72: Val Loss 25030418.00000
Epoch 73: Val Loss 25031304.00000
Epoch 74: Val Loss 25058548.00000
Epoch 75: Val Loss 24989394.00000
Epoch 76: Val Loss 24901614.00000
Epoch 77: Val Loss 24849958.00000
Epoch 78: Val Loss 24783380.00000
Epoch 79: Val Loss 24794568.00000
Epoch 80: Val Loss 24763778.00000
Epoch 81: Val Loss 24707036.00000
Epoch 82: Val Loss 24682772.00000
Epoch 83: Val Loss 24648960.00000
Epoch 84: Val Loss 24541394.00000
Epoch 85: Val Loss 24524298.00000
Epoch 86: Val Loss 24498756.00000
Epoch 87: Val Loss 24372250.00000
Epoch 88: Val Loss 24370136.00000
Epoch 89: Val Loss 24336344.00000
Epoch 90: Val Loss 24273508.00000
Epoch 91: Val Loss 24176516.00000
Epoch 92: Val Loss 24168966.00000
Epoch 93: Val Loss 24051978.00000
Epoch 94: Val Loss 24008340.00000
Epoch 95: Val Loss 23933592.00000
Epoch 96: Val Loss 23855480.00000
Epoch 97: Val Loss 23805764.00000
Epoch 98: Val Loss 23733782.00000
Epoch 99: Val Loss 23661350.00000
{'MSE - mean': 23454218.367710993, 'MSE - std': 324427.43230685836, 'R2 - mean': 0.09188396708779015, 'R2 - std': 0.014075961306122816} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 23454218.367710993 and parameters: {'dim': 256, 'depth': 6, 'heads': 8, 'weight_decay': -2, 'learning_rate': -5, 'dropout': 0.2}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161739472.00000
Epoch 1: Val Loss 161894544.00000
Epoch 2: Val Loss 161796240.00000
Epoch 3: Val Loss 161702080.00000
Epoch 4: Val Loss 161666096.00000
Epoch 5: Val Loss 161715840.00000
Epoch 6: Val Loss 161624768.00000
Epoch 7: Val Loss 161666064.00000
Epoch 8: Val Loss 161648576.00000
Epoch 9: Val Loss 161814240.00000
Epoch 10: Val Loss 161702496.00000
Epoch 11: Val Loss 161599600.00000
Epoch 12: Val Loss 161615760.00000
Epoch 13: Val Loss 161645776.00000
Epoch 14: Val Loss 161705312.00000
Epoch 15: Val Loss 161708944.00000
Epoch 16: Val Loss 161615216.00000
Epoch 17: Val Loss 161638176.00000
Epoch 18: Val Loss 161696944.00000
Epoch 19: Val Loss 161519552.00000
Epoch 20: Val Loss 161601408.00000
Epoch 21: Val Loss 161588464.00000
Epoch 22: Val Loss 161585456.00000
Epoch 23: Val Loss 161469040.00000
Epoch 24: Val Loss 161574704.00000
Epoch 25: Val Loss 161548656.00000
Epoch 26: Val Loss 161472736.00000
Epoch 27: Val Loss 161558272.00000
Epoch 28: Val Loss 161525744.00000
Epoch 29: Val Loss 161412656.00000
Epoch 30: Val Loss 161426848.00000
Epoch 31: Val Loss 161468992.00000
Epoch 32: Val Loss 161360976.00000
Epoch 33: Val Loss 161378240.00000
Epoch 34: Val Loss 161335136.00000
Epoch 35: Val Loss 161161648.00000
Epoch 36: Val Loss 161220576.00000
Epoch 37: Val Loss 161116816.00000
Epoch 38: Val Loss 161023760.00000
Epoch 39: Val Loss 161111856.00000
Epoch 40: Val Loss 160885776.00000
Epoch 41: Val Loss 160779152.00000
Epoch 42: Val Loss 160810032.00000
Epoch 43: Val Loss 160678112.00000
Epoch 44: Val Loss 160528720.00000
Epoch 45: Val Loss 160375040.00000
Epoch 46: Val Loss 160306112.00000
Epoch 47: Val Loss 160202160.00000
Epoch 48: Val Loss 160074816.00000
Epoch 49: Val Loss 159908736.00000
Epoch 50: Val Loss 159794928.00000
Epoch 51: Val Loss 159484336.00000
Epoch 52: Val Loss 159412176.00000
Epoch 53: Val Loss 159260560.00000
Epoch 54: Val Loss 159006496.00000
Epoch 55: Val Loss 158928400.00000
Epoch 56: Val Loss 158567472.00000
Epoch 57: Val Loss 158269808.00000
Epoch 58: Val Loss 158091968.00000
Epoch 59: Val Loss 157843360.00000
Epoch 60: Val Loss 157489312.00000
Epoch 61: Val Loss 157170224.00000
Epoch 62: Val Loss 156892960.00000
Epoch 63: Val Loss 156539920.00000
Epoch 64: Val Loss 156132544.00000
Epoch 65: Val Loss 155672368.00000
Epoch 66: Val Loss 155239856.00000
Epoch 67: Val Loss 154752720.00000
Epoch 68: Val Loss 154154160.00000
Epoch 69: Val Loss 153757984.00000
Epoch 70: Val Loss 153236992.00000
Epoch 71: Val Loss 152716048.00000
Epoch 72: Val Loss 152027696.00000
Epoch 73: Val Loss 151345040.00000
Epoch 74: Val Loss 150750112.00000
Epoch 75: Val Loss 149966240.00000
Epoch 76: Val Loss 149203936.00000
Epoch 77: Val Loss 148405392.00000
Epoch 78: Val Loss 147580720.00000
Epoch 79: Val Loss 146534176.00000
Epoch 80: Val Loss 145664160.00000
Epoch 81: Val Loss 144616832.00000
Epoch 82: Val Loss 143471232.00000
Epoch 83: Val Loss 142180016.00000
Epoch 84: Val Loss 141036208.00000
Epoch 85: Val Loss 139515952.00000
Epoch 86: Val Loss 138107536.00000
Epoch 87: Val Loss 136766976.00000
Epoch 88: Val Loss 135358144.00000
Epoch 89: Val Loss 133800528.00000
Epoch 90: Val Loss 132290392.00000
Epoch 91: Val Loss 130685608.00000
Epoch 92: Val Loss 128824432.00000
Epoch 93: Val Loss 127141272.00000
Epoch 94: Val Loss 125254864.00000
Epoch 95: Val Loss 123262408.00000
Epoch 96: Val Loss 121361912.00000
Epoch 97: Val Loss 119300264.00000
Epoch 98: Val Loss 117220184.00000
Epoch 99: Val Loss 115088320.00000
{'MSE - mean': 114323134.37963063, 'MSE - std': 0.0, 'R2 - mean': -3.4752893066806125, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162321504.00000
Epoch 1: Val Loss 162369632.00000
Epoch 2: Val Loss 162232848.00000
Epoch 3: Val Loss 162330400.00000
Epoch 4: Val Loss 162324192.00000
Epoch 5: Val Loss 162340576.00000
Epoch 6: Val Loss 162422832.00000
Epoch 7: Val Loss 162417904.00000
Epoch 8: Val Loss 162276368.00000
Epoch 9: Val Loss 162519776.00000
Epoch 10: Val Loss 162305600.00000
Epoch 11: Val Loss 162434800.00000
Epoch 12: Val Loss 162415472.00000
Epoch 13: Val Loss 162309152.00000
Epoch 14: Val Loss 162336288.00000
Epoch 15: Val Loss 162428464.00000
Epoch 16: Val Loss 162296256.00000
Epoch 17: Val Loss 162302528.00000
Epoch 18: Val Loss 162299072.00000
Epoch 19: Val Loss 162270656.00000
Epoch 20: Val Loss 162252832.00000
Epoch 21: Val Loss 162177024.00000
Epoch 22: Val Loss 162337696.00000
Epoch 23: Val Loss 162242976.00000
Epoch 24: Val Loss 162200896.00000
Epoch 25: Val Loss 162188368.00000
Epoch 26: Val Loss 162170080.00000
Epoch 27: Val Loss 162101200.00000
Epoch 28: Val Loss 162145760.00000
Epoch 29: Val Loss 162044672.00000
Epoch 30: Val Loss 161961696.00000
Epoch 31: Val Loss 161981744.00000
Epoch 32: Val Loss 162023616.00000
Epoch 33: Val Loss 161884416.00000
Epoch 34: Val Loss 161800112.00000
Epoch 35: Val Loss 161648736.00000
Epoch 36: Val Loss 161626048.00000
Epoch 37: Val Loss 161597984.00000
Epoch 38: Val Loss 161473472.00000
Epoch 39: Val Loss 161356848.00000
Epoch 40: Val Loss 161250064.00000
Epoch 41: Val Loss 161217248.00000
Epoch 42: Val Loss 161039312.00000
Epoch 43: Val Loss 161006912.00000
Epoch 44: Val Loss 160879616.00000
Epoch 45: Val Loss 160575904.00000
Epoch 46: Val Loss 160562032.00000
Epoch 47: Val Loss 160455216.00000
Epoch 48: Val Loss 160021616.00000
Epoch 49: Val Loss 160042272.00000
Epoch 50: Val Loss 159718432.00000
Epoch 51: Val Loss 159433072.00000
Epoch 52: Val Loss 159200880.00000
Epoch 53: Val Loss 158960288.00000
Epoch 54: Val Loss 158702368.00000
Epoch 55: Val Loss 158421072.00000
Epoch 56: Val Loss 157992768.00000
Epoch 57: Val Loss 157783136.00000
Epoch 58: Val Loss 157374400.00000
Epoch 59: Val Loss 156822848.00000
Epoch 60: Val Loss 156493936.00000
Epoch 61: Val Loss 156034816.00000
Epoch 62: Val Loss 155578960.00000
Epoch 63: Val Loss 155042080.00000
Epoch 64: Val Loss 154573552.00000
Epoch 65: Val Loss 154006272.00000
Epoch 66: Val Loss 153373392.00000
Epoch 67: Val Loss 152804400.00000
Epoch 68: Val Loss 152127520.00000
Epoch 69: Val Loss 151303504.00000
Epoch 70: Val Loss 150567072.00000
Epoch 71: Val Loss 149719424.00000
Epoch 72: Val Loss 149006176.00000
Epoch 73: Val Loss 147977200.00000
Epoch 74: Val Loss 147011184.00000
Epoch 75: Val Loss 146164240.00000
Epoch 76: Val Loss 145085680.00000
Epoch 77: Val Loss 144018752.00000
Epoch 78: Val Loss 142824272.00000
Epoch 79: Val Loss 141638208.00000
Epoch 80: Val Loss 140330176.00000
Epoch 81: Val Loss 138875216.00000
Epoch 82: Val Loss 137541280.00000
Epoch 83: Val Loss 136137392.00000
Epoch 84: Val Loss 134875600.00000
Epoch 85: Val Loss 133052288.00000
Epoch 86: Val Loss 131373416.00000
Epoch 87: Val Loss 129624720.00000
Epoch 88: Val Loss 127915720.00000
Epoch 89: Val Loss 126033736.00000
Epoch 90: Val Loss 124058696.00000
Epoch 91: Val Loss 122102872.00000
Epoch 92: Val Loss 120122896.00000
Epoch 93: Val Loss 117840112.00000
Epoch 94: Val Loss 115696584.00000
Epoch 95: Val Loss 113301864.00000
Epoch 96: Val Loss 110890136.00000
Epoch 97: Val Loss 108489072.00000
Epoch 98: Val Loss 105978888.00000
Epoch 99: Val Loss 103495496.00000
{'MSE - mean': 108532314.67892992, 'MSE - std': 5790819.700700708, 'R2 - mean': -3.2250940511747466, 'R2 - std': 0.25019525550586597} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162297712.00000
Epoch 1: Val Loss 162300400.00000
Epoch 2: Val Loss 162293312.00000
Epoch 3: Val Loss 162383632.00000
Epoch 4: Val Loss 162217760.00000
Epoch 5: Val Loss 162350608.00000
Epoch 6: Val Loss 162384688.00000
Epoch 7: Val Loss 162366464.00000
Epoch 8: Val Loss 162226272.00000
Epoch 9: Val Loss 162407856.00000
Epoch 10: Val Loss 162298560.00000
Epoch 11: Val Loss 162346384.00000
Epoch 12: Val Loss 162336464.00000
Epoch 13: Val Loss 162400784.00000
Epoch 14: Val Loss 162256512.00000
Epoch 15: Val Loss 162348176.00000
Epoch 16: Val Loss 162361056.00000
Epoch 17: Val Loss 162370592.00000
Epoch 18: Val Loss 162339744.00000
Epoch 19: Val Loss 162186288.00000
Epoch 20: Val Loss 162334608.00000
Epoch 21: Val Loss 162350832.00000
Epoch 22: Val Loss 162321808.00000
Epoch 23: Val Loss 162226512.00000
Epoch 24: Val Loss 162253488.00000
Epoch 25: Val Loss 162087264.00000
Epoch 26: Val Loss 162143984.00000
Epoch 27: Val Loss 162082592.00000
Epoch 28: Val Loss 162205888.00000
Epoch 29: Val Loss 162094960.00000
Epoch 30: Val Loss 162193376.00000
Epoch 31: Val Loss 162057568.00000
Epoch 32: Val Loss 162009968.00000
Epoch 33: Val Loss 162106160.00000
Epoch 34: Val Loss 161966624.00000
Epoch 35: Val Loss 161984880.00000
Epoch 36: Val Loss 161861904.00000
Epoch 37: Val Loss 161864864.00000
Epoch 38: Val Loss 161793424.00000
Epoch 39: Val Loss 161727216.00000
Epoch 40: Val Loss 161684576.00000
Epoch 41: Val Loss 161585568.00000
Epoch 42: Val Loss 161650160.00000
Epoch 43: Val Loss 161494032.00000
Epoch 44: Val Loss 161394848.00000
Epoch 45: Val Loss 161327840.00000
Epoch 46: Val Loss 161088144.00000
Epoch 47: Val Loss 161091520.00000
Epoch 48: Val Loss 161215728.00000
Epoch 49: Val Loss 161063280.00000
Epoch 50: Val Loss 160809376.00000
Epoch 51: Val Loss 160653904.00000
Epoch 52: Val Loss 160581424.00000
Epoch 53: Val Loss 160456912.00000
Epoch 54: Val Loss 160190272.00000
Epoch 55: Val Loss 160011200.00000
Epoch 56: Val Loss 159883440.00000
Epoch 57: Val Loss 159713184.00000
Epoch 58: Val Loss 159464992.00000
Epoch 59: Val Loss 159251088.00000
Epoch 60: Val Loss 159037568.00000
Epoch 61: Val Loss 158651968.00000
Epoch 62: Val Loss 158398048.00000
Epoch 63: Val Loss 158125632.00000
Epoch 64: Val Loss 157873312.00000
Epoch 65: Val Loss 157458544.00000
Epoch 66: Val Loss 157243424.00000
Epoch 67: Val Loss 156754960.00000
Epoch 68: Val Loss 156513328.00000
Epoch 69: Val Loss 156027216.00000
Epoch 70: Val Loss 155440976.00000
Epoch 71: Val Loss 154924272.00000
Epoch 72: Val Loss 154535040.00000
Epoch 73: Val Loss 154003216.00000
Epoch 74: Val Loss 153387392.00000
Epoch 75: Val Loss 152702192.00000
Epoch 76: Val Loss 152119648.00000
Epoch 77: Val Loss 151567856.00000
Epoch 78: Val Loss 150731200.00000
Epoch 79: Val Loss 149951904.00000
Epoch 80: Val Loss 149105744.00000
Epoch 81: Val Loss 148335072.00000
Epoch 82: Val Loss 147398720.00000
Epoch 83: Val Loss 146454752.00000
Epoch 84: Val Loss 145441840.00000
Epoch 85: Val Loss 144529392.00000
Epoch 86: Val Loss 143438224.00000
Epoch 87: Val Loss 142289184.00000
Epoch 88: Val Loss 141128176.00000
Epoch 89: Val Loss 139854336.00000
Epoch 90: Val Loss 138609024.00000
Epoch 91: Val Loss 137112304.00000
Epoch 92: Val Loss 135876448.00000
Epoch 93: Val Loss 134396688.00000
Epoch 94: Val Loss 132888824.00000
Epoch 95: Val Loss 131242144.00000
Epoch 96: Val Loss 129502000.00000
Epoch 97: Val Loss 127788400.00000
Epoch 98: Val Loss 126001824.00000
Epoch 99: Val Loss 124103360.00000
{'MSE - mean': 113568760.5909235, 'MSE - std': 8549111.26294556, 'R2 - mean': -3.4140785250901438, 'R2 - std': 0.3363956599213526} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 160507568.00000
Epoch 1: Val Loss 160446576.00000
Epoch 2: Val Loss 160430176.00000
Epoch 3: Val Loss 160496848.00000
Epoch 4: Val Loss 160546672.00000
Epoch 5: Val Loss 160493632.00000
Epoch 6: Val Loss 160455296.00000
Epoch 7: Val Loss 160650672.00000
Epoch 8: Val Loss 160421776.00000
Epoch 9: Val Loss 160497520.00000
Epoch 10: Val Loss 160605616.00000
Epoch 11: Val Loss 160445200.00000
Epoch 12: Val Loss 160444608.00000
Epoch 13: Val Loss 160513008.00000
Epoch 14: Val Loss 160495088.00000
Epoch 15: Val Loss 160405856.00000
Epoch 16: Val Loss 160633168.00000
Epoch 17: Val Loss 160522864.00000
Epoch 18: Val Loss 160533024.00000
Epoch 19: Val Loss 160378480.00000
Epoch 20: Val Loss 160387424.00000
Epoch 21: Val Loss 160459184.00000
Epoch 22: Val Loss 160475648.00000
Epoch 23: Val Loss 160378064.00000
Epoch 24: Val Loss 160343584.00000
Epoch 25: Val Loss 160343024.00000
Epoch 26: Val Loss 160330736.00000
Epoch 27: Val Loss 160322176.00000
Epoch 28: Val Loss 160394400.00000
Epoch 29: Val Loss 160411904.00000
Epoch 30: Val Loss 160353264.00000
Epoch 31: Val Loss 160266672.00000
Epoch 32: Val Loss 160294080.00000
Epoch 33: Val Loss 160240112.00000
Epoch 34: Val Loss 160174352.00000
Epoch 35: Val Loss 160131088.00000
Epoch 36: Val Loss 160345968.00000
Epoch 37: Val Loss 160012688.00000
Epoch 38: Val Loss 160142352.00000
Epoch 39: Val Loss 159975216.00000
Epoch 40: Val Loss 160036672.00000
Epoch 41: Val Loss 159850656.00000
Epoch 42: Val Loss 159878288.00000
Epoch 43: Val Loss 159792512.00000
Epoch 44: Val Loss 159668832.00000
Epoch 45: Val Loss 159622992.00000
Epoch 46: Val Loss 159607168.00000
Epoch 47: Val Loss 159391760.00000
Epoch 48: Val Loss 159485648.00000
Epoch 49: Val Loss 159317456.00000
Epoch 50: Val Loss 159251376.00000
Epoch 51: Val Loss 159065904.00000
Epoch 52: Val Loss 158914464.00000
Epoch 53: Val Loss 158748560.00000
Epoch 54: Val Loss 158731936.00000
Epoch 55: Val Loss 158559872.00000
Epoch 56: Val Loss 158335872.00000
Epoch 57: Val Loss 158331088.00000
Epoch 58: Val Loss 157930416.00000
Epoch 59: Val Loss 157803536.00000
Epoch 60: Val Loss 157658400.00000
Epoch 61: Val Loss 157454400.00000
Epoch 62: Val Loss 157151168.00000
Epoch 63: Val Loss 156841424.00000
Epoch 64: Val Loss 156630400.00000
Epoch 65: Val Loss 156369728.00000
Epoch 66: Val Loss 156041776.00000
Epoch 67: Val Loss 155591296.00000
Epoch 68: Val Loss 155301088.00000
Epoch 69: Val Loss 154930768.00000
Epoch 70: Val Loss 154578608.00000
Epoch 71: Val Loss 154249632.00000
Epoch 72: Val Loss 153725168.00000
Epoch 73: Val Loss 153170288.00000
Epoch 74: Val Loss 152664832.00000
Epoch 75: Val Loss 152244752.00000
Epoch 76: Val Loss 151621984.00000
Epoch 77: Val Loss 151107888.00000
Epoch 78: Val Loss 150389568.00000
Epoch 79: Val Loss 149883744.00000
Epoch 80: Val Loss 149062704.00000
Epoch 81: Val Loss 148269456.00000
Epoch 82: Val Loss 147539776.00000
Epoch 83: Val Loss 146826336.00000
Epoch 84: Val Loss 145921824.00000
Epoch 85: Val Loss 145054784.00000
Epoch 86: Val Loss 144145504.00000
Epoch 87: Val Loss 143103376.00000
Epoch 88: Val Loss 142093728.00000
Epoch 89: Val Loss 140998464.00000
Epoch 90: Val Loss 139729376.00000
Epoch 91: Val Loss 138663216.00000
Epoch 92: Val Loss 137412496.00000
Epoch 93: Val Loss 136216432.00000
Epoch 94: Val Loss 134903264.00000
Epoch 95: Val Loss 133376520.00000
Epoch 96: Val Loss 131795008.00000
Epoch 97: Val Loss 130509408.00000
Epoch 98: Val Loss 128803160.00000
Epoch 99: Val Loss 127092584.00000
{'MSE - mean': 116885569.19609635, 'MSE - std': 9371186.451549917, 'R2 - mean': -3.5363513395352237, 'R2 - std': 0.36017142233581595} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161659024.00000
Epoch 1: Val Loss 161687360.00000
Epoch 2: Val Loss 161784448.00000
Epoch 3: Val Loss 161722032.00000
Epoch 4: Val Loss 161651088.00000
Epoch 5: Val Loss 161666272.00000
Epoch 6: Val Loss 161786240.00000
Epoch 7: Val Loss 161701280.00000
Epoch 8: Val Loss 161726688.00000
Epoch 9: Val Loss 161592512.00000
Epoch 10: Val Loss 161644992.00000
Epoch 11: Val Loss 161763776.00000
Epoch 12: Val Loss 161705632.00000
Epoch 13: Val Loss 161609600.00000
Epoch 14: Val Loss 161657344.00000
Epoch 15: Val Loss 161637376.00000
Epoch 16: Val Loss 161672464.00000
Epoch 17: Val Loss 161596048.00000
Epoch 18: Val Loss 161706416.00000
Epoch 19: Val Loss 161651184.00000
Epoch 20: Val Loss 161714912.00000
Epoch 21: Val Loss 161629232.00000
Epoch 22: Val Loss 161664592.00000
Epoch 23: Val Loss 161398736.00000
Epoch 24: Val Loss 161439632.00000
Epoch 25: Val Loss 161491936.00000
Epoch 26: Val Loss 161385520.00000
Epoch 27: Val Loss 161540512.00000
Epoch 28: Val Loss 161494928.00000
Epoch 29: Val Loss 161428640.00000
Epoch 30: Val Loss 161525648.00000
Epoch 31: Val Loss 161340048.00000
Epoch 32: Val Loss 161287392.00000
Epoch 33: Val Loss 161374896.00000
Epoch 34: Val Loss 161267984.00000
Epoch 35: Val Loss 161273440.00000
Epoch 36: Val Loss 161212160.00000
Epoch 37: Val Loss 161106000.00000
Epoch 38: Val Loss 161007152.00000
Epoch 39: Val Loss 161048592.00000
Epoch 40: Val Loss 160933888.00000
Epoch 41: Val Loss 160884128.00000
Epoch 42: Val Loss 160812160.00000
Epoch 43: Val Loss 160764128.00000
Epoch 44: Val Loss 160662688.00000
Epoch 45: Val Loss 160398480.00000
Epoch 46: Val Loss 160325840.00000
Epoch 47: Val Loss 160155296.00000
Epoch 48: Val Loss 160080896.00000
Epoch 49: Val Loss 160056032.00000
Epoch 50: Val Loss 159790784.00000
Epoch 51: Val Loss 159713504.00000
Epoch 52: Val Loss 159449584.00000
Epoch 53: Val Loss 159315184.00000
Epoch 54: Val Loss 159000016.00000
Epoch 55: Val Loss 158819568.00000
Epoch 56: Val Loss 158676912.00000
Epoch 57: Val Loss 158436624.00000
Epoch 58: Val Loss 158106528.00000
Epoch 59: Val Loss 157827360.00000
Epoch 60: Val Loss 157575440.00000
Epoch 61: Val Loss 157344496.00000
Epoch 62: Val Loss 156808976.00000
Epoch 63: Val Loss 156594608.00000
Epoch 64: Val Loss 156147984.00000
Epoch 65: Val Loss 155740000.00000
Epoch 66: Val Loss 155321104.00000
Epoch 67: Val Loss 154832352.00000
Epoch 68: Val Loss 154371712.00000
Epoch 69: Val Loss 153740432.00000
Epoch 70: Val Loss 153352208.00000
Epoch 71: Val Loss 152733392.00000
Epoch 72: Val Loss 152096256.00000
Epoch 73: Val Loss 151399120.00000
Epoch 74: Val Loss 150546560.00000
Epoch 75: Val Loss 149952592.00000
Epoch 76: Val Loss 149155424.00000
Epoch 77: Val Loss 148444400.00000
Epoch 78: Val Loss 147519072.00000
Epoch 79: Val Loss 146684176.00000
Epoch 80: Val Loss 145648592.00000
Epoch 81: Val Loss 144673152.00000
Epoch 82: Val Loss 143693232.00000
Epoch 83: Val Loss 142500880.00000
Epoch 84: Val Loss 141427680.00000
Epoch 85: Val Loss 140266528.00000
Epoch 86: Val Loss 138937072.00000
Epoch 87: Val Loss 137580464.00000
Epoch 88: Val Loss 136228480.00000
Epoch 89: Val Loss 134846064.00000
Epoch 90: Val Loss 133256872.00000
Epoch 91: Val Loss 131632744.00000
Epoch 92: Val Loss 130087736.00000
Epoch 93: Val Loss 128423320.00000
Epoch 94: Val Loss 126672784.00000
Epoch 95: Val Loss 124847976.00000
Epoch 96: Val Loss 122800640.00000
Epoch 97: Val Loss 121057776.00000
Epoch 98: Val Loss 119041920.00000
Epoch 99: Val Loss 116860088.00000
{'MSE - mean': 116763243.07014892, 'MSE - std': 8385413.712639788, 'R2 - mean': -3.5207493210764556, 'R2 - std': 0.3236548388235139} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 116763243.07014892 and parameters: {'dim': 32, 'depth': 1, 'heads': 4, 'weight_decay': -6, 'learning_rate': -6, 'dropout': 0.4}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161489344.00000
Epoch 1: Val Loss 159497712.00000
Epoch 2: Val Loss 152520576.00000
Epoch 3: Val Loss 135163200.00000
Epoch 4: Val Loss 103131176.00000
Epoch 5: Val Loss 60864376.00000
Epoch 6: Val Loss 30426854.00000
Epoch 7: Val Loss 25755978.00000
Epoch 8: Val Loss 25735084.00000
Epoch 9: Val Loss 25657864.00000
Epoch 10: Val Loss 25673176.00000
Epoch 11: Val Loss 25684840.00000
Epoch 12: Val Loss 25682090.00000
Epoch 13: Val Loss 25658314.00000
Epoch 14: Val Loss 25649716.00000
Epoch 15: Val Loss 25592388.00000
Epoch 16: Val Loss 25657600.00000
Epoch 17: Val Loss 25617270.00000
Epoch 18: Val Loss 25629014.00000
Epoch 19: Val Loss 25623454.00000
Epoch 20: Val Loss 25644184.00000
Epoch 21: Val Loss 25591468.00000
Epoch 22: Val Loss 25611346.00000
Epoch 23: Val Loss 25625194.00000
Epoch 24: Val Loss 25570798.00000
Epoch 25: Val Loss 25581998.00000
Epoch 26: Val Loss 25588744.00000
Epoch 27: Val Loss 25578868.00000
Epoch 28: Val Loss 25515950.00000
Epoch 29: Val Loss 25522494.00000
Epoch 30: Val Loss 25546240.00000
Epoch 31: Val Loss 25498344.00000
Epoch 32: Val Loss 25465822.00000
Epoch 33: Val Loss 25454786.00000
Epoch 34: Val Loss 25438986.00000
Epoch 35: Val Loss 25477008.00000
Epoch 36: Val Loss 25419222.00000
Epoch 37: Val Loss 25421874.00000
Epoch 38: Val Loss 25390306.00000
Epoch 39: Val Loss 25360944.00000
Epoch 40: Val Loss 25410454.00000
Epoch 41: Val Loss 25396674.00000
Epoch 42: Val Loss 25351330.00000
Epoch 43: Val Loss 25370848.00000
Epoch 44: Val Loss 25359284.00000
Epoch 45: Val Loss 25314074.00000
Epoch 46: Val Loss 25336148.00000
Epoch 47: Val Loss 25284626.00000
Epoch 48: Val Loss 25259420.00000
Epoch 49: Val Loss 25283674.00000
Epoch 50: Val Loss 25229992.00000
Epoch 51: Val Loss 25209242.00000
Epoch 52: Val Loss 25218934.00000
Epoch 53: Val Loss 25167024.00000
Epoch 54: Val Loss 25220880.00000
Epoch 55: Val Loss 25140258.00000
Epoch 56: Val Loss 25122176.00000
Epoch 57: Val Loss 25139828.00000
Epoch 58: Val Loss 25090462.00000
Epoch 59: Val Loss 25121942.00000
Epoch 60: Val Loss 25077764.00000
Epoch 61: Val Loss 25108248.00000
Epoch 62: Val Loss 25036086.00000
Epoch 63: Val Loss 25016940.00000
Epoch 64: Val Loss 24986168.00000
Epoch 65: Val Loss 24980786.00000
Epoch 66: Val Loss 24957398.00000
Epoch 67: Val Loss 24908044.00000
Epoch 68: Val Loss 24904132.00000
Epoch 69: Val Loss 24897450.00000
Epoch 70: Val Loss 24810140.00000
Epoch 71: Val Loss 24820952.00000
Epoch 72: Val Loss 24841906.00000
Epoch 73: Val Loss 24811166.00000
Epoch 74: Val Loss 24786906.00000
Epoch 75: Val Loss 24751428.00000
Epoch 76: Val Loss 24734596.00000
Epoch 77: Val Loss 24652492.00000
Epoch 78: Val Loss 24676800.00000
Epoch 79: Val Loss 24646942.00000
Epoch 80: Val Loss 24650982.00000
Epoch 81: Val Loss 24583822.00000
Epoch 82: Val Loss 24510408.00000
Epoch 83: Val Loss 24524502.00000
Epoch 84: Val Loss 24493278.00000
Epoch 85: Val Loss 24471894.00000
Epoch 86: Val Loss 24394290.00000
Epoch 87: Val Loss 24411430.00000
Epoch 88: Val Loss 24363348.00000
Epoch 89: Val Loss 24394066.00000
Epoch 90: Val Loss 24241498.00000
Epoch 91: Val Loss 24307520.00000
Epoch 92: Val Loss 24229828.00000
Epoch 93: Val Loss 24181520.00000
Epoch 94: Val Loss 24115218.00000
Epoch 95: Val Loss 24131562.00000
Epoch 96: Val Loss 24078926.00000
Epoch 97: Val Loss 24027682.00000
Epoch 98: Val Loss 23991646.00000
Epoch 99: Val Loss 23966490.00000
{'MSE - mean': 23798469.88802778, 'MSE - std': 0.0, 'R2 - mean': 0.06838595369873213, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162375424.00000
Epoch 1: Val Loss 161090768.00000
Epoch 2: Val Loss 156540656.00000
Epoch 3: Val Loss 145815904.00000
Epoch 4: Val Loss 125094792.00000
Epoch 5: Val Loss 94610856.00000
Epoch 6: Val Loss 63605220.00000
Epoch 7: Val Loss 44765080.00000
Epoch 8: Val Loss 32634444.00000
Epoch 9: Val Loss 26707148.00000
Epoch 10: Val Loss 26089204.00000
Epoch 11: Val Loss 26113218.00000
Epoch 12: Val Loss 26067666.00000
Epoch 13: Val Loss 26126774.00000
Epoch 14: Val Loss 26107470.00000
Epoch 15: Val Loss 26110890.00000
Epoch 16: Val Loss 26041338.00000
Epoch 17: Val Loss 25989190.00000
Epoch 18: Val Loss 26073990.00000
Epoch 19: Val Loss 26031468.00000
Epoch 20: Val Loss 26046844.00000
Epoch 21: Val Loss 26035070.00000
Epoch 22: Val Loss 26002448.00000
Epoch 23: Val Loss 26007316.00000
Epoch 24: Val Loss 26008064.00000
Epoch 25: Val Loss 25959780.00000
Epoch 26: Val Loss 25912470.00000
Epoch 27: Val Loss 25990536.00000
Epoch 28: Val Loss 25933188.00000
Epoch 29: Val Loss 25911948.00000
Epoch 30: Val Loss 25947360.00000
Epoch 31: Val Loss 25857546.00000
Epoch 32: Val Loss 25902648.00000
Epoch 33: Val Loss 25865496.00000
Epoch 34: Val Loss 25925672.00000
Epoch 35: Val Loss 25882394.00000
Epoch 36: Val Loss 25909672.00000
Epoch 37: Val Loss 25875340.00000
Epoch 38: Val Loss 25881692.00000
Epoch 39: Val Loss 25874654.00000
Epoch 40: Val Loss 25786238.00000
Epoch 41: Val Loss 25840588.00000
Epoch 42: Val Loss 25765634.00000
Epoch 43: Val Loss 25728958.00000
Epoch 44: Val Loss 25730432.00000
Epoch 45: Val Loss 25733446.00000
Epoch 46: Val Loss 25766380.00000
Epoch 47: Val Loss 25766810.00000
Epoch 48: Val Loss 25678816.00000
Epoch 49: Val Loss 25683332.00000
Epoch 50: Val Loss 25682708.00000
Epoch 51: Val Loss 25637288.00000
Epoch 52: Val Loss 25670636.00000
Epoch 53: Val Loss 25663822.00000
Epoch 54: Val Loss 25655488.00000
Epoch 55: Val Loss 25638992.00000
Epoch 56: Val Loss 25554180.00000
Epoch 57: Val Loss 25574926.00000
Epoch 58: Val Loss 25536696.00000
Epoch 59: Val Loss 25529334.00000
Epoch 60: Val Loss 25513058.00000
Epoch 61: Val Loss 25463684.00000
Epoch 62: Val Loss 25522806.00000
Epoch 63: Val Loss 25485798.00000
Epoch 64: Val Loss 25463020.00000
Epoch 65: Val Loss 25496642.00000
Epoch 66: Val Loss 25389724.00000
Epoch 67: Val Loss 25400128.00000
Epoch 68: Val Loss 25341190.00000
Epoch 69: Val Loss 25347140.00000
Epoch 70: Val Loss 25368864.00000
Epoch 71: Val Loss 25364028.00000
Epoch 72: Val Loss 25322946.00000
Epoch 73: Val Loss 25239606.00000
Epoch 74: Val Loss 25298118.00000
Epoch 75: Val Loss 25288586.00000
Epoch 76: Val Loss 25223188.00000
Epoch 77: Val Loss 25242780.00000
Epoch 78: Val Loss 25166202.00000
Epoch 79: Val Loss 25161594.00000
Epoch 80: Val Loss 25110846.00000
Epoch 81: Val Loss 25114668.00000
Epoch 82: Val Loss 25044014.00000
Epoch 83: Val Loss 25053242.00000
Epoch 84: Val Loss 24998244.00000
Epoch 85: Val Loss 24989888.00000
Epoch 86: Val Loss 25001700.00000
Epoch 87: Val Loss 24987476.00000
Epoch 88: Val Loss 24964608.00000
Epoch 89: Val Loss 24975406.00000
Epoch 90: Val Loss 24887312.00000
Epoch 91: Val Loss 24871022.00000
Epoch 92: Val Loss 24859460.00000
Epoch 93: Val Loss 24838284.00000
Epoch 94: Val Loss 24777898.00000
Epoch 95: Val Loss 24753454.00000
Epoch 96: Val Loss 24756344.00000
Epoch 97: Val Loss 24695610.00000
Epoch 98: Val Loss 24686968.00000
Epoch 99: Val Loss 24597344.00000
{'MSE - mean': 24104629.55645884, 'MSE - std': 306159.6684310585, 'R2 - mean': 0.061986412637397126, 'R2 - std': 0.006399541061335001} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162145568.00000
Epoch 1: Val Loss 161293072.00000
Epoch 2: Val Loss 157253008.00000
Epoch 3: Val Loss 147601808.00000
Epoch 4: Val Loss 128661976.00000
Epoch 5: Val Loss 97673472.00000
Epoch 6: Val Loss 59998764.00000
Epoch 7: Val Loss 32106186.00000
Epoch 8: Val Loss 26000340.00000
Epoch 9: Val Loss 25986786.00000
Epoch 10: Val Loss 25941642.00000
Epoch 11: Val Loss 25931404.00000
Epoch 12: Val Loss 25963054.00000
Epoch 13: Val Loss 25899954.00000
Epoch 14: Val Loss 25958774.00000
Epoch 15: Val Loss 25973684.00000
Epoch 16: Val Loss 25943828.00000
Epoch 17: Val Loss 25923282.00000
Epoch 18: Val Loss 25891920.00000
Epoch 19: Val Loss 25870802.00000
Epoch 20: Val Loss 25906380.00000
Epoch 21: Val Loss 25884712.00000
Epoch 22: Val Loss 25844024.00000
Epoch 23: Val Loss 25894918.00000
Epoch 24: Val Loss 25865326.00000
Epoch 25: Val Loss 25875224.00000
Epoch 26: Val Loss 25852258.00000
Epoch 27: Val Loss 25890738.00000
Epoch 28: Val Loss 25844152.00000
Epoch 29: Val Loss 25882364.00000
Epoch 30: Val Loss 25807470.00000
Epoch 31: Val Loss 25852242.00000
Epoch 32: Val Loss 25811412.00000
Epoch 33: Val Loss 25820046.00000
Epoch 34: Val Loss 25840996.00000
Epoch 35: Val Loss 25835168.00000
Epoch 36: Val Loss 25755506.00000
Epoch 37: Val Loss 25752102.00000
Epoch 38: Val Loss 25854422.00000
Epoch 39: Val Loss 25773710.00000
Epoch 40: Val Loss 25748882.00000
Epoch 41: Val Loss 25775626.00000
Epoch 42: Val Loss 25813164.00000
Epoch 43: Val Loss 25707368.00000
Epoch 44: Val Loss 25730714.00000
Epoch 45: Val Loss 25740772.00000
Epoch 46: Val Loss 25759084.00000
Epoch 47: Val Loss 25756066.00000
Epoch 48: Val Loss 25692950.00000
Epoch 49: Val Loss 25725672.00000
Epoch 50: Val Loss 25676804.00000
Epoch 51: Val Loss 25675420.00000
Epoch 52: Val Loss 25687596.00000
Epoch 53: Val Loss 25630742.00000
Epoch 54: Val Loss 25634998.00000
Epoch 55: Val Loss 25646432.00000
Epoch 56: Val Loss 25645196.00000
Epoch 57: Val Loss 25675114.00000
Epoch 58: Val Loss 25622758.00000
Epoch 59: Val Loss 25556032.00000
Epoch 60: Val Loss 25631238.00000
Epoch 61: Val Loss 25629100.00000
Epoch 62: Val Loss 25548246.00000
Epoch 63: Val Loss 25571458.00000
Epoch 64: Val Loss 25524226.00000
Epoch 65: Val Loss 25534216.00000
Epoch 66: Val Loss 25550474.00000
Epoch 67: Val Loss 25531952.00000
Epoch 68: Val Loss 25510298.00000
Epoch 69: Val Loss 25482092.00000
Epoch 70: Val Loss 25469800.00000
Epoch 71: Val Loss 25429414.00000
Epoch 72: Val Loss 25391690.00000
Epoch 73: Val Loss 25477306.00000
Epoch 74: Val Loss 25452474.00000
Epoch 75: Val Loss 25421720.00000
Epoch 76: Val Loss 25447264.00000
Epoch 77: Val Loss 25417862.00000
Epoch 78: Val Loss 25355500.00000
Epoch 79: Val Loss 25355240.00000
Epoch 80: Val Loss 25305464.00000
Epoch 81: Val Loss 25359456.00000
Epoch 82: Val Loss 25346702.00000
Epoch 83: Val Loss 25307764.00000
Epoch 84: Val Loss 25252758.00000
Epoch 85: Val Loss 25250060.00000
Epoch 86: Val Loss 25269484.00000
Epoch 87: Val Loss 25269090.00000
Epoch 88: Val Loss 25169354.00000
Epoch 89: Val Loss 25172730.00000
Epoch 90: Val Loss 25232626.00000
Epoch 91: Val Loss 25167638.00000
Epoch 92: Val Loss 25181858.00000
Epoch 93: Val Loss 25132506.00000
Epoch 94: Val Loss 25181728.00000
Epoch 95: Val Loss 25077682.00000
Epoch 96: Val Loss 25112732.00000
Epoch 97: Val Loss 25037186.00000
Epoch 98: Val Loss 25028706.00000
Epoch 99: Val Loss 25015090.00000
{'MSE - mean': 24364627.231674705, 'MSE - std': 444619.7745840709, 'R2 - mean': 0.05316861063690755, 'R2 - std': 0.01352072537884529} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 160191552.00000
Epoch 1: Val Loss 158501200.00000
Epoch 2: Val Loss 151431728.00000
Epoch 3: Val Loss 132582360.00000
Epoch 4: Val Loss 98054512.00000
Epoch 5: Val Loss 54256240.00000
Epoch 6: Val Loss 28202274.00000
Epoch 7: Val Loss 26053316.00000
Epoch 8: Val Loss 26021710.00000
Epoch 9: Val Loss 26086674.00000
Epoch 10: Val Loss 26008476.00000
Epoch 11: Val Loss 26043828.00000
Epoch 12: Val Loss 26052986.00000
Epoch 13: Val Loss 26021518.00000
Epoch 14: Val Loss 26017950.00000
Epoch 15: Val Loss 26014888.00000
Epoch 16: Val Loss 25974246.00000
Epoch 17: Val Loss 25995852.00000
Epoch 18: Val Loss 25954006.00000
Epoch 19: Val Loss 26015078.00000
Epoch 20: Val Loss 25925174.00000
Epoch 21: Val Loss 25941264.00000
Epoch 22: Val Loss 25900046.00000
Epoch 23: Val Loss 25883018.00000
Epoch 24: Val Loss 25904478.00000
Epoch 25: Val Loss 25877378.00000
Epoch 26: Val Loss 25893436.00000
Epoch 27: Val Loss 25897710.00000
Epoch 28: Val Loss 25837178.00000
Epoch 29: Val Loss 25823342.00000
Epoch 30: Val Loss 25827182.00000
Epoch 31: Val Loss 25876852.00000
Epoch 32: Val Loss 25804748.00000
Epoch 33: Val Loss 25841662.00000
Epoch 34: Val Loss 25772316.00000
Epoch 35: Val Loss 25779506.00000
Epoch 36: Val Loss 25785634.00000
Epoch 37: Val Loss 25712274.00000
Epoch 38: Val Loss 25732440.00000
Epoch 39: Val Loss 25748196.00000
Epoch 40: Val Loss 25749112.00000
Epoch 41: Val Loss 25630376.00000
Epoch 42: Val Loss 25683514.00000
Epoch 43: Val Loss 25681870.00000
Epoch 44: Val Loss 25647026.00000
Epoch 45: Val Loss 25610758.00000
Epoch 46: Val Loss 25634302.00000
Epoch 47: Val Loss 25594878.00000
Epoch 48: Val Loss 25559186.00000
Epoch 49: Val Loss 25547490.00000
Epoch 50: Val Loss 25581520.00000
Epoch 51: Val Loss 25520032.00000
Epoch 52: Val Loss 25515136.00000
Epoch 53: Val Loss 25481264.00000
Epoch 54: Val Loss 25479796.00000
Epoch 55: Val Loss 25381310.00000
Epoch 56: Val Loss 25438976.00000
Epoch 57: Val Loss 25356126.00000
Epoch 58: Val Loss 25397412.00000
Epoch 59: Val Loss 25358220.00000
Epoch 60: Val Loss 25315314.00000
Epoch 61: Val Loss 25384442.00000
Epoch 62: Val Loss 25312308.00000
Epoch 63: Val Loss 25224626.00000
Epoch 64: Val Loss 25298072.00000
Epoch 65: Val Loss 25250034.00000
Epoch 66: Val Loss 25185872.00000
Epoch 67: Val Loss 25212260.00000
Epoch 68: Val Loss 25159200.00000
Epoch 69: Val Loss 25125738.00000
Epoch 70: Val Loss 25102258.00000
Epoch 71: Val Loss 25052920.00000
Epoch 72: Val Loss 25040342.00000
Epoch 73: Val Loss 25018416.00000
Epoch 74: Val Loss 24974742.00000
Epoch 75: Val Loss 24984010.00000
Epoch 76: Val Loss 24895560.00000
Epoch 77: Val Loss 24913426.00000
Epoch 78: Val Loss 24814174.00000
Epoch 79: Val Loss 24823314.00000
Epoch 80: Val Loss 24787662.00000
Epoch 81: Val Loss 24751040.00000
Epoch 82: Val Loss 24772794.00000
Epoch 83: Val Loss 24672382.00000
Epoch 84: Val Loss 24643572.00000
Epoch 85: Val Loss 24657512.00000
Epoch 86: Val Loss 24603064.00000
Epoch 87: Val Loss 24574458.00000
Epoch 88: Val Loss 24524292.00000
Epoch 89: Val Loss 24462258.00000
Epoch 90: Val Loss 24419088.00000
Epoch 91: Val Loss 24400848.00000
Epoch 92: Val Loss 24359240.00000
Epoch 93: Val Loss 24304632.00000
Epoch 94: Val Loss 24239342.00000
Epoch 95: Val Loss 24183256.00000
Epoch 96: Val Loss 24145522.00000
Epoch 97: Val Loss 24055012.00000
Epoch 98: Val Loss 24008836.00000
Epoch 99: Val Loss 23986132.00000
{'MSE - mean': 24229036.489665423, 'MSE - std': 451020.62693326664, 'R2 - mean': 0.05964881974626532, 'R2 - std': 0.01621995195370885} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161306672.00000
Epoch 1: Val Loss 156621328.00000
Epoch 2: Val Loss 140227152.00000
Epoch 3: Val Loss 104240408.00000
Epoch 4: Val Loss 54827576.00000
Epoch 5: Val Loss 27692016.00000
Epoch 6: Val Loss 26300166.00000
Epoch 7: Val Loss 26270720.00000
Epoch 8: Val Loss 26211350.00000
Epoch 9: Val Loss 26195236.00000
Epoch 10: Val Loss 26229500.00000
Epoch 11: Val Loss 26213064.00000
Epoch 12: Val Loss 26182156.00000
Epoch 13: Val Loss 26181430.00000
Epoch 14: Val Loss 26153868.00000
Epoch 15: Val Loss 26143408.00000
Epoch 16: Val Loss 26106498.00000
Epoch 17: Val Loss 26112174.00000
Epoch 18: Val Loss 26101970.00000
Epoch 19: Val Loss 26095344.00000
Epoch 20: Val Loss 26091976.00000
Epoch 21: Val Loss 26025686.00000
Epoch 22: Val Loss 26095108.00000
Epoch 23: Val Loss 25984844.00000
Epoch 24: Val Loss 25996592.00000
Epoch 25: Val Loss 25940516.00000
Epoch 26: Val Loss 25983142.00000
Epoch 27: Val Loss 25916326.00000
Epoch 28: Val Loss 25943274.00000
Epoch 29: Val Loss 25876222.00000
Epoch 30: Val Loss 25847968.00000
Epoch 31: Val Loss 25870688.00000
Epoch 32: Val Loss 25865494.00000
Epoch 33: Val Loss 25862706.00000
Epoch 34: Val Loss 25812612.00000
Epoch 35: Val Loss 25785054.00000
Epoch 36: Val Loss 25831320.00000
Epoch 37: Val Loss 25792472.00000
Epoch 38: Val Loss 25689938.00000
Epoch 39: Val Loss 25704396.00000
Epoch 40: Val Loss 25682370.00000
Epoch 41: Val Loss 25624922.00000
Epoch 42: Val Loss 25630740.00000
Epoch 43: Val Loss 25649796.00000
Epoch 44: Val Loss 25563036.00000
Epoch 45: Val Loss 25509440.00000
Epoch 46: Val Loss 25487626.00000
Epoch 47: Val Loss 25500376.00000
Epoch 48: Val Loss 25460942.00000
Epoch 49: Val Loss 25414406.00000
Epoch 50: Val Loss 25442882.00000
Epoch 51: Val Loss 25407364.00000
Epoch 52: Val Loss 25346180.00000
Epoch 53: Val Loss 25321542.00000
Epoch 54: Val Loss 25289130.00000
Epoch 55: Val Loss 25304964.00000
Epoch 56: Val Loss 25234514.00000
Epoch 57: Val Loss 25211062.00000
Epoch 58: Val Loss 25127464.00000
Epoch 59: Val Loss 25123382.00000
Epoch 60: Val Loss 25110266.00000
Epoch 61: Val Loss 24993820.00000
Epoch 62: Val Loss 24984226.00000
Epoch 63: Val Loss 24960956.00000
Epoch 64: Val Loss 24929460.00000
Epoch 65: Val Loss 24826612.00000
Epoch 66: Val Loss 24838544.00000
Epoch 67: Val Loss 24765698.00000
Epoch 68: Val Loss 24774506.00000
Epoch 69: Val Loss 24698410.00000
Epoch 70: Val Loss 24612010.00000
Epoch 71: Val Loss 24629312.00000
Epoch 72: Val Loss 24501466.00000
Epoch 73: Val Loss 24493598.00000
Epoch 74: Val Loss 24442154.00000
Epoch 75: Val Loss 24350478.00000
Epoch 76: Val Loss 24303406.00000
Epoch 77: Val Loss 24246742.00000
Epoch 78: Val Loss 24215086.00000
Epoch 79: Val Loss 24138216.00000
Epoch 80: Val Loss 24065680.00000
Epoch 81: Val Loss 24038386.00000
Epoch 82: Val Loss 23871266.00000
Epoch 83: Val Loss 23865526.00000
Epoch 84: Val Loss 23817086.00000
Epoch 85: Val Loss 23702678.00000
Epoch 86: Val Loss 23663690.00000
Epoch 87: Val Loss 23529916.00000
Epoch 88: Val Loss 23481888.00000
Epoch 89: Val Loss 23364358.00000
Epoch 90: Val Loss 23239576.00000
Epoch 91: Val Loss 23155358.00000
Epoch 92: Val Loss 23077352.00000
Epoch 93: Val Loss 22973428.00000
Epoch 94: Val Loss 22858768.00000
Epoch 95: Val Loss 22763264.00000
Epoch 96: Val Loss 22634938.00000
Epoch 97: Val Loss 22536312.00000
Epoch 98: Val Loss 22444008.00000
Epoch 99: Val Loss 22354730.00000
{'MSE - mean': 23820017.620607335, 'MSE - std': 912097.2676845667, 'R2 - mean': 0.0775973836353048, 'R2 - std': 0.03871786739762698} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 23820017.620607335 and parameters: {'dim': 256, 'depth': 12, 'heads': 4, 'weight_decay': -4, 'learning_rate': -5, 'dropout': 0.2}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 25500054.00000
Epoch 1: Val Loss 25406848.00000
Epoch 2: Val Loss 25254304.00000
Epoch 3: Val Loss 25165136.00000
Epoch 4: Val Loss 24977288.00000
Epoch 5: Val Loss 24736280.00000
Epoch 6: Val Loss 24505886.00000
Epoch 7: Val Loss 24151240.00000
Epoch 8: Val Loss 23754362.00000
Epoch 9: Val Loss 23318438.00000
Epoch 10: Val Loss 22603218.00000
Epoch 11: Val Loss 21758056.00000
Epoch 12: Val Loss 20698976.00000
Epoch 13: Val Loss 19531374.00000
Epoch 14: Val Loss 18448116.00000
Epoch 15: Val Loss 17523876.00000
Epoch 16: Val Loss 16879782.00000
Epoch 17: Val Loss 16408058.00000
Epoch 18: Val Loss 16004654.00000
Epoch 19: Val Loss 15668927.00000
Epoch 20: Val Loss 15364039.00000
Epoch 21: Val Loss 15049628.00000
Epoch 22: Val Loss 14788677.00000
Epoch 23: Val Loss 14582577.00000
Epoch 24: Val Loss 14407723.00000
Epoch 25: Val Loss 14212941.00000
Epoch 26: Val Loss 14085518.00000
Epoch 27: Val Loss 13960474.00000
Epoch 28: Val Loss 13857856.00000
Epoch 29: Val Loss 13754440.00000
Epoch 30: Val Loss 13736848.00000
Epoch 31: Val Loss 13613960.00000
Epoch 32: Val Loss 13548162.00000
Epoch 33: Val Loss 13486071.00000
Epoch 34: Val Loss 13424598.00000
Epoch 35: Val Loss 13364597.00000
Epoch 36: Val Loss 13339048.00000
Epoch 37: Val Loss 13272125.00000
Epoch 38: Val Loss 13246584.00000
Epoch 39: Val Loss 13205639.00000
Epoch 40: Val Loss 13164410.00000
Epoch 41: Val Loss 13200616.00000
Epoch 42: Val Loss 13125032.00000
Epoch 43: Val Loss 13131031.00000
Epoch 44: Val Loss 13102043.00000
Epoch 45: Val Loss 13076213.00000
Epoch 46: Val Loss 13051448.00000
Epoch 47: Val Loss 13059464.00000
Epoch 48: Val Loss 13058135.00000
Epoch 49: Val Loss 13029713.00000
Epoch 50: Val Loss 13039793.00000
Epoch 51: Val Loss 13016781.00000
Epoch 52: Val Loss 13031330.00000
Epoch 53: Val Loss 13002570.00000
Epoch 54: Val Loss 13024985.00000
Epoch 55: Val Loss 13003523.00000
Epoch 56: Val Loss 12986093.00000
Epoch 57: Val Loss 12999838.00000
Epoch 58: Val Loss 12976222.00000
Epoch 59: Val Loss 13000856.00000
Epoch 60: Val Loss 12989219.00000
Epoch 61: Val Loss 12967119.00000
Epoch 62: Val Loss 12981361.00000
Epoch 63: Val Loss 12955881.00000
Epoch 64: Val Loss 12981114.00000
Epoch 65: Val Loss 12975386.00000
Epoch 66: Val Loss 12973375.00000
Epoch 67: Val Loss 12963508.00000
Epoch 68: Val Loss 12960656.00000
Epoch 69: Val Loss 12952754.00000
Epoch 70: Val Loss 12984080.00000
Epoch 71: Val Loss 12966630.00000
Epoch 72: Val Loss 12956797.00000
Epoch 73: Val Loss 12935920.00000
Epoch 74: Val Loss 12934208.00000
Epoch 75: Val Loss 12953650.00000
Epoch 76: Val Loss 12965873.00000
Epoch 77: Val Loss 12946394.00000
Epoch 78: Val Loss 12953447.00000
Epoch 79: Val Loss 12938619.00000
Epoch 80: Val Loss 12947827.00000
Epoch 81: Val Loss 12956702.00000
Epoch 82: Val Loss 12952684.00000
Epoch 83: Val Loss 12944987.00000
Epoch 84: Val Loss 12926876.00000
Epoch 85: Val Loss 12923801.00000
Epoch 86: Val Loss 12914508.00000
Epoch 87: Val Loss 12917508.00000
Epoch 88: Val Loss 12934314.00000
Epoch 89: Val Loss 12937692.00000
Epoch 90: Val Loss 12937101.00000
Epoch 91: Val Loss 12926143.00000
Epoch 92: Val Loss 12926866.00000
Epoch 93: Val Loss 12964168.00000
Epoch 94: Val Loss 12939763.00000
Epoch 95: Val Loss 12909211.00000
Epoch 96: Val Loss 12902603.00000
Epoch 97: Val Loss 12926677.00000
Epoch 98: Val Loss 12934376.00000
Epoch 99: Val Loss 12935273.00000
{'MSE - mean': 12915416.410443796, 'MSE - std': 0.0, 'R2 - mean': 0.49441357371247074, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 29584828.00000
Epoch 1: Val Loss 26364222.00000
Epoch 2: Val Loss 25413102.00000
Epoch 3: Val Loss 25177750.00000
Epoch 4: Val Loss 24824242.00000
Epoch 5: Val Loss 24460468.00000
Epoch 6: Val Loss 23911336.00000
Epoch 7: Val Loss 23157712.00000
Epoch 8: Val Loss 22200050.00000
Epoch 9: Val Loss 20986086.00000
Epoch 10: Val Loss 19638564.00000
Epoch 11: Val Loss 18423744.00000
Epoch 12: Val Loss 17536004.00000
Epoch 13: Val Loss 16890334.00000
Epoch 14: Val Loss 16401621.00000
Epoch 15: Val Loss 15998156.00000
Epoch 16: Val Loss 15661857.00000
Epoch 17: Val Loss 15338178.00000
Epoch 18: Val Loss 15078377.00000
Epoch 19: Val Loss 14841091.00000
Epoch 20: Val Loss 14687483.00000
Epoch 21: Val Loss 14498096.00000
Epoch 22: Val Loss 14366339.00000
Epoch 23: Val Loss 14269876.00000
Epoch 24: Val Loss 14154380.00000
Epoch 25: Val Loss 14104036.00000
Epoch 26: Val Loss 14015358.00000
Epoch 27: Val Loss 13946773.00000
Epoch 28: Val Loss 13885126.00000
Epoch 29: Val Loss 13837075.00000
Epoch 30: Val Loss 13780158.00000
Epoch 31: Val Loss 13710449.00000
Epoch 32: Val Loss 13688981.00000
Epoch 33: Val Loss 13640092.00000
Epoch 34: Val Loss 13586535.00000
Epoch 35: Val Loss 13534445.00000
Epoch 36: Val Loss 13522003.00000
Epoch 37: Val Loss 13469316.00000
Epoch 38: Val Loss 13466214.00000
Epoch 39: Val Loss 13426606.00000
Epoch 40: Val Loss 13402556.00000
Epoch 41: Val Loss 13394416.00000
Epoch 42: Val Loss 13365031.00000
Epoch 43: Val Loss 13318759.00000
Epoch 44: Val Loss 13323759.00000
Epoch 45: Val Loss 13340197.00000
Epoch 46: Val Loss 13270222.00000
Epoch 47: Val Loss 13242108.00000
Epoch 48: Val Loss 13238381.00000
Epoch 49: Val Loss 13234989.00000
Epoch 50: Val Loss 13241356.00000
Epoch 51: Val Loss 13227889.00000
Epoch 52: Val Loss 13209955.00000
Epoch 53: Val Loss 13217261.00000
Epoch 54: Val Loss 13235940.00000
Epoch 55: Val Loss 13191862.00000
Epoch 56: Val Loss 13226004.00000
Epoch 57: Val Loss 13194959.00000
Epoch 58: Val Loss 13199847.00000
Epoch 59: Val Loss 13194807.00000
Epoch 60: Val Loss 13158722.00000
Epoch 61: Val Loss 13177990.00000
Epoch 62: Val Loss 13171807.00000
Epoch 63: Val Loss 13158957.00000
Epoch 64: Val Loss 13140897.00000
Epoch 65: Val Loss 13162286.00000
Epoch 66: Val Loss 13152170.00000
Epoch 67: Val Loss 13142637.00000
Epoch 68: Val Loss 13137509.00000
Epoch 69: Val Loss 13129491.00000
Epoch 70: Val Loss 13131990.00000
Epoch 71: Val Loss 13155005.00000
Epoch 72: Val Loss 13132169.00000
Epoch 73: Val Loss 13123100.00000
Epoch 74: Val Loss 13132209.00000
Epoch 75: Val Loss 13114513.00000
Epoch 76: Val Loss 13144569.00000
Epoch 77: Val Loss 13103740.00000
Epoch 78: Val Loss 13135720.00000
Epoch 79: Val Loss 13116168.00000
Epoch 80: Val Loss 13119034.00000
Epoch 81: Val Loss 13107172.00000
Epoch 82: Val Loss 13113066.00000
Epoch 83: Val Loss 13114445.00000
Epoch 84: Val Loss 13094307.00000
Epoch 85: Val Loss 13097295.00000
Epoch 86: Val Loss 13082072.00000
Epoch 87: Val Loss 13072297.00000
Epoch 88: Val Loss 13101273.00000
Epoch 89: Val Loss 13085864.00000
Epoch 90: Val Loss 13089719.00000
Epoch 91: Val Loss 13125704.00000
Epoch 92: Val Loss 13076395.00000
Epoch 93: Val Loss 13086438.00000
Epoch 94: Val Loss 13087001.00000
Epoch 95: Val Loss 13080574.00000
Epoch 96: Val Loss 13099567.00000
Epoch 97: Val Loss 13056668.00000
Epoch 98: Val Loss 13060559.00000
Epoch 99: Val Loss 13090462.00000
{'MSE - mean': 12994916.09237479, 'MSE - std': 79499.68193099368, 'R2 - mean': 0.4942930078306598, 'R2 - std': 0.00012056588181091898} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 25779450.00000
Epoch 1: Val Loss 25691346.00000
Epoch 2: Val Loss 25618490.00000
Epoch 3: Val Loss 25419718.00000
Epoch 4: Val Loss 25255002.00000
Epoch 5: Val Loss 25083708.00000
Epoch 6: Val Loss 24904960.00000
Epoch 7: Val Loss 24576146.00000
Epoch 8: Val Loss 24217660.00000
Epoch 9: Val Loss 23789006.00000
Epoch 10: Val Loss 23245292.00000
Epoch 11: Val Loss 22508148.00000
Epoch 12: Val Loss 21619738.00000
Epoch 13: Val Loss 20592110.00000
Epoch 14: Val Loss 19367028.00000
Epoch 15: Val Loss 18340364.00000
Epoch 16: Val Loss 17602312.00000
Epoch 17: Val Loss 17001772.00000
Epoch 18: Val Loss 16583541.00000
Epoch 19: Val Loss 16236325.00000
Epoch 20: Val Loss 15931472.00000
Epoch 21: Val Loss 15614076.00000
Epoch 22: Val Loss 15375511.00000
Epoch 23: Val Loss 15138194.00000
Epoch 24: Val Loss 14942628.00000
Epoch 25: Val Loss 14742180.00000
Epoch 26: Val Loss 14537795.00000
Epoch 27: Val Loss 14407992.00000
Epoch 28: Val Loss 14298860.00000
Epoch 29: Val Loss 14175696.00000
Epoch 30: Val Loss 14096326.00000
Epoch 31: Val Loss 14012518.00000
Epoch 32: Val Loss 13917224.00000
Epoch 33: Val Loss 13866939.00000
Epoch 34: Val Loss 13793395.00000
Epoch 35: Val Loss 13739429.00000
Epoch 36: Val Loss 13662511.00000
Epoch 37: Val Loss 13642227.00000
Epoch 38: Val Loss 13585128.00000
Epoch 39: Val Loss 13544411.00000
Epoch 40: Val Loss 13537304.00000
Epoch 41: Val Loss 13467281.00000
Epoch 42: Val Loss 13449865.00000
Epoch 43: Val Loss 13407912.00000
Epoch 44: Val Loss 13395798.00000
Epoch 45: Val Loss 13366294.00000
Epoch 46: Val Loss 13352583.00000
Epoch 47: Val Loss 13374314.00000
Epoch 48: Val Loss 13309022.00000
Epoch 49: Val Loss 13327011.00000
Epoch 50: Val Loss 13304759.00000
Epoch 51: Val Loss 13298188.00000
Epoch 52: Val Loss 13280105.00000
Epoch 53: Val Loss 13274963.00000
Epoch 54: Val Loss 13262662.00000
Epoch 55: Val Loss 13250619.00000
Epoch 56: Val Loss 13280929.00000
Epoch 57: Val Loss 13246384.00000
Epoch 58: Val Loss 13248919.00000
Epoch 59: Val Loss 13232734.00000
Epoch 60: Val Loss 13233385.00000
Epoch 61: Val Loss 13215227.00000
Epoch 62: Val Loss 13229701.00000
Epoch 63: Val Loss 13210584.00000
Epoch 64: Val Loss 13202577.00000
Epoch 65: Val Loss 13203692.00000
Epoch 66: Val Loss 13228985.00000
Epoch 67: Val Loss 13211607.00000
Epoch 68: Val Loss 13193309.00000
Epoch 69: Val Loss 13201873.00000
Epoch 70: Val Loss 13202012.00000
Epoch 71: Val Loss 13177625.00000
Epoch 72: Val Loss 13202284.00000
Epoch 73: Val Loss 13192909.00000
Epoch 74: Val Loss 13198955.00000
Epoch 75: Val Loss 13170252.00000
Epoch 76: Val Loss 13171702.00000
Epoch 77: Val Loss 13174634.00000
Epoch 78: Val Loss 13197091.00000
Epoch 79: Val Loss 13169132.00000
Epoch 80: Val Loss 13169808.00000
Epoch 81: Val Loss 13188334.00000
Epoch 82: Val Loss 13182594.00000
Epoch 83: Val Loss 13178408.00000
Epoch 84: Val Loss 13156587.00000
Epoch 85: Val Loss 13161359.00000
Epoch 86: Val Loss 13190509.00000
Epoch 87: Val Loss 13179235.00000
Epoch 88: Val Loss 13162057.00000
Epoch 89: Val Loss 13167672.00000
Epoch 90: Val Loss 13158753.00000
Epoch 91: Val Loss 13172724.00000
Epoch 92: Val Loss 13151735.00000
Epoch 93: Val Loss 13170289.00000
Epoch 94: Val Loss 13167918.00000
Epoch 95: Val Loss 13165669.00000
Epoch 96: Val Loss 13149366.00000
Epoch 97: Val Loss 13142853.00000
Epoch 98: Val Loss 13150060.00000
Epoch 99: Val Loss 13154744.00000
{'MSE - mean': 13047228.951029867, 'MSE - std': 98421.22050698518, 'R2 - mean': 0.4929507854529884, 'R2 - std': 0.0019007400076852044} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 29452790.00000
Epoch 1: Val Loss 26159690.00000
Epoch 2: Val Loss 25628690.00000
Epoch 3: Val Loss 25468244.00000
Epoch 4: Val Loss 25288244.00000
Epoch 5: Val Loss 25098874.00000
Epoch 6: Val Loss 24812666.00000
Epoch 7: Val Loss 24478442.00000
Epoch 8: Val Loss 24080730.00000
Epoch 9: Val Loss 23539322.00000
Epoch 10: Val Loss 22796600.00000
Epoch 11: Val Loss 21959560.00000
Epoch 12: Val Loss 20903898.00000
Epoch 13: Val Loss 19832334.00000
Epoch 14: Val Loss 18778920.00000
Epoch 15: Val Loss 17974184.00000
Epoch 16: Val Loss 17358544.00000
Epoch 17: Val Loss 16889344.00000
Epoch 18: Val Loss 16504007.00000
Epoch 19: Val Loss 16188610.00000
Epoch 20: Val Loss 15907914.00000
Epoch 21: Val Loss 15632708.00000
Epoch 22: Val Loss 15376441.00000
Epoch 23: Val Loss 15155415.00000
Epoch 24: Val Loss 14970429.00000
Epoch 25: Val Loss 14788590.00000
Epoch 26: Val Loss 14672602.00000
Epoch 27: Val Loss 14570978.00000
Epoch 28: Val Loss 14470966.00000
Epoch 29: Val Loss 14339813.00000
Epoch 30: Val Loss 14292880.00000
Epoch 31: Val Loss 14229036.00000
Epoch 32: Val Loss 14156054.00000
Epoch 33: Val Loss 14058304.00000
Epoch 34: Val Loss 13985738.00000
Epoch 35: Val Loss 13943898.00000
Epoch 36: Val Loss 13887078.00000
Epoch 37: Val Loss 13829655.00000
Epoch 38: Val Loss 13819375.00000
Epoch 39: Val Loss 13767252.00000
Epoch 40: Val Loss 13722485.00000
Epoch 41: Val Loss 13666682.00000
Epoch 42: Val Loss 13630468.00000
Epoch 43: Val Loss 13597669.00000
Epoch 44: Val Loss 13567982.00000
Epoch 45: Val Loss 13555312.00000
Epoch 46: Val Loss 13530172.00000
Epoch 47: Val Loss 13493960.00000
Epoch 48: Val Loss 13497173.00000
Epoch 49: Val Loss 13491367.00000
Epoch 50: Val Loss 13431958.00000
Epoch 51: Val Loss 13410640.00000
Epoch 52: Val Loss 13403199.00000
Epoch 53: Val Loss 13371147.00000
Epoch 54: Val Loss 13364840.00000
Epoch 55: Val Loss 13350129.00000
Epoch 56: Val Loss 13333894.00000
Epoch 57: Val Loss 13324098.00000
Epoch 58: Val Loss 13325048.00000
Epoch 59: Val Loss 13317541.00000
Epoch 60: Val Loss 13306490.00000
Epoch 61: Val Loss 13306723.00000
Epoch 62: Val Loss 13289208.00000
Epoch 63: Val Loss 13274289.00000
Epoch 64: Val Loss 13246720.00000
Epoch 65: Val Loss 13246329.00000
Epoch 66: Val Loss 13223115.00000
Epoch 67: Val Loss 13237029.00000
Epoch 68: Val Loss 13233277.00000
Epoch 69: Val Loss 13213451.00000
Epoch 70: Val Loss 13238711.00000
Epoch 71: Val Loss 13260514.00000
Epoch 72: Val Loss 13189315.00000
Epoch 73: Val Loss 13211441.00000
Epoch 74: Val Loss 13214487.00000
Epoch 75: Val Loss 13183653.00000
Epoch 76: Val Loss 13161787.00000
Epoch 77: Val Loss 13172814.00000
Epoch 78: Val Loss 13181057.00000
Epoch 79: Val Loss 13175734.00000
Epoch 80: Val Loss 13140135.00000
Epoch 81: Val Loss 13144038.00000
Epoch 82: Val Loss 13149583.00000
Epoch 83: Val Loss 13155005.00000
Epoch 84: Val Loss 13180491.00000
Epoch 85: Val Loss 13165270.00000
Epoch 86: Val Loss 13134996.00000
Epoch 87: Val Loss 13126566.00000
Epoch 88: Val Loss 13125794.00000
Epoch 89: Val Loss 13129719.00000
Epoch 90: Val Loss 13128283.00000
Epoch 91: Val Loss 13115934.00000
Epoch 92: Val Loss 13124232.00000
Epoch 93: Val Loss 13112352.00000
Epoch 94: Val Loss 13175156.00000
Epoch 95: Val Loss 13130831.00000
Epoch 96: Val Loss 13122448.00000
Epoch 97: Val Loss 13083617.00000
Epoch 98: Val Loss 13079349.00000
Epoch 99: Val Loss 13103414.00000
{'MSE - mean': 13057851.326793717, 'MSE - std': 87198.37776684159, 'R2 - mean': 0.4932089528142395, 'R2 - std': 0.0017057434132534074} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 27821042.00000
Epoch 1: Val Loss 25868620.00000
Epoch 2: Val Loss 25743442.00000
Epoch 3: Val Loss 25523778.00000
Epoch 4: Val Loss 25249878.00000
Epoch 5: Val Loss 24897206.00000
Epoch 6: Val Loss 24422530.00000
Epoch 7: Val Loss 23855444.00000
Epoch 8: Val Loss 23077828.00000
Epoch 9: Val Loss 22102340.00000
Epoch 10: Val Loss 21010724.00000
Epoch 11: Val Loss 19816734.00000
Epoch 12: Val Loss 18801224.00000
Epoch 13: Val Loss 17966182.00000
Epoch 14: Val Loss 17448398.00000
Epoch 15: Val Loss 16998048.00000
Epoch 16: Val Loss 16597062.00000
Epoch 17: Val Loss 16219817.00000
Epoch 18: Val Loss 15912568.00000
Epoch 19: Val Loss 15646826.00000
Epoch 20: Val Loss 15391177.00000
Epoch 21: Val Loss 15116414.00000
Epoch 22: Val Loss 14933946.00000
Epoch 23: Val Loss 14753543.00000
Epoch 24: Val Loss 14600164.00000
Epoch 25: Val Loss 14469834.00000
Epoch 26: Val Loss 14343096.00000
Epoch 27: Val Loss 14253217.00000
Epoch 28: Val Loss 14147257.00000
Epoch 29: Val Loss 14044640.00000
Epoch 30: Val Loss 13970298.00000
Epoch 31: Val Loss 13893582.00000
Epoch 32: Val Loss 13846440.00000
Epoch 33: Val Loss 13749083.00000
Epoch 34: Val Loss 13696750.00000
Epoch 35: Val Loss 13646904.00000
Epoch 36: Val Loss 13599457.00000
Epoch 37: Val Loss 13543873.00000
Epoch 38: Val Loss 13523997.00000
Epoch 39: Val Loss 13447211.00000
Epoch 40: Val Loss 13424946.00000
Epoch 41: Val Loss 13396272.00000
Epoch 42: Val Loss 13351681.00000
Epoch 43: Val Loss 13316731.00000
Epoch 44: Val Loss 13312450.00000
Epoch 45: Val Loss 13339343.00000
Epoch 46: Val Loss 13264189.00000
Epoch 47: Val Loss 13278299.00000
Epoch 48: Val Loss 13291821.00000
Epoch 49: Val Loss 13240839.00000
Epoch 50: Val Loss 13223700.00000
Epoch 51: Val Loss 13213292.00000
Epoch 52: Val Loss 13224561.00000
Epoch 53: Val Loss 13198101.00000
Epoch 54: Val Loss 13226670.00000
Epoch 55: Val Loss 13199294.00000
Epoch 56: Val Loss 13152272.00000
Epoch 57: Val Loss 13183588.00000
Epoch 58: Val Loss 13169090.00000
Epoch 59: Val Loss 13144035.00000
Epoch 60: Val Loss 13148138.00000
Epoch 61: Val Loss 13158727.00000
Epoch 62: Val Loss 13138223.00000
Epoch 63: Val Loss 13131052.00000
Epoch 64: Val Loss 13140691.00000
Epoch 65: Val Loss 13114447.00000
Epoch 66: Val Loss 13118760.00000
Epoch 67: Val Loss 13125219.00000
Epoch 68: Val Loss 13128101.00000
Epoch 69: Val Loss 13123751.00000
Epoch 70: Val Loss 13115834.00000
Epoch 71: Val Loss 13247409.00000
Epoch 72: Val Loss 13120136.00000
Epoch 73: Val Loss 13096700.00000
Epoch 74: Val Loss 13111281.00000
Epoch 75: Val Loss 13105355.00000
Epoch 76: Val Loss 13110171.00000
Epoch 77: Val Loss 13072654.00000
Epoch 78: Val Loss 13107666.00000
Epoch 79: Val Loss 13086726.00000
Epoch 80: Val Loss 13091154.00000
Epoch 81: Val Loss 13080142.00000
Epoch 82: Val Loss 13128980.00000
Epoch 83: Val Loss 13055780.00000
Epoch 84: Val Loss 13081238.00000
Epoch 85: Val Loss 13092134.00000
Epoch 86: Val Loss 13084502.00000
Epoch 87: Val Loss 13057382.00000
Epoch 88: Val Loss 13075674.00000
Epoch 89: Val Loss 13096145.00000
Epoch 90: Val Loss 13085102.00000
Epoch 91: Val Loss 13048945.00000
Epoch 92: Val Loss 13059687.00000
Epoch 93: Val Loss 13055086.00000
Epoch 94: Val Loss 13067203.00000
Epoch 95: Val Loss 13059460.00000
Epoch 96: Val Loss 13047664.00000
Epoch 97: Val Loss 13047314.00000
Epoch 98: Val Loss 13055632.00000
Epoch 99: Val Loss 13056552.00000
{'MSE - mean': 13056080.537716288, 'MSE - std': 78072.96871492732, 'R2 - mean': 0.49449850718047517, 'R2 - std': 0.0029965730972888504} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 13056080.537716288 and parameters: {'dim': 128, 'depth': 6, 'heads': 8, 'weight_decay': -4, 'learning_rate': -4, 'dropout': 0}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161650992.00000
Epoch 1: Val Loss 160137024.00000
Epoch 2: Val Loss 154550000.00000
Epoch 3: Val Loss 139746800.00000
Epoch 4: Val Loss 109538112.00000
Epoch 5: Val Loss 66568528.00000
Epoch 6: Val Loss 32704320.00000
Epoch 7: Val Loss 26547182.00000
Epoch 8: Val Loss 26487014.00000
Epoch 9: Val Loss 26364592.00000
Epoch 10: Val Loss 26284880.00000
Epoch 11: Val Loss 26250190.00000
Epoch 12: Val Loss 26389486.00000
Epoch 13: Val Loss 26123958.00000
Epoch 14: Val Loss 26345200.00000
Epoch 15: Val Loss 26249456.00000
Epoch 16: Val Loss 26169370.00000
Epoch 17: Val Loss 26172968.00000
Epoch 18: Val Loss 26035858.00000
Epoch 19: Val Loss 26075200.00000
Epoch 20: Val Loss 26107594.00000
Epoch 21: Val Loss 26019596.00000
Epoch 22: Val Loss 25991760.00000
Epoch 23: Val Loss 26013536.00000
Epoch 24: Val Loss 25879314.00000
Epoch 25: Val Loss 25870284.00000
Epoch 26: Val Loss 26022192.00000
Epoch 27: Val Loss 25836124.00000
Epoch 28: Val Loss 25811930.00000
Epoch 29: Val Loss 25911912.00000
Epoch 30: Val Loss 25820990.00000
Epoch 31: Val Loss 25792558.00000
Epoch 32: Val Loss 25695298.00000
Epoch 33: Val Loss 25779240.00000
Epoch 34: Val Loss 25646888.00000
Epoch 35: Val Loss 25793322.00000
Epoch 36: Val Loss 25657610.00000
Epoch 37: Val Loss 25713260.00000
Epoch 38: Val Loss 25559910.00000
Epoch 39: Val Loss 25568370.00000
Epoch 40: Val Loss 25597272.00000
Epoch 41: Val Loss 25534690.00000
Epoch 42: Val Loss 25556876.00000
Epoch 43: Val Loss 25527156.00000
Epoch 44: Val Loss 25542316.00000
Epoch 45: Val Loss 25370098.00000
Epoch 46: Val Loss 25424770.00000
Epoch 47: Val Loss 25341654.00000
Epoch 48: Val Loss 25361364.00000
Epoch 49: Val Loss 25344446.00000
Epoch 50: Val Loss 25236742.00000
Epoch 51: Val Loss 25236972.00000
Epoch 52: Val Loss 25178852.00000
Epoch 53: Val Loss 25200150.00000
Epoch 54: Val Loss 25158356.00000
Epoch 55: Val Loss 25099286.00000
Epoch 56: Val Loss 25136282.00000
Epoch 57: Val Loss 25068684.00000
Epoch 58: Val Loss 25051960.00000
Epoch 59: Val Loss 24967510.00000
Epoch 60: Val Loss 24976220.00000
Epoch 61: Val Loss 24966876.00000
Epoch 62: Val Loss 24944948.00000
Epoch 63: Val Loss 24786570.00000
Epoch 64: Val Loss 24775784.00000
Epoch 65: Val Loss 24729786.00000
Epoch 66: Val Loss 24682936.00000
Epoch 67: Val Loss 24664880.00000
Epoch 68: Val Loss 24658118.00000
Epoch 69: Val Loss 24631974.00000
Epoch 70: Val Loss 24604824.00000
Epoch 71: Val Loss 24545234.00000
Epoch 72: Val Loss 24501918.00000
Epoch 73: Val Loss 24399866.00000
Epoch 74: Val Loss 24346838.00000
Epoch 75: Val Loss 24320992.00000
Epoch 76: Val Loss 24240430.00000
Epoch 77: Val Loss 24231078.00000
Epoch 78: Val Loss 24163054.00000
Epoch 79: Val Loss 24124550.00000
Epoch 80: Val Loss 24097354.00000
Epoch 81: Val Loss 23973592.00000
Epoch 82: Val Loss 23926518.00000
Epoch 83: Val Loss 23871030.00000
Epoch 84: Val Loss 23850518.00000
Epoch 85: Val Loss 23820780.00000
Epoch 86: Val Loss 23704756.00000
Epoch 87: Val Loss 23736538.00000
Epoch 88: Val Loss 23560712.00000
Epoch 89: Val Loss 23609112.00000
Epoch 90: Val Loss 23450580.00000
Epoch 91: Val Loss 23397316.00000
Epoch 92: Val Loss 23363168.00000
Epoch 93: Val Loss 23297820.00000
Epoch 94: Val Loss 23196262.00000
Epoch 95: Val Loss 23085416.00000
Epoch 96: Val Loss 23013238.00000
Epoch 97: Val Loss 22901806.00000
Epoch 98: Val Loss 22898724.00000
Epoch 99: Val Loss 22821792.00000
{'MSE - mean': 22382860.476079263, 'MSE - std': 0.0, 'R2 - mean': 0.12380134882507843, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162341216.00000
Epoch 1: Val Loss 161635888.00000
Epoch 2: Val Loss 158271968.00000
Epoch 3: Val Loss 148513568.00000
Epoch 4: Val Loss 127188880.00000
Epoch 5: Val Loss 91447656.00000
Epoch 6: Val Loss 50621508.00000
Epoch 7: Val Loss 28574930.00000
Epoch 8: Val Loss 26844328.00000
Epoch 9: Val Loss 26868504.00000
Epoch 10: Val Loss 26823108.00000
Epoch 11: Val Loss 26711778.00000
Epoch 12: Val Loss 26699380.00000
Epoch 13: Val Loss 26702126.00000
Epoch 14: Val Loss 26559886.00000
Epoch 15: Val Loss 26560804.00000
Epoch 16: Val Loss 26494754.00000
Epoch 17: Val Loss 26579638.00000
Epoch 18: Val Loss 26510316.00000
Epoch 19: Val Loss 26460860.00000
Epoch 20: Val Loss 26489986.00000
Epoch 21: Val Loss 26439310.00000
Epoch 22: Val Loss 26389578.00000
Epoch 23: Val Loss 26440274.00000
Epoch 24: Val Loss 26434358.00000
Epoch 25: Val Loss 26441604.00000
Epoch 26: Val Loss 26359554.00000
Epoch 27: Val Loss 26415454.00000
Epoch 28: Val Loss 26328110.00000
Epoch 29: Val Loss 26290190.00000
Epoch 30: Val Loss 26299318.00000
Epoch 31: Val Loss 26296044.00000
Epoch 32: Val Loss 26233596.00000
Epoch 33: Val Loss 26241002.00000
Epoch 34: Val Loss 26247730.00000
Epoch 35: Val Loss 26143446.00000
Epoch 36: Val Loss 26177940.00000
Epoch 37: Val Loss 26130848.00000
Epoch 38: Val Loss 26087058.00000
Epoch 39: Val Loss 26105884.00000
Epoch 40: Val Loss 26057028.00000
Epoch 41: Val Loss 26084864.00000
Epoch 42: Val Loss 26089046.00000
Epoch 43: Val Loss 25968470.00000
Epoch 44: Val Loss 26030592.00000
Epoch 45: Val Loss 25975894.00000
Epoch 46: Val Loss 26035882.00000
Epoch 47: Val Loss 25884234.00000
Epoch 48: Val Loss 25870178.00000
Epoch 49: Val Loss 25859886.00000
Epoch 50: Val Loss 25954224.00000
Epoch 51: Val Loss 25909358.00000
Epoch 52: Val Loss 25782148.00000
Epoch 53: Val Loss 25841428.00000
Epoch 54: Val Loss 25797654.00000
Epoch 55: Val Loss 25867258.00000
Epoch 56: Val Loss 25777082.00000
Epoch 57: Val Loss 25705504.00000
Epoch 58: Val Loss 25728694.00000
Epoch 59: Val Loss 25741844.00000
Epoch 60: Val Loss 25634598.00000
Epoch 61: Val Loss 25681266.00000
Epoch 62: Val Loss 25588646.00000
Epoch 63: Val Loss 25592556.00000
Epoch 64: Val Loss 25620022.00000
Epoch 65: Val Loss 25561920.00000
Epoch 66: Val Loss 25548886.00000
Epoch 67: Val Loss 25520474.00000
Epoch 68: Val Loss 25377412.00000
Epoch 69: Val Loss 25418798.00000
Epoch 70: Val Loss 25333494.00000
Epoch 71: Val Loss 25302218.00000
Epoch 72: Val Loss 25320802.00000
Epoch 73: Val Loss 25314806.00000
Epoch 74: Val Loss 25297780.00000
Epoch 75: Val Loss 25276482.00000
Epoch 76: Val Loss 25244556.00000
Epoch 77: Val Loss 25177242.00000
Epoch 78: Val Loss 25183450.00000
Epoch 79: Val Loss 25108704.00000
Epoch 80: Val Loss 25149144.00000
Epoch 81: Val Loss 25090640.00000
Epoch 82: Val Loss 25026274.00000
Epoch 83: Val Loss 25044398.00000
Epoch 84: Val Loss 24874662.00000
Epoch 85: Val Loss 24882398.00000
Epoch 86: Val Loss 24794956.00000
Epoch 87: Val Loss 24795760.00000
Epoch 88: Val Loss 24776006.00000
Epoch 89: Val Loss 24736198.00000
Epoch 90: Val Loss 24774904.00000
Epoch 91: Val Loss 24750434.00000
Epoch 92: Val Loss 24585212.00000
Epoch 93: Val Loss 24540926.00000
Epoch 94: Val Loss 24480542.00000
Epoch 95: Val Loss 24507304.00000
Epoch 96: Val Loss 24400260.00000
Epoch 97: Val Loss 24405662.00000
Epoch 98: Val Loss 24399208.00000
Epoch 99: Val Loss 24334240.00000
{'MSE - mean': 23101584.209901173, 'MSE - std': 718723.7338219099, 'R2 - mean': 0.1011164830883326, 'R2 - std': 0.02268486573674583} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162225664.00000
Epoch 1: Val Loss 161138432.00000
Epoch 2: Val Loss 156823216.00000
Epoch 3: Val Loss 143750848.00000
Epoch 4: Val Loss 116348776.00000
Epoch 5: Val Loss 74730880.00000
Epoch 6: Val Loss 37144448.00000
Epoch 7: Val Loss 26812052.00000
Epoch 8: Val Loss 26607126.00000
Epoch 9: Val Loss 26602388.00000
Epoch 10: Val Loss 26725632.00000
Epoch 11: Val Loss 26607908.00000
Epoch 12: Val Loss 26555228.00000
Epoch 13: Val Loss 26700554.00000
Epoch 14: Val Loss 26464538.00000
Epoch 15: Val Loss 26500998.00000
Epoch 16: Val Loss 26398782.00000
Epoch 17: Val Loss 26537008.00000
Epoch 18: Val Loss 26463842.00000
Epoch 19: Val Loss 26477492.00000
Epoch 20: Val Loss 26464060.00000
Epoch 21: Val Loss 26369140.00000
Epoch 22: Val Loss 26355704.00000
Epoch 23: Val Loss 26357254.00000
Epoch 24: Val Loss 26280974.00000
Epoch 25: Val Loss 26232384.00000
Epoch 26: Val Loss 26295190.00000
Epoch 27: Val Loss 26326086.00000
Epoch 28: Val Loss 26306812.00000
Epoch 29: Val Loss 26191470.00000
Epoch 30: Val Loss 26179826.00000
Epoch 31: Val Loss 26128630.00000
Epoch 32: Val Loss 26201052.00000
Epoch 33: Val Loss 26173558.00000
Epoch 34: Val Loss 26166966.00000
Epoch 35: Val Loss 26100546.00000
Epoch 36: Val Loss 26117450.00000
Epoch 37: Val Loss 26117738.00000
Epoch 38: Val Loss 26073760.00000
Epoch 39: Val Loss 26054928.00000
Epoch 40: Val Loss 25987898.00000
Epoch 41: Val Loss 25982632.00000
Epoch 42: Val Loss 26090450.00000
Epoch 43: Val Loss 25956584.00000
Epoch 44: Val Loss 25919484.00000
Epoch 45: Val Loss 25983742.00000
Epoch 46: Val Loss 25959350.00000
Epoch 47: Val Loss 25827826.00000
Epoch 48: Val Loss 25813684.00000
Epoch 49: Val Loss 25836060.00000
Epoch 50: Val Loss 25807006.00000
Epoch 51: Val Loss 25813742.00000
Epoch 52: Val Loss 25854648.00000
Epoch 53: Val Loss 25751494.00000
Epoch 54: Val Loss 25780342.00000
Epoch 55: Val Loss 25735814.00000
Epoch 56: Val Loss 25700298.00000
Epoch 57: Val Loss 25694160.00000
Epoch 58: Val Loss 25745610.00000
Epoch 59: Val Loss 25623366.00000
Epoch 60: Val Loss 25580710.00000
Epoch 61: Val Loss 25567424.00000
Epoch 62: Val Loss 25609250.00000
Epoch 63: Val Loss 25507512.00000
Epoch 64: Val Loss 25540248.00000
Epoch 65: Val Loss 25427066.00000
Epoch 66: Val Loss 25469420.00000
Epoch 67: Val Loss 25440168.00000
Epoch 68: Val Loss 25427536.00000
Epoch 69: Val Loss 25357614.00000
Epoch 70: Val Loss 25345436.00000
Epoch 71: Val Loss 25283882.00000
Epoch 72: Val Loss 25337194.00000
Epoch 73: Val Loss 25238864.00000
Epoch 74: Val Loss 25226760.00000
Epoch 75: Val Loss 25248140.00000
Epoch 76: Val Loss 25155302.00000
Epoch 77: Val Loss 25154804.00000
Epoch 78: Val Loss 25154024.00000
Epoch 79: Val Loss 25060800.00000
Epoch 80: Val Loss 25115130.00000
Epoch 81: Val Loss 25031746.00000
Epoch 82: Val Loss 24918870.00000
Epoch 83: Val Loss 24886572.00000
Epoch 84: Val Loss 24847400.00000
Epoch 85: Val Loss 24896954.00000
Epoch 86: Val Loss 24825684.00000
Epoch 87: Val Loss 24844098.00000
Epoch 88: Val Loss 24717312.00000
Epoch 89: Val Loss 24630240.00000
Epoch 90: Val Loss 24680026.00000
Epoch 91: Val Loss 24584408.00000
Epoch 92: Val Loss 24584092.00000
Epoch 93: Val Loss 24597570.00000
Epoch 94: Val Loss 24525464.00000
Epoch 95: Val Loss 24459228.00000
Epoch 96: Val Loss 24360088.00000
Epoch 97: Val Loss 24379012.00000
Epoch 98: Val Loss 24298942.00000
Epoch 99: Val Loss 24283324.00000
{'MSE - mean': 23323414.83147219, 'MSE - std': 665427.3211296424, 'R2 - mean': 0.09369311232610807, 'R2 - std': 0.021290411518091655} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 160318464.00000
Epoch 1: Val Loss 158657440.00000
Epoch 2: Val Loss 153390640.00000
Epoch 3: Val Loss 139565744.00000
Epoch 4: Val Loss 113129680.00000
Epoch 5: Val Loss 75459544.00000
Epoch 6: Val Loss 42624868.00000
Epoch 7: Val Loss 34172820.00000
Epoch 8: Val Loss 33817688.00000
Epoch 9: Val Loss 33784172.00000
Epoch 10: Val Loss 33475628.00000
Epoch 11: Val Loss 33220886.00000
Epoch 12: Val Loss 32775798.00000
Epoch 13: Val Loss 32234150.00000
Epoch 14: Val Loss 32099274.00000
Epoch 15: Val Loss 31382598.00000
Epoch 16: Val Loss 30981248.00000
Epoch 17: Val Loss 30378922.00000
Epoch 18: Val Loss 29671542.00000
Epoch 19: Val Loss 29214228.00000
Epoch 20: Val Loss 28489802.00000
Epoch 21: Val Loss 27864196.00000
Epoch 22: Val Loss 27636574.00000
Epoch 23: Val Loss 27121168.00000
Epoch 24: Val Loss 27082158.00000
Epoch 25: Val Loss 27047124.00000
Epoch 26: Val Loss 26921920.00000
Epoch 27: Val Loss 26962646.00000
Epoch 28: Val Loss 26916798.00000
Epoch 29: Val Loss 26801554.00000
Epoch 30: Val Loss 26777774.00000
Epoch 31: Val Loss 26836712.00000
Epoch 32: Val Loss 26696288.00000
Epoch 33: Val Loss 26670632.00000
Epoch 34: Val Loss 26683672.00000
Epoch 35: Val Loss 26507612.00000
Epoch 36: Val Loss 26590012.00000
Epoch 37: Val Loss 26558348.00000
Epoch 38: Val Loss 26452252.00000
Epoch 39: Val Loss 26439230.00000
Epoch 40: Val Loss 26381732.00000
Epoch 41: Val Loss 26279662.00000
Epoch 42: Val Loss 26416150.00000
Epoch 43: Val Loss 26257300.00000
Epoch 44: Val Loss 26318736.00000
Epoch 45: Val Loss 26313638.00000
Epoch 46: Val Loss 26261884.00000
Epoch 47: Val Loss 26254548.00000
Epoch 48: Val Loss 26209052.00000
Epoch 49: Val Loss 26124130.00000
Epoch 50: Val Loss 26034996.00000
Epoch 51: Val Loss 26070678.00000
Epoch 52: Val Loss 26042938.00000
Epoch 53: Val Loss 26002390.00000
Epoch 54: Val Loss 26026290.00000
Epoch 55: Val Loss 25962350.00000
Epoch 56: Val Loss 25893050.00000
Epoch 57: Val Loss 25938936.00000
Epoch 58: Val Loss 25810774.00000
Epoch 59: Val Loss 25771482.00000
Epoch 60: Val Loss 25813054.00000
Epoch 61: Val Loss 25680356.00000
Epoch 62: Val Loss 25706260.00000
Epoch 63: Val Loss 25656510.00000
Epoch 64: Val Loss 25631768.00000
Epoch 65: Val Loss 25609830.00000
Epoch 66: Val Loss 25514456.00000
Epoch 67: Val Loss 25457272.00000
Epoch 68: Val Loss 25525592.00000
Epoch 69: Val Loss 25396704.00000
Epoch 70: Val Loss 25488632.00000
Epoch 71: Val Loss 25405566.00000
Epoch 72: Val Loss 25375216.00000
Epoch 73: Val Loss 25301218.00000
Epoch 74: Val Loss 25173286.00000
Epoch 75: Val Loss 25161970.00000
Epoch 76: Val Loss 25275058.00000
Epoch 77: Val Loss 25147154.00000
Epoch 78: Val Loss 25060620.00000
Epoch 79: Val Loss 25052544.00000
Epoch 80: Val Loss 25024508.00000
Epoch 81: Val Loss 24903504.00000
Epoch 82: Val Loss 25027476.00000
Epoch 83: Val Loss 24923752.00000
Epoch 84: Val Loss 24817444.00000
Epoch 85: Val Loss 24749236.00000
Epoch 86: Val Loss 24746468.00000
Epoch 87: Val Loss 24723050.00000
Epoch 88: Val Loss 24578130.00000
Epoch 89: Val Loss 24498242.00000
Epoch 90: Val Loss 24540780.00000
Epoch 91: Val Loss 24460530.00000
Epoch 92: Val Loss 24420938.00000
Epoch 93: Val Loss 24318476.00000
Epoch 94: Val Loss 24331014.00000
Epoch 95: Val Loss 24218724.00000
Epoch 96: Val Loss 24232714.00000
Epoch 97: Val Loss 24049108.00000
Epoch 98: Val Loss 24052356.00000
Epoch 99: Val Loss 23972194.00000
{'MSE - mean': 23350414.607543338, 'MSE - std': 578171.344415309, 'R2 - mean': 0.09381952577888913, 'R2 - std': 0.01843933724515064} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161327792.00000
Epoch 1: Val Loss 158927728.00000
Epoch 2: Val Loss 150133776.00000
Epoch 3: Val Loss 128934864.00000
Epoch 4: Val Loss 91802608.00000
Epoch 5: Val Loss 49340652.00000
Epoch 6: Val Loss 30133704.00000
Epoch 7: Val Loss 29184710.00000
Epoch 8: Val Loss 29250364.00000
Epoch 9: Val Loss 28888662.00000
Epoch 10: Val Loss 29055684.00000
Epoch 11: Val Loss 28931580.00000
Epoch 12: Val Loss 28770296.00000
Epoch 13: Val Loss 28632532.00000
Epoch 14: Val Loss 28550670.00000
Epoch 15: Val Loss 28675398.00000
Epoch 16: Val Loss 28652034.00000
Epoch 17: Val Loss 28262536.00000
Epoch 18: Val Loss 28390814.00000
Epoch 19: Val Loss 28202772.00000
Epoch 20: Val Loss 28194628.00000
Epoch 21: Val Loss 28110446.00000
Epoch 22: Val Loss 28013786.00000
Epoch 23: Val Loss 27860258.00000
Epoch 24: Val Loss 27852500.00000
Epoch 25: Val Loss 27782322.00000
Epoch 26: Val Loss 27676760.00000
Epoch 27: Val Loss 27495840.00000
Epoch 28: Val Loss 27327190.00000
Epoch 29: Val Loss 27250810.00000
Epoch 30: Val Loss 27381118.00000
Epoch 31: Val Loss 27186246.00000
Epoch 32: Val Loss 26956146.00000
Epoch 33: Val Loss 26816302.00000
Epoch 34: Val Loss 27009554.00000
Epoch 35: Val Loss 26691836.00000
Epoch 36: Val Loss 26729546.00000
Epoch 37: Val Loss 26733392.00000
Epoch 38: Val Loss 26618646.00000
Epoch 39: Val Loss 26527292.00000
Epoch 40: Val Loss 26494504.00000
Epoch 41: Val Loss 26477756.00000
Epoch 42: Val Loss 26390656.00000
Epoch 43: Val Loss 26306786.00000
Epoch 44: Val Loss 26253220.00000
Epoch 45: Val Loss 26117434.00000
Epoch 46: Val Loss 26160636.00000
Epoch 47: Val Loss 26095808.00000
Epoch 48: Val Loss 26107744.00000
Epoch 49: Val Loss 25912946.00000
Epoch 50: Val Loss 25936062.00000
Epoch 51: Val Loss 25804224.00000
Epoch 52: Val Loss 25913082.00000
Epoch 53: Val Loss 25775810.00000
Epoch 54: Val Loss 25738838.00000
Epoch 55: Val Loss 25741448.00000
Epoch 56: Val Loss 25737826.00000
Epoch 57: Val Loss 25634132.00000
Epoch 58: Val Loss 25616410.00000
Epoch 59: Val Loss 25624432.00000
Epoch 60: Val Loss 25445738.00000
Epoch 61: Val Loss 25366872.00000
Epoch 62: Val Loss 25331406.00000
Epoch 63: Val Loss 25327972.00000
Epoch 64: Val Loss 25291920.00000
Epoch 65: Val Loss 25261852.00000
Epoch 66: Val Loss 25120472.00000
Epoch 67: Val Loss 25083436.00000
Epoch 68: Val Loss 25041006.00000
Epoch 69: Val Loss 24908706.00000
Epoch 70: Val Loss 24831036.00000
Epoch 71: Val Loss 24891514.00000
Epoch 72: Val Loss 24884398.00000
Epoch 73: Val Loss 24738452.00000
Epoch 74: Val Loss 24722392.00000
Epoch 75: Val Loss 24500104.00000
Epoch 76: Val Loss 24523714.00000
Epoch 77: Val Loss 24478984.00000
Epoch 78: Val Loss 24441834.00000
Epoch 79: Val Loss 24308386.00000
Epoch 80: Val Loss 24189728.00000
Epoch 81: Val Loss 24120640.00000
Epoch 82: Val Loss 24062962.00000
Epoch 83: Val Loss 23962986.00000
Epoch 84: Val Loss 23776642.00000
Epoch 85: Val Loss 23828312.00000
Epoch 86: Val Loss 23714480.00000
Epoch 87: Val Loss 23608940.00000
Epoch 88: Val Loss 23552054.00000
Epoch 89: Val Loss 23471670.00000
Epoch 90: Val Loss 23343616.00000
Epoch 91: Val Loss 23245342.00000
Epoch 92: Val Loss 23176242.00000
Epoch 93: Val Loss 23062336.00000
Epoch 94: Val Loss 23025044.00000
Epoch 95: Val Loss 22879106.00000
Epoch 96: Val Loss 22837200.00000
Epoch 97: Val Loss 22605398.00000
Epoch 98: Val Loss 22585340.00000
Epoch 99: Val Loss 22404318.00000
{'MSE - mean': 23053198.683385815, 'MSE - std': 787892.698972165, 'R2 - mean': 0.10738491518928202, 'R2 - std': 0.031750377727748635} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 23053198.683385815 and parameters: {'dim': 256, 'depth': 12, 'heads': 2, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0.5}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161156592.00000
Epoch 1: Val Loss 157513040.00000
Epoch 2: Val Loss 144525936.00000
Epoch 3: Val Loss 116117208.00000
Epoch 4: Val Loss 72727592.00000
Epoch 5: Val Loss 34872764.00000
Epoch 6: Val Loss 25706672.00000
Epoch 7: Val Loss 25583124.00000
Epoch 8: Val Loss 25591598.00000
Epoch 9: Val Loss 25623798.00000
Epoch 10: Val Loss 25581888.00000
Epoch 11: Val Loss 25570190.00000
Epoch 12: Val Loss 25520568.00000
Epoch 13: Val Loss 25517244.00000
Epoch 14: Val Loss 25549294.00000
Epoch 15: Val Loss 25541684.00000
Epoch 16: Val Loss 25530382.00000
Epoch 17: Val Loss 25554478.00000
Epoch 18: Val Loss 25513130.00000
Epoch 19: Val Loss 25487042.00000
Epoch 20: Val Loss 25521484.00000
Epoch 21: Val Loss 25499688.00000
Epoch 22: Val Loss 25472120.00000
Epoch 23: Val Loss 25451514.00000
Epoch 24: Val Loss 25464774.00000
Epoch 25: Val Loss 25474286.00000
Epoch 26: Val Loss 25458702.00000
Epoch 27: Val Loss 25480816.00000
Epoch 28: Val Loss 25420170.00000
Epoch 29: Val Loss 25438278.00000
Epoch 30: Val Loss 25445212.00000
Epoch 31: Val Loss 25451580.00000
Epoch 32: Val Loss 25402204.00000
Epoch 33: Val Loss 25440784.00000
Epoch 34: Val Loss 25407830.00000
Epoch 35: Val Loss 25376400.00000
Epoch 36: Val Loss 25416014.00000
Epoch 37: Val Loss 25377304.00000
Epoch 38: Val Loss 25381416.00000
Epoch 39: Val Loss 25319750.00000
Epoch 40: Val Loss 25350590.00000
Epoch 41: Val Loss 25311230.00000
Epoch 42: Val Loss 25331180.00000
Epoch 43: Val Loss 25309028.00000
Epoch 44: Val Loss 25295042.00000
Epoch 45: Val Loss 25315150.00000
Epoch 46: Val Loss 25300266.00000
Epoch 47: Val Loss 25262082.00000
Epoch 48: Val Loss 25301002.00000
Epoch 49: Val Loss 25248626.00000
Epoch 50: Val Loss 25237262.00000
Epoch 51: Val Loss 25228564.00000
Epoch 52: Val Loss 25209120.00000
Epoch 53: Val Loss 25199784.00000
Epoch 54: Val Loss 25176588.00000
Epoch 55: Val Loss 25195968.00000
Epoch 56: Val Loss 25160120.00000
Epoch 57: Val Loss 25166126.00000
Epoch 58: Val Loss 25139018.00000
Epoch 59: Val Loss 25118250.00000
Epoch 60: Val Loss 25146260.00000
Epoch 61: Val Loss 25124336.00000
Epoch 62: Val Loss 25085616.00000
Epoch 63: Val Loss 25058220.00000
Epoch 64: Val Loss 25080094.00000
Epoch 65: Val Loss 25091122.00000
Epoch 66: Val Loss 25050494.00000
Epoch 67: Val Loss 25006932.00000
Epoch 68: Val Loss 25043184.00000
Epoch 69: Val Loss 24981402.00000
Epoch 70: Val Loss 24969218.00000
Epoch 71: Val Loss 24973108.00000
Epoch 72: Val Loss 24989908.00000
Epoch 73: Val Loss 24948770.00000
Epoch 74: Val Loss 24910352.00000
Epoch 75: Val Loss 24888664.00000
Epoch 76: Val Loss 24889606.00000
Epoch 77: Val Loss 24913824.00000
Epoch 78: Val Loss 24843036.00000
Epoch 79: Val Loss 24814528.00000
Epoch 80: Val Loss 24792360.00000
Epoch 81: Val Loss 24810650.00000
Epoch 82: Val Loss 24773818.00000
Epoch 83: Val Loss 24776870.00000
Epoch 84: Val Loss 24750508.00000
Epoch 85: Val Loss 24760256.00000
Epoch 86: Val Loss 24760752.00000
Epoch 87: Val Loss 24721714.00000
Epoch 88: Val Loss 24655976.00000
Epoch 89: Val Loss 24588126.00000
Epoch 90: Val Loss 24645942.00000
Epoch 91: Val Loss 24578696.00000
Epoch 92: Val Loss 24600290.00000
Epoch 93: Val Loss 24560738.00000
Epoch 94: Val Loss 24535124.00000
Epoch 95: Val Loss 24519214.00000
Epoch 96: Val Loss 24543696.00000
Epoch 97: Val Loss 24484028.00000
Epoch 98: Val Loss 24463602.00000
Epoch 99: Val Loss 24461692.00000
{'MSE - mean': 24355917.41171205, 'MSE - std': 0.0, 'R2 - mean': 0.04656413298573825, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162005664.00000
Epoch 1: Val Loss 158869008.00000
Epoch 2: Val Loss 146437952.00000
Epoch 3: Val Loss 118057944.00000
Epoch 4: Val Loss 73875784.00000
Epoch 5: Val Loss 35011272.00000
Epoch 6: Val Loss 25982616.00000
Epoch 7: Val Loss 25904636.00000
Epoch 8: Val Loss 25885468.00000
Epoch 9: Val Loss 25897764.00000
Epoch 10: Val Loss 25878662.00000
Epoch 11: Val Loss 25868072.00000
Epoch 12: Val Loss 25860342.00000
Epoch 13: Val Loss 25881654.00000
Epoch 14: Val Loss 25854550.00000
Epoch 15: Val Loss 25852364.00000
Epoch 16: Val Loss 25834890.00000
Epoch 17: Val Loss 25844328.00000
Epoch 18: Val Loss 25833850.00000
Epoch 19: Val Loss 25828302.00000
Epoch 20: Val Loss 25838462.00000
Epoch 21: Val Loss 25779062.00000
Epoch 22: Val Loss 25804464.00000
Epoch 23: Val Loss 25805674.00000
Epoch 24: Val Loss 25795488.00000
Epoch 25: Val Loss 25809732.00000
Epoch 26: Val Loss 25772290.00000
Epoch 27: Val Loss 25741466.00000
Epoch 28: Val Loss 25754140.00000
Epoch 29: Val Loss 25752578.00000
Epoch 30: Val Loss 25739938.00000
Epoch 31: Val Loss 25720868.00000
Epoch 32: Val Loss 25736740.00000
Epoch 33: Val Loss 25686318.00000
Epoch 34: Val Loss 25683954.00000
Epoch 35: Val Loss 25700240.00000
Epoch 36: Val Loss 25698208.00000
Epoch 37: Val Loss 25671838.00000
Epoch 38: Val Loss 25594324.00000
Epoch 39: Val Loss 25665524.00000
Epoch 40: Val Loss 25623782.00000
Epoch 41: Val Loss 25647032.00000
Epoch 42: Val Loss 25608592.00000
Epoch 43: Val Loss 25625398.00000
Epoch 44: Val Loss 25618404.00000
Epoch 45: Val Loss 25574716.00000
Epoch 46: Val Loss 25567876.00000
Epoch 47: Val Loss 25565084.00000
Epoch 48: Val Loss 25527716.00000
Epoch 49: Val Loss 25564578.00000
Epoch 50: Val Loss 25519496.00000
Epoch 51: Val Loss 25515090.00000
Epoch 52: Val Loss 25514854.00000
Epoch 53: Val Loss 25492592.00000
Epoch 54: Val Loss 25481214.00000
Epoch 55: Val Loss 25475904.00000
Epoch 56: Val Loss 25438436.00000
Epoch 57: Val Loss 25476536.00000
Epoch 58: Val Loss 25397824.00000
Epoch 59: Val Loss 25428078.00000
Epoch 60: Val Loss 25441566.00000
Epoch 61: Val Loss 25405998.00000
Epoch 62: Val Loss 25416548.00000
Epoch 63: Val Loss 25365100.00000
Epoch 64: Val Loss 25338984.00000
Epoch 65: Val Loss 25351530.00000
Epoch 66: Val Loss 25313396.00000
Epoch 67: Val Loss 25344460.00000
Epoch 68: Val Loss 25319806.00000
Epoch 69: Val Loss 25265782.00000
Epoch 70: Val Loss 25324268.00000
Epoch 71: Val Loss 25275128.00000
Epoch 72: Val Loss 25209454.00000
Epoch 73: Val Loss 25179986.00000
Epoch 74: Val Loss 25221086.00000
Epoch 75: Val Loss 25161812.00000
Epoch 76: Val Loss 25190910.00000
Epoch 77: Val Loss 25173880.00000
Epoch 78: Val Loss 25115596.00000
Epoch 79: Val Loss 25090544.00000
Epoch 80: Val Loss 25092584.00000
Epoch 81: Val Loss 25050700.00000
Epoch 82: Val Loss 25063054.00000
Epoch 83: Val Loss 25040362.00000
Epoch 84: Val Loss 24996386.00000
Epoch 85: Val Loss 25014404.00000
Epoch 86: Val Loss 24991760.00000
Epoch 87: Val Loss 24951278.00000
Epoch 88: Val Loss 24943074.00000
Epoch 89: Val Loss 24869550.00000
Epoch 90: Val Loss 24870832.00000
Epoch 91: Val Loss 24888028.00000
Epoch 92: Val Loss 24827364.00000
Epoch 93: Val Loss 24818088.00000
Epoch 94: Val Loss 24793646.00000
Epoch 95: Val Loss 24756444.00000
Epoch 96: Val Loss 24748722.00000
Epoch 97: Val Loss 24678740.00000
Epoch 98: Val Loss 24672310.00000
Epoch 99: Val Loss 24679796.00000
{'MSE - mean': 24488443.187902637, 'MSE - std': 132525.7761905864, 'R2 - mean': 0.04700974874864755, 'R2 - std': 0.0004456157629093016} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 162072928.00000
Epoch 1: Val Loss 159421456.00000
Epoch 2: Val Loss 149800128.00000
Epoch 3: Val Loss 127345136.00000
Epoch 4: Val Loss 90668800.00000
Epoch 5: Val Loss 52918744.00000
Epoch 6: Val Loss 35801520.00000
Epoch 7: Val Loss 29232000.00000
Epoch 8: Val Loss 26256650.00000
Epoch 9: Val Loss 25875906.00000
Epoch 10: Val Loss 25831316.00000
Epoch 11: Val Loss 25859572.00000
Epoch 12: Val Loss 25821534.00000
Epoch 13: Val Loss 25823086.00000
Epoch 14: Val Loss 25878370.00000
Epoch 15: Val Loss 25878402.00000
Epoch 16: Val Loss 25812244.00000
Epoch 17: Val Loss 25816672.00000
Epoch 18: Val Loss 25850874.00000
Epoch 19: Val Loss 25814424.00000
Epoch 20: Val Loss 25835920.00000
Epoch 21: Val Loss 25820270.00000
Epoch 22: Val Loss 25793784.00000
Epoch 23: Val Loss 25775698.00000
Epoch 24: Val Loss 25753812.00000
Epoch 25: Val Loss 25726322.00000
Epoch 26: Val Loss 25800636.00000
Epoch 27: Val Loss 25780122.00000
Epoch 28: Val Loss 25729538.00000
Epoch 29: Val Loss 25707634.00000
Epoch 30: Val Loss 25735480.00000
Epoch 31: Val Loss 25689476.00000
Epoch 32: Val Loss 25713208.00000
Epoch 33: Val Loss 25661358.00000
Epoch 34: Val Loss 25675820.00000
Epoch 35: Val Loss 25718880.00000
Epoch 36: Val Loss 25660996.00000
Epoch 37: Val Loss 25630134.00000
Epoch 38: Val Loss 25656166.00000
Epoch 39: Val Loss 25630024.00000
Epoch 40: Val Loss 25586770.00000
Epoch 41: Val Loss 25592846.00000
Epoch 42: Val Loss 25577722.00000
Epoch 43: Val Loss 25594184.00000
Epoch 44: Val Loss 25525442.00000
Epoch 45: Val Loss 25545638.00000
Epoch 46: Val Loss 25543216.00000
Epoch 47: Val Loss 25512050.00000
Epoch 48: Val Loss 25483186.00000
Epoch 49: Val Loss 25507092.00000
Epoch 50: Val Loss 25500052.00000
Epoch 51: Val Loss 25454082.00000
Epoch 52: Val Loss 25397380.00000
Epoch 53: Val Loss 25424346.00000
Epoch 54: Val Loss 25393596.00000
Epoch 55: Val Loss 25366964.00000
Epoch 56: Val Loss 25381576.00000
Epoch 57: Val Loss 25334134.00000
Epoch 58: Val Loss 25303952.00000
Epoch 59: Val Loss 25308030.00000
Epoch 60: Val Loss 25313026.00000
Epoch 61: Val Loss 25266788.00000
Epoch 62: Val Loss 25247700.00000
Epoch 63: Val Loss 25241586.00000
Epoch 64: Val Loss 25183908.00000
Epoch 65: Val Loss 25187062.00000
Epoch 66: Val Loss 25159982.00000
Epoch 67: Val Loss 25154894.00000
Epoch 68: Val Loss 25079962.00000
Epoch 69: Val Loss 25101214.00000
Epoch 70: Val Loss 25070282.00000
Epoch 71: Val Loss 25013378.00000
Epoch 72: Val Loss 24988942.00000
Epoch 73: Val Loss 24947608.00000
Epoch 74: Val Loss 24961872.00000
Epoch 75: Val Loss 24883256.00000
Epoch 76: Val Loss 24876810.00000
Epoch 77: Val Loss 24818046.00000
Epoch 78: Val Loss 24765116.00000
Epoch 79: Val Loss 24726524.00000
Epoch 80: Val Loss 24734802.00000
Epoch 81: Val Loss 24679850.00000
Epoch 82: Val Loss 24677040.00000
Epoch 83: Val Loss 24647440.00000
Epoch 84: Val Loss 24597616.00000
Epoch 85: Val Loss 24556958.00000
Epoch 86: Val Loss 24509114.00000
Epoch 87: Val Loss 24489862.00000
Epoch 88: Val Loss 24431832.00000
Epoch 89: Val Loss 24407940.00000
Epoch 90: Val Loss 24334284.00000
Epoch 91: Val Loss 24284714.00000
Epoch 92: Val Loss 24290446.00000
Epoch 93: Val Loss 24195582.00000
Epoch 94: Val Loss 24155906.00000
Epoch 95: Val Loss 24125482.00000
Epoch 96: Val Loss 24072520.00000
Epoch 97: Val Loss 23964568.00000
Epoch 98: Val Loss 23979134.00000
Epoch 99: Val Loss 23887544.00000
{'MSE - mean': 24264410.37437795, 'MSE - std': 334798.63197876187, 'R2 - mean': 0.05698544509125558, 'R2 - std': 0.014112456104872342} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 160024448.00000
Epoch 1: Val Loss 155851408.00000
Epoch 2: Val Loss 142862848.00000
Epoch 3: Val Loss 115035080.00000
Epoch 4: Val Loss 72939864.00000
Epoch 5: Val Loss 35662588.00000
Epoch 6: Val Loss 25976612.00000
Epoch 7: Val Loss 25892392.00000
Epoch 8: Val Loss 25899738.00000
Epoch 9: Val Loss 25865898.00000
Epoch 10: Val Loss 25883992.00000
Epoch 11: Val Loss 25852490.00000
Epoch 12: Val Loss 25869384.00000
Epoch 13: Val Loss 25852430.00000
Epoch 14: Val Loss 25837850.00000
Epoch 15: Val Loss 25831968.00000
Epoch 16: Val Loss 25804286.00000
Epoch 17: Val Loss 25810036.00000
Epoch 18: Val Loss 25829442.00000
Epoch 19: Val Loss 25809258.00000
Epoch 20: Val Loss 25803006.00000
Epoch 21: Val Loss 25772288.00000
Epoch 22: Val Loss 25759490.00000
Epoch 23: Val Loss 25756132.00000
Epoch 24: Val Loss 25777504.00000
Epoch 25: Val Loss 25737430.00000
Epoch 26: Val Loss 25752854.00000
Epoch 27: Val Loss 25759654.00000
Epoch 28: Val Loss 25719296.00000
Epoch 29: Val Loss 25696984.00000
Epoch 30: Val Loss 25746676.00000
Epoch 31: Val Loss 25647040.00000
Epoch 32: Val Loss 25682542.00000
Epoch 33: Val Loss 25662944.00000
Epoch 34: Val Loss 25674946.00000
Epoch 35: Val Loss 25633784.00000
Epoch 36: Val Loss 25641374.00000
Epoch 37: Val Loss 25611916.00000
Epoch 38: Val Loss 25616868.00000
Epoch 39: Val Loss 25596798.00000
Epoch 40: Val Loss 25610728.00000
Epoch 41: Val Loss 25549230.00000
Epoch 42: Val Loss 25566024.00000
Epoch 43: Val Loss 25558084.00000
Epoch 44: Val Loss 25552934.00000
Epoch 45: Val Loss 25526218.00000
Epoch 46: Val Loss 25556074.00000
Epoch 47: Val Loss 25535490.00000
Epoch 48: Val Loss 25517826.00000
Epoch 49: Val Loss 25504858.00000
Epoch 50: Val Loss 25469564.00000
Epoch 51: Val Loss 25446682.00000
Epoch 52: Val Loss 25474278.00000
Epoch 53: Val Loss 25425026.00000
Epoch 54: Val Loss 25395036.00000
Epoch 55: Val Loss 25416882.00000
Epoch 56: Val Loss 25390714.00000
Epoch 57: Val Loss 25347930.00000
Epoch 58: Val Loss 25333980.00000
Epoch 59: Val Loss 25352992.00000
Epoch 60: Val Loss 25356558.00000
Epoch 61: Val Loss 25276552.00000
Epoch 62: Val Loss 25304202.00000
Epoch 63: Val Loss 25249030.00000
Epoch 64: Val Loss 25260354.00000
Epoch 65: Val Loss 25244546.00000
Epoch 66: Val Loss 25186904.00000
Epoch 67: Val Loss 25190406.00000
Epoch 68: Val Loss 25172182.00000
Epoch 69: Val Loss 25161208.00000
Epoch 70: Val Loss 25116854.00000
Epoch 71: Val Loss 25102524.00000
Epoch 72: Val Loss 25099248.00000
Epoch 73: Val Loss 25072594.00000
Epoch 74: Val Loss 25038732.00000
Epoch 75: Val Loss 25049260.00000
Epoch 76: Val Loss 25037018.00000
Epoch 77: Val Loss 24951984.00000
Epoch 78: Val Loss 25010852.00000
Epoch 79: Val Loss 24951292.00000
Epoch 80: Val Loss 24935980.00000
Epoch 81: Val Loss 24910020.00000
Epoch 82: Val Loss 24894236.00000
Epoch 83: Val Loss 24877470.00000
Epoch 84: Val Loss 24842950.00000
Epoch 85: Val Loss 24803040.00000
Epoch 86: Val Loss 24778712.00000
Epoch 87: Val Loss 24762806.00000
Epoch 88: Val Loss 24727200.00000
Epoch 89: Val Loss 24717316.00000
Epoch 90: Val Loss 24646346.00000
Epoch 91: Val Loss 24639288.00000
Epoch 92: Val Loss 24650090.00000
Epoch 93: Val Loss 24596524.00000
Epoch 94: Val Loss 24532258.00000
Epoch 95: Val Loss 24517406.00000
Epoch 96: Val Loss 24494876.00000
Epoch 97: Val Loss 24491020.00000
Epoch 98: Val Loss 24426764.00000
Epoch 99: Val Loss 24386090.00000
{'MSE - mean': 24280768.80951646, 'MSE - std': 291325.22750644165, 'R2 - mean': 0.05760599625256749, 'R2 - std': 0.012268916586862025} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 161265584.00000
Epoch 1: Val Loss 158379440.00000
Epoch 2: Val Loss 148101296.00000
Epoch 3: Val Loss 125325656.00000
Epoch 4: Val Loss 87940280.00000
Epoch 5: Val Loss 47086668.00000
Epoch 6: Val Loss 27323864.00000
Epoch 7: Val Loss 26171050.00000
Epoch 8: Val Loss 26093972.00000
Epoch 9: Val Loss 26190186.00000
Epoch 10: Val Loss 26142546.00000
Epoch 11: Val Loss 26111160.00000
Epoch 12: Val Loss 26135120.00000
Epoch 13: Val Loss 26119534.00000
Epoch 14: Val Loss 26103212.00000
Epoch 15: Val Loss 26064916.00000
Epoch 16: Val Loss 26108574.00000
Epoch 17: Val Loss 26042978.00000
Epoch 18: Val Loss 26099042.00000
Epoch 19: Val Loss 26116376.00000
Epoch 20: Val Loss 26089218.00000
Epoch 21: Val Loss 26060196.00000
Epoch 22: Val Loss 26076054.00000
Epoch 23: Val Loss 26043380.00000
Epoch 24: Val Loss 26027938.00000
Epoch 25: Val Loss 26026968.00000
Epoch 26: Val Loss 26019936.00000
Epoch 27: Val Loss 26011032.00000
Epoch 28: Val Loss 26028824.00000
Epoch 29: Val Loss 26003290.00000
Epoch 30: Val Loss 26024036.00000
Epoch 31: Val Loss 25961566.00000
Epoch 32: Val Loss 25956058.00000
Epoch 33: Val Loss 25978724.00000
Epoch 34: Val Loss 25991590.00000
Epoch 35: Val Loss 25957826.00000
Epoch 36: Val Loss 25917674.00000
Epoch 37: Val Loss 25902762.00000
Epoch 38: Val Loss 25887080.00000
Epoch 39: Val Loss 25880616.00000
Epoch 40: Val Loss 25921224.00000
Epoch 41: Val Loss 25912216.00000
Epoch 42: Val Loss 25885032.00000
Epoch 43: Val Loss 25873230.00000
Epoch 44: Val Loss 25848312.00000
Epoch 45: Val Loss 25845676.00000
Epoch 46: Val Loss 25843424.00000
Epoch 47: Val Loss 25835132.00000
Epoch 48: Val Loss 25841524.00000
Epoch 49: Val Loss 25812178.00000
Epoch 50: Val Loss 25760498.00000
Epoch 51: Val Loss 25770780.00000
Epoch 52: Val Loss 25758766.00000
Epoch 53: Val Loss 25829234.00000
Epoch 54: Val Loss 25760548.00000
Epoch 55: Val Loss 25786912.00000
Epoch 56: Val Loss 25761852.00000
Epoch 57: Val Loss 25690280.00000
Epoch 58: Val Loss 25693990.00000
Epoch 59: Val Loss 25694758.00000
Epoch 60: Val Loss 25648940.00000
Epoch 61: Val Loss 25714696.00000
Epoch 62: Val Loss 25663576.00000
Epoch 63: Val Loss 25683994.00000
Epoch 64: Val Loss 25651392.00000
Epoch 65: Val Loss 25623818.00000
Epoch 66: Val Loss 25680338.00000
Epoch 67: Val Loss 25602732.00000
Epoch 68: Val Loss 25642818.00000
Epoch 69: Val Loss 25584730.00000
Epoch 70: Val Loss 25558140.00000
Epoch 71: Val Loss 25549882.00000
Epoch 72: Val Loss 25543556.00000
Epoch 73: Val Loss 25523770.00000
Epoch 74: Val Loss 25528068.00000
Epoch 75: Val Loss 25534258.00000
Epoch 76: Val Loss 25459150.00000
Epoch 77: Val Loss 25491756.00000
Epoch 78: Val Loss 25436484.00000
Epoch 79: Val Loss 25463762.00000
Epoch 80: Val Loss 25415182.00000
Epoch 81: Val Loss 25437154.00000
Epoch 82: Val Loss 25413422.00000
Epoch 83: Val Loss 25346626.00000
Epoch 84: Val Loss 25403010.00000
Epoch 85: Val Loss 25360454.00000
Epoch 86: Val Loss 25316358.00000
Epoch 87: Val Loss 25326372.00000
Epoch 88: Val Loss 25279360.00000
Epoch 89: Val Loss 25291320.00000
Epoch 90: Val Loss 25289024.00000
Epoch 91: Val Loss 25245520.00000
Epoch 92: Val Loss 25198622.00000
Epoch 93: Val Loss 25204348.00000
Epoch 94: Val Loss 25210090.00000
Epoch 95: Val Loss 25166122.00000
Epoch 96: Val Loss 25131058.00000
Epoch 97: Val Loss 25148550.00000
Epoch 98: Val Loss 25157224.00000
Epoch 99: Val Loss 25101996.00000
{'MSE - mean': 24429890.606968403, 'MSE - std': 396037.3119373194, 'R2 - mean': 0.054165380190117275, 'R2 - std': 0.012952698828007396} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 24429890.606968403 and parameters: {'dim': 256, 'depth': 12, 'heads': 8, 'weight_decay': -1, 'learning_rate': -5, 'dropout': 0.1}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 18561124.00000
Epoch 1: Val Loss 14077184.00000
Epoch 2: Val Loss 13337809.00000
Epoch 3: Val Loss 13121279.00000
Epoch 4: Val Loss 13078303.00000
Epoch 5: Val Loss 13097965.00000
Epoch 6: Val Loss 13058988.00000
Epoch 7: Val Loss 13037586.00000
Epoch 8: Val Loss 13061012.00000
Epoch 9: Val Loss 13029228.00000
Epoch 10: Val Loss 13062034.00000
Epoch 11: Val Loss 13033777.00000
Epoch 12: Val Loss 13004548.00000
Epoch 13: Val Loss 13000928.00000
Epoch 14: Val Loss 13005443.00000
Epoch 15: Val Loss 13046839.00000
Epoch 16: Val Loss 13013133.00000
Epoch 17: Val Loss 13086530.00000
Epoch 18: Val Loss 13063034.00000
Epoch 19: Val Loss 13019673.00000
Epoch 20: Val Loss 13002527.00000
Epoch 21: Val Loss 13025179.00000
Epoch 22: Val Loss 12990115.00000
Epoch 23: Val Loss 13023239.00000
Epoch 24: Val Loss 13007821.00000
Epoch 25: Val Loss 13031560.00000
Epoch 26: Val Loss 13031428.00000
Epoch 27: Val Loss 13033965.00000
Epoch 28: Val Loss 13057111.00000
Epoch 29: Val Loss 13013352.00000
Epoch 30: Val Loss 12997817.00000
Epoch 31: Val Loss 13011848.00000
Epoch 32: Val Loss 13007426.00000
Epoch 33: Val Loss 13012365.00000
Epoch 34: Val Loss 13010686.00000
Epoch 35: Val Loss 13007738.00000
Epoch 36: Val Loss 12998026.00000
Epoch 37: Val Loss 12990989.00000
Epoch 38: Val Loss 13017513.00000
Epoch 39: Val Loss 13143858.00000
Epoch 40: Val Loss 13040620.00000
Epoch 41: Val Loss 12993745.00000
Epoch 42: Val Loss 13007924.00000
Epoch 43: Val Loss 13105812.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 12999878.863645349, 'MSE - std': 0.0, 'R2 - mean': 0.4911072095572183, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19849962.00000
Epoch 1: Val Loss 14642876.00000
Epoch 2: Val Loss 13661304.00000
Epoch 3: Val Loss 13407880.00000
Epoch 4: Val Loss 13313049.00000
Epoch 5: Val Loss 13351821.00000
Epoch 6: Val Loss 13347963.00000
Epoch 7: Val Loss 13342697.00000
Epoch 8: Val Loss 13281349.00000
Epoch 9: Val Loss 13258081.00000
Epoch 10: Val Loss 13293096.00000
Epoch 11: Val Loss 13310311.00000
Epoch 12: Val Loss 13272829.00000
Epoch 13: Val Loss 13320069.00000
Epoch 14: Val Loss 13249176.00000
Epoch 15: Val Loss 13257586.00000
Epoch 16: Val Loss 13245807.00000
Epoch 17: Val Loss 13321947.00000
Epoch 18: Val Loss 13254346.00000
Epoch 19: Val Loss 13250626.00000
Epoch 20: Val Loss 13302019.00000
Epoch 21: Val Loss 13262325.00000
Epoch 22: Val Loss 13252961.00000
Epoch 23: Val Loss 13269930.00000
Epoch 24: Val Loss 13252658.00000
Epoch 25: Val Loss 13261115.00000
Epoch 26: Val Loss 13316825.00000
Epoch 27: Val Loss 13268880.00000
Epoch 28: Val Loss 13267978.00000
Epoch 29: Val Loss 13276080.00000
Epoch 30: Val Loss 13274107.00000
Epoch 31: Val Loss 13274253.00000
Epoch 32: Val Loss 13277552.00000
Epoch 33: Val Loss 13260767.00000
Epoch 34: Val Loss 13246728.00000
Epoch 35: Val Loss 13281787.00000
Epoch 36: Val Loss 13249183.00000
Epoch 37: Val Loss 13262486.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 13121828.668269832, 'MSE - std': 121949.80462448392, 'R2 - mean': 0.48936364417326667, 'R2 - std': 0.0017435653839516507} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19615124.00000
Epoch 1: Val Loss 14651891.00000
Epoch 2: Val Loss 13687906.00000
Epoch 3: Val Loss 13449661.00000
Epoch 4: Val Loss 13397943.00000
Epoch 5: Val Loss 13337506.00000
Epoch 6: Val Loss 13321467.00000
Epoch 7: Val Loss 13280461.00000
Epoch 8: Val Loss 13295812.00000
Epoch 9: Val Loss 13293680.00000
Epoch 10: Val Loss 13294545.00000
Epoch 11: Val Loss 13269007.00000
Epoch 12: Val Loss 13273183.00000
Epoch 13: Val Loss 13288237.00000
Epoch 14: Val Loss 13276872.00000
Epoch 15: Val Loss 13297068.00000
Epoch 16: Val Loss 13257628.00000
Epoch 17: Val Loss 13338160.00000
Epoch 18: Val Loss 13280228.00000
Epoch 19: Val Loss 13293160.00000
Epoch 20: Val Loss 13274986.00000
Epoch 21: Val Loss 13258822.00000
Epoch 22: Val Loss 13265173.00000
Epoch 23: Val Loss 13279486.00000
Epoch 24: Val Loss 13229265.00000
Epoch 25: Val Loss 13269386.00000
Epoch 26: Val Loss 13259531.00000
Epoch 27: Val Loss 13281298.00000
Epoch 28: Val Loss 13262928.00000
Epoch 29: Val Loss 13277436.00000
Epoch 30: Val Loss 13276895.00000
Epoch 31: Val Loss 13287239.00000
Epoch 32: Val Loss 13293296.00000
Epoch 33: Val Loss 13263627.00000
Epoch 34: Val Loss 13262025.00000
Epoch 35: Val Loss 13267903.00000
Epoch 36: Val Loss 13285347.00000
Epoch 37: Val Loss 13300087.00000
Epoch 38: Val Loss 13280369.00000
Epoch 39: Val Loss 13292802.00000
Epoch 40: Val Loss 13269501.00000
Epoch 41: Val Loss 13270426.00000
Epoch 42: Val Loss 13255177.00000
Epoch 43: Val Loss 13255022.00000
Epoch 44: Val Loss 13268714.00000
Epoch 45: Val Loss 13255499.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 13164985.417518273, 'MSE - std': 116788.3266570429, 'R2 - mean': 0.48837980456167646, 'R2 - std': 0.0019906182276061037} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20631110.00000
Epoch 1: Val Loss 14765064.00000
Epoch 2: Val Loss 13776602.00000
Epoch 3: Val Loss 13359124.00000
Epoch 4: Val Loss 13253781.00000
Epoch 5: Val Loss 13223574.00000
Epoch 6: Val Loss 13213580.00000
Epoch 7: Val Loss 13185255.00000
Epoch 8: Val Loss 13206723.00000
Epoch 9: Val Loss 13195322.00000
Epoch 10: Val Loss 13174660.00000
Epoch 11: Val Loss 13202199.00000
Epoch 12: Val Loss 13187010.00000
Epoch 13: Val Loss 13158149.00000
Epoch 14: Val Loss 13200796.00000
Epoch 15: Val Loss 13185780.00000
Epoch 16: Val Loss 13190753.00000
Epoch 17: Val Loss 13245121.00000
Epoch 18: Val Loss 13160215.00000
Epoch 19: Val Loss 13164873.00000
Epoch 20: Val Loss 13175211.00000
Epoch 21: Val Loss 13173232.00000
Epoch 22: Val Loss 13214065.00000
Epoch 23: Val Loss 13164743.00000
Epoch 24: Val Loss 13155105.00000
Epoch 25: Val Loss 13172452.00000
Epoch 26: Val Loss 13166103.00000
Epoch 27: Val Loss 13173693.00000
Epoch 28: Val Loss 13173599.00000
Epoch 29: Val Loss 13158935.00000
Epoch 30: Val Loss 13171610.00000
Epoch 31: Val Loss 13192114.00000
Epoch 32: Val Loss 13145010.00000
Epoch 33: Val Loss 13192643.00000
Epoch 34: Val Loss 13194523.00000
Epoch 35: Val Loss 13191341.00000
Epoch 36: Val Loss 13161467.00000
Epoch 37: Val Loss 13178325.00000
Epoch 38: Val Loss 13228856.00000
Epoch 39: Val Loss 13180426.00000
Epoch 40: Val Loss 13157178.00000
Epoch 41: Val Loss 13175361.00000
Epoch 42: Val Loss 13182300.00000
Epoch 43: Val Loss 13218570.00000
Epoch 44: Val Loss 13157849.00000
Epoch 45: Val Loss 13237054.00000
Epoch 46: Val Loss 13151232.00000
Epoch 47: Val Loss 13193474.00000
Epoch 48: Val Loss 13152638.00000
Epoch 49: Val Loss 13185393.00000
Epoch 50: Val Loss 13166280.00000
Epoch 51: Val Loss 13176471.00000
Epoch 52: Val Loss 13171516.00000
Epoch 53: Val Loss 13199848.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 13164126.075933263, 'MSE - std': 101152.60914278633, 'R2 - mean': 0.48908652794901025, 'R2 - std': 0.0021143071050989714} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19419410.00000
Epoch 1: Val Loss 14493499.00000
Epoch 2: Val Loss 13659308.00000
Epoch 3: Val Loss 13400252.00000
Epoch 4: Val Loss 13299723.00000
Epoch 5: Val Loss 13337584.00000
Epoch 6: Val Loss 13283700.00000
Epoch 7: Val Loss 13396367.00000
Epoch 8: Val Loss 13259805.00000
Epoch 9: Val Loss 13231961.00000
Epoch 10: Val Loss 13379513.00000
Epoch 11: Val Loss 13237077.00000
Epoch 12: Val Loss 13245207.00000
Epoch 13: Val Loss 13219379.00000
Epoch 14: Val Loss 13279423.00000
Epoch 15: Val Loss 13234356.00000
Epoch 16: Val Loss 13250020.00000
Epoch 17: Val Loss 13200434.00000
Epoch 18: Val Loss 13223669.00000
Epoch 19: Val Loss 13234381.00000
Epoch 20: Val Loss 13219819.00000
Epoch 21: Val Loss 13212690.00000
Epoch 22: Val Loss 13222246.00000
Epoch 23: Val Loss 13209339.00000
Epoch 24: Val Loss 13233203.00000
Epoch 25: Val Loss 13226036.00000
Epoch 26: Val Loss 13207520.00000
Epoch 27: Val Loss 13304730.00000
Epoch 28: Val Loss 13245926.00000
Epoch 29: Val Loss 13224944.00000
Epoch 30: Val Loss 13222252.00000
Epoch 31: Val Loss 13225891.00000
Epoch 32: Val Loss 13228093.00000
Epoch 33: Val Loss 13241772.00000
Epoch 34: Val Loss 13231809.00000
Epoch 35: Val Loss 13249597.00000
Epoch 36: Val Loss 13240240.00000
Epoch 37: Val Loss 13223158.00000
Epoch 38: Val Loss 13268536.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 13172921.419932278, 'MSE - std': 92167.8499985685, 'R2 - mean': 0.48998043792659995, 'R2 - std': 0.002602409618003085} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 13172921.419932278 and parameters: {'dim': 64, 'depth': 3, 'heads': 4, 'weight_decay': -1, 'learning_rate': -3, 'dropout': 0.5}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17560302.00000
Epoch 1: Val Loss 13738233.00000
Epoch 2: Val Loss 13159904.00000
Epoch 3: Val Loss 13042240.00000
Epoch 4: Val Loss 12957438.00000
Epoch 5: Val Loss 13366090.00000
Epoch 6: Val Loss 13036480.00000
Epoch 7: Val Loss 13086565.00000
Epoch 8: Val Loss 12901677.00000
Epoch 9: Val Loss 12907266.00000
Epoch 10: Val Loss 12905550.00000
Epoch 11: Val Loss 12985573.00000
Epoch 12: Val Loss 12900507.00000
Epoch 13: Val Loss 12864492.00000
Epoch 14: Val Loss 13258799.00000
Epoch 15: Val Loss 12871040.00000
Epoch 16: Val Loss 12896966.00000
Epoch 17: Val Loss 12887413.00000
Epoch 18: Val Loss 12920910.00000
Epoch 19: Val Loss 12964193.00000
Epoch 20: Val Loss 12931412.00000
Epoch 21: Val Loss 12884193.00000
Epoch 22: Val Loss 12947726.00000
Epoch 23: Val Loss 12866055.00000
Epoch 24: Val Loss 13026943.00000
Epoch 25: Val Loss 12840776.00000
Epoch 26: Val Loss 12835904.00000
Epoch 27: Val Loss 12882898.00000
Epoch 28: Val Loss 12915979.00000
Epoch 29: Val Loss 12913125.00000
Epoch 30: Val Loss 13068470.00000
Epoch 31: Val Loss 12832292.00000
Epoch 32: Val Loss 12871830.00000
Epoch 33: Val Loss 12876771.00000
Epoch 34: Val Loss 12895437.00000
Epoch 35: Val Loss 12962458.00000
Epoch 36: Val Loss 12855298.00000
Epoch 37: Val Loss 12843303.00000
Epoch 38: Val Loss 12828903.00000
Epoch 39: Val Loss 12840183.00000
Epoch 40: Val Loss 12840732.00000
Epoch 41: Val Loss 12821637.00000
Epoch 42: Val Loss 12824649.00000
Epoch 43: Val Loss 12903767.00000
Epoch 44: Val Loss 12836037.00000
Epoch 45: Val Loss 12826932.00000
Epoch 46: Val Loss 12847803.00000
Epoch 47: Val Loss 12852287.00000
Epoch 48: Val Loss 12850404.00000
Epoch 49: Val Loss 12843634.00000
Epoch 50: Val Loss 12866969.00000
Epoch 51: Val Loss 12827461.00000
Epoch 52: Val Loss 12827209.00000
Epoch 53: Val Loss 12808129.00000
Epoch 54: Val Loss 12810012.00000
Epoch 55: Val Loss 12780172.00000
Epoch 56: Val Loss 12783800.00000
Epoch 57: Val Loss 12736530.00000
Epoch 58: Val Loss 12736050.00000
Epoch 59: Val Loss 12714930.00000
Epoch 60: Val Loss 12686124.00000
Epoch 61: Val Loss 12697955.00000
Epoch 62: Val Loss 12700705.00000
Epoch 63: Val Loss 12687372.00000
Epoch 64: Val Loss 12673211.00000
Epoch 65: Val Loss 12634263.00000
Epoch 66: Val Loss 12606386.00000
Epoch 67: Val Loss 12629646.00000
Epoch 68: Val Loss 12609606.00000
Epoch 69: Val Loss 12597422.00000
Epoch 70: Val Loss 12606809.00000
Epoch 71: Val Loss 12610242.00000
Epoch 72: Val Loss 12578771.00000
Epoch 73: Val Loss 12580267.00000
Epoch 74: Val Loss 12550888.00000
Epoch 75: Val Loss 12545478.00000
Epoch 76: Val Loss 12565819.00000
Epoch 77: Val Loss 12566776.00000
Epoch 78: Val Loss 12550219.00000
Epoch 79: Val Loss 12546140.00000
Epoch 80: Val Loss 12524005.00000
Epoch 81: Val Loss 12611352.00000
Epoch 82: Val Loss 12547607.00000
Epoch 83: Val Loss 12517022.00000
Epoch 84: Val Loss 12533905.00000
Epoch 85: Val Loss 12503196.00000
Epoch 86: Val Loss 12519678.00000
Epoch 87: Val Loss 12555610.00000
Epoch 88: Val Loss 12504734.00000
Epoch 89: Val Loss 12540611.00000
Epoch 90: Val Loss 12552146.00000
Epoch 91: Val Loss 12487408.00000
Epoch 92: Val Loss 12485937.00000
Epoch 93: Val Loss 12465238.00000
Epoch 94: Val Loss 12491030.00000
Epoch 95: Val Loss 12457304.00000
Epoch 96: Val Loss 12435358.00000
Epoch 97: Val Loss 12411075.00000
Epoch 98: Val Loss 12447307.00000
Epoch 99: Val Loss 12403730.00000
{'MSE - mean': 12420559.034088423, 'MSE - std': 0.0, 'R2 - mean': 0.513785242769244, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 18388138.00000
Epoch 1: Val Loss 14090591.00000
Epoch 2: Val Loss 13456090.00000
Epoch 3: Val Loss 13266225.00000
Epoch 4: Val Loss 13265280.00000
Epoch 5: Val Loss 13289043.00000
Epoch 6: Val Loss 13152109.00000
Epoch 7: Val Loss 13143021.00000
Epoch 8: Val Loss 13137019.00000
Epoch 9: Val Loss 13127617.00000
Epoch 10: Val Loss 13094096.00000
Epoch 11: Val Loss 13285337.00000
Epoch 12: Val Loss 13156284.00000
Epoch 13: Val Loss 13157807.00000
Epoch 14: Val Loss 13103618.00000
Epoch 15: Val Loss 13286947.00000
Epoch 16: Val Loss 13143555.00000
Epoch 17: Val Loss 13153256.00000
Epoch 18: Val Loss 13082364.00000
Epoch 19: Val Loss 13089634.00000
Epoch 20: Val Loss 13075336.00000
Epoch 21: Val Loss 13067521.00000
Epoch 22: Val Loss 13077949.00000
Epoch 23: Val Loss 13078262.00000
Epoch 24: Val Loss 13070500.00000
Epoch 25: Val Loss 13073455.00000
Epoch 26: Val Loss 13076629.00000
Epoch 27: Val Loss 13046291.00000
Epoch 28: Val Loss 13069847.00000
Epoch 29: Val Loss 13089362.00000
Epoch 30: Val Loss 13028663.00000
Epoch 31: Val Loss 13097480.00000
Epoch 32: Val Loss 13088377.00000
Epoch 33: Val Loss 13042974.00000
Epoch 34: Val Loss 13038754.00000
Epoch 35: Val Loss 13010438.00000
Epoch 36: Val Loss 13180510.00000
Epoch 37: Val Loss 13033101.00000
Epoch 38: Val Loss 13079210.00000
Epoch 39: Val Loss 13022129.00000
Epoch 40: Val Loss 12955963.00000
Epoch 41: Val Loss 12976429.00000
Epoch 42: Val Loss 12919756.00000
Epoch 43: Val Loss 12895615.00000
Epoch 44: Val Loss 12881000.00000
Epoch 45: Val Loss 12888987.00000
Epoch 46: Val Loss 12986944.00000
Epoch 47: Val Loss 12849937.00000
Epoch 48: Val Loss 12880169.00000
Epoch 49: Val Loss 12812928.00000
Epoch 50: Val Loss 12953713.00000
Epoch 51: Val Loss 12840853.00000
Epoch 52: Val Loss 12860837.00000
Epoch 53: Val Loss 12838266.00000
Epoch 54: Val Loss 12828975.00000
Epoch 55: Val Loss 12812993.00000
Epoch 56: Val Loss 12775018.00000
Epoch 57: Val Loss 12806081.00000
Epoch 58: Val Loss 12751655.00000
Epoch 59: Val Loss 12842338.00000
Epoch 60: Val Loss 12772209.00000
Epoch 61: Val Loss 12743667.00000
Epoch 62: Val Loss 12690935.00000
Epoch 63: Val Loss 12662526.00000
Epoch 64: Val Loss 12681797.00000
Epoch 65: Val Loss 12692389.00000
Epoch 66: Val Loss 12648898.00000
Epoch 67: Val Loss 12612585.00000
Epoch 68: Val Loss 12588130.00000
Epoch 69: Val Loss 12652585.00000
Epoch 70: Val Loss 12568346.00000
Epoch 71: Val Loss 12571871.00000
Epoch 72: Val Loss 12560885.00000
Epoch 73: Val Loss 12551490.00000
Epoch 74: Val Loss 12561965.00000
Epoch 75: Val Loss 12540633.00000
Epoch 76: Val Loss 12539922.00000
Epoch 77: Val Loss 12547565.00000
Epoch 78: Val Loss 12513640.00000
Epoch 79: Val Loss 12521811.00000
Epoch 80: Val Loss 12519658.00000
Epoch 81: Val Loss 12550771.00000
Epoch 82: Val Loss 12529303.00000
Epoch 83: Val Loss 12503287.00000
Epoch 84: Val Loss 12530743.00000
Epoch 85: Val Loss 12552054.00000
Epoch 86: Val Loss 12491368.00000
Epoch 87: Val Loss 12496233.00000
Epoch 88: Val Loss 12519487.00000
Epoch 89: Val Loss 12474648.00000
Epoch 90: Val Loss 12499596.00000
Epoch 91: Val Loss 12477884.00000
Epoch 92: Val Loss 12517378.00000
Epoch 93: Val Loss 12490098.00000
Epoch 94: Val Loss 12481820.00000
Epoch 95: Val Loss 12469633.00000
Epoch 96: Val Loss 12450062.00000
Epoch 97: Val Loss 12462198.00000
Epoch 98: Val Loss 12499012.00000
Epoch 99: Val Loss 12457685.00000
{'MSE - mean': 12445952.662629958, 'MSE - std': 25393.628541534767, 'R2 - mean': 0.5156447236211553, 'R2 - std': 0.0018594808519113437} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17856352.00000
Epoch 1: Val Loss 14150409.00000
Epoch 2: Val Loss 13621665.00000
Epoch 3: Val Loss 13396120.00000
Epoch 4: Val Loss 13236888.00000
Epoch 5: Val Loss 13271527.00000
Epoch 6: Val Loss 13161157.00000
Epoch 7: Val Loss 13156425.00000
Epoch 8: Val Loss 13156178.00000
Epoch 9: Val Loss 13130766.00000
Epoch 10: Val Loss 13133701.00000
Epoch 11: Val Loss 13118713.00000
Epoch 12: Val Loss 13114693.00000
Epoch 13: Val Loss 13178754.00000
Epoch 14: Val Loss 13132728.00000
Epoch 15: Val Loss 13099699.00000
Epoch 16: Val Loss 13093647.00000
Epoch 17: Val Loss 13094572.00000
Epoch 18: Val Loss 13148242.00000
Epoch 19: Val Loss 13094173.00000
Epoch 20: Val Loss 13078370.00000
Epoch 21: Val Loss 13200148.00000
Epoch 22: Val Loss 13089411.00000
Epoch 23: Val Loss 13172203.00000
Epoch 24: Val Loss 13112725.00000
Epoch 25: Val Loss 13061828.00000
Epoch 26: Val Loss 13159218.00000
Epoch 27: Val Loss 13071069.00000
Epoch 28: Val Loss 13086190.00000
Epoch 29: Val Loss 13091428.00000
Epoch 30: Val Loss 13139830.00000
Epoch 31: Val Loss 13087854.00000
Epoch 32: Val Loss 13085542.00000
Epoch 33: Val Loss 13118130.00000
Epoch 34: Val Loss 13080597.00000
Epoch 35: Val Loss 13095350.00000
Epoch 36: Val Loss 13080124.00000
Epoch 37: Val Loss 13076467.00000
Epoch 38: Val Loss 13177829.00000
Epoch 39: Val Loss 13167181.00000
Epoch 40: Val Loss 13064686.00000
Epoch 41: Val Loss 13106616.00000
Epoch 42: Val Loss 13083939.00000
Epoch 43: Val Loss 13077304.00000
Epoch 44: Val Loss 13079154.00000
Epoch 45: Val Loss 13075471.00000
Epoch 46: Val Loss 13074213.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 12657839.017440105, 'MSE - std': 300369.0157508503, 'R2 - mean': 0.5080927440729487, 'R2 - std': 0.010787488256562642} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19065504.00000
Epoch 1: Val Loss 14204465.00000
Epoch 2: Val Loss 13493445.00000
Epoch 3: Val Loss 13284919.00000
Epoch 4: Val Loss 13129286.00000
Epoch 5: Val Loss 13099102.00000
Epoch 6: Val Loss 13078412.00000
Epoch 7: Val Loss 13104592.00000
Epoch 8: Val Loss 13061558.00000
Epoch 9: Val Loss 13272307.00000
Epoch 10: Val Loss 13049811.00000
Epoch 11: Val Loss 13047559.00000
Epoch 12: Val Loss 13042901.00000
Epoch 13: Val Loss 13050098.00000
Epoch 14: Val Loss 13040262.00000
Epoch 15: Val Loss 13008313.00000
Epoch 16: Val Loss 13032035.00000
Epoch 17: Val Loss 12995894.00000
Epoch 18: Val Loss 12986366.00000
Epoch 19: Val Loss 13032535.00000
Epoch 20: Val Loss 13013869.00000
Epoch 21: Val Loss 13009461.00000
Epoch 22: Val Loss 12991669.00000
Epoch 23: Val Loss 13033572.00000
Epoch 24: Val Loss 12992268.00000
Epoch 25: Val Loss 12987310.00000
Epoch 26: Val Loss 12988008.00000
Epoch 27: Val Loss 13178998.00000
Epoch 28: Val Loss 12975799.00000
Epoch 29: Val Loss 13002543.00000
Epoch 30: Val Loss 12981849.00000
Epoch 31: Val Loss 12975101.00000
Epoch 32: Val Loss 13007585.00000
Epoch 33: Val Loss 13122705.00000
Epoch 34: Val Loss 12991543.00000
Epoch 35: Val Loss 13131437.00000
Epoch 36: Val Loss 12980142.00000
Epoch 37: Val Loss 13001313.00000
Epoch 38: Val Loss 13074746.00000
Epoch 39: Val Loss 13006372.00000
Epoch 40: Val Loss 12986525.00000
Epoch 41: Val Loss 12987124.00000
Epoch 42: Val Loss 13010124.00000
Epoch 43: Val Loss 12981268.00000
Epoch 44: Val Loss 12965208.00000
Epoch 45: Val Loss 12991316.00000
Epoch 46: Val Loss 12969744.00000
Epoch 47: Val Loss 13032107.00000
Epoch 48: Val Loss 12959508.00000
Epoch 49: Val Loss 12977277.00000
Epoch 50: Val Loss 12989840.00000
Epoch 51: Val Loss 13036656.00000
Epoch 52: Val Loss 13026290.00000
Epoch 53: Val Loss 12985367.00000
Epoch 54: Val Loss 12978893.00000
Epoch 55: Val Loss 12994716.00000
Epoch 56: Val Loss 12981848.00000
Epoch 57: Val Loss 12993040.00000
Epoch 58: Val Loss 12973263.00000
Epoch 59: Val Loss 12970812.00000
Epoch 60: Val Loss 12975563.00000
Epoch 61: Val Loss 12981923.00000
Epoch 62: Val Loss 12934593.00000
Epoch 63: Val Loss 12917225.00000
Epoch 64: Val Loss 12915402.00000
Epoch 65: Val Loss 12958701.00000
Epoch 66: Val Loss 12876831.00000
Epoch 67: Val Loss 12888744.00000
Epoch 68: Val Loss 12869282.00000
Epoch 69: Val Loss 12897608.00000
Epoch 70: Val Loss 12931838.00000
Epoch 71: Val Loss 12835921.00000
Epoch 72: Val Loss 12837742.00000
Epoch 73: Val Loss 12831505.00000
Epoch 74: Val Loss 12814456.00000
Epoch 75: Val Loss 12777694.00000
Epoch 76: Val Loss 12750085.00000
Epoch 77: Val Loss 12745020.00000
Epoch 78: Val Loss 12744898.00000
Epoch 79: Val Loss 12702526.00000
Epoch 80: Val Loss 12654141.00000
Epoch 81: Val Loss 12656586.00000
Epoch 82: Val Loss 12665684.00000
Epoch 83: Val Loss 12652560.00000
Epoch 84: Val Loss 12683958.00000
Epoch 85: Val Loss 12607115.00000
Epoch 86: Val Loss 12590813.00000
Epoch 87: Val Loss 12592112.00000
Epoch 88: Val Loss 12584931.00000
Epoch 89: Val Loss 12543497.00000
Epoch 90: Val Loss 12541902.00000
Epoch 91: Val Loss 12528991.00000
Epoch 92: Val Loss 12521098.00000
Epoch 93: Val Loss 12505126.00000
Epoch 94: Val Loss 12539054.00000
Epoch 95: Val Loss 12503464.00000
Epoch 96: Val Loss 12552665.00000
Epoch 97: Val Loss 12475565.00000
Epoch 98: Val Loss 12510198.00000
Epoch 99: Val Loss 12496514.00000
{'MSE - mean': 12614400.751192667, 'MSE - std': 270789.23195190186, 'R2 - mean': 0.5104184901082527, 'R2 - std': 0.01017372650636455} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17516134.00000
Epoch 1: Val Loss 13965439.00000
Epoch 2: Val Loss 13406972.00000
Epoch 3: Val Loss 13200860.00000
Epoch 4: Val Loss 13250468.00000
Epoch 5: Val Loss 13126774.00000
Epoch 6: Val Loss 13139016.00000
Epoch 7: Val Loss 13174162.00000
Epoch 8: Val Loss 13138069.00000
Epoch 9: Val Loss 13071746.00000
Epoch 10: Val Loss 13145559.00000
Epoch 11: Val Loss 13064806.00000
Epoch 12: Val Loss 13042534.00000
Epoch 13: Val Loss 13057189.00000
Epoch 14: Val Loss 13065191.00000
Epoch 15: Val Loss 13103865.00000
Epoch 16: Val Loss 13041475.00000
Epoch 17: Val Loss 13042593.00000
Epoch 18: Val Loss 13046721.00000
Epoch 19: Val Loss 13017551.00000
Epoch 20: Val Loss 13083304.00000
Epoch 21: Val Loss 13006874.00000
Epoch 22: Val Loss 13020013.00000
Epoch 23: Val Loss 13146391.00000
Epoch 24: Val Loss 13001304.00000
Epoch 25: Val Loss 13030129.00000
Epoch 26: Val Loss 13014533.00000
Epoch 27: Val Loss 13009632.00000
Epoch 28: Val Loss 12993807.00000
Epoch 29: Val Loss 13115189.00000
Epoch 30: Val Loss 13016763.00000
Epoch 31: Val Loss 12999534.00000
Epoch 32: Val Loss 13024153.00000
Epoch 33: Val Loss 13029943.00000
Epoch 34: Val Loss 13125788.00000
Epoch 35: Val Loss 13004866.00000
Epoch 36: Val Loss 13002563.00000
Epoch 37: Val Loss 13065048.00000
Epoch 38: Val Loss 13014259.00000
Epoch 39: Val Loss 13008304.00000
Epoch 40: Val Loss 12992508.00000
Epoch 41: Val Loss 13008492.00000
Epoch 42: Val Loss 13034718.00000
Epoch 43: Val Loss 12999794.00000
Epoch 44: Val Loss 13049621.00000
Epoch 45: Val Loss 13016052.00000
Epoch 46: Val Loss 13053201.00000
Epoch 47: Val Loss 13117106.00000
Epoch 48: Val Loss 13017794.00000
Epoch 49: Val Loss 13022208.00000
Epoch 50: Val Loss 13032694.00000
Epoch 51: Val Loss 13001114.00000
Epoch 52: Val Loss 12990990.00000
Epoch 53: Val Loss 12999157.00000
Epoch 54: Val Loss 12983359.00000
Epoch 55: Val Loss 13002268.00000
Epoch 56: Val Loss 12992656.00000
Epoch 57: Val Loss 13045613.00000
Epoch 58: Val Loss 13012050.00000
Epoch 59: Val Loss 13018487.00000
Epoch 60: Val Loss 13010253.00000
Epoch 61: Val Loss 13009877.00000
Epoch 62: Val Loss 13006941.00000
Epoch 63: Val Loss 13061048.00000
Epoch 64: Val Loss 13005830.00000
Epoch 65: Val Loss 13012932.00000
Epoch 66: Val Loss 13007979.00000
Epoch 67: Val Loss 13002367.00000
Epoch 68: Val Loss 13007618.00000
Epoch 69: Val Loss 13005587.00000
Epoch 70: Val Loss 12986287.00000
Epoch 71: Val Loss 13018914.00000
Epoch 72: Val Loss 13006297.00000
Epoch 73: Val Loss 13027901.00000
Epoch 74: Val Loss 12991837.00000
Epoch 75: Val Loss 13008238.00000
Validation loss has not improved for 20 steps!
Early stopping applies.
{'MSE - mean': 12691505.5427309, 'MSE - std': 287127.2227175568, 'R2 - mean': 0.5086424599006211, 'R2 - std': 0.009768362299522016} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 12691505.5427309 and parameters: {'dim': 256, 'depth': 1, 'heads': 8, 'weight_decay': -3, 'learning_rate': -3, 'dropout': 0}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19515558.00000
Epoch 1: Val Loss 14164154.00000
Epoch 2: Val Loss 13361503.00000
Epoch 3: Val Loss 13110621.00000
Epoch 4: Val Loss 13082761.00000
Epoch 5: Val Loss 13010991.00000
Epoch 6: Val Loss 13012590.00000
Epoch 7: Val Loss 12984304.00000
Epoch 8: Val Loss 12969550.00000
Epoch 9: Val Loss 12971155.00000
Epoch 10: Val Loss 12930666.00000
Epoch 11: Val Loss 12912111.00000
Epoch 12: Val Loss 12910044.00000
Epoch 13: Val Loss 12875813.00000
Epoch 14: Val Loss 12840367.00000
Epoch 15: Val Loss 12891396.00000
Epoch 16: Val Loss 12779913.00000
Epoch 17: Val Loss 12765840.00000
Epoch 18: Val Loss 12733451.00000
Epoch 19: Val Loss 12705058.00000
Epoch 20: Val Loss 12679979.00000
Epoch 21: Val Loss 12643639.00000
Epoch 22: Val Loss 12640044.00000
Epoch 23: Val Loss 12645798.00000
Epoch 24: Val Loss 12586805.00000
Epoch 25: Val Loss 12517546.00000
Epoch 26: Val Loss 12644738.00000
Epoch 27: Val Loss 12482869.00000
Epoch 28: Val Loss 12491320.00000
Epoch 29: Val Loss 12442602.00000
Epoch 30: Val Loss 12435599.00000
Epoch 31: Val Loss 12420825.00000
Epoch 32: Val Loss 12434131.00000
Epoch 33: Val Loss 12396416.00000
Epoch 34: Val Loss 12423401.00000
Epoch 35: Val Loss 12453734.00000
Epoch 36: Val Loss 12396107.00000
Epoch 37: Val Loss 12383055.00000
Epoch 38: Val Loss 12403452.00000
Epoch 39: Val Loss 12463158.00000
Epoch 40: Val Loss 12386160.00000
Epoch 41: Val Loss 12350112.00000
Epoch 42: Val Loss 12405185.00000
Epoch 43: Val Loss 12375006.00000
Epoch 44: Val Loss 12356512.00000
Epoch 45: Val Loss 12379066.00000
Epoch 46: Val Loss 12353884.00000
Epoch 47: Val Loss 12373762.00000
Epoch 48: Val Loss 12323718.00000
Epoch 49: Val Loss 12317103.00000
Epoch 50: Val Loss 12332395.00000
Epoch 51: Val Loss 12319788.00000
Epoch 52: Val Loss 12361532.00000
Epoch 53: Val Loss 12312583.00000
Epoch 54: Val Loss 12320415.00000
Epoch 55: Val Loss 12302321.00000
Epoch 56: Val Loss 12291861.00000
Epoch 57: Val Loss 12300800.00000
Epoch 58: Val Loss 12302222.00000
Epoch 59: Val Loss 12357170.00000
Epoch 60: Val Loss 12290583.00000
Epoch 61: Val Loss 12312674.00000
Epoch 62: Val Loss 12287621.00000
Epoch 63: Val Loss 12294943.00000
Epoch 64: Val Loss 12283647.00000
Epoch 65: Val Loss 12292304.00000
Epoch 66: Val Loss 12283328.00000
Epoch 67: Val Loss 12287475.00000
Epoch 68: Val Loss 12255657.00000
Epoch 69: Val Loss 12266445.00000
Epoch 70: Val Loss 12267734.00000
Epoch 71: Val Loss 12257980.00000
Epoch 72: Val Loss 12272485.00000
Epoch 73: Val Loss 12280237.00000
Epoch 74: Val Loss 12289242.00000
Epoch 75: Val Loss 12285603.00000
Epoch 76: Val Loss 12266198.00000
Epoch 77: Val Loss 12275492.00000
Epoch 78: Val Loss 12266579.00000
Epoch 79: Val Loss 12254547.00000
Epoch 80: Val Loss 12303832.00000
Epoch 81: Val Loss 12285425.00000
Epoch 82: Val Loss 12248021.00000
Epoch 83: Val Loss 12247831.00000
Epoch 84: Val Loss 12251174.00000
Epoch 85: Val Loss 12230055.00000
Epoch 86: Val Loss 12271935.00000
Epoch 87: Val Loss 12259947.00000
Epoch 88: Val Loss 12242626.00000
Epoch 89: Val Loss 12249341.00000
Epoch 90: Val Loss 12288563.00000
Epoch 91: Val Loss 12258262.00000
Epoch 92: Val Loss 12238792.00000
Epoch 93: Val Loss 12261957.00000
Epoch 94: Val Loss 12253424.00000
Epoch 95: Val Loss 12265960.00000
Epoch 96: Val Loss 12244265.00000
Epoch 97: Val Loss 12248528.00000
Epoch 98: Val Loss 12261365.00000
Epoch 99: Val Loss 12235329.00000
{'MSE - mean': 12245495.855526162, 'MSE - std': 0.0, 'R2 - mean': 0.5206382596609223, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17571210.00000
Epoch 1: Val Loss 13974510.00000
Epoch 2: Val Loss 13506458.00000
Epoch 3: Val Loss 13329882.00000
Epoch 4: Val Loss 13203781.00000
Epoch 5: Val Loss 13250552.00000
Epoch 6: Val Loss 13172266.00000
Epoch 7: Val Loss 13301713.00000
Epoch 8: Val Loss 13190304.00000
Epoch 9: Val Loss 13269660.00000
Epoch 10: Val Loss 13131970.00000
Epoch 11: Val Loss 13117400.00000
Epoch 12: Val Loss 13127544.00000
Epoch 13: Val Loss 13091657.00000
Epoch 14: Val Loss 13106027.00000
Epoch 15: Val Loss 13086566.00000
Epoch 16: Val Loss 13101815.00000
Epoch 17: Val Loss 13260098.00000
Epoch 18: Val Loss 13056262.00000
Epoch 19: Val Loss 13025947.00000
Epoch 20: Val Loss 13121801.00000
Epoch 21: Val Loss 13024237.00000
Epoch 22: Val Loss 13018169.00000
Epoch 23: Val Loss 12983800.00000
Epoch 24: Val Loss 13058064.00000
Epoch 25: Val Loss 12988080.00000
Epoch 26: Val Loss 13000786.00000
Epoch 27: Val Loss 13085174.00000
Epoch 28: Val Loss 13020306.00000
Epoch 29: Val Loss 12952296.00000
Epoch 30: Val Loss 12942239.00000
Epoch 31: Val Loss 12938314.00000
Epoch 32: Val Loss 12912457.00000
Epoch 33: Val Loss 12914239.00000
Epoch 34: Val Loss 12925512.00000
Epoch 35: Val Loss 12904030.00000
Epoch 36: Val Loss 12861464.00000
Epoch 37: Val Loss 12871435.00000
Epoch 38: Val Loss 12852254.00000
Epoch 39: Val Loss 12863079.00000
Epoch 40: Val Loss 12822908.00000
Epoch 41: Val Loss 12804594.00000
Epoch 42: Val Loss 12835925.00000
Epoch 43: Val Loss 12812350.00000
Epoch 44: Val Loss 12779204.00000
Epoch 45: Val Loss 12765903.00000
Epoch 46: Val Loss 12839235.00000
Epoch 47: Val Loss 12747578.00000
Epoch 48: Val Loss 12735062.00000
Epoch 49: Val Loss 12783254.00000
Epoch 50: Val Loss 12735506.00000
Epoch 51: Val Loss 12752191.00000
Epoch 52: Val Loss 12732703.00000
Epoch 53: Val Loss 12741184.00000
Epoch 54: Val Loss 12713996.00000
Epoch 55: Val Loss 12743318.00000
Epoch 56: Val Loss 12688236.00000
Epoch 57: Val Loss 12725792.00000
Epoch 58: Val Loss 12698890.00000
Epoch 59: Val Loss 12688017.00000
Epoch 60: Val Loss 12828556.00000
Epoch 61: Val Loss 12736976.00000
Epoch 62: Val Loss 12700788.00000
Epoch 63: Val Loss 12675796.00000
Epoch 64: Val Loss 12716416.00000
Epoch 65: Val Loss 12669170.00000
Epoch 66: Val Loss 12643147.00000
Epoch 67: Val Loss 12678221.00000
Epoch 68: Val Loss 12680300.00000
Epoch 69: Val Loss 12657858.00000
Epoch 70: Val Loss 12690145.00000
Epoch 71: Val Loss 12613701.00000
Epoch 72: Val Loss 12608130.00000
Epoch 73: Val Loss 12627928.00000
Epoch 74: Val Loss 12610557.00000
Epoch 75: Val Loss 12602007.00000
Epoch 76: Val Loss 12577643.00000
Epoch 77: Val Loss 12588776.00000
Epoch 78: Val Loss 12564985.00000
Epoch 79: Val Loss 12563487.00000
Epoch 80: Val Loss 12596974.00000
Epoch 81: Val Loss 12556327.00000
Epoch 82: Val Loss 12570337.00000
Epoch 83: Val Loss 12562449.00000
Epoch 84: Val Loss 12581202.00000
Epoch 85: Val Loss 12559516.00000
Epoch 86: Val Loss 12527164.00000
Epoch 87: Val Loss 12552713.00000
Epoch 88: Val Loss 12658455.00000
Epoch 89: Val Loss 12497594.00000
Epoch 90: Val Loss 12514219.00000
Epoch 91: Val Loss 12514530.00000
Epoch 92: Val Loss 12500058.00000
Epoch 93: Val Loss 12495218.00000
Epoch 94: Val Loss 12516874.00000
Epoch 95: Val Loss 12504630.00000
Epoch 96: Val Loss 12501580.00000
Epoch 97: Val Loss 12542658.00000
Epoch 98: Val Loss 12508479.00000
Epoch 99: Val Loss 12536660.00000
{'MSE - mean': 12374441.188964633, 'MSE - std': 128945.33343847096, 'R2 - mean': 0.518451440245966, 'R2 - std': 0.0021868194149562425} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 22944082.00000
Epoch 1: Val Loss 15164916.00000
Epoch 2: Val Loss 13892472.00000
Epoch 3: Val Loss 13480085.00000
Epoch 4: Val Loss 13284618.00000
Epoch 5: Val Loss 13264147.00000
Epoch 6: Val Loss 13254738.00000
Epoch 7: Val Loss 13232190.00000
Epoch 8: Val Loss 13181600.00000
Epoch 9: Val Loss 13189729.00000
Epoch 10: Val Loss 13153691.00000
Epoch 11: Val Loss 13171596.00000
Epoch 12: Val Loss 13220615.00000
Epoch 13: Val Loss 13143335.00000
Epoch 14: Val Loss 13131534.00000
Epoch 15: Val Loss 13135869.00000
Epoch 16: Val Loss 13132549.00000
Epoch 17: Val Loss 13156747.00000
Epoch 18: Val Loss 13175905.00000
Epoch 19: Val Loss 13125720.00000
Epoch 20: Val Loss 13156933.00000
Epoch 21: Val Loss 13116958.00000
Epoch 22: Val Loss 13132284.00000
Epoch 23: Val Loss 13159831.00000
Epoch 24: Val Loss 13095121.00000
Epoch 25: Val Loss 13104535.00000
Epoch 26: Val Loss 13091609.00000
Epoch 27: Val Loss 13079187.00000
Epoch 28: Val Loss 13089671.00000
Epoch 29: Val Loss 13058208.00000
Epoch 30: Val Loss 13075570.00000
Epoch 31: Val Loss 13035690.00000
Epoch 32: Val Loss 13050880.00000
Epoch 33: Val Loss 13061986.00000
Epoch 34: Val Loss 13058895.00000
Epoch 35: Val Loss 12994043.00000
Epoch 36: Val Loss 12995199.00000
Epoch 37: Val Loss 12979853.00000
Epoch 38: Val Loss 12993154.00000
Epoch 39: Val Loss 12974591.00000
Epoch 40: Val Loss 12927392.00000
Epoch 41: Val Loss 12928206.00000
Epoch 42: Val Loss 12962730.00000
Epoch 43: Val Loss 12964623.00000
Epoch 44: Val Loss 12938761.00000
Epoch 45: Val Loss 12870170.00000
Epoch 46: Val Loss 12874318.00000
Epoch 47: Val Loss 12897774.00000
Epoch 48: Val Loss 12919494.00000
Epoch 49: Val Loss 12861802.00000
Epoch 50: Val Loss 12836737.00000
Epoch 51: Val Loss 12840684.00000
Epoch 52: Val Loss 12842101.00000
Epoch 53: Val Loss 12807078.00000
Epoch 54: Val Loss 12824929.00000
Epoch 55: Val Loss 12805415.00000
Epoch 56: Val Loss 12778122.00000
Epoch 57: Val Loss 12773124.00000
Epoch 58: Val Loss 12784679.00000
Epoch 59: Val Loss 12750545.00000
Epoch 60: Val Loss 12763791.00000
Epoch 61: Val Loss 12755934.00000
Epoch 62: Val Loss 12712498.00000
Epoch 63: Val Loss 12750338.00000
Epoch 64: Val Loss 12733288.00000
Epoch 65: Val Loss 12772827.00000
Epoch 66: Val Loss 12710341.00000
Epoch 67: Val Loss 12692298.00000
Epoch 68: Val Loss 12697700.00000
Epoch 69: Val Loss 12695001.00000
Epoch 70: Val Loss 12693787.00000
Epoch 71: Val Loss 12680553.00000
Epoch 72: Val Loss 12675395.00000
Epoch 73: Val Loss 12706319.00000
Epoch 74: Val Loss 12686757.00000
Epoch 75: Val Loss 12675343.00000
Epoch 76: Val Loss 12673279.00000
Epoch 77: Val Loss 12670713.00000
Epoch 78: Val Loss 12655102.00000
Epoch 79: Val Loss 12679312.00000
Epoch 80: Val Loss 12647746.00000
Epoch 81: Val Loss 12669280.00000
Epoch 82: Val Loss 12650567.00000
Epoch 83: Val Loss 12658433.00000
Epoch 84: Val Loss 12644226.00000
Epoch 85: Val Loss 12653381.00000
Epoch 86: Val Loss 12637998.00000
Epoch 87: Val Loss 12678389.00000
Epoch 88: Val Loss 12630892.00000
Epoch 89: Val Loss 12632148.00000
Epoch 90: Val Loss 12674830.00000
Epoch 91: Val Loss 12625630.00000
Epoch 92: Val Loss 12668350.00000
Epoch 93: Val Loss 12625903.00000
Epoch 94: Val Loss 12630537.00000
Epoch 95: Val Loss 12675630.00000
Epoch 96: Val Loss 12627991.00000
Epoch 97: Val Loss 12647730.00000
Epoch 98: Val Loss 12638490.00000
Epoch 99: Val Loss 12610021.00000
{'MSE - mean': 12457563.864874506, 'MSE - std': 157807.97775949686, 'R2 - mean': 0.5158783229864096, 'R2 - std': 0.0040533917030812095} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19847368.00000
Epoch 1: Val Loss 14400606.00000
Epoch 2: Val Loss 13506957.00000
Epoch 3: Val Loss 13263916.00000
Epoch 4: Val Loss 13385070.00000
Epoch 5: Val Loss 13175427.00000
Epoch 6: Val Loss 13155712.00000
Epoch 7: Val Loss 13145555.00000
Epoch 8: Val Loss 13092548.00000
Epoch 9: Val Loss 13073536.00000
Epoch 10: Val Loss 13052938.00000
Epoch 11: Val Loss 13059416.00000
Epoch 12: Val Loss 13047910.00000
Epoch 13: Val Loss 13028272.00000
Epoch 14: Val Loss 13043093.00000
Epoch 15: Val Loss 13034253.00000
Epoch 16: Val Loss 13020136.00000
Epoch 17: Val Loss 13107166.00000
Epoch 18: Val Loss 12983908.00000
Epoch 19: Val Loss 12959298.00000
Epoch 20: Val Loss 12961265.00000
Epoch 21: Val Loss 12939082.00000
Epoch 22: Val Loss 12928798.00000
Epoch 23: Val Loss 12904827.00000
Epoch 24: Val Loss 12896921.00000
Epoch 25: Val Loss 12917237.00000
Epoch 26: Val Loss 12873951.00000
Epoch 27: Val Loss 12836326.00000
Epoch 28: Val Loss 12832477.00000
Epoch 29: Val Loss 12820432.00000
Epoch 30: Val Loss 12785408.00000
Epoch 31: Val Loss 12808842.00000
Epoch 32: Val Loss 12801910.00000
Epoch 33: Val Loss 12876963.00000
Epoch 34: Val Loss 12713941.00000
Epoch 35: Val Loss 12723076.00000
Epoch 36: Val Loss 12726526.00000
Epoch 37: Val Loss 12713429.00000
Epoch 38: Val Loss 12681864.00000
Epoch 39: Val Loss 12682523.00000
Epoch 40: Val Loss 12671388.00000
Epoch 41: Val Loss 12646738.00000
Epoch 42: Val Loss 12655352.00000
Epoch 43: Val Loss 12660204.00000
Epoch 44: Val Loss 12646117.00000
Epoch 45: Val Loss 12636812.00000
Epoch 46: Val Loss 12628221.00000
Epoch 47: Val Loss 12635488.00000
Epoch 48: Val Loss 12631701.00000
Epoch 49: Val Loss 12610311.00000
Epoch 50: Val Loss 12614891.00000
Epoch 51: Val Loss 12605184.00000
Epoch 52: Val Loss 12590638.00000
Epoch 53: Val Loss 12634434.00000
Epoch 54: Val Loss 12621538.00000
Epoch 55: Val Loss 12572688.00000
Epoch 56: Val Loss 12591256.00000
Epoch 57: Val Loss 12569859.00000
Epoch 58: Val Loss 12602942.00000
Epoch 59: Val Loss 12575140.00000
Epoch 60: Val Loss 12576448.00000
Epoch 61: Val Loss 12628568.00000
Epoch 62: Val Loss 12571815.00000
Epoch 63: Val Loss 12561599.00000
Epoch 64: Val Loss 12589177.00000
Epoch 65: Val Loss 12548620.00000
Epoch 66: Val Loss 12564541.00000
Epoch 67: Val Loss 12539694.00000
Epoch 68: Val Loss 12538530.00000
Epoch 69: Val Loss 12539462.00000
Epoch 70: Val Loss 12542359.00000
Epoch 71: Val Loss 12559016.00000
Epoch 72: Val Loss 12533354.00000
Epoch 73: Val Loss 12618367.00000
Epoch 74: Val Loss 12542743.00000
Epoch 75: Val Loss 12528983.00000
Epoch 76: Val Loss 12549781.00000
Epoch 77: Val Loss 12579143.00000
Epoch 78: Val Loss 12554640.00000
Epoch 79: Val Loss 12500625.00000
Epoch 80: Val Loss 12509426.00000
Epoch 81: Val Loss 12553430.00000
Epoch 82: Val Loss 12516635.00000
Epoch 83: Val Loss 12512993.00000
Epoch 84: Val Loss 12527742.00000
Epoch 85: Val Loss 12545523.00000
Epoch 86: Val Loss 12540064.00000
Epoch 87: Val Loss 12503511.00000
Epoch 88: Val Loss 12503783.00000
Epoch 89: Val Loss 12505940.00000
Epoch 90: Val Loss 12496744.00000
Epoch 91: Val Loss 12489766.00000
Epoch 92: Val Loss 12512040.00000
Epoch 93: Val Loss 12539076.00000
Epoch 94: Val Loss 12567199.00000
Epoch 95: Val Loss 12520665.00000
Epoch 96: Val Loss 12484986.00000
Epoch 97: Val Loss 12488291.00000
Epoch 98: Val Loss 12472520.00000
Epoch 99: Val Loss 12501978.00000
{'MSE - mean': 12462555.415923798, 'MSE - std': 136938.91020830948, 'R2 - mean': 0.5163210329031804, 'R2 - std': 0.003593113473740744} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20476772.00000
Epoch 1: Val Loss 14644290.00000
Epoch 2: Val Loss 13610780.00000
Epoch 3: Val Loss 13333617.00000
Epoch 4: Val Loss 13253967.00000
Epoch 5: Val Loss 13212007.00000
Epoch 6: Val Loss 13166343.00000
Epoch 7: Val Loss 13214609.00000
Epoch 8: Val Loss 13100909.00000
Epoch 9: Val Loss 13128655.00000
Epoch 10: Val Loss 13154074.00000
Epoch 11: Val Loss 13073402.00000
Epoch 12: Val Loss 13064422.00000
Epoch 13: Val Loss 13107037.00000
Epoch 14: Val Loss 13085089.00000
Epoch 15: Val Loss 13063907.00000
Epoch 16: Val Loss 13053837.00000
Epoch 17: Val Loss 13039888.00000
Epoch 18: Val Loss 13012441.00000
Epoch 19: Val Loss 12996711.00000
Epoch 20: Val Loss 12989506.00000
Epoch 21: Val Loss 13007669.00000
Epoch 22: Val Loss 12983636.00000
Epoch 23: Val Loss 12977213.00000
Epoch 24: Val Loss 12960408.00000
Epoch 25: Val Loss 12944827.00000
Epoch 26: Val Loss 12899059.00000
Epoch 27: Val Loss 12896126.00000
Epoch 28: Val Loss 12868074.00000
Epoch 29: Val Loss 12830718.00000
Epoch 30: Val Loss 12789807.00000
Epoch 31: Val Loss 12793431.00000
Epoch 32: Val Loss 12755747.00000
Epoch 33: Val Loss 12715948.00000
Epoch 34: Val Loss 12700463.00000
Epoch 35: Val Loss 12669390.00000
Epoch 36: Val Loss 12644244.00000
Epoch 37: Val Loss 12623935.00000
Epoch 38: Val Loss 12655211.00000
Epoch 39: Val Loss 12684309.00000
Epoch 40: Val Loss 12639537.00000
Epoch 41: Val Loss 12569128.00000
Epoch 42: Val Loss 12577459.00000
Epoch 43: Val Loss 12575525.00000
Epoch 44: Val Loss 12595879.00000
Epoch 45: Val Loss 12551054.00000
Epoch 46: Val Loss 12546599.00000
Epoch 47: Val Loss 12542543.00000
Epoch 48: Val Loss 12528314.00000
Epoch 49: Val Loss 12509712.00000
Epoch 50: Val Loss 12537528.00000
Epoch 51: Val Loss 12508572.00000
Epoch 52: Val Loss 12634779.00000
Epoch 53: Val Loss 12524017.00000
Epoch 54: Val Loss 12500513.00000
Epoch 55: Val Loss 12508481.00000
Epoch 56: Val Loss 12510193.00000
Epoch 57: Val Loss 12535989.00000
Epoch 58: Val Loss 12578349.00000
Epoch 59: Val Loss 12506346.00000
Epoch 60: Val Loss 12511917.00000
Epoch 61: Val Loss 12488522.00000
Epoch 62: Val Loss 12481140.00000
Epoch 63: Val Loss 12492925.00000
Epoch 64: Val Loss 12494584.00000
Epoch 65: Val Loss 12470212.00000
Epoch 66: Val Loss 12472759.00000
Epoch 67: Val Loss 12467046.00000
Epoch 68: Val Loss 12467470.00000
Epoch 69: Val Loss 12482065.00000
Epoch 70: Val Loss 12477293.00000
Epoch 71: Val Loss 12458021.00000
Epoch 72: Val Loss 12474789.00000
Epoch 73: Val Loss 12460565.00000
Epoch 74: Val Loss 12469195.00000
Epoch 75: Val Loss 12447326.00000
Epoch 76: Val Loss 12458242.00000
Epoch 77: Val Loss 12472878.00000
Epoch 78: Val Loss 12446646.00000
Epoch 79: Val Loss 12452406.00000
Epoch 80: Val Loss 12455508.00000
Epoch 81: Val Loss 12525154.00000
Epoch 82: Val Loss 12483655.00000
Epoch 83: Val Loss 12464652.00000
Epoch 84: Val Loss 12467882.00000
Epoch 85: Val Loss 12434086.00000
Epoch 86: Val Loss 12467144.00000
Epoch 87: Val Loss 12467355.00000
Epoch 88: Val Loss 12436246.00000
Epoch 89: Val Loss 12427792.00000
Epoch 90: Val Loss 12447234.00000
Epoch 91: Val Loss 12448386.00000
Epoch 92: Val Loss 12535509.00000
Epoch 93: Val Loss 12462755.00000
Epoch 94: Val Loss 12447166.00000
Epoch 95: Val Loss 12437737.00000
Epoch 96: Val Loss 12462978.00000
Epoch 97: Val Loss 12437182.00000
Epoch 98: Val Loss 12435158.00000
Epoch 99: Val Loss 12460488.00000
{'MSE - mean': 12457356.85544509, 'MSE - std': 122922.38293913245, 'R2 - mean': 0.5176847407848072, 'R2 - std': 0.004215111895761048} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 12457356.85544509 and parameters: {'dim': 128, 'depth': 2, 'heads': 8, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 2 with value: 12443717.191287499.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19298324.00000
Epoch 1: Val Loss 14093405.00000
Epoch 2: Val Loss 13353048.00000
Epoch 3: Val Loss 13176044.00000
Epoch 4: Val Loss 13030968.00000
Epoch 5: Val Loss 12998643.00000
Epoch 6: Val Loss 13040700.00000
Epoch 7: Val Loss 13060180.00000
Epoch 8: Val Loss 12980115.00000
Epoch 9: Val Loss 12942959.00000
Epoch 10: Val Loss 12930938.00000
Epoch 11: Val Loss 12908159.00000
Epoch 12: Val Loss 12909831.00000
Epoch 13: Val Loss 12909328.00000
Epoch 14: Val Loss 12899280.00000
Epoch 15: Val Loss 12868546.00000
Epoch 16: Val Loss 12843884.00000
Epoch 17: Val Loss 12853287.00000
Epoch 18: Val Loss 12858967.00000
Epoch 19: Val Loss 12830453.00000
Epoch 20: Val Loss 12812008.00000
Epoch 21: Val Loss 12790559.00000
Epoch 22: Val Loss 12824231.00000
Epoch 23: Val Loss 12798833.00000
Epoch 24: Val Loss 12790233.00000
Epoch 25: Val Loss 12757914.00000
Epoch 26: Val Loss 12793251.00000
Epoch 27: Val Loss 12767913.00000
Epoch 28: Val Loss 12711553.00000
Epoch 29: Val Loss 12753362.00000
Epoch 30: Val Loss 12751791.00000
Epoch 31: Val Loss 12676257.00000
Epoch 32: Val Loss 12681103.00000
Epoch 33: Val Loss 12644784.00000
Epoch 34: Val Loss 12649020.00000
Epoch 35: Val Loss 12624672.00000
Epoch 36: Val Loss 12613056.00000
Epoch 37: Val Loss 12576953.00000
Epoch 38: Val Loss 12560827.00000
Epoch 39: Val Loss 12559917.00000
Epoch 40: Val Loss 12556761.00000
Epoch 41: Val Loss 12540023.00000
Epoch 42: Val Loss 12531400.00000
Epoch 43: Val Loss 12514030.00000
Epoch 44: Val Loss 12484318.00000
Epoch 45: Val Loss 12472498.00000
Epoch 46: Val Loss 12485127.00000
Epoch 47: Val Loss 12469929.00000
Epoch 48: Val Loss 12465273.00000
Epoch 49: Val Loss 12412262.00000
Epoch 50: Val Loss 12432945.00000
Epoch 51: Val Loss 12419185.00000
Epoch 52: Val Loss 12477207.00000
Epoch 53: Val Loss 12431624.00000
Epoch 54: Val Loss 12421407.00000
Epoch 55: Val Loss 12428573.00000
Epoch 56: Val Loss 12434625.00000
Epoch 57: Val Loss 12386356.00000
Epoch 58: Val Loss 12395852.00000
Epoch 59: Val Loss 12416141.00000
Epoch 60: Val Loss 12448831.00000
Epoch 61: Val Loss 12374332.00000
Epoch 62: Val Loss 12416108.00000
Epoch 63: Val Loss 12409180.00000
Epoch 64: Val Loss 12367857.00000
Epoch 65: Val Loss 12364931.00000
Epoch 66: Val Loss 12356598.00000
Epoch 67: Val Loss 12390685.00000
Epoch 68: Val Loss 12381446.00000
Epoch 69: Val Loss 12366707.00000
Epoch 70: Val Loss 12349509.00000
Epoch 71: Val Loss 12351653.00000
Epoch 72: Val Loss 12352961.00000
Epoch 73: Val Loss 12350212.00000
Epoch 74: Val Loss 12346537.00000
Epoch 75: Val Loss 12343390.00000
Epoch 76: Val Loss 12342143.00000
Epoch 77: Val Loss 12349927.00000
Epoch 78: Val Loss 12342327.00000
Epoch 79: Val Loss 12314293.00000
Epoch 80: Val Loss 12307855.00000
Epoch 81: Val Loss 12410193.00000
Epoch 82: Val Loss 12406731.00000
Epoch 83: Val Loss 12331287.00000
Epoch 84: Val Loss 12309706.00000
Epoch 85: Val Loss 12335451.00000
Epoch 86: Val Loss 12317731.00000
Epoch 87: Val Loss 12316958.00000
Epoch 88: Val Loss 12294814.00000
Epoch 89: Val Loss 12297006.00000
Epoch 90: Val Loss 12359321.00000
Epoch 91: Val Loss 12291479.00000
Epoch 92: Val Loss 12325259.00000
Epoch 93: Val Loss 12314877.00000
Epoch 94: Val Loss 12296044.00000
Epoch 95: Val Loss 12320459.00000
Epoch 96: Val Loss 12277020.00000
Epoch 97: Val Loss 12289388.00000
Epoch 98: Val Loss 12298775.00000
Epoch 99: Val Loss 12271441.00000
{'MSE - mean': 12275009.258263394, 'MSE - std': 0.0, 'R2 - mean': 0.5194829290588492, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 18966512.00000
Epoch 1: Val Loss 14196879.00000
Epoch 2: Val Loss 13444111.00000
Epoch 3: Val Loss 13329482.00000
Epoch 4: Val Loss 13243463.00000
Epoch 5: Val Loss 13203904.00000
Epoch 6: Val Loss 13218540.00000
Epoch 7: Val Loss 13172629.00000
Epoch 8: Val Loss 13169986.00000
Epoch 9: Val Loss 13202966.00000
Epoch 10: Val Loss 13153438.00000
Epoch 11: Val Loss 13173511.00000
Epoch 12: Val Loss 13203138.00000
Epoch 13: Val Loss 13112781.00000
Epoch 14: Val Loss 13197072.00000
Epoch 15: Val Loss 13125287.00000
Epoch 16: Val Loss 13129333.00000
Epoch 17: Val Loss 13203650.00000
Epoch 18: Val Loss 13074522.00000
Epoch 19: Val Loss 13063372.00000
Epoch 20: Val Loss 13073813.00000
Epoch 21: Val Loss 13073988.00000
Epoch 22: Val Loss 13023899.00000
Epoch 23: Val Loss 13023133.00000
Epoch 24: Val Loss 13030973.00000
Epoch 25: Val Loss 12976773.00000
Epoch 26: Val Loss 12951983.00000
Epoch 27: Val Loss 12929299.00000
Epoch 28: Val Loss 12904195.00000
Epoch 29: Val Loss 12867581.00000
Epoch 30: Val Loss 12853276.00000
Epoch 31: Val Loss 12839317.00000
Epoch 32: Val Loss 12785526.00000
Epoch 33: Val Loss 12728083.00000
Epoch 34: Val Loss 12706231.00000
Epoch 35: Val Loss 12711599.00000
Epoch 36: Val Loss 12671738.00000
Epoch 37: Val Loss 12691532.00000
Epoch 38: Val Loss 12623237.00000
Epoch 39: Val Loss 12624279.00000
Epoch 40: Val Loss 12600812.00000
Epoch 41: Val Loss 12595420.00000
Epoch 42: Val Loss 12566323.00000
Epoch 43: Val Loss 12582221.00000
Epoch 44: Val Loss 12568911.00000
Epoch 45: Val Loss 12536428.00000
Epoch 46: Val Loss 12544772.00000
Epoch 47: Val Loss 12541919.00000
Epoch 48: Val Loss 12527165.00000
Epoch 49: Val Loss 12521321.00000
Epoch 50: Val Loss 12527621.00000
Epoch 51: Val Loss 12529496.00000
Epoch 52: Val Loss 12502180.00000
Epoch 53: Val Loss 12533141.00000
Epoch 54: Val Loss 12530289.00000
Epoch 55: Val Loss 12504239.00000
Epoch 56: Val Loss 12494954.00000
Epoch 57: Val Loss 12485136.00000
Epoch 58: Val Loss 12515584.00000
Epoch 59: Val Loss 12493444.00000
Epoch 60: Val Loss 12517041.00000
Epoch 61: Val Loss 12474641.00000
Epoch 62: Val Loss 12473268.00000
Epoch 63: Val Loss 12504420.00000
Epoch 64: Val Loss 12477696.00000
Epoch 65: Val Loss 12486403.00000
Epoch 66: Val Loss 12507280.00000
Epoch 67: Val Loss 12463376.00000
Epoch 68: Val Loss 12509812.00000
Epoch 69: Val Loss 12458527.00000
Epoch 70: Val Loss 12468034.00000
Epoch 71: Val Loss 12450602.00000
Epoch 72: Val Loss 12524558.00000
Epoch 73: Val Loss 12457723.00000
Epoch 74: Val Loss 12454303.00000
Epoch 75: Val Loss 12485884.00000
Epoch 76: Val Loss 12507808.00000
Epoch 77: Val Loss 12535878.00000
Epoch 78: Val Loss 12435634.00000
Epoch 79: Val Loss 12523132.00000
Epoch 80: Val Loss 12436032.00000
Epoch 81: Val Loss 12430733.00000
Epoch 82: Val Loss 12448962.00000
Epoch 83: Val Loss 12483799.00000
Epoch 84: Val Loss 12432765.00000
Epoch 85: Val Loss 12438076.00000
Epoch 86: Val Loss 12436063.00000
Epoch 87: Val Loss 12431178.00000
Epoch 88: Val Loss 12438217.00000
Epoch 89: Val Loss 12432989.00000
Epoch 90: Val Loss 12422275.00000
Epoch 91: Val Loss 12428525.00000
Epoch 92: Val Loss 12413160.00000
Epoch 93: Val Loss 12412716.00000
Epoch 94: Val Loss 12441372.00000
Epoch 95: Val Loss 12446389.00000
Epoch 96: Val Loss 12424349.00000
Epoch 97: Val Loss 12439512.00000
Epoch 98: Val Loss 12478786.00000
Epoch 99: Val Loss 12400265.00000
{'MSE - mean': 12344580.450240195, 'MSE - std': 69571.19197680149, 'R2 - mean': 0.5195999500305586, 'R2 - std': 0.00011702097170940284} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19981254.00000
Epoch 1: Val Loss 14570977.00000
Epoch 2: Val Loss 13624247.00000
Epoch 3: Val Loss 13385052.00000
Epoch 4: Val Loss 13303209.00000
Epoch 5: Val Loss 13240654.00000
Epoch 6: Val Loss 13236947.00000
Epoch 7: Val Loss 13190803.00000
Epoch 8: Val Loss 13155491.00000
Epoch 9: Val Loss 13138045.00000
Epoch 10: Val Loss 13121189.00000
Epoch 11: Val Loss 13096082.00000
Epoch 12: Val Loss 13116714.00000
Epoch 13: Val Loss 13053075.00000
Epoch 14: Val Loss 13043098.00000
Epoch 15: Val Loss 13047420.00000
Epoch 16: Val Loss 13007771.00000
Epoch 17: Val Loss 13018041.00000
Epoch 18: Val Loss 12979689.00000
Epoch 19: Val Loss 12943281.00000
Epoch 20: Val Loss 12946453.00000
Epoch 21: Val Loss 12948731.00000
Epoch 22: Val Loss 12901059.00000
Epoch 23: Val Loss 12908270.00000
Epoch 24: Val Loss 12886291.00000
Epoch 25: Val Loss 12867110.00000
Epoch 26: Val Loss 12851126.00000
Epoch 27: Val Loss 12868185.00000
Epoch 28: Val Loss 12813600.00000
Epoch 29: Val Loss 12793278.00000
Epoch 30: Val Loss 12778491.00000
Epoch 31: Val Loss 12769659.00000
Epoch 32: Val Loss 12758671.00000
Epoch 33: Val Loss 12769752.00000
Epoch 34: Val Loss 12711615.00000
Epoch 35: Val Loss 12729929.00000
Epoch 36: Val Loss 12698978.00000
Epoch 37: Val Loss 12719382.00000
Epoch 38: Val Loss 12703285.00000
Epoch 39: Val Loss 12726939.00000
Epoch 40: Val Loss 12740356.00000
Epoch 41: Val Loss 12680592.00000
Epoch 42: Val Loss 12671927.00000
Epoch 43: Val Loss 12673228.00000
Epoch 44: Val Loss 12671154.00000
Epoch 45: Val Loss 12660450.00000
Epoch 46: Val Loss 12743641.00000
Epoch 47: Val Loss 12657948.00000
Epoch 48: Val Loss 12686106.00000
Epoch 49: Val Loss 12642261.00000
Epoch 50: Val Loss 12642155.00000
Epoch 51: Val Loss 12648131.00000
Epoch 52: Val Loss 12680367.00000
Epoch 53: Val Loss 12660108.00000
Epoch 54: Val Loss 12633171.00000
Epoch 55: Val Loss 12630721.00000
Epoch 56: Val Loss 12652311.00000
Epoch 57: Val Loss 12619345.00000
Epoch 58: Val Loss 12610949.00000
Epoch 59: Val Loss 12623490.00000
Epoch 60: Val Loss 12676418.00000
Epoch 61: Val Loss 12683009.00000
Epoch 62: Val Loss 12629431.00000
Epoch 63: Val Loss 12606876.00000
Epoch 64: Val Loss 12635688.00000
Epoch 65: Val Loss 12630793.00000
Epoch 66: Val Loss 12617595.00000
Epoch 67: Val Loss 12608793.00000
Epoch 68: Val Loss 12645015.00000
Epoch 69: Val Loss 12603046.00000
Epoch 70: Val Loss 12599944.00000
Epoch 71: Val Loss 12629197.00000
Epoch 72: Val Loss 12599437.00000
Epoch 73: Val Loss 12586158.00000
Epoch 74: Val Loss 12567344.00000
Epoch 75: Val Loss 12634377.00000
Epoch 76: Val Loss 12574498.00000
Epoch 77: Val Loss 12591155.00000
Epoch 78: Val Loss 12605439.00000
Epoch 79: Val Loss 12590878.00000
Epoch 80: Val Loss 12588229.00000
Epoch 81: Val Loss 12568171.00000
Epoch 82: Val Loss 12567976.00000
Epoch 83: Val Loss 12557326.00000
Epoch 84: Val Loss 12576418.00000
Epoch 85: Val Loss 12579694.00000
Epoch 86: Val Loss 12579086.00000
Epoch 87: Val Loss 12572461.00000
Epoch 88: Val Loss 12550669.00000
Epoch 89: Val Loss 12615186.00000
Epoch 90: Val Loss 12573629.00000
Epoch 91: Val Loss 12582599.00000
Epoch 92: Val Loss 12564276.00000
Epoch 93: Val Loss 12558025.00000
Epoch 94: Val Loss 12577904.00000
Epoch 95: Val Loss 12576037.00000
Epoch 96: Val Loss 12574495.00000
Epoch 97: Val Loss 12530057.00000
Epoch 98: Val Loss 12554130.00000
Epoch 99: Val Loss 12563843.00000
{'MSE - mean': 12412464.273319667, 'MSE - std': 111549.06561906118, 'R2 - mean': 0.5176203931239806, 'R2 - std': 0.00280114625905916} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 21613968.00000
Epoch 1: Val Loss 14980504.00000
Epoch 2: Val Loss 13794027.00000
Epoch 3: Val Loss 13362323.00000
Epoch 4: Val Loss 13251066.00000
Epoch 5: Val Loss 13199046.00000
Epoch 6: Val Loss 13144389.00000
Epoch 7: Val Loss 13130947.00000
Epoch 8: Val Loss 13114478.00000
Epoch 9: Val Loss 13101870.00000
Epoch 10: Val Loss 13083401.00000
Epoch 11: Val Loss 13099877.00000
Epoch 12: Val Loss 13085037.00000
Epoch 13: Val Loss 13067926.00000
Epoch 14: Val Loss 13043532.00000
Epoch 15: Val Loss 13060635.00000
Epoch 16: Val Loss 13125890.00000
Epoch 17: Val Loss 13069576.00000
Epoch 18: Val Loss 13031631.00000
Epoch 19: Val Loss 13008022.00000
Epoch 20: Val Loss 13029979.00000
Epoch 21: Val Loss 12999965.00000
Epoch 22: Val Loss 13005274.00000
Epoch 23: Val Loss 13053651.00000
Epoch 24: Val Loss 12993442.00000
Epoch 25: Val Loss 12987546.00000
Epoch 26: Val Loss 12979487.00000
Epoch 27: Val Loss 12959894.00000
Epoch 28: Val Loss 12953450.00000
Epoch 29: Val Loss 12987124.00000
Epoch 30: Val Loss 12941015.00000
Epoch 31: Val Loss 12933471.00000
Epoch 32: Val Loss 12909545.00000
Epoch 33: Val Loss 12892001.00000
Epoch 34: Val Loss 12876343.00000
Epoch 35: Val Loss 12858148.00000
Epoch 36: Val Loss 12841835.00000
Epoch 37: Val Loss 12830975.00000
Epoch 38: Val Loss 12825551.00000
Epoch 39: Val Loss 12856803.00000
Epoch 40: Val Loss 12791917.00000
Epoch 41: Val Loss 12782745.00000
Epoch 42: Val Loss 12773022.00000
Epoch 43: Val Loss 12752341.00000
Epoch 44: Val Loss 12798708.00000
Epoch 45: Val Loss 12742245.00000
Epoch 46: Val Loss 12692433.00000
Epoch 47: Val Loss 12685918.00000
Epoch 48: Val Loss 12669459.00000
Epoch 49: Val Loss 12677986.00000
Epoch 50: Val Loss 12658050.00000
Epoch 51: Val Loss 12664676.00000
Epoch 52: Val Loss 12614451.00000
Epoch 53: Val Loss 12587016.00000
Epoch 54: Val Loss 12567631.00000
Epoch 55: Val Loss 12557188.00000
Epoch 56: Val Loss 12530227.00000
Epoch 57: Val Loss 12552971.00000
Epoch 58: Val Loss 12532080.00000
Epoch 59: Val Loss 12525222.00000
Epoch 60: Val Loss 12507881.00000
Epoch 61: Val Loss 12509022.00000
Epoch 62: Val Loss 12502996.00000
Epoch 63: Val Loss 12494775.00000
Epoch 64: Val Loss 12588555.00000
Epoch 65: Val Loss 12678793.00000
Epoch 66: Val Loss 12477889.00000
Epoch 67: Val Loss 12458315.00000
Epoch 68: Val Loss 12467697.00000
Epoch 69: Val Loss 12498409.00000
Epoch 70: Val Loss 12487190.00000
Epoch 71: Val Loss 12468051.00000
Epoch 72: Val Loss 12451114.00000
Epoch 73: Val Loss 12461429.00000
Epoch 74: Val Loss 12460625.00000
Epoch 75: Val Loss 12439515.00000
Epoch 76: Val Loss 12447069.00000
Epoch 77: Val Loss 12442929.00000
Epoch 78: Val Loss 12457012.00000
Epoch 79: Val Loss 12411920.00000
Epoch 80: Val Loss 12436682.00000
Epoch 81: Val Loss 12441984.00000
Epoch 82: Val Loss 12445369.00000
Epoch 83: Val Loss 12432601.00000
Epoch 84: Val Loss 12430690.00000
Epoch 85: Val Loss 12434753.00000
Epoch 86: Val Loss 12427912.00000
Epoch 87: Val Loss 12426179.00000
Epoch 88: Val Loss 12431938.00000
Epoch 89: Val Loss 12427793.00000
Epoch 90: Val Loss 12408100.00000
Epoch 91: Val Loss 12462392.00000
Epoch 92: Val Loss 12406124.00000
Epoch 93: Val Loss 12422180.00000
Epoch 94: Val Loss 12435067.00000
Epoch 95: Val Loss 12440097.00000
Epoch 96: Val Loss 12390532.00000
Epoch 97: Val Loss 12407040.00000
Epoch 98: Val Loss 12418017.00000
Epoch 99: Val Loss 12412366.00000
{'MSE - mean': 12410217.662692701, 'MSE - std': 96682.66291477391, 'R2 - mean': 0.5183432551728371, 'R2 - std': 0.00272990913740949} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19061004.00000
Epoch 1: Val Loss 14417764.00000
Epoch 2: Val Loss 13553908.00000
Epoch 3: Val Loss 13272420.00000
Epoch 4: Val Loss 13193390.00000
Epoch 5: Val Loss 13217189.00000
Epoch 6: Val Loss 13181301.00000
Epoch 7: Val Loss 13142987.00000
Epoch 8: Val Loss 13170658.00000
Epoch 9: Val Loss 13114507.00000
Epoch 10: Val Loss 13102309.00000
Epoch 11: Val Loss 13080712.00000
Epoch 12: Val Loss 13073722.00000
Epoch 13: Val Loss 13083644.00000
Epoch 14: Val Loss 13061775.00000
Epoch 15: Val Loss 13070741.00000
Epoch 16: Val Loss 13100755.00000
Epoch 17: Val Loss 13067090.00000
Epoch 18: Val Loss 13030126.00000
Epoch 19: Val Loss 13009618.00000
Epoch 20: Val Loss 13002046.00000
Epoch 21: Val Loss 12981362.00000
Epoch 22: Val Loss 12995912.00000
Epoch 23: Val Loss 13000844.00000
Epoch 24: Val Loss 12942342.00000
Epoch 25: Val Loss 13035442.00000
Epoch 26: Val Loss 12994257.00000
Epoch 27: Val Loss 12933546.00000
Epoch 28: Val Loss 12918528.00000
Epoch 29: Val Loss 12936253.00000
Epoch 30: Val Loss 12901242.00000
Epoch 31: Val Loss 12902710.00000
Epoch 32: Val Loss 12932028.00000
Epoch 33: Val Loss 12845753.00000
Epoch 34: Val Loss 12882049.00000
Epoch 35: Val Loss 12826372.00000
Epoch 36: Val Loss 12815161.00000
Epoch 37: Val Loss 12831878.00000
Epoch 38: Val Loss 12814326.00000
Epoch 39: Val Loss 12778694.00000
Epoch 40: Val Loss 12839990.00000
Epoch 41: Val Loss 12778903.00000
Epoch 42: Val Loss 12794414.00000
Epoch 43: Val Loss 12792726.00000
Epoch 44: Val Loss 12763642.00000
Epoch 45: Val Loss 12751327.00000
Epoch 46: Val Loss 12746809.00000
Epoch 47: Val Loss 12768416.00000
Epoch 48: Val Loss 12733351.00000
Epoch 49: Val Loss 12722743.00000
Epoch 50: Val Loss 12686067.00000
Epoch 51: Val Loss 12701446.00000
Epoch 52: Val Loss 12688533.00000
Epoch 53: Val Loss 12638594.00000
Epoch 54: Val Loss 12659770.00000
Epoch 55: Val Loss 12650671.00000
Epoch 56: Val Loss 12631647.00000
Epoch 57: Val Loss 12622485.00000
Epoch 58: Val Loss 12615866.00000
Epoch 59: Val Loss 12584460.00000
Epoch 60: Val Loss 12586849.00000
Epoch 61: Val Loss 12558736.00000
Epoch 62: Val Loss 12553550.00000
Epoch 63: Val Loss 12587453.00000
Epoch 64: Val Loss 12561194.00000
Epoch 65: Val Loss 12539555.00000
Epoch 66: Val Loss 12508847.00000
Epoch 67: Val Loss 12521777.00000
Epoch 68: Val Loss 12547734.00000
Epoch 69: Val Loss 12500519.00000
Epoch 70: Val Loss 12519025.00000
Epoch 71: Val Loss 12522302.00000
Epoch 72: Val Loss 12499467.00000
Epoch 73: Val Loss 12513310.00000
Epoch 74: Val Loss 12510045.00000
Epoch 75: Val Loss 12502181.00000
Epoch 76: Val Loss 12512169.00000
Epoch 77: Val Loss 12462714.00000
Epoch 78: Val Loss 12510841.00000
Epoch 79: Val Loss 12505735.00000
Epoch 80: Val Loss 12533987.00000
Epoch 81: Val Loss 12501228.00000
Epoch 82: Val Loss 12517720.00000
Epoch 83: Val Loss 12487724.00000
Epoch 84: Val Loss 12542340.00000
Epoch 85: Val Loss 12460616.00000
Epoch 86: Val Loss 12586309.00000
Epoch 87: Val Loss 12469802.00000
Epoch 88: Val Loss 12478615.00000
Epoch 89: Val Loss 12483644.00000
Epoch 90: Val Loss 12496947.00000
Epoch 91: Val Loss 12477333.00000
Epoch 92: Val Loss 12478734.00000
Epoch 93: Val Loss 12509988.00000
Epoch 94: Val Loss 12475036.00000
Epoch 95: Val Loss 12504647.00000
Epoch 96: Val Loss 12489525.00000
Epoch 97: Val Loss 12481523.00000
Epoch 98: Val Loss 12472700.00000
Epoch 99: Val Loss 12453579.00000
{'MSE - mean': 12421735.392065033, 'MSE - std': 89491.12920401992, 'R2 - mean': 0.5190629205265049, 'R2 - std': 0.0028343598931484886} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 12421735.392065033 and parameters: {'dim': 128, 'depth': 2, 'heads': 8, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.3}. Best is trial 13 with value: 12421735.392065033.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 17506036.00000
Epoch 1: Val Loss 13827408.00000
Epoch 2: Val Loss 13223419.00000
Epoch 3: Val Loss 13177117.00000
Epoch 4: Val Loss 13033018.00000
Epoch 5: Val Loss 13005836.00000
Epoch 6: Val Loss 13021033.00000
Epoch 7: Val Loss 12956178.00000
Epoch 8: Val Loss 12958828.00000
Epoch 9: Val Loss 12972150.00000
Epoch 10: Val Loss 12924358.00000
Epoch 11: Val Loss 12974566.00000
Epoch 12: Val Loss 12951014.00000
Epoch 13: Val Loss 12942795.00000
Epoch 14: Val Loss 12911987.00000
Epoch 15: Val Loss 12879619.00000
Epoch 16: Val Loss 12880423.00000
Epoch 17: Val Loss 12838654.00000
Epoch 18: Val Loss 12837780.00000
Epoch 19: Val Loss 12870405.00000
Epoch 20: Val Loss 12826880.00000
Epoch 21: Val Loss 12789531.00000
Epoch 22: Val Loss 12809388.00000
Epoch 23: Val Loss 12757136.00000
Epoch 24: Val Loss 12743029.00000
Epoch 25: Val Loss 12720194.00000
Epoch 26: Val Loss 12750452.00000
Epoch 27: Val Loss 12867984.00000
Epoch 28: Val Loss 12715848.00000
Epoch 29: Val Loss 12683436.00000
Epoch 30: Val Loss 12744594.00000
Epoch 31: Val Loss 12655777.00000
Epoch 32: Val Loss 12669772.00000
Epoch 33: Val Loss 12636298.00000
Epoch 34: Val Loss 12613305.00000
Epoch 35: Val Loss 12620147.00000
Epoch 36: Val Loss 12611038.00000
Epoch 37: Val Loss 12576435.00000
Epoch 38: Val Loss 12663596.00000
Epoch 39: Val Loss 12593796.00000
Epoch 40: Val Loss 12625085.00000
Epoch 41: Val Loss 12602086.00000
Epoch 42: Val Loss 12512991.00000
Epoch 43: Val Loss 12478990.00000
Epoch 44: Val Loss 12522565.00000
Epoch 45: Val Loss 12521148.00000
Epoch 46: Val Loss 12631105.00000
Epoch 47: Val Loss 12446527.00000
Epoch 48: Val Loss 12446572.00000
Epoch 49: Val Loss 12472931.00000
Epoch 50: Val Loss 12443523.00000
Epoch 51: Val Loss 12426908.00000
Epoch 52: Val Loss 12417390.00000
Epoch 53: Val Loss 12415140.00000
Epoch 54: Val Loss 12413879.00000
Epoch 55: Val Loss 12423835.00000
Epoch 56: Val Loss 12442092.00000
Epoch 57: Val Loss 12417981.00000
Epoch 58: Val Loss 12427982.00000
Epoch 59: Val Loss 12420052.00000
Epoch 60: Val Loss 12390967.00000
Epoch 61: Val Loss 12401776.00000
Epoch 62: Val Loss 12383007.00000
Epoch 63: Val Loss 12392893.00000
Epoch 64: Val Loss 12405687.00000
Epoch 65: Val Loss 12394135.00000
Epoch 66: Val Loss 12382622.00000
Epoch 67: Val Loss 12385616.00000
Epoch 68: Val Loss 12414171.00000
Epoch 69: Val Loss 12375867.00000
Epoch 70: Val Loss 12397343.00000
Epoch 71: Val Loss 12376538.00000
Epoch 72: Val Loss 12377605.00000
Epoch 73: Val Loss 12372902.00000
Epoch 74: Val Loss 12355793.00000
Epoch 75: Val Loss 12376997.00000
Epoch 76: Val Loss 12394231.00000
Epoch 77: Val Loss 12383852.00000
Epoch 78: Val Loss 12462288.00000
Epoch 79: Val Loss 12435529.00000
Epoch 80: Val Loss 12357444.00000
Epoch 81: Val Loss 12367581.00000
Epoch 82: Val Loss 12376027.00000
Epoch 83: Val Loss 12375244.00000
Epoch 84: Val Loss 12370389.00000
Epoch 85: Val Loss 12382097.00000
Epoch 86: Val Loss 12376026.00000
Epoch 87: Val Loss 12389832.00000
Epoch 88: Val Loss 12354888.00000
Epoch 89: Val Loss 12357996.00000
Epoch 90: Val Loss 12379475.00000
Epoch 91: Val Loss 12379108.00000
Epoch 92: Val Loss 12380717.00000
Epoch 93: Val Loss 12403363.00000
Epoch 94: Val Loss 12350624.00000
Epoch 95: Val Loss 12360137.00000
Epoch 96: Val Loss 12356321.00000
Epoch 97: Val Loss 12368044.00000
Epoch 98: Val Loss 12481438.00000
Epoch 99: Val Loss 12341162.00000
{'MSE - mean': 12355825.103801984, 'MSE - std': 0.0, 'R2 - mean': 0.5163193148760172, 'R2 - std': 0.0} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20304428.00000
Epoch 1: Val Loss 14420901.00000
Epoch 2: Val Loss 13537513.00000
Epoch 3: Val Loss 13317125.00000
Epoch 4: Val Loss 13309915.00000
Epoch 5: Val Loss 13256211.00000
Epoch 6: Val Loss 13211798.00000
Epoch 7: Val Loss 13187522.00000
Epoch 8: Val Loss 13239752.00000
Epoch 9: Val Loss 13159821.00000
Epoch 10: Val Loss 13170343.00000
Epoch 11: Val Loss 13139497.00000
Epoch 12: Val Loss 13140494.00000
Epoch 13: Val Loss 13124409.00000
Epoch 14: Val Loss 13117474.00000
Epoch 15: Val Loss 13123363.00000
Epoch 16: Val Loss 13138377.00000
Epoch 17: Val Loss 13119522.00000
Epoch 18: Val Loss 13127278.00000
Epoch 19: Val Loss 13117598.00000
Epoch 20: Val Loss 13089358.00000
Epoch 21: Val Loss 13085274.00000
Epoch 22: Val Loss 13078415.00000
Epoch 23: Val Loss 13085656.00000
Epoch 24: Val Loss 13067533.00000
Epoch 25: Val Loss 13084937.00000
Epoch 26: Val Loss 13087075.00000
Epoch 27: Val Loss 13073991.00000
Epoch 28: Val Loss 13062376.00000
Epoch 29: Val Loss 13118248.00000
Epoch 30: Val Loss 13059782.00000
Epoch 31: Val Loss 13057034.00000
Epoch 32: Val Loss 13023100.00000
Epoch 33: Val Loss 13071005.00000
Epoch 34: Val Loss 13052593.00000
Epoch 35: Val Loss 13044776.00000
Epoch 36: Val Loss 13032813.00000
Epoch 37: Val Loss 13017704.00000
Epoch 38: Val Loss 13038598.00000
Epoch 39: Val Loss 13060436.00000
Epoch 40: Val Loss 13051136.00000
Epoch 41: Val Loss 13016542.00000
Epoch 42: Val Loss 12999475.00000
Epoch 43: Val Loss 13042213.00000
Epoch 44: Val Loss 13009867.00000
Epoch 45: Val Loss 12994116.00000
Epoch 46: Val Loss 12983219.00000
Epoch 47: Val Loss 12985156.00000
Epoch 48: Val Loss 12966594.00000
Epoch 49: Val Loss 12981492.00000
Epoch 50: Val Loss 12970367.00000
Epoch 51: Val Loss 12972296.00000
Epoch 52: Val Loss 13003577.00000
Epoch 53: Val Loss 12939829.00000
Epoch 54: Val Loss 12935871.00000
Epoch 55: Val Loss 12938950.00000
Epoch 56: Val Loss 12958317.00000
Epoch 57: Val Loss 12926295.00000
Epoch 58: Val Loss 12971773.00000
Epoch 59: Val Loss 12929473.00000
Epoch 60: Val Loss 12931900.00000
Epoch 61: Val Loss 12907109.00000
Epoch 62: Val Loss 12901054.00000
Epoch 63: Val Loss 12941140.00000
Epoch 64: Val Loss 12913772.00000
Epoch 65: Val Loss 12932882.00000
Epoch 66: Val Loss 12896412.00000
Epoch 67: Val Loss 12876313.00000
Epoch 68: Val Loss 12899407.00000
Epoch 69: Val Loss 12877719.00000
Epoch 70: Val Loss 12865382.00000
Epoch 71: Val Loss 12853487.00000
Epoch 72: Val Loss 12826706.00000
Epoch 73: Val Loss 12826988.00000
Epoch 74: Val Loss 12821228.00000
Epoch 75: Val Loss 12774236.00000
Epoch 76: Val Loss 12788132.00000
Epoch 77: Val Loss 12743154.00000
Epoch 78: Val Loss 12717458.00000
Epoch 79: Val Loss 12743574.00000
Epoch 80: Val Loss 12697857.00000
Epoch 81: Val Loss 12699314.00000
Epoch 82: Val Loss 12727594.00000
Epoch 83: Val Loss 12680281.00000
Epoch 84: Val Loss 12648389.00000
Epoch 85: Val Loss 12649460.00000
Epoch 86: Val Loss 12694963.00000
Epoch 87: Val Loss 12651247.00000
Epoch 88: Val Loss 12620972.00000
Epoch 89: Val Loss 12617603.00000
Epoch 90: Val Loss 12608089.00000
Epoch 91: Val Loss 12605415.00000
Epoch 92: Val Loss 12613692.00000
Epoch 93: Val Loss 12610738.00000
Epoch 94: Val Loss 12567442.00000
Epoch 95: Val Loss 12598335.00000
Epoch 96: Val Loss 12586840.00000
Epoch 97: Val Loss 12587498.00000
Epoch 98: Val Loss 12562211.00000
Epoch 99: Val Loss 12619768.00000
{'MSE - mean': 12461113.322493043, 'MSE - std': 105288.21869105846, 'R2 - mean': 0.5150729943398679, 'R2 - std': 0.00124632053614937} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 20819190.00000
Epoch 1: Val Loss 14635329.00000
Epoch 2: Val Loss 13664389.00000
Epoch 3: Val Loss 13342245.00000
Epoch 4: Val Loss 13298838.00000
Epoch 5: Val Loss 13254856.00000
Epoch 6: Val Loss 13230903.00000
Epoch 7: Val Loss 13202157.00000
Epoch 8: Val Loss 13168133.00000
Epoch 9: Val Loss 13177331.00000
Epoch 10: Val Loss 13227271.00000
Epoch 11: Val Loss 13173855.00000
Epoch 12: Val Loss 13139164.00000
Epoch 13: Val Loss 13181655.00000
Epoch 14: Val Loss 13120133.00000
Epoch 15: Val Loss 13138734.00000
Epoch 16: Val Loss 13175827.00000
Epoch 17: Val Loss 13131242.00000
Epoch 18: Val Loss 13106002.00000
Epoch 19: Val Loss 13125323.00000
Epoch 20: Val Loss 13104242.00000
Epoch 21: Val Loss 13084655.00000
Epoch 22: Val Loss 13072544.00000
Epoch 23: Val Loss 13062465.00000
Epoch 24: Val Loss 13069436.00000
Epoch 25: Val Loss 13027148.00000
Epoch 26: Val Loss 13027813.00000
Epoch 27: Val Loss 13044769.00000
Epoch 28: Val Loss 13138785.00000
Epoch 29: Val Loss 13025398.00000
Epoch 30: Val Loss 13059495.00000
Epoch 31: Val Loss 13041919.00000
Epoch 32: Val Loss 12998455.00000
Epoch 33: Val Loss 12989274.00000
Epoch 34: Val Loss 13004131.00000
Epoch 35: Val Loss 12986196.00000
Epoch 36: Val Loss 12968509.00000
Epoch 37: Val Loss 12983099.00000
Epoch 38: Val Loss 12946439.00000
Epoch 39: Val Loss 12940091.00000
Epoch 40: Val Loss 12953356.00000
Epoch 41: Val Loss 12916775.00000
Epoch 42: Val Loss 12939646.00000
Epoch 43: Val Loss 12935235.00000
Epoch 44: Val Loss 12930818.00000
Epoch 45: Val Loss 12902616.00000
Epoch 46: Val Loss 12898790.00000
Epoch 47: Val Loss 12897930.00000
Epoch 48: Val Loss 12879969.00000
Epoch 49: Val Loss 12856371.00000
Epoch 50: Val Loss 12861725.00000
Epoch 51: Val Loss 12853433.00000
Epoch 52: Val Loss 12825951.00000
Epoch 53: Val Loss 12790758.00000
Epoch 54: Val Loss 12797898.00000
Epoch 55: Val Loss 12771103.00000
Epoch 56: Val Loss 12747939.00000
Epoch 57: Val Loss 12760561.00000
Epoch 58: Val Loss 12738898.00000
Epoch 59: Val Loss 12723751.00000
Epoch 60: Val Loss 12729858.00000
Epoch 61: Val Loss 12703374.00000
Epoch 62: Val Loss 12676974.00000
Epoch 63: Val Loss 12659715.00000
Epoch 64: Val Loss 12710337.00000
Epoch 65: Val Loss 12670272.00000
Epoch 66: Val Loss 12630114.00000
Epoch 67: Val Loss 12653230.00000
Epoch 68: Val Loss 12629779.00000
Epoch 69: Val Loss 12637471.00000
Epoch 70: Val Loss 12649738.00000
Epoch 71: Val Loss 12616142.00000
Epoch 72: Val Loss 12664787.00000
Epoch 73: Val Loss 12625687.00000
Epoch 74: Val Loss 12609732.00000
Epoch 75: Val Loss 12603233.00000
Epoch 76: Val Loss 12594075.00000
Epoch 77: Val Loss 12604641.00000
Epoch 78: Val Loss 12640112.00000
Epoch 79: Val Loss 12582901.00000
Epoch 80: Val Loss 12585923.00000
Epoch 81: Val Loss 12577918.00000
Epoch 82: Val Loss 12701887.00000
Epoch 83: Val Loss 12604963.00000
Epoch 84: Val Loss 12568575.00000
Epoch 85: Val Loss 12573261.00000
Epoch 86: Val Loss 12584409.00000
Epoch 87: Val Loss 12647461.00000
Epoch 88: Val Loss 12574690.00000
Epoch 89: Val Loss 12564205.00000
Epoch 90: Val Loss 12563898.00000
Epoch 91: Val Loss 12579125.00000
Epoch 92: Val Loss 12544900.00000
Epoch 93: Val Loss 12579534.00000
Epoch 94: Val Loss 12553066.00000
Epoch 95: Val Loss 12576714.00000
Epoch 96: Val Loss 12563024.00000
Epoch 97: Val Loss 12584253.00000
Epoch 98: Val Loss 12546777.00000
Epoch 99: Val Loss 12606512.00000
{'MSE - mean': 12491089.684545616, 'MSE - std': 95851.81563523614, 'R2 - mean': 0.5145661134390477, 'R2 - std': 0.0012447488693646121} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19027980.00000
Epoch 1: Val Loss 14220000.00000
Epoch 2: Val Loss 13445943.00000
Epoch 3: Val Loss 13247006.00000
Epoch 4: Val Loss 13149786.00000
Epoch 5: Val Loss 13169069.00000
Epoch 6: Val Loss 13102725.00000
Epoch 7: Val Loss 13116556.00000
Epoch 8: Val Loss 13116340.00000
Epoch 9: Val Loss 13082038.00000
Epoch 10: Val Loss 13052081.00000
Epoch 11: Val Loss 13056395.00000
Epoch 12: Val Loss 13032225.00000
Epoch 13: Val Loss 13036390.00000
Epoch 14: Val Loss 13011611.00000
Epoch 15: Val Loss 13002628.00000
Epoch 16: Val Loss 12985170.00000
Epoch 17: Val Loss 12940497.00000
Epoch 18: Val Loss 12951627.00000
Epoch 19: Val Loss 12952768.00000
Epoch 20: Val Loss 12943872.00000
Epoch 21: Val Loss 12903481.00000
Epoch 22: Val Loss 12901106.00000
Epoch 23: Val Loss 12893472.00000
Epoch 24: Val Loss 12868240.00000
Epoch 25: Val Loss 12865056.00000
Epoch 26: Val Loss 12887648.00000
Epoch 27: Val Loss 12840874.00000
Epoch 28: Val Loss 12805888.00000
Epoch 29: Val Loss 12788298.00000
Epoch 30: Val Loss 12773379.00000
Epoch 31: Val Loss 12796996.00000
Epoch 32: Val Loss 12760635.00000
Epoch 33: Val Loss 12741527.00000
Epoch 34: Val Loss 12737607.00000
Epoch 35: Val Loss 12722348.00000
Epoch 36: Val Loss 12731298.00000
Epoch 37: Val Loss 12741009.00000
Epoch 38: Val Loss 12725002.00000
Epoch 39: Val Loss 12705884.00000
Epoch 40: Val Loss 12686516.00000
Epoch 41: Val Loss 12679951.00000
Epoch 42: Val Loss 12667890.00000
Epoch 43: Val Loss 12633676.00000
Epoch 44: Val Loss 12624073.00000
Epoch 45: Val Loss 12635246.00000
Epoch 46: Val Loss 12687172.00000
Epoch 47: Val Loss 12621389.00000
Epoch 48: Val Loss 12612964.00000
Epoch 49: Val Loss 12578225.00000
Epoch 50: Val Loss 12638786.00000
Epoch 51: Val Loss 12592946.00000
Epoch 52: Val Loss 12584554.00000
Epoch 53: Val Loss 12559871.00000
Epoch 54: Val Loss 12548836.00000
Epoch 55: Val Loss 12555846.00000
Epoch 56: Val Loss 12553256.00000
Epoch 57: Val Loss 12551944.00000
Epoch 58: Val Loss 12547398.00000
Epoch 59: Val Loss 12563637.00000
Epoch 60: Val Loss 12534901.00000
Epoch 61: Val Loss 12500435.00000
Epoch 62: Val Loss 12531378.00000
Epoch 63: Val Loss 12517848.00000
Epoch 64: Val Loss 12519432.00000
Epoch 65: Val Loss 12513193.00000
Epoch 66: Val Loss 12517147.00000
Epoch 67: Val Loss 12494954.00000
Epoch 68: Val Loss 12527626.00000
Epoch 69: Val Loss 12596919.00000
Epoch 70: Val Loss 12485828.00000
Epoch 71: Val Loss 12517527.00000
Epoch 72: Val Loss 12522129.00000
Epoch 73: Val Loss 12502306.00000
Epoch 74: Val Loss 12495486.00000
Epoch 75: Val Loss 12483878.00000
Epoch 76: Val Loss 12504153.00000
Epoch 77: Val Loss 12484469.00000
Epoch 78: Val Loss 12489471.00000
Epoch 79: Val Loss 12499798.00000
Epoch 80: Val Loss 12488722.00000
Epoch 81: Val Loss 12483899.00000
Epoch 82: Val Loss 12508361.00000
Epoch 83: Val Loss 12543977.00000
Epoch 84: Val Loss 12491336.00000
Epoch 85: Val Loss 12481976.00000
Epoch 86: Val Loss 12465558.00000
Epoch 87: Val Loss 12537594.00000
Epoch 88: Val Loss 12479979.00000
Epoch 89: Val Loss 12477125.00000
Epoch 90: Val Loss 12487542.00000
Epoch 91: Val Loss 12451819.00000
Epoch 92: Val Loss 12466410.00000
Epoch 93: Val Loss 12459366.00000
Epoch 94: Val Loss 12474377.00000
Epoch 95: Val Loss 12478772.00000
Epoch 96: Val Loss 12467434.00000
Epoch 97: Val Loss 12467421.00000
Epoch 98: Val Loss 12514318.00000
Epoch 99: Val Loss 12467961.00000
{'MSE - mean': 12484877.835159978, 'MSE - std': 83704.47502253421, 'R2 - mean': 0.5154459652640313, 'R2 - std': 0.0018666728456319796} 
 

In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 19604192.00000
Epoch 1: Val Loss 14422024.00000
Epoch 2: Val Loss 13536887.00000
Epoch 3: Val Loss 13273644.00000
Epoch 4: Val Loss 13218269.00000
Epoch 5: Val Loss 13167836.00000
Epoch 6: Val Loss 13164115.00000
Epoch 7: Val Loss 13142558.00000
Epoch 8: Val Loss 13111746.00000
Epoch 9: Val Loss 13183002.00000
Epoch 10: Val Loss 13176710.00000
Epoch 11: Val Loss 13104316.00000
Epoch 12: Val Loss 13051616.00000
Epoch 13: Val Loss 13031018.00000
Epoch 14: Val Loss 13095193.00000
Epoch 15: Val Loss 13059004.00000
Epoch 16: Val Loss 13015098.00000
Epoch 17: Val Loss 13026551.00000
Epoch 18: Val Loss 12968613.00000
Epoch 19: Val Loss 12990484.00000
Epoch 20: Val Loss 12938088.00000
Epoch 21: Val Loss 12921837.00000
Epoch 22: Val Loss 12891659.00000
Epoch 23: Val Loss 12906609.00000
Epoch 24: Val Loss 12880165.00000
Epoch 25: Val Loss 12832611.00000
Epoch 26: Val Loss 12825332.00000
Epoch 27: Val Loss 12785072.00000
Epoch 28: Val Loss 12776533.00000
Epoch 29: Val Loss 12773319.00000
Epoch 30: Val Loss 12747364.00000
Epoch 31: Val Loss 12742664.00000
Epoch 32: Val Loss 12781310.00000
Epoch 33: Val Loss 12714685.00000
Epoch 34: Val Loss 12711150.00000
Epoch 35: Val Loss 12702128.00000
Epoch 36: Val Loss 12719098.00000
Epoch 37: Val Loss 12666427.00000
Epoch 38: Val Loss 12678386.00000
Epoch 39: Val Loss 12679539.00000
Epoch 40: Val Loss 12652813.00000
Epoch 41: Val Loss 12655241.00000
Epoch 42: Val Loss 12686792.00000
Epoch 43: Val Loss 12630862.00000
Epoch 44: Val Loss 12653913.00000
Epoch 45: Val Loss 12625417.00000
Epoch 46: Val Loss 12590066.00000
Epoch 47: Val Loss 12595736.00000
Epoch 48: Val Loss 12605722.00000
Epoch 49: Val Loss 12593377.00000
