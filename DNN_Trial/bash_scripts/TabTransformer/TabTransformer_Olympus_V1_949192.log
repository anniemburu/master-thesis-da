

----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/boston.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/boston.yml', data_parallel=False, dataset='Boston', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[3], num_classes=1, num_features=13, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Boston...
Dataset loaded! 

X b4 encoding : [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01
 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00] 

(506, 13)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [3]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [3]
Cat Idx Part II: [3] 
ENDE 
 

X after Nominal Encoding: [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01
 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00] 
 

Scaling the data...
X after Scaling: [-0.41978194  0.28482986 -1.2879095   0.         -0.14421743  0.41367189
 -0.12001342  0.1402136  -0.98284286 -0.66660821 -1.45900038  0.44105193
 -1.0755623 ] 
 

One Hot Encoding...
X after One Hot Encoding: [ 1.          0.         -0.41978194  0.28482986 -1.2879095  -0.14421743
  0.41367189 -0.12001342  0.1402136  -0.98284286 -0.66660821 -1.45900038
  0.44105193 -1.0755623 ] 
 

args.num_features: 14
args.cat_idx: None
Cat Dims: []
New Shape: (506, 14)
False 
 

Using an existing study with name 'TabTransformer_Boston' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 543.46167
Epoch 1: Val Loss 543.46112
Epoch 2: Val Loss 543.46051
Epoch 3: Val Loss 543.45990
Epoch 4: Val Loss 543.45935
Epoch 5: Val Loss 543.45880
Epoch 6: Val Loss 543.45819
Epoch 7: Val Loss 543.45764
Epoch 8: Val Loss 543.45703
Epoch 9: Val Loss 543.45654
Epoch 10: Val Loss 543.45587
Epoch 11: Val Loss 543.45538
Epoch 12: Val Loss 543.45477
Epoch 13: Val Loss 543.45422
Epoch 14: Val Loss 543.45361
Epoch 15: Val Loss 543.45300
Epoch 16: Val Loss 543.45245
Epoch 17: Val Loss 543.45184
Epoch 18: Val Loss 543.45129
Epoch 19: Val Loss 543.45068
Epoch 20: Val Loss 543.45013
Epoch 21: Val Loss 543.44952
Epoch 22: Val Loss 543.44891
Epoch 23: Val Loss 543.44836
Epoch 24: Val Loss 543.44775
Epoch 25: Val Loss 543.44714
Epoch 26: Val Loss 543.44666
Epoch 27: Val Loss 543.44604
Epoch 28: Val Loss 543.44543
Epoch 29: Val Loss 543.44489
Epoch 30: Val Loss 543.44427
Epoch 31: Val Loss 543.44373
Epoch 32: Val Loss 543.44312
Epoch 33: Val Loss 543.44250
Epoch 34: Val Loss 543.44196
Epoch 35: Val Loss 543.44141
Epoch 36: Val Loss 543.44080
Epoch 37: Val Loss 543.44025
Epoch 38: Val Loss 543.43970
Epoch 39: Val Loss 543.43909
Epoch 40: Val Loss 543.43854
Epoch 41: Val Loss 543.43799
Epoch 42: Val Loss 543.43738
Epoch 43: Val Loss 543.43677
Epoch 44: Val Loss 543.43622
Epoch 45: Val Loss 543.43567
Epoch 46: Val Loss 543.43506
Epoch 47: Val Loss 543.43445
Epoch 48: Val Loss 543.43390
Epoch 49: Val Loss 543.43329
Epoch 50: Val Loss 543.43274
Epoch 51: Val Loss 543.43213
Epoch 52: Val Loss 543.43164
Epoch 53: Val Loss 543.43103
Epoch 54: Val Loss 543.43042
Epoch 55: Val Loss 543.42987
Epoch 56: Val Loss 543.42926
Epoch 57: Val Loss 543.42871
Epoch 58: Val Loss 543.42810
Epoch 59: Val Loss 543.42755
Epoch 60: Val Loss 543.42700
Epoch 61: Val Loss 543.42645
Epoch 62: Val Loss 543.42584
Epoch 63: Val Loss 543.42529
Epoch 64: Val Loss 543.42462
Epoch 65: Val Loss 543.42407
Epoch 66: Val Loss 543.42352
Epoch 67: Val Loss 543.42291
Epoch 68: Val Loss 543.42236
Epoch 69: Val Loss 543.42175
Epoch 70: Val Loss 543.42120
Epoch 71: Val Loss 543.42065
Epoch 72: Val Loss 543.42010
Epoch 73: Val Loss 543.41949
Epoch 74: Val Loss 543.41895
Epoch 75: Val Loss 543.41840
Epoch 76: Val Loss 543.41779
Epoch 77: Val Loss 543.41724
Epoch 78: Val Loss 543.41663
Epoch 79: Val Loss 543.41608
Epoch 80: Val Loss 543.41553
Epoch 81: Val Loss 543.41486
Epoch 82: Val Loss 543.41431
Epoch 83: Val Loss 543.41376
Epoch 84: Val Loss 543.41315
Epoch 85: Val Loss 543.41260
Epoch 86: Val Loss 543.41199
Epoch 87: Val Loss 543.41138
Epoch 88: Val Loss 543.41089
Epoch 89: Val Loss 543.41028
Epoch 90: Val Loss 543.40973
Epoch 91: Val Loss 543.40912
Epoch 92: Val Loss 543.40857
Epoch 93: Val Loss 543.40802
Epoch 94: Val Loss 543.40747
Epoch 95: Val Loss 543.40686
Epoch 96: Val Loss 543.40631
Epoch 97: Val Loss 543.40576
Epoch 98: Val Loss 543.40521
Epoch 99: Val Loss 543.40460
Trial 1 failed with parameters: {'dim': 64, 'depth': 1, 'heads': 4, 'weight_decay': -4, 'learning_rate': -6, 'dropout': 0.3} because of the following error: AttributeError("module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations").
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 52, in cross_validation
    curr_model.predict(X_test)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/basemodel_torch.py", line 129, in predict
    self.predictions = self.predict_helper(X)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabtransformer.py", line 155, in predict_helper
    X = np.array(X, dtype=np.float)
  File "/home/mburu/.local/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
Trial 1 failed with value None.


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/socmob.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/socmob.yml', data_parallel=False, dataset='Socmob', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 2, 3], num_classes=1, num_features=5, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Socmob...
Dataset loaded! 

X b4 encoding : ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 22.9] 

(1156, 5)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 2, 3]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 2, 3]
Cat Idx Part II: [0, 1, 2, 3] 
ENDE 
 

X after Nominal Encoding: ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 22.9] 
 

Scaling the data...
X after Scaling: ['Professional_Self-Employed' 'Professional_Self-Employed' 'intact'
 'white' 0.13440189107338416] 
 

One Hot Encoding...
X after One Hot Encoding: [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0
 0.0 1.0 0.13440189107338416] 
 

args.num_features: 39
args.cat_idx: None
Cat Dims: []
New Shape: (1156, 39)
False 
 

Using an existing study with name 'TabTransformer_Socmob' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 128 and batch size 128
On Device: cuda
Epoch 0: Val Loss 1276.65576
Epoch 1: Val Loss 1276.63330
Epoch 2: Val Loss 1276.61072
Epoch 3: Val Loss 1276.58862
Epoch 4: Val Loss 1276.56628
Epoch 5: Val Loss 1276.54407
Epoch 6: Val Loss 1276.52197
Epoch 7: Val Loss 1276.49976
Epoch 8: Val Loss 1276.47681
Epoch 9: Val Loss 1276.45422
Epoch 10: Val Loss 1276.43201
Epoch 11: Val Loss 1276.40967
Epoch 12: Val Loss 1276.38647
Epoch 13: Val Loss 1276.36316
Epoch 14: Val Loss 1276.34045
Epoch 15: Val Loss 1276.31812
Epoch 16: Val Loss 1276.29504
Epoch 17: Val Loss 1276.27197
Epoch 18: Val Loss 1276.25000
Epoch 19: Val Loss 1276.22742
Epoch 20: Val Loss 1276.20447
Epoch 21: Val Loss 1276.18103
Epoch 22: Val Loss 1276.15747
Epoch 23: Val Loss 1276.13416
Epoch 24: Val Loss 1276.11194
Epoch 25: Val Loss 1276.08936
Epoch 26: Val Loss 1276.06677
Epoch 27: Val Loss 1276.04346
Epoch 28: Val Loss 1276.02002
Epoch 29: Val Loss 1275.99670
Epoch 30: Val Loss 1275.97229
Epoch 31: Val Loss 1275.94849
Epoch 32: Val Loss 1275.92480
Epoch 33: Val Loss 1275.90137
Epoch 34: Val Loss 1275.87854
Epoch 35: Val Loss 1275.85510
Epoch 36: Val Loss 1275.83167
Epoch 37: Val Loss 1275.80823
Epoch 38: Val Loss 1275.78418
Epoch 39: Val Loss 1275.76013
Epoch 40: Val Loss 1275.73596
Epoch 41: Val Loss 1275.71228
Epoch 42: Val Loss 1275.68896
Epoch 43: Val Loss 1275.66528
Epoch 44: Val Loss 1275.64148
Epoch 45: Val Loss 1275.61768
Epoch 46: Val Loss 1275.59363
Epoch 47: Val Loss 1275.57056
Epoch 48: Val Loss 1275.54712
Epoch 49: Val Loss 1275.52393
Epoch 50: Val Loss 1275.49951
Epoch 51: Val Loss 1275.47534
Epoch 52: Val Loss 1275.45142
Epoch 53: Val Loss 1275.42798
Epoch 54: Val Loss 1275.40271
Epoch 55: Val Loss 1275.37878
Epoch 56: Val Loss 1275.35498
Epoch 57: Val Loss 1275.32996
Epoch 58: Val Loss 1275.30591
Epoch 59: Val Loss 1275.28210
Epoch 60: Val Loss 1275.25769
Epoch 61: Val Loss 1275.23303
Epoch 62: Val Loss 1275.20813
Epoch 63: Val Loss 1275.18359
Epoch 64: Val Loss 1275.15918
Epoch 65: Val Loss 1275.13513
Epoch 66: Val Loss 1275.11047
Epoch 67: Val Loss 1275.08606
Epoch 68: Val Loss 1275.06140
Epoch 69: Val Loss 1275.03723
Epoch 70: Val Loss 1275.01270
Epoch 71: Val Loss 1274.98816
Epoch 72: Val Loss 1274.96362
Epoch 73: Val Loss 1274.93884
Epoch 74: Val Loss 1274.91406
Epoch 75: Val Loss 1274.88904
Epoch 76: Val Loss 1274.86389
Epoch 77: Val Loss 1274.83911
Epoch 78: Val Loss 1274.81335
Epoch 79: Val Loss 1274.78821
Epoch 80: Val Loss 1274.76270
Epoch 81: Val Loss 1274.73804
Epoch 82: Val Loss 1274.71313
Epoch 83: Val Loss 1274.68774
Epoch 84: Val Loss 1274.66284
Epoch 85: Val Loss 1274.63843
Epoch 86: Val Loss 1274.61389
Epoch 87: Val Loss 1274.58850
Epoch 88: Val Loss 1274.56250
Epoch 89: Val Loss 1274.53638
Epoch 90: Val Loss 1274.50964
Epoch 91: Val Loss 1274.48389
Epoch 92: Val Loss 1274.45801
Epoch 93: Val Loss 1274.43250
Epoch 94: Val Loss 1274.40735
Epoch 95: Val Loss 1274.38074
Epoch 96: Val Loss 1274.35571
Epoch 97: Val Loss 1274.33057
Epoch 98: Val Loss 1274.30469
Epoch 99: Val Loss 1274.27820
Trial 1 failed with parameters: {'dim': 128, 'depth': 6, 'heads': 4, 'weight_decay': -2, 'learning_rate': -5, 'dropout': 0} because of the following error: AttributeError("module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations").
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 52, in cross_validation
    curr_model.predict(X_test)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/basemodel_torch.py", line 129, in predict
    self.predictions = self.predict_helper(X)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabtransformer.py", line 155, in predict_helper
    X = np.array(X, dtype=np.float)
  File "/home/mburu/.local/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
Trial 1 failed with value None.


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/sensory.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/sensory.yml', data_parallel=False, dataset='Sensory', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], num_classes=1, num_features=11, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=False, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Sensory...
Dataset loaded! 

X b4 encoding : [1 1 1 1 1 1 3 3 1 2 1] 

(576, 11)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Cat Idx Part II: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 
ENDE 
 

X after Nominal Encoding: [1 1 1 1 1 1 3 3 1 2 1] 
 

X after Scaling: [1 1 1 1 1 1 3 3 1 2 1] 
 

One Hot Encoding...
X after One Hot Encoding: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.] 
 

args.num_features: 36
args.cat_idx: None
Cat Dims: []
New Shape: (576, 36)
False 
 

Using an existing study with name 'TabTransformer_Sensory' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
In get_device
()
On Device: cuda
Using dim 64 and batch size 128
On Device: cuda
Epoch 0: Val Loss 220.86786
Epoch 1: Val Loss 220.86601
Epoch 2: Val Loss 220.86415
Epoch 3: Val Loss 220.86230
Epoch 4: Val Loss 220.86046
Epoch 5: Val Loss 220.85861
Epoch 6: Val Loss 220.85678
Epoch 7: Val Loss 220.85493
Epoch 8: Val Loss 220.85307
Epoch 9: Val Loss 220.85123
Epoch 10: Val Loss 220.84940
Epoch 11: Val Loss 220.84755
Epoch 12: Val Loss 220.84569
Epoch 13: Val Loss 220.84386
Epoch 14: Val Loss 220.84200
Epoch 15: Val Loss 220.84015
Epoch 16: Val Loss 220.83832
Epoch 17: Val Loss 220.83647
Epoch 18: Val Loss 220.83463
Epoch 19: Val Loss 220.83279
Epoch 20: Val Loss 220.83095
Epoch 21: Val Loss 220.82910
Epoch 22: Val Loss 220.82726
Epoch 23: Val Loss 220.82539
Epoch 24: Val Loss 220.82356
Epoch 25: Val Loss 220.82173
Epoch 26: Val Loss 220.81987
Epoch 27: Val Loss 220.81802
Epoch 28: Val Loss 220.81616
Epoch 29: Val Loss 220.81433
Epoch 30: Val Loss 220.81250
Epoch 31: Val Loss 220.81065
Epoch 32: Val Loss 220.80879
Epoch 33: Val Loss 220.80695
Epoch 34: Val Loss 220.80508
Epoch 35: Val Loss 220.80325
Epoch 36: Val Loss 220.80141
Epoch 37: Val Loss 220.79955
Epoch 38: Val Loss 220.79771
Epoch 39: Val Loss 220.79587
Epoch 40: Val Loss 220.79401
Epoch 41: Val Loss 220.79216
Epoch 42: Val Loss 220.79031
Epoch 43: Val Loss 220.78848
Epoch 44: Val Loss 220.78661
Epoch 45: Val Loss 220.78474
Epoch 46: Val Loss 220.78290
Epoch 47: Val Loss 220.78108
Epoch 48: Val Loss 220.77921
Epoch 49: Val Loss 220.77736
Epoch 50: Val Loss 220.77553
Epoch 51: Val Loss 220.77367
Epoch 52: Val Loss 220.77182
Epoch 53: Val Loss 220.76997
Epoch 54: Val Loss 220.76811
Epoch 55: Val Loss 220.76627
Epoch 56: Val Loss 220.76440
Epoch 57: Val Loss 220.76256
Epoch 58: Val Loss 220.76071
Epoch 59: Val Loss 220.75885
Epoch 60: Val Loss 220.75700
Epoch 61: Val Loss 220.75517
Epoch 62: Val Loss 220.75331
Epoch 63: Val Loss 220.75145
Epoch 64: Val Loss 220.74962
Epoch 65: Val Loss 220.74774
Epoch 66: Val Loss 220.74593
Epoch 67: Val Loss 220.74405
Epoch 68: Val Loss 220.74220
Epoch 69: Val Loss 220.74037
Epoch 70: Val Loss 220.73849
Epoch 71: Val Loss 220.73666
Epoch 72: Val Loss 220.73482
Epoch 73: Val Loss 220.73296
Epoch 74: Val Loss 220.73111
Epoch 75: Val Loss 220.72925
Epoch 76: Val Loss 220.72740
Epoch 77: Val Loss 220.72557
Epoch 78: Val Loss 220.72371
Epoch 79: Val Loss 220.72186
Epoch 80: Val Loss 220.72003
Epoch 81: Val Loss 220.71817
Epoch 82: Val Loss 220.71634
Epoch 83: Val Loss 220.71448
Epoch 84: Val Loss 220.71263
Epoch 85: Val Loss 220.71077
Epoch 86: Val Loss 220.70894
Epoch 87: Val Loss 220.70709
Epoch 88: Val Loss 220.70525
Epoch 89: Val Loss 220.70340
Epoch 90: Val Loss 220.70154
Epoch 91: Val Loss 220.69969
Epoch 92: Val Loss 220.69786
Epoch 93: Val Loss 220.69601
Epoch 94: Val Loss 220.69417
Epoch 95: Val Loss 220.69232
Epoch 96: Val Loss 220.69046
Epoch 97: Val Loss 220.68861
Epoch 98: Val Loss 220.68675
Epoch 99: Val Loss 220.68491
Trial 1 failed with parameters: {'dim': 64, 'depth': 2, 'heads': 2, 'weight_decay': -5, 'learning_rate': -6, 'dropout': 0.1} because of the following error: AttributeError("module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations").
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 52, in cross_validation
    curr_model.predict(X_test)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/basemodel_torch.py", line 129, in predict
    self.predictions = self.predict_helper(X)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabtransformer.py", line 155, in predict_helper
    X = np.array(X, dtype=np.float)
  File "/home/mburu/.local/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
Trial 1 failed with value None.


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/moneyball.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/moneyball.yml', data_parallel=False, dataset='Moneyball', direction='minimize', dropna_idx=[9, 10, 12, 13], early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 1, 8], num_classes=1, num_features=14, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Moneyball...
Dataset loaded! 

X b4 encoding : ['ARI' 'NL' 2012 688 81 0.3279999999999999 0.418 0.259 0 162] 

(1232, 10)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 1, 8]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [0, 1, 8]
Cat Idx Part II: [0, 1, 8] 
ENDE 
 

X after Nominal Encoding: ['ARI' 'NL' 2012 688 81 0.3279999999999999 0.418 0.259 0 162] 
 

Scaling the data...
X after Scaling: ['ARI' 'NL' 1.5554755871677342 -0.2910721732671802 0.008362450087033452
 0.11120590052485849 0.6212382045299186 -0.0211383889172301 0
 0.13005495722996097] 
 

One Hot Encoding...
X after One Hot Encoding: [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.5554755871677342 -0.2910721732671802
 0.008362450087033452 0.11120590052485849 0.6212382045299186
 -0.0211383889172301 0.13005495722996097] 
 

args.num_features: 50
args.cat_idx: None
Cat Dims: []
New Shape: (1232, 50)
False 
 

Using an existing study with name 'TabTransformer_Moneyball' instead of creating a new one.
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
()
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 516269.65625
Epoch 1: Val Loss 516092.03125
Epoch 2: Val Loss 515735.12500
Epoch 3: Val Loss 514946.25000
Epoch 4: Val Loss 513278.71875
Epoch 5: Val Loss 510057.84375
Epoch 6: Val Loss 504293.34375
Epoch 7: Val Loss 494677.71875
Epoch 8: Val Loss 479732.28125
Epoch 9: Val Loss 457710.71875
Epoch 10: Val Loss 426431.81250
Epoch 11: Val Loss 384031.90625
Epoch 12: Val Loss 330456.31250
Epoch 13: Val Loss 268181.00000
Epoch 14: Val Loss 201365.73438
Epoch 15: Val Loss 138679.95312
Epoch 16: Val Loss 87833.96094
Epoch 17: Val Loss 53559.28906
Epoch 18: Val Loss 34927.35938
Epoch 19: Val Loss 25952.17188
Epoch 20: Val Loss 21393.65234
Epoch 21: Val Loss 18246.27734
Epoch 22: Val Loss 15872.86523
Epoch 23: Val Loss 14027.89258
Epoch 24: Val Loss 12610.10840
Epoch 25: Val Loss 11405.51172
Epoch 26: Val Loss 10448.64160
Epoch 27: Val Loss 9610.87695
Epoch 28: Val Loss 8952.83789
Epoch 29: Val Loss 8366.27930
Epoch 30: Val Loss 7873.06299
Epoch 31: Val Loss 7438.87354
Epoch 32: Val Loss 7063.96973
Epoch 33: Val Loss 6740.48730
Epoch 34: Val Loss 6463.64990
Epoch 35: Val Loss 6214.32764
Epoch 36: Val Loss 6005.73389
Epoch 37: Val Loss 5803.60400
Epoch 38: Val Loss 5628.87207
Epoch 39: Val Loss 5469.33740
Epoch 40: Val Loss 5333.13477
Epoch 41: Val Loss 5209.85986
Epoch 42: Val Loss 5088.88086
Epoch 43: Val Loss 4974.58936
Epoch 44: Val Loss 4877.68848
Epoch 45: Val Loss 4782.99268
Epoch 46: Val Loss 4699.02734
Epoch 47: Val Loss 4611.13477
Epoch 48: Val Loss 4532.52832
Epoch 49: Val Loss 4453.86377
Epoch 50: Val Loss 4384.60645
Epoch 51: Val Loss 4315.53076
Epoch 52: Val Loss 4248.91748
Epoch 53: Val Loss 4194.50879
Epoch 54: Val Loss 4126.26318
Epoch 55: Val Loss 4082.47754
Epoch 56: Val Loss 4023.12378
Epoch 57: Val Loss 3978.85278
Epoch 58: Val Loss 3922.20190
Epoch 59: Val Loss 3880.09985
Epoch 60: Val Loss 3840.36353
Epoch 61: Val Loss 3805.36963
Epoch 62: Val Loss 3761.03052
Epoch 63: Val Loss 3728.74048
Epoch 64: Val Loss 3683.46265
Epoch 65: Val Loss 3656.47119
Epoch 66: Val Loss 3617.30933
Epoch 67: Val Loss 3603.46875
Epoch 68: Val Loss 3566.90649
Epoch 69: Val Loss 3535.66382
Epoch 70: Val Loss 3506.39917
Epoch 71: Val Loss 3479.48047
Epoch 72: Val Loss 3472.89038
Epoch 73: Val Loss 3437.83105
Epoch 74: Val Loss 3415.74048
Epoch 75: Val Loss 3395.25488
Epoch 76: Val Loss 3370.17969
Epoch 77: Val Loss 3348.88379
Epoch 78: Val Loss 3332.83740
Epoch 79: Val Loss 3313.72266
Epoch 80: Val Loss 3289.83228
Epoch 81: Val Loss 3268.38574
Epoch 82: Val Loss 3251.28369
Epoch 83: Val Loss 3239.61890
Epoch 84: Val Loss 3212.20654
Epoch 85: Val Loss 3192.75513
Epoch 86: Val Loss 3179.49097
Epoch 87: Val Loss 3162.39624
Epoch 88: Val Loss 3147.09521
Epoch 89: Val Loss 3120.59180
Epoch 90: Val Loss 3106.02173
Epoch 91: Val Loss 3092.92041
Epoch 92: Val Loss 3075.44678
Epoch 93: Val Loss 3045.07397
Epoch 94: Val Loss 3041.16064
Epoch 95: Val Loss 3022.73438
Epoch 96: Val Loss 3010.94263
Epoch 97: Val Loss 2989.96021
Epoch 98: Val Loss 2969.85522
Epoch 99: Val Loss 2958.13037
Trial 1 failed with parameters: {'dim': 128, 'depth': 6, 'heads': 2, 'weight_decay': -6, 'learning_rate': -3, 'dropout': 0.1} because of the following error: AttributeError("module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations").
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 52, in cross_validation
    curr_model.predict(X_test)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/basemodel_torch.py", line 129, in predict
    self.predictions = self.predict_helper(X)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabtransformer.py", line 155, in predict_helper
    X = np.array(X, dtype=np.float)
  File "/home/mburu/.local/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
Trial 1 failed with value None.


----------------------------------------------------------------------------
Training TabTransformer Vesion 1 with Dataset: config/black_friday.yml 



----------------------------------------------------------------------------
 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/black_friday.yml', data_parallel=False, dataset='Black_Friday', direction='minimize', dropna_idx=None, early_stopping_rounds=20, epochs=100, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabTransformer', n_trials=30, nominal_idx=[0, 2, 3, 5, 6, 7, 8], num_classes=1, num_features=9, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=True, ordinal_idx=[1], scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset Black_Friday...
Dataset loaded! 

X b4 encoding : ['F' '0-17' 10 'A' 2 0 1 6 14] 

(166821, 9)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [0, 2, 3, 5, 6, 7, 8]
Ordinal Idx: [1]
Cat Dims: None 
 

Normonal Idx: [0, 2, 3, 5, 6, 7, 8]
Cat Idx Part II: [0, 1, 2, 3, 5, 6, 7, 8] 
ENDE 
 

X after Nominal Encoding: ['F' '0-17' 10 'A' 2 0 1 6 14] 
 

Scaling the data...
X after Scaling: ['F' '0-17' 10 'A' 0.1076520112629123 0 1 6 14] 
 

Ordinal Idx: [0]
One Hot Encoding...
X after One Hot Encoding: ['0-17' 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
 0.1076520112629123] 
 

args.num_features: 71
args.cat_idx: [0]
Cat Dims: [8]
New Shape: (166821, 71)
True 
 

Using an existing study with name 'TabTransformer_Black_Friday' instead of creating a new one.
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
In get_device
[8]
On Device: cuda
Using dim 8 and batch size 64
On Device: cuda
Epoch 0: Val Loss 30495354.00000
Epoch 1: Val Loss 26297008.00000
Epoch 2: Val Loss 25013704.00000
Epoch 3: Val Loss 24168802.00000
Epoch 4: Val Loss 23073152.00000
Epoch 5: Val Loss 21746362.00000
Epoch 6: Val Loss 20293604.00000
Epoch 7: Val Loss 18798476.00000
Epoch 8: Val Loss 17580038.00000
Epoch 9: Val Loss 16641438.00000
Epoch 10: Val Loss 15959291.00000
Epoch 11: Val Loss 15451817.00000
Epoch 12: Val Loss 15043163.00000
Epoch 13: Val Loss 14631937.00000
Epoch 14: Val Loss 14377812.00000
Epoch 15: Val Loss 14098728.00000
Epoch 16: Val Loss 13961685.00000
Epoch 17: Val Loss 13789894.00000
Epoch 18: Val Loss 13665393.00000
Epoch 19: Val Loss 13564120.00000
Epoch 20: Val Loss 13482676.00000
Epoch 21: Val Loss 13417623.00000
Epoch 22: Val Loss 13350183.00000
Epoch 23: Val Loss 13260653.00000
Epoch 24: Val Loss 13234360.00000
Epoch 25: Val Loss 13186785.00000
Epoch 26: Val Loss 13148273.00000
Epoch 27: Val Loss 13143711.00000
Epoch 28: Val Loss 13110970.00000
Epoch 29: Val Loss 13108127.00000
Epoch 30: Val Loss 13090092.00000
Epoch 31: Val Loss 13056628.00000
Epoch 32: Val Loss 13078623.00000
Epoch 33: Val Loss 13053823.00000
Epoch 34: Val Loss 13036276.00000
Epoch 35: Val Loss 13043999.00000
Epoch 36: Val Loss 13006614.00000
Epoch 37: Val Loss 13000660.00000
Epoch 38: Val Loss 13027084.00000
Epoch 39: Val Loss 13002129.00000
Epoch 40: Val Loss 12993389.00000
Epoch 41: Val Loss 12993285.00000
Epoch 42: Val Loss 12986113.00000
Epoch 43: Val Loss 12984792.00000
Epoch 44: Val Loss 12978838.00000
Epoch 45: Val Loss 12981641.00000
Epoch 46: Val Loss 12980247.00000
Epoch 47: Val Loss 12974236.00000
Epoch 48: Val Loss 12961578.00000
Epoch 49: Val Loss 12945609.00000
Epoch 50: Val Loss 12955378.00000
Epoch 51: Val Loss 12958360.00000
Epoch 52: Val Loss 12949555.00000
Epoch 53: Val Loss 12961523.00000
Epoch 54: Val Loss 12927177.00000
Epoch 55: Val Loss 12932223.00000
Epoch 56: Val Loss 12942786.00000
Epoch 57: Val Loss 12932915.00000
Epoch 58: Val Loss 12923136.00000
Epoch 59: Val Loss 12905800.00000
Epoch 60: Val Loss 12918793.00000
Epoch 61: Val Loss 12897707.00000
Epoch 62: Val Loss 12904094.00000
Epoch 63: Val Loss 12886630.00000
Epoch 64: Val Loss 12906212.00000
Epoch 65: Val Loss 12904889.00000
Epoch 66: Val Loss 12893631.00000
Epoch 67: Val Loss 12852940.00000
Epoch 68: Val Loss 12887921.00000
Epoch 69: Val Loss 12851252.00000
Epoch 70: Val Loss 12867085.00000
Epoch 71: Val Loss 12868710.00000
Epoch 72: Val Loss 12851525.00000
Epoch 73: Val Loss 12862284.00000
Epoch 74: Val Loss 12842246.00000
Epoch 75: Val Loss 12868617.00000
Epoch 76: Val Loss 12844644.00000
Epoch 77: Val Loss 12852010.00000
Epoch 78: Val Loss 12839551.00000
Epoch 79: Val Loss 12838465.00000
Epoch 80: Val Loss 12828675.00000
Epoch 81: Val Loss 12829378.00000
Epoch 82: Val Loss 12830390.00000
Epoch 83: Val Loss 12826012.00000
Epoch 84: Val Loss 12811825.00000
Epoch 85: Val Loss 12803311.00000
Epoch 86: Val Loss 12803177.00000
Epoch 87: Val Loss 12802685.00000
Epoch 88: Val Loss 12825776.00000
Epoch 89: Val Loss 12793908.00000
Epoch 90: Val Loss 12795357.00000
Epoch 91: Val Loss 12776607.00000
Epoch 92: Val Loss 12781252.00000
Epoch 93: Val Loss 12801014.00000
Epoch 94: Val Loss 12778686.00000
Epoch 95: Val Loss 12786691.00000
Epoch 96: Val Loss 12766680.00000
Epoch 97: Val Loss 12776407.00000
Epoch 98: Val Loss 12774683.00000
Epoch 99: Val Loss 12765079.00000
Trial 1 failed with parameters: {'dim': 128, 'depth': 1, 'heads': 2, 'weight_decay': -6, 'learning_rate': -4, 'dropout': 0.1} because of the following error: AttributeError("module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations").
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 52, in cross_validation
    curr_model.predict(X_test)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/basemodel_torch.py", line 129, in predict
    self.predictions = self.predict_helper(X)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabtransformer.py", line 155, in predict_helper
    X = np.array(X, dtype=np.float)
  File "/home/mburu/.local/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
Trial 1 failed with value None.
